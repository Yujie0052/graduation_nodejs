Biol. Cybern. 85, 77±87  2001)
 Mathematical models of eye movements in reading: a possible role for autonomous saccades
Ralf Engbert1Y2, Reinhold Kliegl1
1 Institut fuÈ r Psychologie, UniversitaÈ t Potsdam, PF 601553, 14415 Potsdam, Germany
2 Zentrum fuÈ r Dynamik komplexer Systeme, UniversitaÈ t Potsdam, PF 601553, 14415 Potsdam, Germany Received: 21 March 2000 / Accepted in revised form: 10 January 2001
Abstract. An e cient method for the exact numerical simulation of semi-Markov processes is used to study minimal models of the control of eye movements in reading. When we read a text, typical sequences of ®xations form a rather complicated trajectory ± almost like a random walk. Mathematical models of eye movement control can account for this behavior using stochastic transition rules between few discrete internal states, which represent combinations of certain stages of lexical access and saccade programs. We show that experimentally observed ®xation durations can be explained by residence-time-dependent transition prob- abilities. Stochastic processes with this property are known as semi-Markov processes. For our numerical simulations we use the minimal process method  Gilles- pie algorithm), which is an exact and e cient simulation algorithm for this class of stochastic processes. Within this mathematical framework, we study di erent forms of coupling between eye movements and shifts of covert attention in reading. Our model lends support to the existence of autonomous saccades, i.e., the hypothesis that initiations of saccades are not completely deter- mined by lexical access processes.
1 Introduction
Investigating eye movements in reading may be looked upon as a case study for the more general problem of scanning of visual scenes with higher structural com- plexity. Typical eye movements in reading form a rather complicated trajectory ± almost like a random walk  van Kampen 1981; Gardiner 1990; Gillespie 1992). For theoretical approaches to the control of eye movements, this random walk can be approximated by a series of ®xations. As an illustrative comparison  Reichle
Correspondence to: R. Engbert  e-mail: engbert@rz.uni-potsdam.de)
et al. 1998), we may think of reading as a ``slide show'' where the ``slides''  words) are ®xated for about 200 to 250 ms, separated by executions of saccades  20 to 40 ms).
The experimentally observed random walk over words is the consequence of saccades from wordn to wordk. For k   n, wordn is re®xated, which is typically observed for low-frequency words. If k b n   1, wordn 1 is skipped. Word skipping is very likely for highly fre- quent words like articles. Considering only forward saccades and re®xations, k   n, is motivated by the hy- pothesis that this form of eye movements may represent a ``default'' model of eye movement control. Using this conjecture, a successful class of models, the so-called E- Z Reader 1 to 5, has been proposed recently  Reichle et al. 1998). This model has also been extended to in- clude initial ®xation locations and re®xations  Reichle et al. 1999). In mathematical models of this class, a number of internal states is used to represent combina- tions of di erent stages of lexical access and saccade programs. A stochastic sequence of internal states is related to a certain sentence. The sequence is generated by stochastic transition rules to account for the observed statistical properties of eye movements.
In this paper, we propose a ``minimal'' model of eye movement control in reading to explore modi®cations with respect to di erent assumptions of the E-Z Reader modeling framework  Reichle et al. 1998). Since all our alternative assumptions relate to a fundamental level of model design, the analysis is restricted to the same level of approximations as E-Z Reader 1. As a consequence, experimental data used here are gaze durations, de®ned as the sum of ®rst ®xation duration and all potential re®xations, and probabilities for word skipping. Possible extensions of our model are proposed in Sect. 5.
The model proposed here di ers in three major as- pects from the E-Z Reader model. First, a fundamental assumption in mathematical models of eye movement control is related to the coupling of two subsystems: eye movements and shifts of visual attention. In the E-Z Reader models, lexical processes induce shifts of covert attention. The end of a familiarity check, i.e., a

78
preprocessing stage of lexical access, initiates a saccadic motor program, while a shift of  covert) attention to the next word is induced by full lexical access. In our model, we assume that both the saccadic motor program and the covert shift of attention are initiated at the end of lexical access of the foveal word, i.e., a common mechanism, which synchronizes both subsystems.
Second, in the E-Z Reader models, saccade initiation is completely governed by lexical processes. There is no saccade without successful preprocessing. In our model, we investigate the consequences of relaxing this hy- pothesis of a strong coupling between lexical access and eye movements. In particular, we demonstrate that au- tonomous saccades, i.e., saccade programs, which are initiated without a lexical trigger signal, will lead to rich and psychologically plausible dynamic behavior in a minimal model.
Finally, the third di erence to the E-Z Reader framework is methodologically motivated. We use a generalized approach for stochastic transition rules, which is based on the concept of transition probability rates  van Kampen 1981; Gardiner 1990). Using this framework, we replace assumptions on the distribution of residence times ± like gamma distributions  Reichle et al. 1998) ± by an equation for transition probability rates on the level of internal states of the model. In particular, we show that the experimentally observed statistical properties of eye movements in reading can be explained by semi-Markov processes  Gillespie 1978) with residence-time-dependent transition probability rates.
We start with an introduction to semi-Markov pro- cesses and its exact numerical simulation in Sect. 2. In Sect. 3 we focus on a simple model for word skipping, which is then extended to modeling distributions of ®xation duration  Sect. 4). In Sect. 5 we will address some other di erences between our model and the E-Z Reader model.
2 Semi-Markov processes
In the class of models of eye movement control discussed here, a ®nite number of internal states S1YS2YS3Y... is used to describe di erent stages of processing of words and eye movements. Since transitions between adjoining states are de®ned by stochastic transition rules, a ``random walk'' over the internal states is performed when we use such a model to process a sentence. Di erent runs of the model yield di erent realizations of both the internal random walk and the observed random walk over words  i.e., the series of ®xations).
2.1 Residence-time-dependent transition probability rate
An important concept for stochastic models of random processes is the transition probability rate. To start with a general framework for stochastic transitions, we use the following transition rule  Gillespie 1978): if the system is in the state Sm at time t, having arrived there at time t   s s   0 , the probability that it will step to some
other state Sn in the next in®nitesimal time interval  tYt dt is
Wnm s dt X  1
The most common assumption is that the transition probability rate does not depend on residence time s, i.e., Wnm s    Wnm   const. In this case the random walk is a Markov process  van Kampen 1981). The more general case  1), where the transition probability rate depends on residence time, is referred to as a semi-Markov process. Some techniques for the anal- ysis of Markov processes can be exploited for the study of semi-Markov processes. As an example, Gillespie  1977) derived a generalized master equation for these processes. For numerical simulations of semi- Markov processes, an exact and e cient algorithm has been developed  Gillespie 1978). In the following we discuss why we need to implement residence-time- dependent transition probabilities in our model of eye movements in reading.
2.2 Pausing-time distribution
A fundamental concept for Monte Carlo simulation techniques is the transition probability density function P  sY njmY t . The probability that the system steps next to state Sn in the time interval  t sYt s ds , if it arrived in the state Sm at time t, is given by P  sY njmY t ds. The transition probability density function can be written as a product of two other functions,
P  sY njmY t    p nY m w sjm  Y  2
where p nY m  is the stepping probability from state Sm to state Sn, and w sjm  is the probability density function for the pausing time s in the state Sm.
The stepping probability can be calculated from the relative transition probabilities at time s,
p nY m    Wnm s  X  3  Wm  s
The probability density function for the pausing time is obtained from the transition probability rate  1),
9=
 8< Z s
Wm s0 ds0 Y  4  :;
w sjm    Wm s  exp
where we used the de®nition Wm s    P Wnm s  of the
0
other state Sn  for details see Gillespie 1978).
In the case of a Markov process, where Wm s  is a constant, the pausing time is exponentially distributed: w sjm    wme wms. As an important property of the ex- ponential distribution, the maximum of w is at s   0. The residence times of di erent internal states Sj  with sub-processes Sk 7!Sl ) sum to ®xation durations. There- fore, corresponding distributions of ®xation durations are qualitatively of the exponential type. A typical ex- perimentally observed distribution of ®xation durations
n
transition probability for a transition from Sm to any
additional statistical measures. Alternatively, a speci®- cation of additional states of the model could lead to comparable results. In the case of a ®xed value of /,  7) is in agreement with the assumptions of Reichle et al.  1998) in the E-Z Reader models  see below).
Given a mean value ls of the pausing time, we have to compute a corresponding value for the total transition probability rate Wm for state Sm to all adjoining states Sn. Using the probability distribution function for the pausing time  6), we calculate its mean value,
  p  1a2
79
 ls s0  2w
wm   pa2   pa2 X  9
Y  8  from which parameter wm can be read directly,
 m
  Fig. 1. A typical frequency distribution of ®xation durations in reading  after Rayner 1998)
shows, however, a sharp maximum of relative frequency  probability) at 200 ms  Fig. 1). Therefore, the experi- mentally observed distribution of ®xation durations cannot be explained by a Markov model in a straight- forward way.
To avoid these restrictions of the Markov case, we use the more general approach for the transition probability rate  1). Compared to a constant Wm, we now discuss the next more complicated case: a transition probability which increases linearly with s. As an additional parameter, we introduce a refractory period s0 with vanishing transition probability rate. These two assumptions are described by
We now investigate how variation of / in ̄uences the relation between the mean value ls and the standard deviation rs of the pausing time s. Calculation of the second moment of the distribution  6) and substracting the square of the mean gives
4 p
r2s  E s2  l2s   X  10
2wm
The variance does not depend on /  or s0), since the refractory time simply shifts the distribution of s. The ratio of standard deviation  from Eq. 10) to mean  Eq. 8) is independent of the transition probability parameter wm , i.e.,
s
Since in the E-Z Reader models rsals is ®xed at one- third  Reichle et al. 1998), the corresponding value of / is 0.36. Given a certain mean value for the pausing time in state Sm, and a relation between standard deviation and mean value, we can calculate the model parameters wm and / analytically using  9) and  11).
As stated before, the type of distributions  6) for the sub-processes determines the distribution of ®xation durations. It is shown in Sect. 4 that  5) leads to realistic predictions on the distributions of ®xation durations. In Sect. 3 we review the basic mechanisms for word skipping.
3 Modeling word skipping
The simplest model of how lexical access drives eye movements is a strictly serial one. When a currently ®xated word is lexically processed, a saccade program to the next word is initiated. Its termination signals the execution of the saccade. This assumption leads to strong constraints on the available time for lexical access. The reason for a rather limited time for lexical access is that in such a model the lexical access time and the time required for programming a saccade to the next
 ls  s0 2 l2s 1 / 2
   0 if s ` s0 Wm s   wm s s0  ifs s0 X
 5
Following the general relation  4) between transition probability rate and pausing time distribution,  5) leads to the following probability density function for the pausing time,
w sjm
 0
  ifs`s0 ifs s Y 6
  w s s exp wm s s 2 m0200

rs 4 p 1a2
l   p    1 /  0X52  1 /  X  11
   which is a single-humped distribution and qualitatively in agreement with the experimentally observed percent- age of ®xation durations  Fig. 1). In the following, the refractory time s0 is written as a proportion of mean pausing time ls
s0 /ls with 0 /`1X  7
Some remarks on this assumption are necessary: Our approach  7) permits independent variation of the variance and mean value of the residence time. Modi- ®cations to this relation could be necessary to capture
80
word sum to the ®xation duration of a word. Therefore, the lexical access time can be calculated from the di erence of ®xation duration and saccade program time. From the viewpoint of adaptivity, parallel pro- cessing of lexical access and saccade program to the next word would be much more e cient. We will show in Sect. 5 that ± given the experimentally observed ®xation durations ± the time available for lexical access increases considerably compared to the serial model. The as- sumption of parallel processes has the additional consequence that it predicts skipping of highly frequent words  Morrison 1984; Rayner and Pollatsek 1989).
3.1 A two-state model
Information processing is most e cient in the central 2  of the visual ®eld, the foveal region. Acuity decreases in the parafoveal region, which extends out to 5 , and is even poorer in the peripheral region beyond the parafovea  Rayner 1998). Nevertheless, it will turn out to be a considerable advantage if parafoveal information is used during reading. For processing the word to the rightofacurrently®xatedword,attentionhastoshiftto the parafovea. In this case, the programming of saccade and lexical access are active simultaneously. Therefore, such models show parallel processing  Morrison 1984). Regardless of the number of internal states, the basic mechanisms that are performed by these models are shifts of attention and eye movements.
A minimal model for parallel processing of lexical access and saccade programming consists of two internal states  Fig. 2). In state 1, wordn is ®xated while lexical processing ln is active. It is well known that lexical access time depends on word frequency,
ln   lb   lm log Fn  Y  12
where Fn is the frequency of wordn, and lb and lm are constant parameters. When lexical access is ®nished, the system switches to state 2. This transition implies a shift of attention to wordn 1.
The saccade program to the next wordn 1 starts in state 2. The saccade program is denoted by sn; n is re- placed by n 1  update) when the transition is per- formed. Simultaneously, lexical access ln starts, which is
Fig. 2. The two-state model for word skipping. In state 1  left) lexical access of wordn is active while it is ®xated. When lexical access is complete, the system steps to state 2  right), i.e., a shift of attention occurs. The variable n is updated  n!7 n   1 . In state 2, lexical access of wordn starts by use of parafoveal information. Simultaneously, a saccade program to the same word is initiated. Depending on the frequency Fn of wordn, it can be skipped  ln terminates ®rst) or ®xated  sn terminates ®rst)
possible due to the use of parafoveal information  by the shift of attention). Since the two sub-processes ln and sn of state 2 are in competition with each other, two tran- sitions are possible. The saccade program is assumed to be independent of lexical properties of the text, with a mean value of the programming time of about 150 ms. If the saccade program is faster than lexical access, which is most likely for low-frequency words, the system switches to state 1. This transition signals the execution of a saccade  typical duration 15 to 40 ms). As a result, the ®xation period of wordn begins. In state 1, the lexical access of wordn will be completed.
Alternatively, if lexical access is faster than the sac- cade program, a transition from state 2 to state 2 is performed. In this case, the saccade program to wordn is canceled: there is no reason to ®xate wordn, since it is already lexically processed. Instead a saccade program sn 1 and lexical access ln 1 of the next word are initiated  n is updated during the transition). As a consequence, wordn is skipped. According to the model, this event will occur with higher probability for high-frequency words.
3.2Simulationsofthetwo-statemodel
For numerical simulations of the two-state model and its modi®cations, we use a corpus of sentences as previously discussed in Reichle et al.  1998). In an eye-tracking experiment participants read 48 sentences, each consist- ing of 8 to 14 words  Schilling et al. 1998). For model evaluation we compared numerical simulations of mean ®xation duration and probability for word skipping with corresponding values observed in the experimental study.
Word frequency is the lexical parameter used for model simulations, as in  12). All words of the corpus were divided into ®ve di erent frequency classes. Mean gaze duration and mean probability for skipping for these ®ve classes are the experimental basis used here. Results from the model simulation are given in Fig. 3. Details of the simulation algorithm are discussed in Appendix A. For the execution of saccades we used gamma-distributed random numbers. The mean value was ®xed at 25 ms and the standard deviation was ®xed at one-third of the mean, as suggested by Reichle et al.  1998). The parameter estimation method is described in Appendix B, where mean ®xation durations and skip- ping probabilities are used for the optimization proce- dure  Appendix B.1). Best-®t values for model parameters are lb   271, lm   12X0, sn   170, /l   0X70, and /s   0X47. The obtained minimal value for the de- viation measure,  B3), is h2   0X174. Summarizing we can say that ®xation durations as well as skipping probabilities are in good agreement with experimental data.
The most important restrictions for modeling di er- ent distributions of ®xation durations arise from as- sumptions on the form of the transition probability rate  1). There are two important motivations for the linear s-dependence chosen in our model. First, it is the next more complicated case compared to the Markov

81
  Fig. 3a,b. Fixation durations and probability of word skipping for model simulation  dashed line) and experimental data  solid line, from Reichle et al. 1998): a ®xation durations show a linear dependence on the logarithm of word frequency ± frequency classes represent e ectively a logarithmic scale; b probability for word skipping increases with word frequency. In both panels, model simulations are in good agreement with experimental data
assumption  Wm  s  = const.). Second, the resulting distribution  6) is qualitatively in agreement with typical experimentally observed distributions  Fig. 1); for  6), the probability decreases faster for increasing s than in the case of gamma distributions.
While mean ®xation durations and probabilites of word skipping can be explained with the two-state model, it fails to predict the correct distributions of ®xation durations  see Fig. 6). In Sect. 4 we show how this shortcoming can be solved by increasing model complexity, i.e., by adding a third state to the model.
4 Modeling ®xation durations
4.1 A three-state model
The next step in making the basic two-state model  Fig. 2) more  ̄exible for reproducing distributions of ®xation durations is to distinguish lexical access pro- cesses in states 1 and 2. Since acuity is decreased in the parafoveal region, we assume that lexical processing in state 2 is in a preliminary stage with di erent parame- ters: after the shift of attention, parafoveal information is required in state 2, while foveal information can be
Fig. 4. The three-state model for the control of eye movements in reading. As a generalization of the two-state model  Fig. 2), lexical access parameters are di erent in states 1 and 2. To make the resulting model consistent with respect to lexical access parameters after a word has been skipped, an additional state 3 has to be assumed. The transition from state 1 to state 3 plays an important role for the coupling between lexical processes and saccade programs  see text)
used in state 1. If lexical preprocessing in state 2, lcn   lcb   lcm log Fn , terminates, a transition to an addi- tional state 3 occurs  Fig. 4). In this state, the saccade program to wordn is canceled, i.e., wordn is skipped, and a new saccade program to wordn 1 is initiated; at the same time, lexical access of wordn is completed. We assume that lexical access time is independent of the state where it started. Therefore, lexical access time in states 1 and 3 is reduced by the amount of time lexical preprocessing  in state 2) is performed. As a conse- quence, it is most likely that lexical access ln in state 3 is faster than sn 1, since ln is reduced by lcn  on the basis of mean values). Therefore, we do not add an additional transition for the case in which sn 1 terminates ®rst.
4.2 Autonomous saccade programs
As an additional property of the three-state model, we consider a possible transition from state 1 to state 3: the ``autonomous'' initiation of a saccade program to wordn 1. The hypothesis behind the introduction of this transition is that the visual control system has some autonomy in programming a saccade. This assumption has important consequences for the coupling between shifts of attention and eye movement control. In extant theoretical models, eye movements are completely controlled by lexical processes, i.e., the initiation of a saccade program is determined by the familiarity check  Reichle et al. 1998). We now address the important question of whether we can relax the assumption of this strong form of coupling of the two sub-processes in order to explain the experimental data.
82
Table 1. Results of parameter estimation for two-state  2) and three-state  3A/3B) models. The star  c) were estimated from mean ®xation durations and skipping probabilities only  Appendix B.1)
indicates that model parameters
           Model Fitness
2c 0.174 2 0.491
3A 0.348
3B 0.312
lb lm
271 12.0 287 14.0 255 9.5 280 14.7
lcb lcm /l ±±0.70
±±0.47
sn /s a /a 170 0.47 ± ±
162 0.44 ± ± 113 0.69 ± ± 102 0.79 233 0.47
           188 9.9 187 12.6
0.47 0.28
         Fig. 5a,b. Fixation durations a and probability of word skipping b for numerical simulations  dashed line) of the three-state model  version B) and experimental data  solid line, from Reichle et al. 1998). Results are comparable to those of the two-state model  see Fig. 3)
This investigation is carried out by a comparison of the performance of di erent versions of the three-state model. In model 3A, we exclude transitions from state 1 to state 3, which is equivalent to a ! 1. In model 3B, we assume that the mean residence time to start a sac- cade program sn 1 in state 1 is ®nite and constant, a   const. b 0. In this case, the transition from state 1 to state 3 is comparable to other transitions in the model. Like the mean time sn to program a saccade to wordn, the parameter a is independent of word frequency Fn.
Wenowdiscusstheresultsobtainedfrommodel simulations. Using the same corpus of sentences as for the two-state model, the performance of the models is compared with respect to mean ®xation durations and distributions as well as to the probability of skipping, for ®ve di erent frequency classes  Eq. B5). The results are summarized in Table 1.
A ®rst glance at Table 1 shows that the results for both versions  A and B) of the three-state model are comparable. This robustness of estimates for the pa- rameters is a hint for the structural stability of the models: small modi®cation do not lead to qualitative changes. This property emphasizes the psychological plausibility of the models. For a more detailed com- parison between the two- and three-state models, the results for model 3B are presented in Fig. 5  mean ®x- ation duration and probability for word skipping) and 6  distribution of ®xation durations for 5 di erent classes of word frequency). Generally, model simulations are in good agreement with experimental data.
All models discussed here represent the same class of models, but with di ering model complexity, e.g., number of internal states and parameters. Generally, model performance, i.e., ®tness, increases with model complexity. This a non-trivial result, because the cou- pling between lexical access and saccade programs decreases from model 3A to model 3B. While in model 3A only lexical access can trigger the initiation of a saccade program, in model 3B the saccade programs can start spontaneously. Additionally, we investigated a saccadic sub-system with a periodic forcing, i.e., a periodic variation of the probability for the initiation of saccade programs, which turned out not to desta- bilize model performance. This numerical control study provides further evidence on the robustness of our results.
A further remark concerns the two-state model: while the two-state model can explain the pattern of mean ®xation duration and probability for word skipping, it fails to predict the correct distribution of ®xation du- rations. The corresponding best ®tness value obtained in our simulations was h3   0X491  see Eq. B5), which is a clear indication that the two-state model is too limited to explain the distribution of ®xation durations.
The most important consequence of the introduction of an autonomous saccade program is related to pre- dictions about preview bene®t. This is a qualitatively new property of model 3B that cannot be achieved by model 3A. We discuss these results in Sect. 4.3.
4.3Analysisofpreviewbene®t
It is a well-known experimental observation that lexical processing time of a word is shorter when there was a preview of the word in the parafovea  Rayner 1998). Following Reichle et al.  1998), a key problem in minimal models of eye movement control in reading is
83
 to explain that preview bene®t is modulated by foveal di culty. Therefore, an important success of the E-Z Reader models is to provide a mechanism for this e ect by introducing two stages of lexical processing: a familiarity check and lexical completion.
In contrast to Reichle et al.  1998), our three-state model 3B shows that preview bene®t depends on its in- herent dynamic behavior with a single lexical processing stage  Fig. 4). If lexical access of wordn starts in state 1, then there are two possible transitions  to state 2 and state 3, respectively), whose stepping probabilities are modulated by word frequency. In the case of a foveal high-frequency word, a transition to state 2 will most likely occur. Preview bene®t is, therefore ± to a ®rst approximation ± given by the time required for pro- gramming the saccade to wordn 1. Alternatively, a low- frequency wordn in the fovea induces a high probability for a transition to state 3 via an autonomously triggered saccade. If this happens, a saccade program to wordn 1 is initiated before lexical access of wordn is completed. The consequence for the amount of preview bene®t is that when the system ®nally steps to state 2  there is no alternative), preview bene®t is reduced by the amount of time that the system has spent in state 3. Thus, our model provides an explanation of how preview bene®t
Fig. 6. Distribution of ®xation durations for ®ve di erent word frequency classes  experi- ment, solid lines; model 2, dotted; model 3B, dashed). Despite its performance with respect to mean ®xation durations  Fig. 3a), the two-state model fails to reproduce the distributions of ®xation durations. Simulations of the three- state model are in good agreement with exper- imental data  after Reichle et al. 1998)
can be modulated by foveal processing di culty, in terms of its dynamics.
On the assumption that preview bene®t is minimal if the previous word is skipped, we con®rmed these con- siderations about the underlying qualitative dynamics with numerical simulations. Thus, the di erence between ®xation durations on words following a skipped word and those following a ®xated word should indicate a preview bene®t increasing with word frequency. The di erence between the solid lines in Fig. 7 reveals that this was indeed the case for model 3B. The amount of preview bene®t obtained is in the same order of mag- nitude as in the study by Reichle et al.  1998). The upper limit for preview bene®t in our model is the mean time required to program a saccade, sn, since preview bene®t is given by the residence time in state 2  Fig. 2). The dependence of preview bene®t on word frequency was not observed for model 3A  i.e., the di erence between the dashed lines was roughly constant). The primary source of the increase of preview bene®t with word fre- quency in model 3B is due to the introduction of au- tonomous saccades in this model leading to an increase of the ®xation duration for low-frequency words if the previous word was ®xated  two bottom lines in Fig. 7). Since this e ect is caused by transitions from state 1 to
84
 Fig. 7. Preview bene®t in models 3A  dashed lines) and 3B  solid lines), averaged over 500 realizations. The bottom two lines represent ®xation durations as a function of word frequency class for all cases, in which the previous word was ®xated. Corresponding mean ®xation durations are shorter than in those cases where the previous word was skipped  top two lines), i.e., there was minimal preview. It is an important consequence of the introduction of autonomous saccades that preview bene®t is modulated by word frequency  see text)
state 3  Fig. 4), the mean residence time in state 2 is reduced. As a consequence, this reduction of the preview bene®t is highest for foveal low-frequency words. As expected, there is no or very little frequency-dependent modulation of ®xation durations due to autonomous saccades if the previous word is skipped  top two lines in Fig. 7).
Finally we note that there are phenomena not covered by the E-Z Reader model as well as our present model, such as the in ̄uence of the di culty of the last word ®xated on ®xation duration on the word currently ®x- ated  Kennedy 2000), or extraction of information to the left of the ®xated word  Binder et al. 1999; see also Kennedy et al. 2000 for a recent review). A discussion of these issues is beyond the scope of the current study; our aim here is to demonstrate that complex dynamic be- havior emerging from simple mathematical models can explain experimental ®ndings in a psychologically plausible way.
5 Discussion
Theoretical models of eye movement control success- fully account for several statistical aspect of eye movement pattern during reading  Reichle et al. 1998). A key model assumption relates to the coupling of eye- movement preparation and shift of visual attention. In the E-Z Reader framework  Reichle etal. 1998), detailed mechanisms of lexical processing are proposed  Liversedge and Findlay 2000). This class of models is based on the assumption that saccade programming is strictly governed by lexical processes. Following Deubel et al.  2000), however, ``the alternative notion that low-
level oculomotor processes might be playing the dom- inant role in eye movement control during reading remains a serious possibility''. Based on the analysis of initial landing positions, Reilly and O'Regan  1998) compared di erent word-targeting strategies as an example of this alternative approach to eye movement control.
In this study we restricted our analysis to minimal models investigating a new form of coupling between eye movements and visual attention, where saccade programs can be initiated both by lexical processes and autonomously. Using a three-state model, we showed that relaxing the assumption that lexical processing completely governs the initiation of saccade programs leads to an increase in model performance. Therefore, it seems promising to study further variants or exten- sions of our three-state model with the aim of analyz- ing the relation between shifts of attention and eye movements. Our preliminary results, however, suggest that models without a strong coupling between lexical processes and saccade programs may still be a viable alternative to the successful E-Z Reader models  Reichle et al. 1998).
As a consequence of the introduction of an autono- mous saccade program, even the three-state model shows complex dynamic behavior, which leads to psy- chologically plausible results on preview bene®t. An important property of this model is that it provides an explanation of how preview bene®t is modulated by foveal processing di culty in terms of its dynamics.
An interesting parameter in models of eye movement control in reading is lexical processing time. Fixation duration as a function of word frequency follows ap- proximately the relation tn   303   8X26 log Fn   Fig. 3a, solid line). In a strictly serial model, explained at the beginning of Sect. 3, lexical and saccade processing time simply sum to tn. Assuming a mean saccade program duration of s   100X0 ms yields a lexical processing time lsn  tn  s 203 8X26log Fn . Based on the same ex- perimental data, all models discussed here provide a signi®cant increase in  available) processing time. This increase is due to parallel processing of saccades and lexical access  Morrison 1984). Furthermore, our model suggests that gaze duration is a key measure of lexical processing time, since the slope parameters   9X5 to  14X7) estimated in the model are in good agreement with the empirical slope for the regression of gaze duration on word frequency   8X26).
Using the theoretical framework of semi-Markov processes, we investigated the role of stochastic transi- tion rules for models of eye movement control. In the E- Z Reader framework  Reichle et al. 1998), pausing-time distributions are assumed to be gamma-distributed with a ®xed relation between standard deviation and mean value. As an alternative, we analyzed residence-time- dependent transition probability rates. Besides the fact that transition probability rate is a more fundamental concept for stochastic processes  Gillespie 1978), an additional advantage of this approach is that it can be implemented by an exact algorithm for numerical simulations.
The implementation of stochastic models may result in considerable deviations from the exact results  Feistel 1977). To avoid these problems for the numerical sim- ulations of our mathematical models, we used an exact algorithm  Gillespie 1978) which is a generalization of the minimal process method  Gillespie 1976). This method was proposed originally for numerical simula- tions of chemical reactions, but has been applied to a broad class of systems, e.g., from molecular biology  Elowitz and Leibler 2000), physiology  Fricke and Schnakenberg 1991), or population dynamics  Engbert and Drepper 1994). Therefore, we believe that our ap- proach may be applicable to a variety of problems in the ®eld of eye movement control.
As a further remark, the framework for stochastic simulation introduced here can also be used for numeri- cal simulations of a broader class of models  Reichle et al. 1998). We restricted our analysis to minimal models in order to investigate the coupling of eye movements and shifts of visual attention in detail. For this reason, the three-state model is on the same level of abstraction as E-Z Reader 1. Possible extensions of this three-state model to account for re®xations could be derived in close analogy to the development of E-Z Reader 3 to 5 by Reichle et al.  1998). In particular, a saccadic re®xation program could be introduced as an additional subprocess in state 1  Fig. 4). We note, however, that a more psychologically plausible expla- nation for the occurrence of re®xations should provide a common mechanism for saccade, re®xations, and regressions.
As a ®nal remark, our results suggest that assump- tions about stochastic properties of mathematical mod- els may strongly in ̄uence the performance of the models as well as their complexity  e.g., the number of internal states necessary). A comparison between E-Z Reader 1  eight internal states) with our three-state model suggests that ± if these assumptions are too restrictive ± model complexity  e.g., number of internal states) could be overestimated.
Acknowledgement. We thank AndreÂ Longtin, University of Otta- wa, for valuable discussions and Eric D. Reichle, Carnegie Mellon University, for providing us with the corpus of 48 sentences used in our simulations. R.K. thanks Jan Droesler, University of Regens- burg, for the hospitality during a stay at his unit. This work was supported by Deutsche Forschungsgemeinschaft  DFG grants KL 955/3-1 and KL 955/3-2).
Appendix A: Numerical simulation ± Gillespie algorithm
E cient numerical simulation techniques for Markov processes  Gillespie 1976; Feistel 1977) have been extended to stochastic simulation of processes with residence-time-dependent transition probability. Details of the derivation of the algorithm can be found in Gillespie  1978).
The pausing-time distribution can be calculated from the transition probability rate  4). For numerical simu- lations, we have to create pseudo-random numbers ac- cording to this distribution. The general rule  Gillespie
1978) for the transformation of computer-generated random numbers r to s is given by
Zs  1
Wm s0 ds0 log r Y  A1
0
where r is a random number from the unit-interval uniform distribution  Press et al. 1988). In our case  5), this relation is invertible, so that s r  can be calculated analytically. Realizations of the pausing time s are obtained from
where s0 is the refractory time. Details of the simulation algorithm are discussed in Gillespie  1978).
As a typical case for the models studied in this paper, there are transitions between states with two competing processes. As an example, consider state 2 in the two- state model  Fig. 2): lexical access ln of wordn and the saccade program sn to the same word are active at the same time. If the saccade program terminates before lexical access is complete, then the transition from state 2 to state 1 is performed and ®xation of wordn starts. When the system arrives in state 1, lexical access of wordn has already been active for a time ~s, which is, in this case, the pausing time in the previous state 2. Therefore, the transition probability in state 1 is W  s   ~s , where ~s is the time period for which lexical access has already been active. We now extend  A2) to the case of two competing sub-processes with one and two adjoining states.
A.1. Transitions with one adjoining state
Let us de®ne the di erence s1   s0   ~s of refractory time s0  5) and ~s of the process. The pausing time at state S1 can be transformed from unit-interval uniform random numbers as follows,
2  1
s r  s0  log Y  A2
85
 s
  wm r
s
2  1
s1 0: s s1  log Y  A3
  w1 r s
2 2  1
s1`0: s s1  s1 wlog r X  A4
  1
A.2. Transitions with two adjoining states
In the case of two possible transitions, like state 2 of the
two-state model  Fig. 2), the stochastic simulation of
the pausing time is more complicated. We consider two
competing sub-processes with s   s k    ~s k  k   1Y 2 ,  k  k 0  k
where s0 are refractory times and ~s are the time periods for which the processes have already been active. For simplicity, let us assume s1   s2. The pausing time of such states can be computed as follows:
86
Case1.s1  0,s2  0 s
Reichle et al.  1998). The frequencies in the di erent classes are 0±10  class 1), 11±100  class 2), 101±1000  class 3), 1001±10000  class 4), and 10001±1  class 5).
With the two-state model we aim to explain the ex- perimental results of mean ®xation duration and prob- ability of word skipping. The mean ®xation duration for words of class k obtained from model simulations is denoted by T k ; rT k  is the corresponding standard deviation. This is compared with the experimentally observed value T  k . The deviation of simulated mean ®xation durations from observed mean ®xation dura- tions is de®ned as
X5  T  k  T k  2
hT  r k  Y  B1
k 1 T
i.e., the sum of squared di erences over ®ve di erent
frequency classes.
As the second measure of model performance we use
the probability of word skipping. p k  is the probability of word skipping in class k; the experimental value is

denoted by p  k . Analogously to  B1), the deviation
between simulated and observed probabilities of word skipping can be de®ned as
X5  p  k    p k  2
hp  r k  Y  B2
k 1 p
p
estimation method,
h2  hT hp 1a2X  B3
B.2Distributionof®xationdurations
For the three-state model, we include a measure for the deviation of simulated data from the observed distribu- tion of ®xation durations,
ws  ws  ws  ws 2 2  1  s 11 22  11 22 log
    w1   w2 Case2.s1  0,
w1   w2 w1 r s2 b0
s
 A5
 a s s2 s s1  s21 w2log 1r 1
   b  s b s2
s   w1s1   w2s2
 w1   w2 s

11 22   22   log
r
 w s   w s  2 w s2 2  1
    w1   w2 w1   w2 Case3.s1Ys2 b0Y s1 `s2
w1
 a  s   s2  b  s b s2
s   s1
s                       2  1
log w1 r
 A6
  w2 ds s s1 w  w
  12
s                                                                                       w2ds  2 w2 ds 2 2  1
p k  1   p k  .
Combining these two terms gives a possible ®tness
  w w  w w w wlogr Y 121212
 A7
where ds   s2   s1.
Appendix B: Parameter estimation ± genetic algorithm
approach
For the estimation of model parameters we use a genetic algorithm  GA) approach  Holland 1992; Mitchell 1996). For each sentence, 500 stochastic realizations of the model were run with a new set of pseudo-random numbers. For the genetic algorithm we used a popula- tion of 100 combinations of parameter values which were iterated over 1000 generations. Several runs of the GA were used to test the reliability of the estimates for the model parameters. The parameter values given in Table 1 represent mean values over nine runs of the GA. A separate simulation was performed to produce the data shown in Fig. 3, 5, and 6. A single run for the parameter estimation took approximately 48 hours CPU time on a SUN Ultra 10 computer. The ®tness function used for the GA optimization method is discussed below.
B.1 Mean ®xation duration and skipping probability
The 536 words of the corpus  48 sentences) are divided into ®ve di erent frequency classes, as suggested in
where rp k
function that is used for the genetic algorithm parameter
    hD
X12 k 1
 h  k    h k  2 Y  B4
where h k ,  ho k ) are the relative frequencies of simulated  observed) ®xation durations in a distribution over 12 bins  from Reichle et al. 1998). Therefore, the ®tness function is modi®ed to
h3  hT hp hD 1a2 X  B5  References
Binder KS, Pollatsek A, Rayner K  1999) Extraction of informa- tion to the left of the ®xated word in reading. J Exp Psychol Hum Percept Perform 25: 1162±1172
Deubel H, O'Regan JK, Radach R  2000) Attention, information processing, and eye movement control. In: Kennedy A,
Radach R, Heller D, Pynte J  eds) Reading as a perceptual
process. Elsevier, Oxford
Elowitz MB, Leibler S  2000) A synthetic oscillatory network of
transcriptional regulators. Nature 403: 335±338
Engbert R, Drepper FR  1994) Chance and chaos in population biology ± models of recurrent epidemics and food chain
dynamics. Chaos Solitons Fractals 4: 1147±1169
Feistel R  1977) Betrachtung der Realisierung stochastischer Prozesse aus automatentheoretischer Sicht. Wiss. Z. WPU
Rostock 26: 663±670
Fricke T, Schnakenberg J  1991) Monte-Carlo simulation of an
inhomogeneous reaction-di usion system in the biophysics of
receptor cells. Z Phys B 83: 277±297
Gardiner CW  1990) Handbook of stochastic methods. Springer,
Berlin, Heidelberg, New York
Gillespie DT  1976) A general method for numerically simulating
the stochastic time evolution of coupled chemical reactions.
J Comput Phys 22: 403±434
Gillespie DT  1977) Master equations for random walks with
arbitrary pausing time distributions. Phys Lett A 64: 22±24 Gillespie DT  1978) Monte Carlo simulation of random walks with residence time dependent transition probability rates. J Com-
put Phys 28: 395±407
Gillespie DT  1992) Markov processes. Academic, San Diego Holland JH  1992) Adaptation in natural and arti®cial systems.
MIT Press, Cambridge, Mass
van Kampen NG  1981) Stochastic processes in physics and
chemistry. North-Holland, Amsterdam
Kennedy A  2000) Parafoveal processing in word recognition. Q J
Exp Psychol A 53: 429±455
Kennedy A, Radach R, Heller D, Pynte J  eds)  2000) Reading as a perceptual process. Elsevier, Oxford
Liversedge SP, Findlay JM  2000) Saccadic eye movements and cognition. Trends Cogn Sci 4: 6±14
Mitchell M  1996) An introduction to genetic algorithms. MIT Press, Cambridge, Mass
Morrison RE  1984) Manipulation of stimulus onset delay in reading: evidence for parallel programming of saccades. J Exp Psychol Hum Percept Perform 10: 667±682
Press WH, Flannery BP, Teukolsky SA, Vetterling WT  1998) Numerical Recipes in C. Cambridge University Press, Cambridge
Rayner K  1998) Eye movements in reading and information processing: 20 years of research. Psychol Bull 124: 372±422 Rayner K, Pollatsek A  1989) The psychology of reading. Prentice-
Hall, Englewood-Cli s, N.J
Reichle ED, Pollatsek A, Fisher DL, Rayner K  1998) Toward a
model of eye movement control in reading. Psychol Rev 105:
125±157
Reichle ED, Rayner K, Pollatsek A  1999) Eye movement control
in reading: accounting for initial ®xation locations and re®x- ations within the E-Z Reader model. Vision Res 39: 4403± 4411
Reilly RG, O'Regan JK  1998) Eye movement control during reading: a simulation of some word-targeting strategies. Vision Res 38: 303±317
Schilling HEH, Rayner K, Chumbley JI  1998) Comparing naming, lexical decision, and eye ®xation times: word fre- quency e ects and individual di erences. Mem Cogn 26: 1270±1281
CogInfoCom 2013 • 4th IEEE International Conference on Cognitive Infocommunications • December 2–5, 2013 , Budapest, Hungary
Measuring Reading Comprehension using Eye Movements
Leana Copeland
Research School of Computer Science Australian National University Canberra, Australia leana.copeland@anu.edu.au
Abstract—We investigate eye movement measures and methods for predicting reading comprehension. This builds on previous work on factors affecting reading comprehension, namely perceived familiarity with the text content. We further investigate answer-seeking behavior and present a method for measuring and comparing this behavior. The number of fixations, number of regressions, and total fixation time are an indicator of reading intensity and the intensity of reading is related to comprehension. We show that a feed-forward backpropagation neural network can be used to predict subjective comprehension scores as well as quiz scores. We propose using the degree of answer-seeking behavior to measure how question difficulty and as an implicit measure of how difficult a participant finds a tutorial and quiz. Such information is beneficial to apply in eLearning to create dynamic learning environments that use eye movement to predict implicit question difficulty as well as individual participant difficulty.
Keywords—Reading Analysis; Reading Comprehension; Eye gaze; adaptive eLearning environments; Reading Behavior
I. INTRODUCTION
We investigate measures for analyzing reading behavior and overall comprehension of the underlying meaning and concepts within a piece of text using eye movements. Our objective is to identify eye movement measures that will predict reading comprehension. The intention is to apply them in eLearning to create dynamic learning environments that use eye movement to detect reader comprehension. We begin by looking at the situation where students read tutorial content and answer comprehension questions. We investigate methods for identifying and comparing answer-seeking behavior. Further, we explore how inferences about reading behavior can be used in an online learning environment to optimize learning. This is part of on-going research that will be more broadly aimed at detecting when a student is having difficultly understanding material to ameliorate effective learning. We aim to increase learning, a fundamental human cognitive process, through interactive communication with the learning environment. We therefore contribute to CogInfoCom research by utilizing knowledge about human cognition to develop information transfer techniques between humans and infocommunications systems that are primarily based on Internet technology and for the purpose of education.
We first review the literature on reading analysis using eye movements and how previous work can be applied to inferring
Tom Gedeon
Research School of Computer Science Australian National University Canberra, Australia tom.gedeon@anu.edu.au
comprehension. Then the results of a user study will be presented and discussed. Finally, we will conclude and indicate how these results will be used in the future work of developing an adaptive eLearning environment.
A. EyeMovementsDuringReading
Eye movements can be broadly characterized as fixations and saccades. A fixation is where the eye remains relatively still to take in visual information. A saccade is a rapid movement that transports the eye to another fixation. At the centre of the retina is a special part of the eye that sees in fine detail called the fovea. The foveal region of the eye is very small, being only about 0.2mm in diameter. Around the point of fixation visual acuity extends about 2° [1].
Generally when reading English fixation duration is around 200-300 milliseconds, with a range of 100-500 milliseconds and saccadic movement is between 1 and 15 characters with an average of 7-9 characters [1]. The majority of saccades are to transport the eye forward in the text when reading English, however, a proficient reader exhibits backward saccades to previously read words or lines about 10- 15% of the time [1]. Backward saccades are termed regressions. Short regressions can occur within words or a few words back and may be due to problems in processing the currently fixated word, overshoots in saccades, or oculomotor errors. However, longer regressions occur due to comprehension difficulties, as the reader tends to send their eyes back to the part of the text that caused the difficulty [1].
B. Reading Comprehension and Eye Movements
Reading comprehension is a skill that must be taught and requires constant education. When understanding language, we integrate ideas in the text and form a mental model that is an abstraction of the conglomeration of ideas [2,3]. The actual text read is not remembered verbatim, it is the ideas and constructed representation that are [2,3]. Comprehension of the text can have significant effects on the eye movements observed [1,4]. Studies have shown there are numerous variables that influence eye movements during reading: semantic relationships between words, anaphora and co- reference, lexical ambiguity, phonological ambiguity, discourse factors and stylistic conventions, and syntactic disambiguation. For example, garden-path sentences are syntactically ambiguous and induce regressions to resolve the
978-1-4799-1546-0/13/$31.00 ©2013 IEEE
791
L. Copeland and T. Gedeon • Measuring Reading Comprehension using Eye Movements
comprehension problems [5]. Eye movements have also been shown to reflect text difficulty [4].
C. Related Work
Eye gaze patterns can be used to detect what kind of task the participant is performing [6,7] or whether a person is reading or not [8,9] as well as if they are reading or skimming [10]. Previous studies have shown that even within the activity of reading, eye gaze patterns can be used to differentiate when individuals are reading different types of content [11].
One method of using eye gaze in human-computer interaction is to integrate eye gaze implicitly into the use of an interface. In such a system the user may not even be aware that their eye gaze is being used to control or alter the use of the system. An example of such a system is eye gaze based rendering. This is where the high-resolution image is displayed at the point of the users fixation [12]. This type of display exploits the fact that fine detail vision occurs only in the fovea and so only high resolution of image display is necessary at the point of fixation. Another example is the use of eye gaze to provide feedback about behavior such as in [13] where eye gaze is recorded to give implicit perceived relevance of pieces of text in a document. There are several applications that are used in reading assistance. iDict is a reading aid designed to help readers of a foreign language [14]. iDict uses eye gaze to predict when a reader is having comprehension difficulties. If the user hesitates whilst reading a word then a translation of the word is provided along with a dictionary meaning. This is somewhat similar to The Reading Assistant [15], which uses eye gaze to predict failure to recognize a word. The Reading Assistant then provides auditory pronunciation of the word to aid in reading.
II. OUR PRIOR WORK
In previous work we analyzed the effect of perceived familiarity with the topic of the tutorial [16]. We found a relationship between perceived familiarity and answering behavior. In the first instance that participants are presented with the text there is no clear objective to the reading of the text other than knowing two questions will be asked about the content. In the second instance, the objective is clear as participants know the questions and can use the text as a reference to find the answers to the questions. This behavior of referencing the text to find the answer is what we have termed answer-seeking behavior. We used the number of fixations and total fixation time recorded for the reading of the second display of the content as indicators of answer-seeking behavior. We found that participants with less perceived familiarity displayed much greater answer-seeking behavior. This was also true for participants that stated they were familiar with the topic but did not answer questions correctly.
This current work is an extension of this research that endeavors to clearly define answer-seeking behavior as a metric that can be used to predict reading comprehension, give information about question and content difficulty, as well as provide a metric for implicit participant performance.
III. METHOD
A user study was conducted to collect participants’ eye gaze as they read a tutorial and completed a quiz based on the tutorial’s content. The tutorial and quiz were coursework from a first year computer science course taken at the Australian National University. There were 15 (6 female, 9 male) participants aged between 17 and 31 that took part in the study.
Participants read 9 slides of content covering a topic of the tutorial (Web Search). Each slide was 400 words long with an average Flesch Kincaid Grade Level of 12. All participants were university students and therefore had at least high school level education indicating that the readability of the slides should not be above their reading abilities. The tutorial content was accessible via the Wattle online learning environment at ANU (a variant of Moodle). After each slide participants answered two questions to measure their comprehension (18 questions in total); one question is multiple-choice and the other is cloze (fill-in-the-blanks). The two types of questions are to assess different forms of comprehension [17]. When presented with the questions, participants were also given the opportunity to re-read the content to aid in answering the questions. Once the participant finished the quiz and before shown their result, participants were asked to subjectively rate their overall comprehension on a scale of 1 to 10 with 10 being complete understanding.
The study was displayed on a 1280x1024 pixel Dell monitor. Eye gaze data was recorded at 60Hz using Seeing Machines FaceLAB 5 infrared cameras mounted at the base of the monitor. EyeWorks was the software used to collect the data. The study involved 9-point calibration sequence.
As the data recorded is a series of gaze points, EyeWorks Analyze was used to pre-process the data to give fixation points. The parameters used for this were a minimum duration of 0.075 seconds and a threshold of 5 pixels.
IV. ANALYSIS OF EYE MOVEMENT MEASURES FOR COMPREHENSION
A. EffectofFirstReadThroughtoLaterReadThrough
During the first presentation the participants read the content with no specific purpose. In the second presentation, participants know the questions and could use the text as a reference for answering the questions. The eye movements differed for the two readings. We hypothesized that there is a correlation between the two reading behaviors. Explicitly, the way a participant read the first display has some effect on how they read it the second time. We expected to see that if the participant read the content more thoroughly in the first presentation then he or she would spend less time reading the second presentation of content.
There is a medium negative correlation between both the number of fixations and the number of regressions (r=-0.4 for both measures), and the total fixation time (r=-0.6) observed in the first read through to the second read through. As more fixations and more regressions were observed in the first reading of the content, fewer fixations and regressions were
792
CogInfoCom 2013 • 4th IEEE International Conference on Cognitive Infocommunications • December 2–5, 2013 , Budapest, Hungary
observed in the second read through. Furthermore, longer total fixation time observed for the first read through was also observed with a decrease in total fixation time observed for the second read through. This indicates that less reading of the second presentation is required to find the answers when more reading was done on the first presentation of the text. More reading is defined as higher numbers of fixations and regressions as well as a longer total fixation time. Whilst, more reading of the text does not guarantee understanding it can be noted that those participants who read the text more thoroughly the first time did not read the second presentation of the text as much.
The number of fixations, number of regressions, and total fixation time are an indicator of reading intensity. Reading intensity can be affected by many different factors such as how interested the reader is in the content, how hard the reader finds the content to understand, as well as external factors such as motivation for reading the content and answering the questions correctly. For these reasons reading intensity is not a direct indicator of reading comprehension. Instead it can be used as a supporting metric for measuring comprehension as we have shown that reading intensity of the first presentation of text is correlated to reading intensity of the second presentation of text. Reading of the second presentation of text is a measure of how confident a participant is in answering a question. More reading of the second presentation of text implies that the participant is not confident answering the question.
B. FurtherExplorationofAnswer-SeekingBehaviour
The areas of text each participant read can be distinguished into four categories; the first presentation of the slide content before questions are known, the multiple choice question, the cloze question, and the second read presentation of the slide content provided for assistance in answering the two questions. In this section we investigate eye movement measures for reading the four sections of text and their relationship to the outcome of answering the questions. When considering the eye movements observed for reading the multiple-choice questions there is a negative correlation between the number of fixations (r=-0.8), the total fixation time (r=-0.8), and the number of regressions (r=-0.7) to the score obtained for the multiple-choice question being read. Similarly for the cloze questions, the outcome of the question is correlated to the eye movements observed. There is a negative correlation between the number of fixations (r=-0.8), the maximum fixation duration (r=-0.8), the total fixation time (r=-0.8), the number of regressions (r=-0.8), and the regression ratio (r=-0.8) to the score obtained for the cloze question being read. Furthermore, there are also moderate negative correlations between eye movement measures observed for the reading of the second presentation of the content to the total marks received for that topic. The number of fixations (r=- 0.5), the maximum fixation duration (r=-0.5), the total fixation time (r=-0.5), and the number of regressions (r=-0.5) are correlated to the score obtained for the both questions the section relates to.
From these correlations we observed that participants who read both the questions and the second presentation of the text
more tended to do worse on the quiz. This observation is parallel to the previous finding that when questions are answered incorrectly more fixations for longer duration are observed. The definition of more reading is high numbers of fixations and regressions as well as a longer total fixation time. We can therefore deduce that more reading is indicative of the participant’s lack of understanding of either the questions or the content. Time spent reading questions and referencing text for the questions is related to the participant’s understanding whereby longer time spent answering the questions indicates less understanding.
Additionally, there are correlations between the eye movement measures observed when reading each type of question and the reading behavior seen when reading the second display of the content. We hypothesis that participants who re-read the question more are having difficulty answering the question and would therefore exhibit similar behavior when reading the second display of content. Indeed, we found positive correlations between the number of fixations observed for reading the multiple-choice question (r=0.7) and the cloze question (r=0.6) to the number of fixations observed for the second display of reading the content. Similarly, a positive correlation was found for total fixation time (r=0.7 and r=0.6, respectively) and number of regressions (r=0.8 and r=0.7, respectively) observed when comparing the eye movement recorded for the multiple-choice and cloze question to the second display of the content.
The participants who do not understand the question or the content well enough to answer the question seek to find the answer by re-reading both the question and the content. We term this answer-seeking behavior. Answer-seeking behavior is indicative of the participant’s confidence in answering the questions. The participant’s confidence is related to his or her actual understanding of the content, their perceived familiarity with the subject matter, as well as their confidence in his or her abilities to answer the questions correctly.
C. Defining Answer-Seeking Behaviour for Feedback
We have established that answer-seeking behavior is an indicator of a participant’s confidence in answering a question. Confidence is a product of many factors such as familiarity with the topic and is related to whether participants answer the questions correctly or incorrectly. More answer-seeking behavior indicates the participant has less confidence and is less likely to know the answer. It is beneficial to measure such behavior so that feedback can then be given based on the existence and extent of the answer seeking observed.
We propose measuring answer-seeking behavior by recording the large jumps between question and content regions and vice versa. Additionally, reading behavior within each region will be detected and recorded. The reading behavior is detected and recorded using the reading detection algorithm as defined by [10]. Both reading and skimming behavior is recorded as reading behavior in this analysis.
There is a strong correlation (r=0.87) between the number of scans between question and content to reading behavior observed. Participants who scanned between question and the reference text frequently also exhibited a larger amount of
793
L. Copeland and T. Gedeon • Measuring Reading Comprehension using Eye Movements
reading fixation transitions comparative to those that had fewer scans. This indicates that participants who scanned between question and text regions did so to read the text as a reference. From this observation we can conclude that region scans can be used as a metric for answer-seeking behavior.
We propose two purposes for measuring answer-seeking behavior. The first is as a feedback tool for instructors about the nature of how students read and answered questions. The second is to provide feedback to instructors about how individual students are performing. We start by elaborating on the first use; feedback about individual questions. The average number of region scans between question and content, the average number of fixation transitions classified as part of reading behavior and the average score for that question are shown in Table I. It can be seen that there is a large range in the average number of region scans for each question. There is also a large range in fixation transitions classified as part of reading behavior for each question. There is a minimum of 3.3 scans and 23.9 reading transitions for question 3 and a maximum of 19.5 scans and 203.3 reading transitions for question 8. From these observations, question 3 was the easiest on average question to answer as fewest region scans and lowest amount of reading was needed to answer the question. Question 8 was the most difficult question to answer on average as the most region scans and the most reading was needed to answer the question.
both measures). Therefore, performance on the question is not an accurate measure of how difficult the participants found the questions. Instead the answer-seeking behavior is a measure of how hard the participants found the questions as well as how much interest they paid to the question. We propose the use of answer-seeking behavior to describe how difficult a question is to answer.
The large standard deviations shown in Table I show that there is a large variation in the observed answer-seeking behavior. This is expected, as there is a large variation in eye movement behavior observed between individuals [1]. Furthermore, we are only considering average performance on questions, as we would expect that some individuals would find questions easier to answer than others. This leads us to the discussion of using answer-seeking behavior to quantify individual student performance.
Next we investigate the use of answer-seeking to establish participant behavior. We have established in previous work that answer-seeking behavior is an indicator of a participant’s confidence in answering the questions. The average number of region scans between question and content, the average number of fixation transitions classified as part of reading behavior, and the total score for that participant are shown in Table II. Note that in Table II the participants are listed in ascending order of average number of scans between questions and content. This ranking of participants’ shows the extent of the variance of answer-seeking behavior each participant exhibits. Once again we can use this information to extrapolate how difficult the individual participant found the tutorial and quiz. The 15th participant showed quite a high amount of answer seeking behavior whilst the 1st participant showed about a seventh of the region scanning and transitions classified as reading to the 15th participant. There is a small negative correlation between the average number of region scans and the participants’ total score (r=-0.34). This indicates that the participants who displayed less answer-seeking behavior were not necessarily correct and may be over confident with their answers.
TABLE I.
ANSWER SEEKING BEHAVIOUR AVERAGES PER QUESTION.
    Question number
   Region Scans between question and content
 Transitions
classified as part of Reading Behavior
  Average score for question
  Mean
STD
  Mean
 STD
 Mean
 STD
 1
  11.1
9.5
  85.9
 79.4
 0.73
 0.46
 2
  12.2
9.2
  115.6
 118.6
 0.90
 0.21
 3
  3.3
3.4
  23.9
 36.7
 1.00
 0.00
 4
  5.9
2.6
  65.5
 52.8
 1.00
 0.00
 5
  4.6
4.6
  36.9
 40.3
 0.80
 0.41
 6
  4.3
3.2
  32
 20.8
 1.00
 0.00
 7
  10.6
8.0
  58.6
 47.7
 0.87
 0.35
 8
  19.5
13.6
  203.3
 173.5
 0.90
 0.21
 9
  6.1
5.0
  73.5
 90.9
 0.73
 0.46
 10
  11.2
8.1
  77.5
 55.0
 0.97
 0.13
 11
  17.3
12.9
  148.1
 107.9
 0.73
 0.46
 12
  17.3
11.5
  146.7
 120.6
 0.97
 0.13
 13
  4.6
4.1
  57.3
 60.6
 0.80
 0.41
 14
  7.9
5.6
  65.9
 80.4
 1.00
 0.00
 15
  5.8
7.7
  64.8
 100.4
 0.67
 0.49
 16
  7.1
4.2
  61.2
 46.0
 0.97
 0.13
 17
  5.5
4.2
  33.7
 32.7
 0.93
 0.26
 18
  9.3
 5.3
   71.7
  42.8
  0.93
  0.18
          TABLE II.
AVERAGE ANSWER SEEKING BEHAVIOUR PER PARTICIPANT.
      Participant
   Region Scans between question and content
Transitions classified as part of Reading Behavior
   Total Score
    Mean
 STD
 Mean
 STD
 1
  2.9
 3.2
 24.9
34.9
  17
 2
  3.2
 3.6
 23.7
21.9
  18
 3
  5.0
 3.4
 67.4
68.3
  17
 4
  5.4
 4.7
 46.6
55
  17
 5
  6.4
 4.7
 40.3
38.9
  16.5
 6
  6.8
 5.8
 47.1
43.1
  13
 7
  6.9
 5.9
 39.4
43.4
  17
 8
  7.8
 6.8
 95.8
108.1
  14.5
 9
  8.6
 5.6
 60.2
51
  18
 10
  10.6
 11.4
 89.7
103.6
  14.5
 11
  11.4
 6.9
 136.2
57.9
  15
 12
  12.3
 8
 56.7
51.9
  13.5
 13
  12.3
 9.7
 143.7
163.8
  16
 14
  14.8
 8.6
 139.4
95.1
  15.5
 15
  22.1
  14.3
  173.9
 129.6
   16
                 On average, participants displayed more region scans and transitions classified as part of reading behavior observed for some questions; that is, more answer-seeking recorded. This indicates that some questions are harder than others to answer. This difficulty could be for several reasons such as ambiguity in the question or technical difficulty of the question. There is no correlation between the number of region scans between questions and content or the amount of reading transitions observed to the score obtained for the question (r=-0.1, for
        794
CogInfoCom 2013 • 4th IEEE International Conference on Cognitive Infocommunications • December 2–5, 2013 , Budapest, Hungary
We propose the use of answer-seeking behavior as an implicit measure of how difficult a participant finds the tutorial and quiz.
Once again there is high variation in the observations as shown by the standard deviations. This is a reflection of the differing complexity of the 18 questions as already discussed and shown in Table I. The standard deviations for each participant can be used to evaluate how consistently difficult that participant found the questions. For example, a low standard deviation indicates low variability and therefore that the participant consistently showed similar answer-seeking behavior. This result indicates that the participant found each question to be similar in complexity. If it is an objective for instructors to construct questions of similar or differing complexity than this information can be used in support of the previous information about question complexity (shown in Table I).
V. PREDICTING READING COMPREHENSION
We now investigate how answer-seeking behavior and eye movement measures can be used to predict participant comprehension. Both the participants' subjective comprehension and quiz score are used as comprehension metrics. The technique used to perform the prediction is a feed-forward backpropagation neural network. Mean square error (MSE) was used to evaluate the performance of the network output. The reported MSE values are the average of 50 runs of the network. Each time 10 participants are selected randomly as the training set and 5 participants randomly selected as the testing set. We used the average answer- seeking behavior and eye movement measures as inputs to the network. The eye movements measures used are: number of fixations, maximum fixation duration, average fixation duration, total fixation duration, number of regressions and regression ratio. There are a total of 16 input neurons to the network and 16 hidden neurons. The average MSE for the training set for predicting participant quiz score is 0.74 and the average MSE for the testing set is 4.3. The average MSE for the training set for predicting participant subjective comprehension score is 0.85 and the average MSE for the testing set is 3.8.
These results show that both subjective and explicit comprehension scores can be predicted using answer-seeking behavior and eye movement measures. The implication of predicting subjective comprehension based on eye movements is that feedback can be given to both instructors and students about their relative understanding of the content and questions. To elaborate on this, if a student is exhibiting eye movement behavior that implies he or she subjectively believes they understand the content however their explicit comprehension scores show otherwise, retargeting of the educational material to the student can be made before giving them a test result. The upshot of this is that the student is not unknowingly demoralized by a low result, which could affect subsequent learning behavior. Instead students would be encouraged to learn the material in a more positive way rather than providing negative feedback.
VI. USING ANSWER-SEEKING BEHAVIOR FOR FEEDBACK
We have defined an indicator of reading intensity by number of fixations, number of regressions, and total fixation time. We have also shown that reading intensity can be used as a supporting indicator for reading comprehension level as more reading intensity for the first read through of the text was seen to correlate to lower reading intensity of the second read through of the text. As stated prior, reading intensity can be affected by many different factors such as how interested the reader is in the content, and how hard the reader finds the content to understand. These factors also contribute to reading comprehension, as readers are more likely to want to learn something they are interested in and are less likely to understand something that is complicated to read. In this case, eye tracking data from students reading learning material can be used to calculate reading intensity of learning material. The reading intensity can then be used along with answer-seeking indicators to extrapolate information about learning content. For example, if a section of text is presented to students and the reading intensity is observed to be low yet later when asked questions based on content, students exhibited high answer-seeking behavior then the instructor can investigate whether the content is interesting enough to keep the students’ attention. Another example is if high reading intensity is observed followed by high answer-seeking this could be a signal that the content is complex.
We have established a method of defining answer-seeking behavior by recording the large scans between questions and content combined with the amount of reading that is performed in the question and content areas. We propose the use of this measure as an indicator to the instructor of question difficulty as well as the participant’s implicit difficulty in completing the quiz. We will now establish the benefits of such information.
There is a range in answer-seeking behavior seen for each of the questions. This shows that some questions were harder to answer then others. The use of answer-seeking behavior as a measure of question difficulty can be used as a feedback system to an instructor. If such information is provided to the instructor then the instructor could gauge how difficult questions are. This difficulty could be due to factors such as the technical nature of the material, and ambiguity in the material. Conversely, the instructor could see that the question is too easy and change it to be more challenging. This information could also be used to weight questions so that more difficult questions are weighted higher than those that are less difficult.
Furthermore, there is a range of answer-seeking behavior seen among the participants. Some found the quiz more challenging than others. It is beneficial for learning if all students are challenged equally. Under-challenged students may get bored and lose interest in the material whilst over- challenged students may become anxious and disheartened by the material. In either case there is a negative impact on the learning process. Using answer-seeking behavior as an implicit measure for a student’s confidence in the material can provide the framework for an adaptive online learning environment. Such an environment can use input from the
795
L. Copeland and T. Gedeon • Measuring Reading Comprehension using Eye Movements
eyes to measure the answer-seeking behavior and alter the learning material and questions in response to the student’s behavior. That is, if a student is found to not be having difficulty completing a quiz then the material can be altered to be more advanced and technical. Conversely, if a student is having difficulty then the material can be altered to be less technical and more basic.
VII. CONCLUSION AND FURTHER WORK
In this study we have investigated answer-seeking behavior and methods for detecting reading comprehension for a tutorial and quiz. This work builds on previous work on the effect of topic familiarity on eye movements during reading. In this research we identified and measured answer-seeking behavior. We found that participants with higher numbers of fixations and regressions, as well as longer total fixation time during reading of the first presentation of the content had lower respective values for reading the second presentation of the content. These eye movement measures can be used as an indicator of reading intensity, which we propose is related to comprehension.
We have shown that more answer-seeking behavior is observed for those participants who did not answer the questions correctly. Answer-seeking behavior is related to the participant’s confidence in answering the question. We have shown that this confidence is also related to whether he or she answered the question correctly or not. Answer-seeking behavior is an indicator of comprehension.
Furthermore, we have proposed the use of answer-seeking behavior to describe how difficult a question is to answer and as an implicit measure of how difficult a participant finds the tutorial and quiz. The eventual goal is to create a tool that will provide feedback to instructors about implicit behavior of students performing a reading task through an online learning environment. For example, if the instructor receives feedback that multiple students are failing to understand specific parts of the text then the instructor can dedicate more time explaining these concepts during face to face teaching time, or could re-word the content to make it easier to understand. Furthermore, the instructor can be given feedback about how students are reading questions and be able to deduce if questions are appropriately worded or are ambiguous and hence causing low scores or confusion. Finally, the information about reading behavior can also be used to dynamically alter tutorial content to personalize the learning experience where students familiar with or excelling at specific content can be given more advanced content to read compared to students that are not familiar with the content or finding it harder to understand.
The outcomes of this current study are aimed to predict a student's reading behavior and understanding of text. We have shown that by using a feed-forward backpropagation neural network both subjective and explicit comprehension scores can be predicted based on answer-seeking metrics and eye movement measures.
This data will be analyzed further to assess how eye movements can be analyzed to assess a participant’s understanding of the content and the questions. This is the first
step in identifying differences in comprehension processes and formulating ways for how to evaluate them using eye movements as an implicit comprehension measure.
The results from this study will be used as the foundation for building a prototype adaptive learning environment. The learning environment will use eye tracking as the input for determining learning rates and behavior during reading so that learning can be personalized for individual students. This is on-going research, which is currently being explored.
VIII. REFERENCES
[1] K. Rayner, “Eye Movements in Reading and Information Processing: 20 Years of Research” in Psychological Bulletin, vol. 124(3), pp. 372-422, 1998.
[2] W. Kintsch, and K.A. Rawson, Comprehension, in The Science of Reading: A Handbook, M.J. Snowling and C. Hulme, Editors, 2005. Blackwell Publishing.
[3] G. Underwood, and V. Batt, Reading and Understanding, 1996. Massachusetts, USA: Blackwell Publishers.
[4] K. Rayner, K.H. Chace, T.J. Slattery, and J. Ashby, “Eye Movements as Reflections of Comprehension Processes in Reading.” in Scientific Studies of Reading, vol. 10(3), pp. 241-255, 2006.
[5] L. Frazier, and K. Rayner, “ Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences.” in Cognitive Psychology, vol. 14(2), pp.178-210, 1982.
[6] S.T. Iqbal, and B.P. Bailey, “ Using Eye Gaze Patterns to Identify User Tasks.” In Proc. The Grace Hopper Celebration of Women in Computing, 2004.
[7] J. Salojarvi, K. Puolamaki, J. Simola, L. Kovanen, I. Kojo, and S. Kaski, “Inferring relevance from eye movements: Feature extraction” in Publications in Computer and Information Science, 2005.
[8] C.S. Campbell, and P.P Maglio., “ A robust algorithm for reading detection.” in Proc of the 2001 workshop on Perceptive user interfaces. 2001. ACM.
[9] C.J. Gustavsson, Real Time Classification of Reading in Gaze Data (Master’s Thesis), 2010, School of Computer Science and Engineering. Royal Institute of Technology. Stockholm, Sweden.
[10] G. Buscher, A. Dengel, and L. Van Elst, “ Eye movements as implicit relevance feedback” in CHI '08 Extended Abstracts on Human Factors in Computing Systems. pp 2991-2996, 2008.
[11] T. Vo, B.S.U. Mendis, and T.D. Gedeon, “Gaze Patterns and Reading Comprehension” in Proc. Of the 17th international conference on Neural information processing: Models and applications, vol. II, pp.124-131, 2010.
[12] R.J. Jacob, and K.S. Karn, “Eye tracking in human-computer interaction and usability research: Ready to deliver the promises.” in Mind, vol. 2(3): p. 4, 2003.
[13] G. Buscher, A. Dengel, R. Biedert, and L. Van Elst, “Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond.” in ACM Transactions on Interactive Intelligent Systems, vol. 1(2), p. Article 9, 2012.
[14] A. Hyrskykari, P. Majaranta, A. Aaltonen, and K.-J. Räihä, “Design issues of iDICT: a gaze-assisted translation aid.” in Proc. of the 2000 symposium on Eye tracking research & applications, pp. 9-14, 2000.
[15] J.L. Sibert, M. Gokturk, and R.A. Lavine, “The Reading Assistant: Eye Gaze Triggered Auditory Prompting for Reading Remediation.” in Proc. of the 13th annual ACM symposium on User interface software and technology, pp. 101-107, 2000.
[16] L. Copeland and T. Gedeon, “The Effect of Subject Familiarity on Comprehension and Eye Movements during Reading”, in Proc. Of the 25th Australian Computer-Human Interaction Conference, 2013
[17] J.M. Fletcher, “Measuring Reading Comprehension” in Scientific Studies of Reading, vol. 10, pp. 323-330, 2006.
Supporting Exploration of Eye Tracking Data: Identifying Changing Behaviour Over Long Durations
Prithiviraj K. Muthumanickam Camilla Forsell Katerina Vrotsou
ABSTRACT
Visual analytics of eye tracking data is a common tool for evaluation studies across diverse fields. In this position pa- per we propose a novel user-driven interactive data explo- ration tool for understanding the characteristics of eye gaze movements and the changes in these behaviours over time. Eye tracking experiments generate multidimensional scan path data with sequential information. Many mathematical methods in the past have analysed one or a few of the at- tributes of the scan path data and derived attributes such as Area of Interest (AoI), statistical measures, geometry, do- main specific features etc. In our work we are interested in visual analytics of one of the derived attributes of sequen- tial data-the: AoI and the sequences of visits to these AoIs over time. In the case of static stimuli, such as images, or dynamic stimuli, like videos, having predefined or fixed AoIs is not an efficient way of analysing scan path patterns. The AoI of a user over a stimulus may evolve over time and hence determining the AoIs dynamically through tem- poral clustering could be a better method for analysing the eye gaze patterns. In this work we primarily focus on the challenges in analysis and visualization of the temporal evo- lution of AoIs. This paper discusses the existing methods, their shortcomings and scope for improvement by adopting visual analytics methods for event-based temporal data to the analysis of eye tracking data.
CCS Concepts
•Human-centered computing → Visualization tech- niques; Visualization design and evaluation meth- ods;
∗All authors are associated with the Linko ̈ping Univer- sity, Norrko ̈ping, Sweden <prithivira j.muthumanickam, camilla.forsell, katerina.vrotsou, jimmy.johansson, matthew.cooper>@liu.se
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
BELIV ’16, October 24 2016, Baltimore, MD, USA
⃝c 2016ACM.ISBN978-1-4503-4818-8/16/10...$15.00 DOI: http://dx.doi.org/10.1145/2993901.2993905
Eye tracking, pattern analysis, scan path, time evolving AoIs, Clustering of Fixations, ActiviTree
1. INTRODUCTION
Eye tracking is a technology which has become increas- ingly popular over the last 15 years as the associated hard- ware has improved and its cost has dramatically reduced. While still not inexpensive, and so hardly ubiquitous, eye- tracking is being used by many groups to perform experi- mental studies in visualization and user interface design as well as film, advertising and related fields [12, 3, 4]. Once correctly calibrated, the eye tracking equipment samples the eye position and orientation within a working volume at a rate typically in excess of 100Hz, often much higher, to determine the gaze direction and so the gaze point on the screen. From this information, for example, the parts of the display which are of greatest interest to the viewer can be determined due to the number and duration of fixations (periods of gaze) as well as the movements (saccades) be- tween fixations as the viewer switches their gaze point. For a visual data analysis task this can enable the experimenter to directly determine those parts of the visual information display which the analyst finds most valuable and interest- ing, the parts of the user interface which are used most fre- quently, or those functions which are most difficult to locate, and to examine the sequence of actions which the analyst makes when carrying out a task.
The data retrieved from the eye-tracking device is gen- erally in the form of fixations, filtered from the raw gaze- point data, accompanied by screen position, time-stamps, duration, a sequence number, and a plethora of other in- formation. This data can then be analysed in many ways, according to the experimenters needs, but most common is to examine areas of interests (AoIs) for the test subject through representations such as heat maps [4], which may be shown for some time window within the data, and ani- mated displays of the sequence of fixations as a scatter plot and the associated saccades using lines or arrows. These displays can be very effective for relatively short tracking experiments, perhaps lasting a few minutes, and so are rou- tinely used for many experiments in visualization, but they can become extremely problematic when longer studies are being carried out with each session lasting, perhaps, a few hours. In such instances these simple displays can become extremely cluttered making the analysis of the eye-tracking data a very laborious and time-consuming task even when specific events are being explored within the data and mak-
Jimmy Johansson
Matthew Cooper
Keywords
∗

ing a purely exploratory analysis of the data close to im- possible. The authors have recently become involved in ex- perimental work being conducted by colleagues within the field of air traffic control [32]. Their experimental tasks typ- ically require eye-tracking studies lasting as long as three hours per session, and they are particularly interested in changes in the behaviour of the test subjects, the air traffic controller, due to events in the (usually simulated) airspace which they are monitoring. Some of these events can be predicted or deliberately inserted into the simulation, per- mitting the experimenter to focus on particular short seg- ments of the eye-tracking data, but there is a need to be able to perform an exploratory analysis of the full duration data to identify unexpected changes in behaviour by the con- troller. To date this analysis has proven too time-consuming to perform, with one three hour experimental study session requiring more than a week to analyse using purely visual means.
The analysis which is desired within these longer studies is to identify the AoIs across the display from the eye-tracking data itself, to identify how these AoIs shift as the task pro- gresses and events occur, to then find patterns of behaviour of the test subjects and to identify how those patterns of behaviour change over time. These AoIs and the changes over time can then be compared between subjects and sim- ilarities and differences in their response can be identified and the reasons behind those differences explored. Identi- fication of AoIs over time and displaying them, while re- taining their sequential and relational information, is then a significant problem in the data analysis due to the exten- sive clutter which can be expected. This problem is further compounded when the study involves comparison between multiple test subjects. To this end, and inspired by previ- ous work in the field of activity data for populations [35], we have begun applying and developing automated and in- teractive methods to assist in analysing these long time se- quences to identify both consistent and shifting AoIs within the recorded eye-tracking data for one subject, sequences of visitations to these different AoIs for a single test subject and changes in that subjects behaviour over time. Having enabled the experimenter to identify such behaviour, and so changes of behaviour over the lengthy duration of the ex- perimental study, we can then produce comparison plots for multiple test subjects to complete the exploratory analysis required for the experimental study. We now review some previous related work and then describe some of our first re- search directions and ideas for future work in this area. The proposed visualization approach will thus work towards,
paper by Winkler et al. [37] standardized a list of attributes for eye tracking data and the need to do the same across fu- ture datasets. The sequential part of the eye tracking data is usually stored in the form of scan paths containing attributes such as (1) fixation information like sequence, position and duration (2) saccadic information like position, length, am- plitude and direction. The existing canon of methods deal- ing with analytics of eye tracking data either operate on the sequential scan path information or on certain derived attributes from them such as (1) AoIs, (2) geometric infor- mation such as circular, linear or mixed patterns, (3) appli- cation specific feature vectors such as median x, y position of eye fixations etc. The data becomes multidimensional when multiple scan paths are collected from different input stimuli (dynamic or static), from multiple users, or from multiple iterations etc. A recent state of the art report on visualiza- tion methods for eye tracking data [4] classifies AoI based visualizations into Timeline AoI representations and Rela- tional AoI representations. Timeline representations (AoI sequence charts, scarf plots, AoI rivers, parallel scan path visualizations etc.) and Relational AoI representations (AoI trees, graphs etc.) do not scale well to a large number of AoIs, or to multiple users. Visual analytics of AoIs can be broadly classified into two categories,
2.
• •
Using clustering methods such as mean shift clustering for identification of temporally evolving AoIs.
User-driven exploration of temporal AoI sequences col- lected from multiple users and visualizing their sequen- tial and relational information in an intuitive manner without visual clutter.
ANALYTICS BASED ON AOI
The stimulus is divided into predefined AoIs and tran- sitions between these different AoIs are analyzed for gaze patterns. The disadvantage of predefined static AoI defini- tion is that it can fail to capture the temporal evolution of the user’s attention over time, particularly when long du- ration experiments are involved. A stimulus can contain a large number of AoIs that increase over time and over mul- tiple user trials. Identifying them in an efficient way, and studying their temporal evolution, has its advantages.
Transition matrix. Transitions between the AoIs are modelled as transition matrices in a tabular form represent- ing the number of transitions to and from each AoI. If the matrix is dense with most of the cells containing transition information, it indicates an extensive search on a display while sparse matrices indicate more efficient and directed search. While the frequency of particular transitions be- tween AoIs is shown, such matrix representations do not convey the temporal aspects of the user behaviour well since the ordering of the transitions is not easy to represent.
Markov models. Transitions between different AoIs can be modelled as first order Markov chains [30] where the tran- sition to a future AoI is dependent only on the present AoI and not on the past AoIs. The Shannon entropy coefficient of the Markov model is then computed to quantify the tran- sition across AoIs. While initial methods presented in [19] were restricted in the number of AoIs due to the error as- sociated with sparse transition matrices, methods like [18] extended the above approach to multiple AoIs and stimuli- independent subdivisions of AoIs. Transition entropy is used to understand the scanning patterns of subjects, with higher entropy indicating more randomness in the sequence of vis- its of AoIs, while a smaller transition entropy value suggests dependencies between two successive AoI visits. Station-
Raw eye tracking data is in the form of gaze coordinates and duration. Typical commercial software such as Tobii Studio [1] convert it into scan paths made of fixations and saccades. Following on from a study of the various data for- mats used across many eye tracking systems [36] a recent
• •
2.1
methods that deal with static predefined AoIs
those that treat AoIs as time evolving entities in static or dynamic stimuli
Static AoI-based analytics
ary entropy presents us the distribution of attention among AoIs, with higher entropy indication that there is a uniform distribution of visual attention among AoIs, while a smaller stationary entropy suggests eye fixations are concentrated on specific AoIs as they attract more visual attention.
Reinforcement learning algorithms. While the tran- sition matrices and Markov models estimate the conditional probability of scan paths only to the first order i.e, only between two AoIs, thereby looking at only one step of the sequence, while higher order matrices suffer from less data (transitions) to calculate an accurate estimate and hence limiting the number of AoIs in a stimulus. Reinforcement learning algorithms learn to predict future scan paths based on past scan paths [13]. Once a transition happens from one AoI to another, instead of simply updating the transi- tion probability from the first to the second AoI, the method associates the first AoI with the second AoI and all expected subsequent AoIs based on prior visits to the second AoI.
Scanpath comparison methods. AoIs are assigned symbol strings and methods such as String edit distance analysis [7] and MultiMatch [10] are used to compare two scan paths but they are constrained in searching for patterns within scan paths. Finding repeated patterns inside a scan path suffers from quantization error due to the fixed sized window constraint that breaks the approximated string rep- resentation of the scan path for comparison [20]. Only a loosely coupled grammar can be derived for finding the pat- terns from the string representation. [2] provides a state of the art report on different scan path comparison methods. Several visualization methods such as circular heat maps [6], AoI rivers [8] etc., have been used to display the AoIs from static stimuli featuring fixation count inside AoIs, transi- tions between AoIs etc.
2.2 Temporal evolution of AoIs
Due to the evolving nature of AoIs over time, visual ana- lytics of gaze patterns in dynamic stimuli, such as videos, is a complex task [23]. Static stimuli are not an exception since new users seeing a stimulus for the first time may find differ- ent areas of the display to be interesting and, once they gain an understanding, the AoIs in their gaze pattern may change altogether [22]. Similarly a complex visual data anysis task may involve several steps, each making use of different parts of the display which will be reflected in different AoIs dur- ing difference phases of the analysis. Annotation of AoIs may be based on fixation counts on a local neighbourhood, the number of transitions between two regions in the stimuli represented and analysed using transition matrices [15]. The first step in the visual analytics of spatio-temporal AoIs is clustering them by taking the temporal aspect into account and the next step is visualizing them using methods that portray their sequential nature, duration of occurrence over multiple users and iterations.
Clustering methods. Hierarchical methods, density- based and grid-based methods are some of the clustering techniques that have been used to group fixations into AoIs. A comprehensive list of clustering methods can be found in [14, 17]. Several similarity metrics have been used to cluster fixations in [24] such as Levenshtein distance, atten- tion distribution-based and AoI transition-based methods. Multilevel visual groups based on agglomerative hierarchi- cal clustering [17] were created on the dynamic targets in the scene [15] where they take into account the inherent hierar-
chical structure present in the AoIs of gaze patterns. Some of the clustering methods relevant in the context of large data are explained in Section 3.1.
Visualization methods. Visual exploration of cluster- ing results can be performed using numerous visualization techniques. The Space-Time Cube (STC) has been used to display the time evolving nature of regions of interest, their temporal duration, underlying stimuli (dynamic/static) and aggregated information from multiple users [25, 11]. The spatio-temporal context of AoIs across multiple users, stim- uli and iterations can be represented in STCs but simultane- ous display of all AoIs leads to visual clutter. Later methods have used a combination of synchronized views to overcome this problem. ISeeCube identifies time evolving clusters of fixations using mean shift clustering and displays them in STC visualization. AoIs are annotated manually based on the identified clusters and displayed using STC and the vi- sual clutter is handled through synchronized scarfplots and timeline views [24]. Among alternative visualizations that try to reduce visual clutter, scarfplots are an efficient way of representing AoIs and their evolution over time, but they still suffer from colour coding, clutter over time and multi- ple user trials. Hence hierarchical visualization approaches that take into account the inherent hierarchies in AoI clus- ters over time are used in [5]. In this recent work multiple visualization tools such as AoI trees, graphs, transition ma- trix and hierarchy diagrams have been used to collectively portray the sequential, relational and temporally evolving nature of AoIs. The AoI Transition Trees approach [26] combines scarfplot with an additional space-filling icicle plot diagram that provides a hierarchy of transition sequences. Thumbnails that represent the AoIs from the stimuli are added to the nodes of the tree. The height of the thumbnail corresponds to the frequency of visits to that AoI. It focuses on the strategy of how AoIs are visited rather than what AoIs were visited. They perform clustering and identifica- tion of AoIs through the ISeeCube interface. eSeeTrack [34] uses a similar visualization structure but, in place of thumb- nails, words were used as tree nodes that were similar to that of WordTree. The purely visual representations described above can suffer from visual clutter, particularly when deal- ing with long duration data. While AoI trees, graphs, scarf- plots etc. suffer from the same big data problem, transition matrices cannot convey the time evolving nature of the AoIs. In AoI trees, for example, the arrows depicting transitions between AoIs can become cluttered over time. In order to overcome this shortcoming, we propose a semi-automatic user driven approach for interactive exploration of AoI se- quences over time. In this paper, we plan to adapt algo- rithms used for exploring sequences in a large event-based data collected from multiple users [35]. A tree inspired clut- ter free representation is used to explore paths of AoI nodes one at a time chosen by the user and a scarfplot like rep- resentation to highlight the AoI across time, duration and multiple users (Refer to Figure 2).
3. INTERACTIVE VISUALIZATION OF TEMPORAL EVOLUTION OF AOI
The first step of our approach is to use fast clustering algorithms such as the mean shift method that can iden- tify temporally evolving AoIs in both static and dynamic stimuli. In the second step, the visual clutter due to over-
plotting in the timeline AoI displays will be addressed using a novel approach. It is new to this field with its origins from algorithms and techniques used for interactive explo- ration of event-based datasets - the ActiviTree method [35]. Using this approach, users can analyse how the transition between AoI sequences have evolved over time and identify AoI sequence paths followed by different users based on their significance determined by the data analyst.
3.1 Identification of AoIs through clustering
Automatic clustering reduces the time consuming manual AoI definition that requires prior knowledge from the ana- lyst. When dealing with raw data, identification of fixations, saccades and smooth pursuits (dynamic stimuli) can be car- ried out effectively using probabilistic models. Online meth- ods such as Bayesian model based clustering algorithms [33] can be used. The advantage of using probabilistic meth- ods over threshold based systems is that they are parameter free and are not limited by thresholds, rather they learn them from the data itself [16]. When huge volumes of eye tracking data that do not fit in the main memory are to be involved in the clustering process, density-based clustering methods based on DBSCAN can be used [28]. As our aim is to find temporally changing AoIs, threshold based algo- rithms that expect input parameters in advance in the form of number of AoIs etc. are not suitable. For clustering the fixations, we initally plan to use the mean shift clustering method [9, 31] that has inherent advantages in finding AoIs that evolve over time and the identification of them is not through an analyst but data driven [11]. The raw eye track- ing data is collected in the form of screen position and time stamp Di = (xi , yi , ti ). Mean shift clustering iteratively moves points to locations of higher density called modes - the weighted mean of neighboring points based on a multivari- ate gaussian kernel. A scale parameter σ is used to specify the kernel, increasing its value results in fewer, larger clus- ters. This parameter does not control the actual size of the density cluster, but it makes sure that no two clusters exist which are closer than this scale parameter. Optimum values for this scale parameter can be determined automatically us- ing methods described in [31, 27]. The algorithm computes clusters of interest that are robust to noise and small out- liers. The method is very effective over traditional methods and is used in the Eyetrace software tool [21]. Irrespective of any methods, while performing clustering for identification of AoIs in a stimulus for multiple users, the clusters should be in sync with the other users also. Coherence should be established across clusters discovered across time and differ- ent users. In this process, user specified threshold values can account for inherent noise in treating two different clusters as a single cluster. Mean shift method can clearly perform robust clustering on data from different users in the presence of noise producing coherent AoIs [31]. The location property of the clusters, such as the centroid, can be used to estab- lish this coherence across different viewers, time periods etc based on a predefined threshold.
3.2 Methodology behind clustering
While clustering the fixation points into AoIs, the empha- sis lies on identification of AoIs based on the geometrical po- sition of the occurrence of fixations across any kind of stim- ulus: static or dynamic - active or passive. We consider the AoI identification across different users and stimuli purely
from a geometrical perspective of the neighboring fixation positions and thereby avoid the process of associating the underlying semantic information of the stimulus with the identified AoIs and the necessity to make the identification of AoIs in sync across different types of stimuli.
Once the data analyst identifies an interesting sequence of AoIs visited by different users, the semantic information of the stimulus that stimulated an Area of Interest for the user can be highlighted on the underlying stimulus in a separate co-ordinated view. In this subsequent step, the nature of the stimulus being static or dynamic - active or passive that simulated this aggregation of fixation points at a particular region can be analyzed. While this step does not necessi- ate the AoI identification process to be in sync with the underlying stimulus, it should be understood that it is not the same with regard to geometric position of occurrence of AoIs. When the identification of AoIs is performed across different users, there is a necessity for uniform labelling of AoIs based on their position of occurrence in the stimulus. Hence the identified AoIs should be in sync across different users purely from the perspective of the geometrical position of AoIs.
3.3 ActiviTree for exploring temporal evolu- tion of AoIs
ActiviTree [35] was developed for exploring sequences in event-based temporal data collected across large numbers of test subjects. The exploration is user-driven and it over- comes the disadvantages of visual clutter in the space-time cube: AoI trees and graphs of increasing size [35]. In this context the event data is related to sequences of visits to the AoIs within the eye tracking data as they share sequence in- formation and duration across multiple users. We can adopt this approach for the analysis of temporally evolving AoIs in an eye tracking data set collected across different users.
ActiviTree for AoI sequence data. Figure 1 illus- trates a snapshot of the ActiviTree visualization framework applied on social science diary (event) data over long du- rations. The visualization is divided into two views - the ActiviTree visual interface and a linked view containing the event sequences with time of occurrence of events mapped against the vertical axis and the data collected from different users mapped against the horizontal axis. In our proposed framework, we can adapt this visualization model wherein the event sequences are replaced with AoIs from eye tracking data. The view containing AoI sequences resembles that of a Scarfplot [29] and the use of the ActiviTree interface not only reduces the visual clutter but also handles the inability of Scarfplots to portray transition information between AoIs. It is a hybrid approach making use of a Timeline AoI and a Relational AoI representation in an interactive and efficient manner. It is more intuitive when compared with AoI trees and graphs where filtering options and thresholding were used to reduce visual clutter. Also the interface can be used to compare eye tracking data collected from different stimuli or users or iterations. Methods such as transition diagrams and recurrence quantification can be employed to convey the high frequency and recurrent patterns in eye gaze data and so guide the user in selecting potential AoIs of interest in the sequence. While in these methods, the term frequency of a certain pattern is synonymous with interestingness in a eye tracking data, ActiviTree enables an analyst to drive the data exploration process to find patterns based on their
 Figure 1: ActiviTree visual interface for a social diary data. a) Tree diagram where each node will correspond to AoIs along with their significance scores computed by Hubs and authorities algorithm. b) Sequence plot similar to Scarfplot with time in vertical axis and user dimension in horizontal axis. The sequence of AoI visits along with their duration and temporal information can be displayed. Image Courtesy: With permission from Vrotsou et.al 2009 [35].
own interest as explained below.
Interactive approach. After the AoIs are identified us-
ing clustering algorithms discussed in the previous section, they can be listed in a table sorted by their significance score, computed based on frequency of user visits. The an- alyst can begin the exploration by selecting any one of the AoIs that they find interesting. It may be based on the sig- nificance score of AoIs computed across all users or specific AoIs corresponding to an interesting region of the stimulus or an AoI identified from the previous exploration of AoI se- quence path etc. A tree based representation is then used to connect the selected AoI with all the possible previous and subsequent AoIs visited by the users, ordered by the signif- icance score. Figure 1(a) portrays the stepwise exploration of potential event-based sequences in social diary data that can be adapted to an AoI sequence exploration. The analyst can repeat this process by adding each AoI of importance at every step (See Figure 2 (a)) and their time of occur- rence along with duration can be displayed in the sequence plot in Figure 2 (b) and (c). The computation of signifi- cant scores for each AoI can be computed at each step using the generalization of hubs and authorities algorithm as de- scribed in [35]. As the term interestingness is subjective to a data analyst, the entire analysis process is user driven. For example, a user can search for highly visited AoI sequence paths, less frequent AoI sequences, identification of an AoI that caused a drift in visual attention over a high frequented sequence path of AoIs etc.
While the ActiviTree based method has many advantages over its peers, it cannot associate the relation of the iden- tified clusters with the underlying semantic information of the stimulus. A co-ordinated view containing the stimulus with the identified AoIs highlighted at the time of their oc- currence can help to alleviate this problem.
4. CONCLUSION
In this paper we discussed the existing methods that deal with visual analytics of derived attributes of eye tracking data - the AoIs. The inability of the state of the art meth- ods such as space time cube, transition matrix, AoI graphs and trees to overcome the problem of visual clutter caused by large volume of eye tracking data is highlighted and a poten- tial solution based on an analyst driven approach, ActiviT- ree, is proposed. The use of efficient clustering approaches such as mean shift for capturing the time evolving nature of the AoIs in a static or dynamic stimuli environment is discussed. Using this method coherence can be established across clusters discovered across time and different users. In our ActiviTree based visualization we propose a hybrid approach using tree inspired interactive exploration of AoI sequences driven by the data analyst. Using a user driven exploration process we can search for highly visited or less frequented AoI sequence paths, identification of an AoI clus- ter that caused a drift in visual attention over a highly or less frequented AoI sequence path etc. Those selective AoI sequences of interest identified by the analyst can be dis- played in a scarfplot like display presenting a timeline view and duration information of AoI sequences. Visual clutter is averted since only the selected scan path sequences based on user interest are displayed in this visualization plot. We believe that this position paper can motivate researchers to deal with visual analytics of temporal AoIs with a more intu- itive, data analyst centred interactive visualization approach in the future.
5. ACKNOWLEDGMENTS
This work is funded by the Swedish Research Council, grant number 2013-4939.
a)
successive AOIs
successive AOIs
       selected AOI
previously visited AOIs successive AOIs
AOI sequence
selected AOI sequence
previously visited AOIs
         b)
Time
                                                                                                                                                                    previously visited AOIs c)
Users
  Time
                                                                                                                                                    Figure 2: Methodology of ActiviTree based visual interface. a) At the initial step, users are free to select a single AoI based on their significance score computed across different users. All the previously visited AoIs and successive AoIs for the selected AoI is represented in a tree diagram. Tree nodes represent the AoIs computed from the clustering step and they are arranged in their increasing order of significance from right to left. Data analysts trying to find interesting AoI sequences can interactively select a subsequent AoI and construct an AoI sequence path. The sequence of AoI visits along with their duration and temporal information are displayed in a scarfplot like sequence plot to the right as shown in b) and c).
Users
6. REFERENCES
[1] Tobii studio users manual. Technical Report Version 3.4.5, Tobii AB, January 2016.
[2] N. C. Anderson, F. Anderson, A. Kingstone, and
W. F. Bischof. A comparison of scanpath comparison methods. Behavior research methods, 47(4):1377–1392, 2015.
[3] G. Andrienko, N. Andrienko, M. Burch, and
D. Weiskopf. Visual analytics methodology for eye movement studies. Visualization and Computer Graphics, IEEE Transactions on, 18(12):2889–2898, 2012.
[4] T. Blascheck, K. Kurzhals, M. Raschke, M. Burch, D. Weiskopf, and T. Ertl. State-of-the-art of visualization for eye tracking data. In Proceedings of EuroVis, volume 2014, 2014.
[5] T. Blascheck, K. Kurzhals, M. Raschke, S. Strohmaier, D. Weiskopf, and T. Ertl. Aoi hierarchies for visual exploration of fixation sequences. In Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research & Applications, pages 111–118. ACM, 2016.
[6] T. Blascheck, M. Raschke, and T. Ertl. Circular heat map transition diagram. In Proceedings of the 2013 Conference on Eye Tracking South Africa, pages 58–61. ACM, 2013.
[7] S. A. Brandt and L. W. Stark. Spontaneous eye movements during visual imagery reflect the content of the visual scene. Journal of cognitive neuroscience, 9(1):27–38, 1997.
[8] M. Burch, A. Kull, and D. Weiskopf. Aoi rivers for visualizing dynamic eye gaze frequencies. In Computer Graphics Forum, volume 32, pages 281–290. Wiley Online Library, 2013.
[9] D. Comaniciu and P. Meer. Mean shift: A robust approach toward feature space analysis. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 24(5):603–619, 2002.
[10] R. Dewhurst, M. Nystro ̈m, H. Jarodzka, T. Foulsham, R. Johansson, and K. Holmqvist. It depends on how you look at it: Scanpath comparison in multiple dimensions with multimatch, a vector-based approach. Behavior research methods, 44(4):1079–1100, 2012.
[11] G. Drusch, J. Bastien, and S. Paris. Analysing eye-tracking data: From scanpaths and heatmaps to the dynamic visualisation of areas of interest. Advances in Science, Technology, Higher Education and Society in the Conceptual Age: STHESCA, 20:205, 2014.
[12] A. T. Duchowski. A breadth-first survey of eye-tracking applications. Behavior Research Methods, Instruments, & Computers, 34(4):455–470, 2002.
[13] T. R. Hayes, A. A. Petrov, and P. B. Sederberg. A novel method for analyzing sequential eye movements reveals strategic influence on raven’s advanced progressive matrices. Journal of Vision, 11(10):10–10, 2011.
[14] A. K. Jain, M. N. Murty, and P. J. Flynn. Data clustering: a review. ACM computing surveys (CSUR), 31(3):264–323, 1999.
[15] Z. Kang and S. J. Landry. An eye movement analysis algorithm for a multielement target tracking task: Maximum transition-based agglomerative hierarchical
clustering. Human-Machine Systems, IEEE
Transactions on, 45(1):13–24, 2015.
[16] E. Kasneci, G. Kasneci, T. C. Ku ̈bler, and
W. Rosenstiel. The applicability of probabilistic methods to the online recognition of fixations and saccades in dynamic scenes. In Proceedings of the Symposium on Eye Tracking Research and Applications, pages 323–326. ACM, 2014.
[17] S. Kotsiantis and P. Pintelas. Recent advances in clustering: A brief survey. WSEAS Transactions on Information Science and Applications, 1(1):73–81, 2004.
[18] K. Krejtz, A. Duchowski, T. Szmidt, I. Krejtz,
F. Gonz ́alez Perilli, A. Pires, A. Vilaro, and
N. Villalobos. Gaze transition entropy. ACM Transactions on Applied Perception (TAP), 13(1):4, 2015.
[19] K. Krejtz, T. Szmidt, A. T. Duchowski, and I. Krejtz. Entropy-based statistical analysis of eye movement transitions. In Proceedings of the Symposium on Eye Tracking Research and Applications, pages 159–166. ACM, 2014.
[20] T. C. Ku ̈bler, E. Kasneci, and W. Rosenstiel. Subsmatch: Scanpath similarity in dynamic scenes based on subsequence frequencies. In Proceedings of the Symposium on Eye Tracking Research and Applications, pages 319–322. ACM, 2014.
[21] T. C. Ku ̈bler, K. Sippel, W. Fuhl, G. Schievelbein,
J. Aufreiter, R. Rosenberg, W. Rosenstiel, and
E. Kasneci. Analysis of eye movements with eyetrace. In Biomedical Engineering Systems and Technologies, pages 458–471. Springer, 2015.
[22] K. Kurzhals, C. F. Bopp, J. Ba ̈ssler, F. Ebinger, and D. Weiskopf. Benchmark data for evaluating visualization and analysis techniques for eye tracking for video stimuli. In Proceedings of the Fifth Workshop on Beyond Time and Errors: Novel Evaluation Methods for Visualization, pages 54–60. ACM, 2014.
[23] K. Kurzhals, B. Fisher, M. Burch, and D. Weiskopf. Eye tracking evaluation of visual analytics. Information Visualization, page 1473871615609787, 2015.
[24] K. Kurzhals, F. Heimerl, and D. Weiskopf. Iseecube: Visual analysis of gaze data for video. In Proceedings of the Symposium on Eye Tracking Research and Applications, pages 43–50. ACM, 2014.
[25] K. Kurzhals and D. Weiskopf. Space-time visual analytics of eye-tracking data for dynamic stimuli. Visualization and Computer Graphics, IEEE Transactions on, 19(12):2129–2138, 2013.
[26] K. Kurzhals and D. Weiskopf. Aoi transition trees. In Proceedings of the 41st Graphics Interface Conference, pages 41–48. Canadian Information Processing Society, 2015.
[27] T. Lindeberg. Scale-space theory in computer vision, volume 256. Springer Science & Business Media, 2013.
[28] I. Peca, G. Fuchs, K. Vrotsou, N. Andrienko, and G. Andrienko. Scalable cluster analysis of spatial events. In Proceedings of International Workshop on Visual Analytics (EuroVA 2012), pages 19–23, 2012.
[29] D. C. Richardson and R. Dale. Looking to understand: The coupling between speakers’ and listeners’ eye
movements and its relationship to discourse comprehension. Cognitive science, 29(6):1045–1060, 2005.
[30] S. M. Ross et al. Stochastic processes, volume 2. John Wiley & Sons New York, 1996.
[31] A. Santella and D. DeCarlo. Robust clustering of eye movement recordings for quantification of visual interest. In Proceedings of the 2004 symposium on Eye tracking research & applications, pages 27–34. ACM, 2004.
[32]  ̊A. Svensson. Air traffic controller’s work-pattern during air traffic control tower simulations: A eye-tracking study of air traffic controller’s eye-movements during arrivals. Master’s thesis, Linko ̈ping University, Sweden, 2015.
[33] E. Tafaj, G. Kasneci, W. Rosenstiel, and M. Bogdan. Bayesian online clustering of eye movement data. In Proceedings of the Symposium on Eye Tracking Research and Applications, pages 285–288. ACM, 2012.
[34] H. Y. Tsang, M. Tory, and C. Swindells. eseetrack - visualizing sequential fixation patterns. Visualization and Computer Graphics, IEEE Transactions on, 16(6):953–962, 2010.
[35] K. Vrotsou, J. Johansson, and M. Cooper. Activitree: interactive visual exploration of sequences in event-based data using graph similarity. Visualization and Computer Graphics, IEEE Transactions on, 15(6):945–952, 2009.
[36] S. Winkler and S. Ramanathan. Overview of eye tracking datasets. In QoMEX, pages 212–217, 2013.
[37] S. Winkler, F. M. Savoy, and R. Subramanian. X-eye: A reference format for eye tracking data to facilitate analyses across databases. In IS&T/SPIE Electronic Imaging, pages 90140L–90140L. International Society for Optics and Photonics, 2014.
A Robust Realtime Reading-Skimming Classifier
Distinguishing whether eye tracking data reflects reading or skim- ming already proved to be of high analytical value. But with a po- tentially more widespread usage of eye tracking systems at home, in the office or on the road the amount of environmental and exper- imental control tends to decrease. This in turn leads to an increase in eye tracking noise and inaccuracies which are difficult to address with current reading detection algorithms. In this paper we pro- pose a method for constructing and training a classifier that is able to robustly distinguish reading from skimming patterns. It oper- ates in real time, considering a window of saccades and computing features such as the average forward speed and angularity. The algorithm inherently deals with distorted eye tracking data and pro- vides a robust, linear classification into the two classes read and skimmed. It facilitates reaction times of 750ms on average, is ad- justable in its horizontal sensitivity and provides confidence values for its classification results; it is also straightforward to implement. Trained on a set of six users and evaluated on an independent test set of six different users it achieved a 86% classification accuracy and it outperformed two other methods.
CR Categories: I.5.4 [Computing Methodologies]: PATTERN RECOGNITION—Applications;
Keywords: eyetracking,reading,skimming,machinelearning 1 Introduction
The preferred way to detect reading behavior, i.e., deciding whether observed eye movement patterns are compatible with patterns com- monly defined as reading, is to measure fixation progress on words expressed in character units [Hyrskykari 2006], [Buscher et al. 2008]. This approach is thoroughly studied [Rayner 1998a] and works well in cases where the eye tracking accuracy is high enough to provide word level resolution.
However, many eye tracking systems and situations are unable to provide such a high resolution, especially due to stochastic noise, measurement drift, miscalibrations, and all of these in combination with small fonts and narrow line spacing. Figure 1 depicts several of these issues. Because of this noise, even though a text has been read, the individually measured gaze points can miss most words and the overall pattern usually jumps diagonally across multiple lines, rather than following a single line continuously.
∗ ralf.biedert@dfki.de
† joern.hees@dfki.de
‡ andreas.dengel@dfki.de § georgbu@microsoft.com
Copyright © 2012 by the Association for Computing Machinery, Inc.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1 (212) 869-0481 or e-mail permissions@acm.org.
ETRA 2012, Santa Barbara, CA, March 28 – 30, 2012. © 2012 ACM 978-1-4503-1225-7/12/0003 $10.00
Figure 1: Sample fixation data displaying problems commonly en- countered when dealing with eye tracking data on text. Most of the fixations are off and it is unclear to which line of the text they apply to. Also, most saccades are not strictly horizontal but rather jump diagonally across the text.
In psychological studies the experiment settings can be controlled sufficiently, either through the use of head fixation, the manual re- alignment of gaze data with the help of control points, or by pre- senting just a single line of text at a time. However, we believe the principal issue of noisy eye tracking data will be of special con- cern in mostly uncontrolled interactive scenarios. There, usually normal-sized texts are read with a partially degraded calibration, and no manual correction can be performed. We anticipate that especially future handheld eye tracking systems, similar to the re- cently developed C121 unit, will be affected by these issues. This device class suffers from three additional degrees of freedom (de- vice rotation) in comparison to desktop mounted devices, and when being hold will be more prone to tilts, shifts and shaking.
But even in such situations where the point of gaze cannot be deter- mined exactly, it can be desirable to automatically decide to what extent eye movements resemble a reading pattern or a skimming pattern in order to automatically respond to this behavior. Exam- ples applications include ScentHighlight [Chi et al. 2005], which highlights related sentences during reading; the eyeBook [Biedert et al. 2010a], where ambient effects are to be triggered in proxim- ity of the reading position; or QuickSkim [Biedert et al. 2010c], where non-content words may be faded out in real time with an in- crease of skimming speed to make reading more efficient. Also, in the domain of information retrieval it has been shown that ac- quiring implicit feedback from a reading and skimming detection can significantly improve search accuracy through personalization [Buscher 2010].
In this respect it is now an interesting observation that human judges who inspect noisy eye tracking scanpaths are often able to classify what segments of the scanpath belong to reading or skimming be- havior, even though the fixations do not match the underlying text2 (e.g., consider the scanpath in Figure 2). More specifically one can
1See Tobii http://tobii.com
2Although the knowledge that the scan path was related to text—while
Ralf Biedert∗ Jo ̈rn Hees† Andreas Dengel‡
German Research Center for Artificial Intelligence
Georg Buscher§ Microsoft Bing
 Abstract
   123
 Figure 2: Fixation data with the underlying surface removed. Even though the text is not visible to a human judge it is possible to estimate what parts of it point to reading behavior. The overall pattern clearly indicates reading-like behavior, the line height is visible, and since most fonts have a certain aspect ratio also the character width can be estimated, which in turn gives an estimate if the pattern rather indicates reading or skimming.
notice that a classification of eye tracking data can usually be done already when only some limited context of the saccade in question is provided.
Based on this observation we propose a reading detector that learns to recognize reading, similar to human experts, through an analy- sis mostly based on scanpaths. This should enable it to perform a classification even under unfavorable conditions. Since some of our use cases also require a real time response, with reaction times less than one second, the algorithm should also be able to classify incoming gaze data with as little forward-context (i.e, fixations and which happened after the saccade being considered) as possible. Furthermore, it should be reasonably simple in terms of parameters and their meaning and be adjustable in how permissive it is with respect to various reading speeds. In its very principal structure the algorithm we propose is somewhat similar to the real time reading detection described by [Campbell and Maglio 2001], with a num- ber of important differences. Most notably it does not distinguish icon search from reading, but is rather a method capable of dis- tinguishing intensive, careful textual perusal (reading for meaning, compare [Reichle et al. 1998]) from less intensive, rather searching textual perusal (skimming, ditto).
However, it should be kept in mind that it is not our intention to construct an algorithm that emits the labels reading or skimming in terms of a ground truth on its own. It rather rates how closely its input resembles what it previously learned about these classes. Their actual definition might depend on a specific context, such as book reading in a Germanic language.
The rest of the paper is now structured as follows. We start with pre- senting an explorative study for generating a ground truth gaze data set for reading and skimming behavior. Using the training set we construct a classifier to detect and differentiate reading from skim- ming, based on features described in section Features & Training. In the Evaluation section we describe our results on these features and classifiers and present a linear model to discriminate both be- haviors. This is followed by our Conclusion and an Outlook onto future work.
2 Explorative Study
In order to construct a classifier that can learn to distinguish the classes reading and skimming we need to collect a data set con- taining both. Thus, we start with an explorative study in which we record eye movements for a number of persons that are asked to
not strictly necessary—will help to reduce false positives. However, the scenario we mainly target is when due to drift and errors the measured gaze position cannot be reliably matched to the text. While the system usually has knowledge that the user is considering text (and not icons or images) it cannot reliably determine character progress.
read various texts in different modes. These recording constitute our training and evaluation dataset.
For the study participants are asked to assume the role of a news paper editor who has to read various types of articles under time pressure in order to answer questions at the end of each trial. A single experiment consists of four trials and each trial started with the presentation of a fictional mail from their main editor asking them to write about the given topic. Each mail also contains two to three attached documents on which their report for the trial should be based upon. In order to elicit reading and skimming behavior, some passages presented in each trail contain relevant text which has to be read intensively to successfully complete the task. Others contain irrelevant information which must be skimmed or skipped to finish in time. On average each document contains one paragraph that is related to the topic while the remaining ones are unrelated.
For our actual study we invited 12 participants to read each of the texts, their average age was 24.6 years (ranging from 20 to 31), 11 of them male, 1 female and most of them were students at the local university. Since all texts are written in German we also ensured the participant’s primary language was German. Each user was in- structed verbally on the tasks and was given an initial training trial which was discarded for the subsequent evaluation. The experiment took place in a web browser with a font size of 16pt, the interaction was recorded with a Tobii 1750 tracker and processed with the Text 2.0 Framework [Biedert et al. 2010b].
Overall the experiment yielded 12 ∗ 8 = 96 read document passes, and for our ground truth the recorded eye tracking data has to be manually classified into the main categories read and skimmed. With reading we subsume line-wise gaze behavior with real or as- sumed average saccade distances below approximately 10 charac- ters (real when the fixations clearly matched text, assumed when it did not match the text but the distances could be extrapolated from words nearby; newline regressions within a read context also count as reading). Skimming subsumes all other gaze behavior that occurs on text, such as line-wise perusal with offsets larger than 10 char- acters and obvious vertical movements. Our distinction between reading and skimming is mostly influenced by the approximate size of the word identification span (compare, for example [Underwood 1985] and more generally [Rayner 1998b]), and we assume that a number of saccades larger than this span are an indicator that the texts covered by these saccades are most likely not read for mean- ing.
To label the data we next created a rating application. For each round it presents the visual representation of a randomly selected window of eight subsequent fixations and their saccades, including the text, compare Figure 3. The saccade in the middle is called the saccade under consideration, while each of the three surrounding saccades are called their past- and future context, respectively. The saccade in the middle is highlighted, and the experimenter is asked
 124
 to give a judgement to which class the marked saccades and its context belongs.
In our study, for each of the read documents approximately 15 sac- cades under consideration are randomly selected. This results in a total of 1407 saccades for the training and testing, which are even- tually presented to two human experts for labeling. Both experts have more than one year of eye tracking experience and rated the set independently. In the end both results are merged, and only matching ratings and considered as valid, while differing ratings were not considered for training. The agreement rate for both ex- perts was 74.9%, resulting in 1055 saccades to which both experts gave the same class of either reading or skimming. From the re- maining 352 saccades, 1.8% (25) were labeled as unknown3 by both experts, 8% (113) were labeled as unknown by at least one expert, the remaining 15.2% (214) had truly differing ratings in terms of that they were labeled as skimming by one expert and reading by the other. Overall 40.6% (428) saccades had an agreed rating of reading, 59.4% (627) an agreed rating of skimming. Ignoring sac- cades that were unknown to one user, both reviewers achieved an agreement rate of 83.1% (κ = 0.65). A more detailed look at the differing saccades revealed that while in some instances the given classification was most probably mistaken by one expert, many of them were borderline cases where, depending on how the context has been interpreted, both ratings appeared to be justifiable4. The saccades that were labeled as unknown by only one expert were in most cases (85%, 96 instances) rated as skimming by the other.
We now extract two subsets from this set of rated saccades . The saccades of six randomly selected users become part of the train- ing set, the saccades of the six remaining users become part of the independent test set, forming two almost equally sized sets that al- lows us to investigate how a trained classifier is likely to perform on unknown users, i.e., generalize. With the training set we next continued to construct a set of feature vectors and train a classifier learning on them.
3 Features & Training
To build a function capable of distinguishing reading from skim- ming when given raw eye tracking in screen coordinates, some pre- processing is required to transform the incoming measurements into a more suitable representation. Given the eye tracker emits a set of raw eye tracking points E′ = e′1, ..., e′cur we first filter and denoise the data using a virtual median filter E = vm(E′), so that
ei = (medianx(e′i−2, ..., e′i+2), mediany(e′i−2, ..., e′i+2))
which allows us to robustly rule out measurement outliers without affecting the filtered position. To the filtered data we then apply a dispersion window based fixation detection algorithm f ixdis
F′ = f1′,...fn′ = fixdis(E)
with 100ms temporal and 25px spacial dimensioning, which trans- forms them into screen fixations. Next we transform each of these screen fixations f ′ into document fixations f by converting them into the document coordinate system based on the document’s win- dow geometry and viewport at the time of each fixation:
F = todoc(F′)
3The raters agreed that the label unknown should only be given when the observed pattern was unusual and likely neither reading nor skimming, or could not clearly be identified in the evaluation tool.
4For example, similar to Figure 3, past contexts of skimming that were just about to enter reading-for-meaning, or skim-alike-outliers in an other- wise reading-like stream.
Figure 3: One of the 1400 slices of fixations that were rated inde- pendently by two eye tracking experts. The red saccade (approxi- mately horizontal in the middle of the screen) had the focus while the saccades before and after were the contributing context. The given example was rated as skimming by both experts, since it was at the end of a vertical skimming pattern, just about to enter fast reading.
It should be noted that in terms of mapped fixations in real world scenarios the stream F usually contains a number of empty fixations f∅ , since not every fixation observed on the screen actually hit the document and could therefore be converted, effectively representing
outliers in the stream of observed document fixations.
The stream of mapped fixations now serves as the basic input on which we build our classifiers and evaluate feature vectors. As we are interested in a real time classification we also define a window function
wa,b(F) := fa, ..., fb ∈ F
which returns a consecutive slice of fixations. Due to the real time demand the effective window size to consider is usually relatively small and we define wa,b to return the empty set ∅ when one of the converted elements was outside the document area, i.e., f∅ ∈ F, effectively signaling that the user was probably visually distracted and no further classification should be done for the window. Hence from here on we can assume for simplicity that each extracted win- dow contains the same defined amount of elements.
Now for the purpose of building feature vectors for machine learn- ing, the outputs of function wa,b are the principal elements we con- sider. In training and evaluation they are constructed around a rated fixation extracted from the gaze stream such that a = x − δ and b = x + δ, where x is some position in the stream, and 2δ the mentioned window size comprising δ saccade elements of past and δ − 1 elements future context. During a realtime classification task x would naturally represent the (δ+1)-latest observed fixation, and
 125
δ would be selected as the smallest value that still gives acceptable results in terms of overall accuracy. In a non-realtime scenario the value should be selected so that it simply maximizes the accuracy. For example, the samples used in training, like depicted in Figure 3, correspond to wx−δ,x+δ slices with δ = 4.
From here on we explore three main variants to construct feature vectors. The first variant, baseline1, is a traditional reading detec- tion based on actual character offsets. It maps the observed fixa- tions to words and measure the progress in the document expressed in character units. The second variant, baseline2, is some sort of a na ̈ıve implementation of the idea not to express saccades in real character progress, but rather define and detect reading in terms of relative saccade patterns. These first two variants serve as our base- lines against which our actual feature generator will be compared. Our main feature generator, the contextual saccade shape, uses the relative saccade patterns, but tries to interpret implicitly encoded semantical information to simplify the actual classification task. Its main idea is to express the shape of w with as few but as expressive parameters as possible.
We consider baseline1 to approximately reflect the state of the art of character-based classification methods [Buscher et al. 2008] [Hyrskykari 2006] in quasi-realtime scenarios, in which no manual correction of gaze data is or can be performed. In addition baseline2 serves as a comparison for the contactual saccade shape algorithm to test if the transformation of gaze data into a more abstract for- mat yields any additional benefit. Very coarsely it mimics some of the vector based approaches reported earlier [Campbell and Maglio 2001] [Kollmorgen and Holmqvist 2007] [Holmqvist et al. 2003] [Holmqvist et al. 2011]5.
Technically all three variants accept an emitted window w and re- turn a feature vector v = ν1 , ..., νm , which usually consists of one or more numeric values νi. These values, along with a label, can then be given to classifiers such as SVMs.
3.1 Character Based Reading Detection
As mentioned reading behavior is traditionally described by the av- erage amount of character positions the reader advances with a sac- cade, measured sequentially from the start of the given block of text. Thus for our baseline1 we implemented a reading detection algorithm similar to [Buscher et al. 2008]. Given the set of fixa- tions in w, each one is magnetically mapped [Hyrskykari 2006] to the closest word in the document and its character position with re- spect to the whole text is extracted. An actual feature vector vb1 is therefore composed of the character offsets o1 , ..., om−1 in the docu- ment
vb1 =o1,...,om−1
between the fixations f1,..., fm of the given window. It should be noted that in contrast to the original algorithm, due to the high amount of vertical saccades in our scenarios, the employed mag- netic match had to be considered on a word level basis and not on a line level. Thus in a setting where skimming mostly consists of horizontal saccades and realtime is of less concern a line based magnetic or sticky approach (again, compare [Hyrskykari 2006]) will probably perform better than baseline1.
3.2 Normalized Saccade Vectors
Since it is our intention to perform a classification mostly based on the gaze data alone and with as little surface knowledge as neces- sary we implemented what could be considered as a na ̈ıve approach
5Also see vld∗ in Section 4.1 and the detailed discussion in Section 5 on comparability.
for baseline2. In it we use the fixation pattern almost directly, only applying a conversion into a relative representation.
Comparing baseline1 against baseline2 serves both the purpose of verifying if not a relative conversion alone might already be suf- ficient and if further processing the raw data into a more compact format was justified, since such a compression could either remove data necessary for a proper classification, or it could denoise the data and improve accuracy.
For the creation of the feature vector in baseline2 we first trans- form the sequence of fixations within w into a sequence of saccades S = s1 , ..., sm−1 . In S , each saccade is represented in polar coordi- nates,si =θi,ri,denotingtheangleandnormalizedlengthinvirtual character units from fi to fi+1.
A virtual character unit, in turn, specifies how many pixels width a single character of the underlying text approximately requires. It is computed by acquiring all word boxes that were covered by the saccades in w in the document area, and the sum of their widths in pixels is divided by the sum of their lengths in character units. For the computation of ri we therefore divide the length of pixels of si by the amount of character units.
An actual feature vector vb2 thus consists of the individual angles θ and normalized lengths r.
vb2 =θ1,r1,...,θn,rn
While this feature vector serves as the baseline2, it also serves as the
foundation for the computation of our shape based representation.
3.3 Contextual Saccade Shape
After working with slices of fixations, their visualizations, and comparing the data labeled as read with the data labeled as skimmed it appears there are two major factors that contribute to this distinc- tion. One can best be described as the average angularity of the saccade and its context, the other is its average forward speed, also compare Figure 4. Hence we decided to use these two attributes as the main features for our classifier.
Angularity h in this respect denotes how bent or vertical the win- dow’s saccades, when considering their concatenated vector shape, on average appear in contrast to a single horizontal line. Obviously a number of subsequent fixations within one line of text should ideally deliver almost no angularity, while vertical perusal of text should yield a high value. For a given window w we compute it as
∑(⃗) h = atan2( |sx| )
where s are all normalized saccades generated from the window as described in the previous section. This means a high horizontality will be expressed in values close to 0, while a mostly vertical pattern yields values up to π .
2
The average forward speed denotes how much progress in the read- ing direction of the text was made within the given window. It reflects how many characters have approximately been read on av- erage during saccades compatible with reading, where the compat- ibility is estimated by angular bounds.
p= {ri ∈w:|θi|< π} 3
In other words the speed p is computed as the average length of all
saccades which point approximately towards the right. The bounds
for this check are deliberately set to the large limits of ± π since the 3
s∈w |sy|
    126
 vcss vb1
δ=2 vb2 vcss
vb1 δ=3 vb2
vcss vb1
δ=4 vb2 vcss
testing δ=3
rel
8192 5.02 .58 10−4 .58 .0025
339.4 .06 2.86 10−4
Read Skim
Vector C γ P R P R Acc.
vb1 .12 10−4 .63
.74 .79 .69 .71 .66 .77 .81 .75 .82 .86 .82 .82 .83 .86 .82 .77 .69 .78 .82 .76 .82 .88 .89 .86 .79 .84 .81 .8 .66 .78 .88 .79 .82 .88 .92 .88 .64 .77 .89 .79
.6 .75 .91 .77
.78 .84 .9 .85
 δ=1 vb2 8192 10−4
.71 .76 .78 .73 .85 .75
  339.4 .012 .8
339.4 .58 .58 8192
.012 .88
10−4 .82 .0025 .83 10−4 .86
   −−.84.82.86.88.86 −−.75.49.69.87.70
vb1
vb2
vcss − − .86 .86
.89 .89 .88 .75 .77 .72
.79 .74 .74
  Figure 4: Graphical outline of the generation of the vcss feature vectors. For a given saccade we consider a window around it and compute the average lengths, expressed in virtual character units, of all saccades that have an approximate angle aligned with the direction of the text and the angle of the sum of all absolute vectors. Both values form the basis for classification.
angularity feature should already sufficiently represent and handle the vertical and diagonal skimming patterns. Now our final set of feature vectors for the contextual saccade shape can be expressed as
vcss =h,p
With this set of features we next perform an evaluation of the algo-
rithms’ performances.
4 Evaluation
As mentioned the recorded and labeled data set was split into a training set including six users, and an independent testing set, in- cluding six other users. Even though generated from different users, both sets had similar distributions of reading to skimming patterns. The training set consists of 56% skimming data, while the testing set contains 51%. For the actual evaluation the feature vectors are generated on both sets. While the training set is used to find the best classifier and parameters the testing set is left untouched until optimal parameters are found and only eventually used to verify the findings.
4.1 Overall Performance Measures
There are two main tasks we need the training set for. The first is to find the best possible classifiers for the given feature vector representation, with the best variants we then perform an analysis of the window size δ. While on the feature level we already se- lected a number of variants, for the classifier we put our focus on a
Table 1: The upper part contains the results on the training set when testing for window sizes (δ) and features with tenfold cross- validation. The best values for C and γ parameters were deter- mined by a grid search, precision (P) and recall (R) values are given for the classes reading and skimming, as well as the overall accuracy. The middle part contains the results of the best trained classifiers on the testing set, i.e., the results a classifier trained with one set of users achieved when exposed to a new set of users. In the lower part we show the results for two feature vectors (consisting of saccade length and fixation duration) similar to those described in related work.
RBF-SVM for which we conducted a grid search over C and γ; a method fairly established in machine learning. Also, since the av- erage reaction time of the final classifier should be below 1000ms on average, this threshold determines the upper bounds for the win- dow size to consider. Assuming an average fixation time of 250ms, δ was therefore limited to 4 in this study6.
Using the labeled training we first do an initial comparison of the methods vb1, vb2 and vcss. The upper part of Table 1 lists the results of the classification runs on the training data as well as the best found parameters for the RBF-SVM, the precision and recall values for the reading and skimming class as well as the overall accuracy. It can be seen that in all cases the accuracy delivered by the vcss feature set is superior to the character based feature vector baseline1 and the raw feature set baseline2, the same holds for the precision and recall values of both classes.
We also investigate the influence of the window size on the overall classification accuracy, compare Figure 5. For all three investigated feature types the accuracy increases from δ = 1, which equals approximately 250ms reaction time after the fixation beginning the saccade under consideration, up to δ = 3, approximately 750ms reaction time, beyond which the accuracy drops again. This means on the training set, that if after three more fixations a classification is performed on a saccade under consideration, approximately nine out of ten of the label emitted by vcss-SVM classifier are correct and also approximately in nine out of ten true instances of skimming (eight out of ten for reading) the classifier manages to detect these.
Based on these findings we select δ = 3 as the preferred window size and for each feature set the best trained classifier is chosen
6It should be noted that the actual reaction time can be significantly slower or faster, since the fixations times are merely assumed averages and can vary from less than 100ms to several hundreds of milliseconds (com- pare [Just and Carpenter 1980]), thus the true reaction times could range from below 300ms to over 3000ms.
vld3 1.63 4 .68 .66 vld4 1.37 3.07 .69 .74
 127
  1.0
                   vb1 vb2 vcss rnd
                 0.8
0.6
0.4
0.2
0.0 012345
Window Size (δ)
 Figure 5: Influence of the window size on the best classification accuracy for the given feature type. The overall optimum was pro- duced by vcss at δ = 3 with an overall accuracy of 88%. Consider- ing a window size of 4 already decreases the performance slightly. Each step of window size changes the reaction time approximately 250ms on average. The absolute baseline is set by rnd, which equals the guessing level base distribution of reading to skimming on our training set.
and evaluated on the test set with containing the six other users, compare the middle part of Table 1. For the feature set vb1 an RBF- SVM with C = 2.86 and γ = 10−4 is the selected candidate, and for the vb2 and vcss features the ones with the values C = 339.4, γ = 0.012 respectively. Using these classifiers it turns out that while the performance of the classifier trained on the vcss feature set achieves the same overall accuracy of 0.88, the performance of the character based approach increases significantly, while the performance of the classifier trained on the raw feature set drops down to 0.7; we will discuss these findings in the conclusion.
Eventually we also compute two variants of a feature type we find in the related work. The features vld3 and vld4 (compare the lower part of Table 1) are similar to the feature Q′ described in [Kollmorgen and Holmqvist 2007]. The vector vld4 is built using a window of four fixations around the saccade under consideration. It consists of the normalized lengths ri, as well as the fixation durations of all fixations involved. In contrast to Q′ we did not take into account blink events. The vector vld3 is similar to vld4, with the exception that it has a window size of 3, the optimal value we found for our other feature types so far. Both features are trained using grid search on a RBF-SVM in 10-fold cross validation. We can see the overall accuracy values with 0.72 and 0.74 for both vld types are somewhat low compared to the baselines. We will also come back to this in the conclusion.
4.2 Linear Classification and Model Adjustments
Since we now established somewhat of a peak classification accu- racy at which the separability of both classes was highest our last step is to simplify the classification method itself. Considering that the feature set vcss yields the best results, visualizing it reveals that both classes (compare Figure 6), while being partially overlapping, apparently have a shape simple enough for linear classification.
In contrast to a SVM a linear model with similar performance mea- sures has several advantages, amongst them that it is easier to im- plement and more economically to execute, especially on small scale devices like tablets.
So taking the training set we construct a linear classifier and evalu- ate it with tenfold cross-validation, which gives an overall accuracy of 85.5% and precision/recall values of 0.81 / 0.86 for reading and
0.90 / 0.85 for skimming respectively. Evaluating the same classi- fier on the six users of the testing set yields an overall accuracy of 86.1% and almost the same precision/recall values. The final model is generated as
class = −2.97 + 5.36h + 0.17p
where class yields values greater than 0 in case the pattern resem- bles skimming, and values smaller than 0 in case of reading. Since class reflects the distance of a point from the classification bound- ary its absolute value of also reflects, to some extent, the certainty of the classification result. The closer the value is to 0 the more likely it could fall into the other class. Another very practical ad- vantage of the linear classification is that the space of vvss becomes, in contrast to SVMs, predictably partitioned, which allows for an adjustment of the speed axis for the purpose of individualization. While on sufficiently large horizontal text a vertical pattern is most certainly an indicator against reading, the specific threshold when to classify a horizontal saccade length still as reading is dependent on the context and task for which this classification is being per- formed7. With this in mind we introduce an additional parameter β, which is being used to adjust the speed values prior to their classi- fication, generating a derived speed:
p′ = β p
The parameter can best be interpreted as a dilatation factor on the forwards-speed axis, effectively shifting data points with low angu- larity towards one side of the classification boundary. With it the influence of the horizontal saccades on the classification result can be adjusted, while at the same time the points with a high angularity remain mostly unaffected.
 1.2
1.0
0.8
0.6
0.4
0.2
0.0
0 5 10 15 20 25 30 35
Forward Speed p [vcu]
              Read Skimmed Boundary
                           Figure 6: Visualization of our ground truth when expressed as an- gularity and forward speed for the window size of δ = 3 and the trained linear classification boundary. Adjusting β will shift the points to the left or right and the less angularity there is involved the more impact the parameter has.
5 Conclusion
Starting from the assumption that gaze patterns alone should con- tain most information needed to recognize and discriminate be- tween reading and skimming patterns we first performed a data ac- quisition experiment. We invited a number of participants to read and comprehend a number of passages, embedded in mostly un- related text, to elicit various forms of reading-for-comprehension
7In addition to other factor like for example the rate of non-newline re- gressions.
 128
Angularity h [deg]
Accuracy
and searching patterns. We selected saccades as the base unit of consideration and two eye tracking experts labeled approximately 1400 of them independently into the classes reading and skimming. This resulted in 1055 saccades that we considered as ground truth for the construction, training and testing of an automated classifier. We investigated the expressiveness of three different feature types, a character-offset based feature type, somewhat in line to currently established classification methods and psychological knowledge; a raw vector feature type, only considering angles and normalized lengths; and a feature set we call contextual saccade shape, which condenses the overall shape of a saccade and its adjacencies into the two parameters forward speed and angularity. For all three feature sets we performed SVM grid searches to find the best parameters and window sizes and eventually presented a linear classifier that delivers a comparable performance.
Based on our findings there are a number of conclusions to draw. Comparing the three investigated vector representations, the con- textual saccade shape appears to be most robust in terms of gen- eralization and accuracy. On the training set it gives significantly better results than both other methods, even on small window sizes. On the testing set this observation also holds.
While the character based approach comes close on testing, its higher volatility over different window sizes, and also compared between both sets, are an indicator of a latent instability. However, it is unclear if this instability emerges from a principal problem of overlapping classes in the feature space, the variations in a-priori class distributions or an insufficient number of training examples for the given feature size. At a window size of δ = 3 the vector vb1 consists of 6 attributes while vcss of only 3.
The impact of the window size on the overall accuracy is notice- able for all classifiers and in our study a context of three saccade into both direction gives the best results. Smaller windows are most likely to cut off parts of the actual perusal process (e.g., when considering only two saccades a newline regression might be hard to distinguish from a scanning pattern), while larger windows are likely to introduce too much unrelated noise. In case of both base- lines an increased window size inherently leads to bigger feature vectors, which could probably be compensated by an simultaneous increase of training data, while in case of the overall saccade shape the expressiveness of h and p becomes diluted. All in all however, a window size of 3, thus 6 saccades or approximately 2 seconds of gaze data appears to be a reasonable size to distinguish reading from skimming.
Comparing our approach to existing work we outline a straightfor- ward way to obtain and evaluate reading and skimming data on text. Presented to and labeled by two independent experts we generate ground truth based on fixations and saccades. The data serves as the foundation for an comparable assessment of the precision and recall of different algorithms. With the design of our baselines and implementation of vld∗ we also tried provide at least a basic level of comparability to previous approaches. There are, however, certain, limits. Of all the papers addressing an automated reading detec- tion we are aware of, only few target explicitly the discrimination of reading-for-meaning from skimming (such as fast perusal while still maintaining a mostly line-wise pattern) and can thus be com- pared directly.
The usage of character based offsets, presented in [Buscher et al. 2008], inspired our design of baseline1. This work, however, ex- pressly targets a line-based non-realtime detection and was not eval- uated against labeled ground truth.
The length-blink-duration approach presented by [Kollmorgen and Holmqvist 2007] focusses on detecting reading vs. non-reading, where the latter consisted of activities such as “editing periods [where a writer’s] gaze may rest on text ROIs, although she is not reading”. It was trained and evaluated on a set of apparently one hour of labeled gaze data. The authors report an accuracy of 0.86
(precision / recall 0.8) for realtime classification on their validation set. They also employed a neural network for classification and training which achieved precision and recall values of 0.86 for the reading class with reported training times of several hours. As men- tioned earlier we implemented their feature set Q′ as vld4 and vld3, with the exception of not taking into account blink events. On our reading and skimming data this set performed with an overall accu- racy of 0.74 and precision / recall values for reading of 0.69 / 0.74 respectively at a window size of 4.
Comparing these results one might be tempted to draw another con- clusion: The main difference between vb2 and vld∗ is the move from saccade angles to fixation durations. Hence, it appears that to dis- tinguish reading from skimming the saccade angles hold more pre- dictive power than the fixation duration. While in principle we con- sider this statement to hold, one should keep in mind that for the generation of ground truth the eye tracking experts did not consider the fixation duration. This might, in turn, favor a feature set similar to those observed by the experts.
A very comprehensive report on the classification into three cat- egories was published by [Simola et al. 2008]. In an experiment the users had to perform word search, question answering and a search / reading for the most interesting items in lists. While the word search was similar to skimming, the latter two categories ap- parently required intense amounts of reading for meaning. For their most predictive classifier the authors used a feature vector combi- nation of fixation duration, saccade length and direction, as well as a binary regression flag. On their testing data a trained HMM was able to achieve an accuracy of 0.6, however on a much harder three- class problem with two similar categories. Treating their class W as skimming and A + I as reading instead (see Table 2 in [Simola et al. 2008]) we can compute an overall accuracy of 0.8 based on their reported data.
The reading detector described in [Holmqvist et al. 2003] and [Holmqvist et al. 2011] appears to consider saccade windows with a context of size 1 and is calibrated with samples from each recording to classify. It aims at detecting reading in contrast to non-reading, i.e., mostly scanning in their case. However, no classification re- sults on labeled ground truth are being reported.
The approach described by [Campbell and Maglio 2001] aims at distinguishing reading from icon search. In contrast to all other ap- proaches it does not use fixation and saccade data, but aggregated data over 100ms segments for both x- and y-directions. The relative movements are then evaluated according to some length-criterion and rated. Reading is considered once a certain threshold of aggre- gated points are reached. In an experiment where four participants either read a text carefully or performed a search for the Windows Excel document icon achieved approximately 0.90 accuracy. The reported recognition speed of their algorithm ranged “from 200 to 3000ms, with an average of just over one second (1106ms)”.
Considering our achieved accuracy levels, the experts’ agreement and surveying the reported publications we got the impression that we came close to a realistic peak classification accuracy on our la- beled data, similar to [Kollmorgen and Holmqvist 2007]. While an overall accuracy in the ‘upper nineties’ would, for ideas such a QuickSkim, surely be advantageous, we have to consider that both experts also only agreed to 83% on unproblematic data. In light of a limited saccade context and finite classification resources some patterns are apparently just too ambiguous. On the other hand this finding is hardly surprising. The transition from slow skimming to fast reading is a soft one. Both classes do not exist as well de- fined, crisp entities, but instead as regions around a zone of con- fluence. Hence in any classification scenario one should not only rely on the computed classification output, but also on the classi- fiers’ confidence—in our case the distance from the classification boundary.
In overall we present a classification method that can be trained to
129
distinguish both patterns with high a precision, recall and overall accuracy. The low variance of the the feature vector we propose, in contrast to both baselines, is a good indicator that the algorithm should be reasonably robust, even considering new users. By vary- ing the window size it allows for a certain tradeoff between latency and overall accuracy. It is furthermore straightforward to imple- ment and requires only a few lines of code, making it a suitable candidate for small scale devices or embedded hardware. The clear notions of its features simplifies its understanding. With the intro- duction of β it can also be adjusted in its sensitivity regarding its assignment of inconclusive gaze data to ’fast reading’ and ’slow skimming’.
Eventually we would like to point out that our method requires little knowledge about the underlying text. Ideally it should be provided with information whether in the region was text after all8 and the approximate character size. However, it can also be a suitable can- didate in scenarios where such information is unavailable or expen- sive to generate. Possible examples could be mobile eye tracking applications, where a textual stimulus is read in the real world and only recorded by a scene camera. A reading result might then yield candidates where to perform an OCR analysis.
6 Outlook
In our scenario we are able to train a classifier on one set of users that is capable of generalizing to the reading patterns of other users. Being able to do so was not unexpected since the general pattern of reading for meaning on sufficiently large text in a given language is, to a large degree, influenced by the general layout and grammar. The main question was rather to what extent such a generalization would be possible, especially when disregarding most of the tex- tual stimulus. However, when these preconditions are not given anymore, the overall patterns evoked will look significantly differ- ent, examples include small columns of text, bullet point lists or any other scenario where a mostly horizontal reading pattern has no space to evolve. Investigating a feature representation and clas- sification method that can deal with or consider these layout details will expand the field of use of such a real time classifier.
Also we have to acknowledge that the our set of users was, with regard to the general public, obviously not representative, consid- ering that the majority of our users were male university students. Therefore two interesting steps would be to analyze how our ac- tually reported linear classifier performs on a completely different user group, and how a newly trained classifier including these or both groups would perform. First results from demo applications in which we integrated the presented linear classifier and used it on a set of different texts, font sizes (e.g., 11pt) and languages (English instead of German) indicate that even in these cases its generaliza- tion to different users does work.
Lastly we think that having a classifier capable of robustly distin- guishing reading from skimming patterns on text in real time facil- itates many new interactive and reading-context enriched end user application. They can use the knowledge about what has been read and what currently is being read to contextualize personalized infor- mation processing and change the interface according to the user’s current state of mind.
7 Acknowledgements
We would like to thank Horst Bunke and Seiichi Uchida for their valueable remarks on SVM classification. This work was supported
8For example, not to misinterpret a skimming result when the user just performed GUI tasks such as icon search instead. In fact for most desktop scenarios, when there was no text nearby, the algorithm does not even need to run.
by the German Federal Ministry of Education, Science, Research and Technology (bmb+f), (project Perspecting, grant 01IW08002).
References
Biedert, R., Buscher, G., and Dengel, A. 2010. The eyeBook - Us- ing eye tracking to enhance the reading experience. Informatik- Spektrum 33, 3 (June), 272–281.
Biedert, R., Buscher, G., Lottermann, T., Schwarz, S., Mo ̈ller, M., and Dengel, A. 2010. The Text 2.0 Framework. Workshop on Eye Gaze in Intelligent Human Machine Interaction.
Biedert, R., Buscher, G., Schwarz, S., Hees, J., and Dengel, A. 2010. Text 2.0. CHI EA ’10: Proceedings of the 28th of the international conference extended abstracts on Human factors in computing systems (Apr.), 4003–4008.
Buscher, G., Dengel, A., and Elst, L. v. 2008. Eye movements as implicit relevance feedback. CHI ’08 extended abstracts on Human factors in computing systems, 2991–2996.
Buscher, G. 2010. Attention-Based Information Retrieval. PhD thesis, University Kaiserslautern, Kaiserslautern.
Campbell, C. S., and Maglio, P. P. 2001. A robust algorithm for reading detection. In Proceedings of the 2001 workshop on Per- ceptive user interfaces, New York, NY, USA, 1–7.
Chi, E. H., Hong, L., Gumbrecht, M., and Card, S. K. 2005. Scen- tHighlights: highlighting conceptually-related sentences during reading. Proceedings of the 10th international conference on Intelligent user interfaces, 274.
Holmqvist, K., Holsanova, J., and Barthelson, M. 2003. Reading or scanning? A study of newspaper and net paper reading. In The Mind’s Eye: Cognitive and Applied Aspects of Eye Movement Research. Elsevier Science Ltd.
Holmqvist, K., Nystrom, M., Andersson, R., Dewhurst, R., Jaro- dzka, H., and van de Weijer, J. 2011. Eye Tracking. A Compre- hensive Guide to Methods and Measures. Oxford Univ Pr, Nov.
Hyrskykari, A. 2006. Eyes in Attentive Interfaces: Experiences from Creating iDict, a Gaze-Aware Reading Aid. acta.uta.fi.
Just, M. A., and Carpenter, P. A. 1980. A theory of reading: From eye fixations to comprehension. Psychological Review 87, 329–354.
Kollmorgen, S., and Holmqvist, K. 2007. Automatically detecting reading in eye tracking data. Lund University Cognitive Studies.
Rayner, K. 1998. Eye movements in reading and information pro- cessing: 20 years of research. Psychological Bulletin 124, 3, 372–422.
Rayner, K. 1998. Eye movements in reading and information pro- cessing: 20 years of research. Psychological Bulletin.
Reichle, E., Pollatsek, A., Fisher, D., and Rayner, K. 1998. To- ward a model of eye movement control in reading. PSYCHO- LOGICAL REVIEW-NEW YORK-.
Simola, J., Saloja ̈rvi, J., and Kojo, I. 2008. Using hidden Markov model to uncover processing states from eye movements in in- formation search tasks. Cognitive Systems Research 9, 4 (Oct.), 237–251.
Underwood, N. 1985. Perceptual span for letter distinctions during reading. Reading Research Quarterly, 20, 153–162.
Leveraging the Crowd CSCW 2015, March 14-18, 2015, Vancouver, BC, Canada
Social Eye Tracking: Gaze Recall with Online Crowds
Shiwei Chenga,b, Zhiqiang Sun aZhejiang University of Technology swc@zjut.edu.cn,
sunzhiqiang89@163.com
ABSTRACT
Eye tracking is a compelling tool for revealing people’s spatial-temporal distribution of visual attention. But quality eye tracking hardware is expensive and can only be used with one person at a time. Further, webcam eye tracking systems have significant limitations on head movement and lighting conditions that result in significant data loss and inaccuracies. To address these drawbacks, we introduce a new approach that harnesses the crowd to understand allocation of visual attention. In our approach, crowdsourcing participants use mouse clicks to self-report the positions and trajectory for the following valuable eye tracking measures: first gaze, last gaze and all gazes. We validate our crowdsourcing approach with a user study, which demonstrated good accuracy when compared to a real eye tracker. We then deployed our prototype, GazeCrowd, in a crowdsourcing setting, and showed that it accurately generated gaze heatmaps and trajectory maps. Such an approach will allow designers to evaluate and refine their visual design without requiring the use of limited/expensive eye trackers.
Author Keywords
visual attention; fixation; crowdsourcing.
ACM Classification Keywords
H.5.m. Information interfaces and presentation (e.g., HCI): Miscellaneous.
INTRODUCTION
Eye tracking can measure and record gaze positions and movements of an individual. More researchers are using this technology to obtain insights into users’ behavioral and cognitive processes used in problem solving [1], such as to explore usability problems in user interfaces, and improve a user’s efficiency in searching, navigating and manipulating. However, traditional eye tracking hardware is not suitable for use in all contexts. It has three main shortcomings [28]: 1) most commercial eye trackers are too expensive for anyone other than researchers and practitioners to own; 2) a limited number of users from narrow demographic groups can participate in eye tracking experiments, as they must be co-located with the eye tracker, and only one user can use it
Xiaojuan Ma
Noah’s Ark Lab xiaojuan.ma@gmail.com
Jodi Forlizzi, Scott Hudson, Anind Dey
bCarnegie Mellon University {forlizzi, scott.hudson, anind}@ cs.cmu.edu
at a time; 3) cheaper web-camera based eye trackers greatly constrain the location and motion of a user’s head, are not robust to different lighting, have low sample rates, leading to significant data loss and inaccuracies [6].
We propose a device-independent and easily accessible method to complement or even replace current eye trackers for some measurements. In particular, our method leverages crowdsourcing in which multiple users self-report the regions (or gaze points) of stimuli that they focus on visually. These results can be aggregated to generate an overall understanding of how people allocate their visual attention on these stimuli. This approach can be executed cheaply, as it requires no expensive equipment beyond a computer and an Internet connection, and can be used by many users in parallel, with varying demographics.
Our approach collects a range of eye tracking data: location of first gaze, location of last gaze and location and order of all gazes. These eye movement measures are important for understanding the spatial-temporal distribution of visual attention. For example, first gaze usually reveals the most salient part of a visual stimulus, and can be used to select between multiple graphical user interface (GUI) design solutions, e.g., to determine whether an important menu can be easily found. Last gaze can indicate objects that have low visual saliency, e.g., an icon located in the bottom-left corner of an interface may be the last item seen by a user. The sequence of all gazes can help understand how visual attention is allocated over time. By having users recall (i.e., self-report) their gaze trajectory in the presence or absence of a given task, we can also identify the use of different kinds of visual attention mechanisms, such as bottom-up and top-down [7], and identify problems with UI designs.
Our approach displays a visual stimulus and asks users to recall which parts of it they looked at. To validate the accuracy of these self-reports, we conducted a laboratory experiment (user study 1) where we compared the recall results of first gaze, last gaze, and all gazes to those obtained from an eye tracker, and found high consistency between each individual’s self-reports and actual gaze data. User study 1 validated that an individual’s recall data for a user was similar to his/her own eye-tracking data. We conducted a second experiment in a crowdsourcing setting (user study 2), to then validate that aggregated crowd data is similar to the aggregated eye-tracking data from study 1. We obtained gaze recall data and aggregated crowdsourced results to generate gaze heatmaps and trajectory maps comparable to eye trackers and models of visual attention.
 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org.
CSCW '15, March 14 - 18, 2015, Vancouver, BC, Canada. Copyright 2015 ACM 978-1-4503-2922-4/15/03...$15.00. http://dx.doi.org/10.1145/2675133.2675249
454
Leveraging the Crowd CSCW 2015, March 14-18, 2015, Vancouver, BC, Canada
User study 2 validated that the recall approach also had good performance on a crowdsouricng platform. Finally we discuss how to reduce the limitations of our approach and apply it to guide visual design or design evaluation tasks.
Our contributions of this paper include:
  Validated the use of structured participant recall for
recording gaze data (first gaze, last gaze, all gazes) with accuracy comparable to infra-red web camera-based eye trackers. This gaze recall data can be captured in task- oriented and browsing situations, leveraging both bottom- up and top-down visual cognition mechanisms.
  Developed a prototype crowdsourcing system for collecting aggregate measures of visual attention. Our approach is efficient, inexpensive, and induces low workload in collecting eye motion data in a real crowdsourcing environment. Most importantly, it produces heatmaps and trajectory maps, comparable to eye tracking systems and models of visual attention.
RELATED WORK
Traditional commercial eye trackers are expensive (usually US $10,000 to $30,000 each) and allow only a single user at a time. These issues make it difficult to acquire gaze data from a large number of users with high efficiency. Hence, different methods were proposed to address these issues.
Webcam-based eye tracking. Compared to high quality, commercial eye trackers, webcam-based eye tracking systems use much cheaper and simpler cameras built into computers to detect users’ gaze locations. For example, the ITU Gaze Tracker (gazegroup.org/downloads) performs remote eye tracking using a web camera with a modified camera lens and two infrared lights (total cost is US$100). It has a sampling rate of 30 Hz and a mean error of 59 pixels, approximately twice the error of the Tobii T60 binocular eye tracker (sampling rate of 60 Hz, mean error of 31 pixels) [20]. There are also a number of open source eye trackers that use unmodified webcams, such as Opengazer (inference.phy.cam.ac.uk/opengazer/), but they tend to have even worse errors.
Developers have improved these open source eye trackers and deployed them as services for conducting remote user study service, such as GazeHawk (gazehawk.com) and EyeTrackShop (eyetrackshop.com), for evaluating advertisements, website designs, etc. Using these systems, gaze data can be recorded on participants’ computers and then uploaded to a study server, for the generation of analytic reports and visualizations. However, these systems still have some limitations: 1) low data sampling frequency and accuracy. A typical commercial eye tracker has accuracy of less than one degree of visual angle and 60 Hz sampling frequency at least; in contrast, due to imperfect lighting conditions, poor quality webcams, on-screen distractions, an infrared webcam produces results with accuracies of 1.5 to 2 degrees of visual angle at best, but usually with less accuracy, such as 2 to 5 degrees of visual angle and use at most a sampling frequency of 30 Hz [20];
2) low tracking robustness. In addition, participant head movement and a lack of robustness to imperfect lighting conditions, leads to a greater proportion of participants whose data will be lost [6]; 3) device dependent. For example, the ITU Gaze Tracker requires a special camera lens and infrared illuminations; 4) hard to use. For example, the technical sophistication required to compile a package like Opengazer with a number of external dependencies and then configure the system, is quite high; 5) users’ limitations. Ordinary web-cam based eye trackers do not perform well if users wear glasses.
Visual attention model-based eye tracking. Visual attention is a process that enables humans to quickly select the most relevant regions of a scene. Visual attention models usually take into account a bottom-up cognition mechanism that highlights regions that are different from their surroundings. They are used to predict eye movement data (e.g., fixations), and to generate salience maps. Although these models can accurately predict fixations when compared to real eye tracking data and can be executed very quickly [8], most of them do not focus on the top-down cognitive process that is driven by task. This results in saliency maps that are quite different from actual eye movement when people observe images or videos with interactive, complex, and dynamic tasks [7].
Mouse movement-based eye tracking. There is some coordination between eye movements and mouse movements. Chen et al. [9] asked users to browse web pages, and used an eye tracker to record their gaze and mouse movement data at the same time. They found that of the regions that a mouse cursor visited, 84% of them were also visited by gaze, and for the regions that the eye gaze did not visit, 88% of them were also not visited by the mouse cursor. However, just five participants were involved in this study, and they were given no specific tasks when browsing the web pages. Most users moved their mouse according to their eye gaze, only when they had specific tasks, such as clicking highly relevant target results on web search result pages [27].
Another similar method, restricted focus viewer (RFV) [5], blurred a test image, but allowed users to see a small region of the image in focus, with the region being dynamic based on the position of the mouse. By assuming that users only focus on the unblurred region of the stimulus the tracked mouse movement can be considered as equivalent to gaze movement. Although the RFV was introduced as a complementary tool for eye tracking, it could not track users’ visual attention well when solving complex problems like identifying as many bugs in a program as possible [4]. In these situations, the visual attention data recorded using the RFV and eye tracker differed significantly. Moreover, the blurring of the stimuli changed users’ natural visual attention strategies and increased mental and physical workload. Jones and Mewhort [21] suggested that the size and characteristics of the view in focus affected the
455
Leveraging the Crowd CSCW 2015, March 14-18, 2015, Vancouver, BC, Canada
information search strategies, and the users who relied on peripheral data would have to exert more effort to search for information using the mouse.
Social computing -based eye tracking. As mentioned above, these methods aim to be a complementary or even alternative tool for real eye trackers, but they have not yet achieved high accuracy, efficiency and convenience. Recently, a social computing-based eye tracking method has been proposed. It applies a human computation mechanism to involve crowds in completing specific and simple tasks, and aggregates the crowd’s interaction data to indicate eye movement. For example, researchers developed an online game system where users are shown a small part of an image, and are asked to click where it belongs in the original full image [2]. The system considered the most well mapped image regions to be the most salient parts, and generated a saliency map. An evaluation revealed that the saliency map contained the most essential visual elements and features of the image. However, elements that people pay attention to such as human faces or traffic signs, are not always chosen in saliency maps based on traditional visual attention models. In addition, unlike with real eye trackers, this method cannot capture gaze trajectories (gaze sequences over time).
Rudoy et al. [28] proposed a different crowdsourcing method, which acquires gaze data using self-reports of what users were looking at in video clips. Users viewed an input video for a few seconds, and immediately after the video ended, were shown a labeled grid where the video was displayed. The grid was displayed for a short period of time and disappeared afterwards, at which point, users were asked to enter the grid label corresponding to the region of the video they had seen most clearly. These self-reports were shown to be comparable to eye-tracking data. However, this kind of approach still has three limitations:
(1) This method only asked each participant to recall the last gaze on the screen and did not track the first gaze, and all the gazes and their trajectories over the whole period of time when the stimuli was displayed. The first gaze is more important than last gaze, as it typically reveals the most salient region, and the remaining gazes are chosen to maximize information gain, given the first gaze [13] [18]. In addition, the information about all gazes, such as how many there are, their positions, and the overall trajectory (sequences of gaze points over time) is also important for us to understand spatial-temporal distributions of visual attention. For example, gaze trajectories (also called scan paths) may indicate higher cognitive strategies (e.g., visual search strategies for UIs) and states [24] [16].
(2) Participants were just asked to recall gaze positions based on the bottom-up visual mechanism, but use of top- down mechanisms were not considered. Bottom-up is mainly based on characteristics of visual scene (stimulus- driven) [25], and is involuntary [7]. Areas of interest that attract our visual attention in a bottom-up manner must be
sufficiently distinctive with respect to surrounding features. In contrast, top-down (goal-driven or task-driven) is voluntary, determined by cognitive phenomena like knowledge, expectations, tasks and goals [7] [17]. So eye movements will differ considerably for the same scene if users were or were not assigned a task, or if assigned different tasks [29]. The bottom-up mechanism can only explain a small fraction of eye movements since the majority of them are driven by the top-down mechanisms [14]. Hence, supporting the collection of gaze information based on the bottom-up mechanisms alone is not sufficient.
(3) The mechanism for specifying recalled gaze can be made more efficient. Participants were asked to enter a label consisting of several characters for reporting their gaze locations, and this can increase both physical and mental workload and reduce efficiency, unnecessarily.
We propose a method to facilitate gaze recall with online crowds that builds on this past work of Rudoy et al. [28]. We address the shortcomings discussed above: 1) Besides recording the last gaze, we also record the first gaze, all gazes and their trajectories; 2) In addition to simply browsing images (bottom-up), our approach also applies to task-driven recall (top-down); 3) Instead of having to recall multi-character labels, participants are asked to only click on regions of interest when recalling gaze. The scope of our method is broader, making the results more applicable across different application domains and scenarios. Furthermore, we performed a detailed analysis of the accuracy of our approach, comparing it to eye tracking data. Finally we validated our method with a prototype system deployed on Amazon Mechanical Turk. We now describe our approach in detail and the evaluation of our approach.
USER STUDY 1: GAZE RECALL EVALUATION
To determine if recall of what a participant views in an image can approximate gaze data, we used an eye tracker to collect users’ gaze data as ground truth, and recorded users’ recalled gaze points (mouse clicks) simultaneously. Then we evaluated users’ recall performance for their first gaze, last gaze, all gazes, and trajectory of gazes compared with the ground truth.
Participants & Apparatus
We recruited 20 participants (8 female and 12 male) from a local participant pool. They were aged between 18 and 61 years (M=28.6, SD=10.7), and none of them were color- blind or had any visual impairments. We used the SMITM iViewX RED remote eye tracker to record gaze data. The eye tracker has two infrared cameras and the gaze sampling rate was configured to 250Hz. Its operating distance is 50cm to 70cm, and it supports free head movement (40cm × 20cm at 70cm distance to screen). In this context, the gaze data sampling accuracy is 0.5 degrees of visual angle. It works with most glasses and contact lenses. Study stimuli were shown to participants on a 21-inch LCD screen with a resolution of 1680×1050.
456
Leveraging the Crowd CSCW 2015, March 14-18, 2015, Vancouver, BC, Canada
Procedure
We first gave the participants a brief introduction to our experiment, and then asked them to sit in front of the monitor about 50cm (30° of visual angle) away from their eyes. We performed an 8-point calibration for each participant with the eye tracker. Our study had 4 different sessions: training session (5 images), first gaze recall session (16 images), last gaze recall session (8 images), all gazes recall session (with specific tasks (3 images) and without specific tasks (14 images)). Without specific tasks, a bottom-up, stimulus-driven approach is used. In contrast, to explore task-driven (top-down) gaze recall, we gave participants two kinds of tasks that have high complexity: detection tasks and problem-solving tasks [12]. For detection tasks, we asked participants to locate predefined objects embedded in the stimuli; for problem-solving tasks, we ask participants to answer one cognitive-ability question, such as reporting the number of people in an image. The study lasted between 30 to 50 minutes for each participant.
...
Grid image Test image
Mask image Instruction
Figure 1. Sequence of stimuli in each experiment session.
A recall session is shown in Figure 1. During each session, there are four different types of frames displayed on the screen: instruction, mask image, test image and grid image.
  Instruction: describes the task and how to recall the gaze
and click the grids. For example, for first gaze, the instruction provided was “Please look at the test image; then click on the one grid cell (in the grid image) that best matches what you first looked at in the test image”.
  Mask image: a blank gray image displayed for 1 second. It acts as a visual mask, allowing any high contrast stimuli from the previous screen to fade, reducing the persistence of vision [15]. In addition, it can also reduce visual fatigue.
  Test image: image resolution was 1152×864, and displayed for a given duration (e.g., 250 milliseconds). We used a total of 20 different test images collected from Internet, including abstract salient scenes (e.g., one yellow dot among green dots), human faces, sports activities, flowers, posters and advertisements, etc.
  Grid image: a blank white image with a 9×9 grid placed over it. Each grid cell was marked with characters, which had no specific meaning but could help participants to discriminate different cells easily. We provide the grid as a visual aid, to help participants locate their gaze positions, but record the actual pixel location of their clicks. Participants recalled their gaze(s) from the
previous displayed test image and clicked one (or more) grid cells to indicate their gaze based on the instructions. The size of the grid image was equivalent to the test image, which ensured the grid covered the test image.
All experimental sessions followed a similar sequence, and we only varied the duration for displaying the test image. For the first gaze recall session, the test image duration was set to 250ms, 500ms and 1000ms. After a stimulus is attended to for a short while (e.g., 100-1000ms), inhibition of return (IOR) [22] promotes exploration of new, previously unattended regions in the scene by preventing attention from returning to already-attended regions. So if we display the test image for a long period of time, the participant will shift her gaze to other regions of the stimulus. Thus, we displayed the test image no longer than 1000ms to capture first gaze. For the last gaze recall session, the test image duration time was 1000 and 2000ms. Here, we leveraged the inhibition of return phenomenon in our favor. By showing the test image for longer periods, users will shift their gaze beyond their initial gaze point. For the all gaze recall sessions without a task, the test image duration time was also 1000 and 2000ms, so participants had enough time to browse more of the test image and recall multiple gazes. For the all gaze recall session with a task, the duration was unlimited, so a participant could close the image manually when she finished the task (but the completion time was M=7.3 seconds, SD=7.6).
Additionally, before each experimental session, we displayed 2 or 3 images (not repeated in the experimental sessions themselves) to train participants how to perform different kinds of gaze recall. We provided additional instructions to the participants who did not understand how to recall gazes or made mistakes during recall (e.g., clicking two grid cells when recalling only the first).
The nature of our recall-based experiments allowed users to explicitly recall their gazes. To ensure that the recall did not impact how they allocated visual attention, we incorporated an additional data collection during the first training session. We displayed test images that would later be seen during the experimental settings. Participants were provided with no instructions other than to browse the images (i.e., no recall). The gaze data (e.g., fixation count) was treated as ground truth and compared to the later browsing of these images with explicit recall. We found that there was no significant difference in the fixation count for the “no recall” and “recall” conditions, for example, for one test image with natural browsing and the first gaze recall, a paired samples t-test showed no significant differences (t(19)=-.665, p=0.257). These results suggest that our explicit recall method did not impact participants’ visual attention.
Results
Results of first gaze recall
We measured the geometrical distances between each participant’s first gaze points (eye tracking fixations) and her recalled gaze points (mouse clicks) to evaluate the
         457
Leveraging the Crowd CSCW 2015, March 14-18, 2015, Vancouver, BC, Canada
accuracy of gaze point recall. The smaller the distance, the more accurate the recall. Across all test images, the distance ranged from 6 pixels to 393 pixels (M=116, SD=83). A 100- pixel distance equates to a visual angle error of ~3 degrees.
The distribution of distances revealed that distances shorter than 20 pixels (1.2% of the screen’s width) accounted for only 3.4% of the distances, corresponding to 0.5 degrees (equivalent to the accuracy of most commercial eye trackers, e.g., SMITM iViewX RED) of visual angle (with the viewing distance between participant’s eyes and screen set to 50cm). Distances corresponding to 1, 2, 3, 4, and 5 degrees accounted for 15.9%, 32.5%, 54.1%, 62.8% and 73.4%, respectively. The most effective webcam eye trackers have accuracies of 1.5 to 2 degrees (but often 3 to 5). However, these suffer from requiring modification to the camera to be infrared-based, and data loss due to imperfect lighting, webcam quality, and head motion. While our accuracy for first gaze recall is comparable to the infra-red webcam systems, it does not have these data loss issues.
For each test image, a one-way within subjects (or repeated measures) ANOVA was conducted to compare the effect of test image display duration on distance for the 250ms, 500ms and 1000ms conditions. There were significant effects of display duration for all the test images. For example, in one test image that had an abstract scene, there was a significant effect of display duration, F (2, 18) = 10.68, p =.001. Three paired samples t-tests were used to make post-hoc comparisons between conditions. The first paired samples t-test found a significant difference in the distances for the 250ms (M=156.59, SD=101.93) and 500ms (M=64.13, SD=56.81) conditions; t(19)=4.74, p<0.001. A second paired samples t-test indicated that there was a significant difference in the distances for the 250ms (M=156.59, SD=101.93) and 1000ms (M=77.5, SD=71.37) conditions; t(4)=2.97, p = .008. But a third paired samples t- test indicated that there was no significant difference for the 500ms and 1000ms conditions; t(19)=-.599, p=.556.
These results suggest that when a test image was displayed to participants for 250ms, their recall was significantly less accurate (higher distances) than when the test image was displayed for 500ms and 1000ms. These results were consistent across our set of test images. One reason for these results is that when a test image is only displayed for 250ms, it is too short for a participant to be conscious of her first gaze, so she cannot recall its position with high accuracy. When the test image is displayed longer, such as 500/1000ms, the participant has time to be aware of the first gaze made, and can better recall its position.
However, the first gaze recall still has some errors when compared to the first fixation data from an eye tracker. We found that if there were some highly salient objects around the first fixation, people tended to remember them rather than the actual first fixation. This is because people are sensitive to salient objects around their fovea centralis, the part of the eye responsible for sharp central vision.
We did observe some interesting differences across our images and participants. Across all conditions of test image display duration, on average, participants were more accurate with some images compared to others. The average visual angle distance on each image from participant recall to the eye tracker data was 3.7 degrees (SD=1.2), but ranged from 2.1 to 5.7 degrees. Similarly, there was variance among our participants. The average visual angle distance was 3.7 degrees across the participants (SD=1.2), but some participants averaged as low as 1.8 degrees across all the images, while others were as high as 5.5 degrees.
Results of last gaze recall
The metric for the last gaze recall is the same as for the first gaze recall. We calculated the distance between the last gaze point and the recalled gaze point. For all test images, the distances ranged from 5 pixels to 689 pixels (M=102, SD=105). The distribution of distances revealed that distances shorter than 20 pixels represented 5.6% of all distances, corresponding to a visual angle of 0.5 degrees. Distances corresponding to 1, 2, 3, 4, and 5 degrees accounted for 20.0%, 53.1%, 68.8%, 73.1% and 81.9%, respectively. Our last gaze recall accuracy results are again comparable to web camera eye trackers.
For each test image, a paired samples t-test was conducted to compare the effect of test image display duration on the values of distance for the 1000ms and 2000ms conditions. There were no significant effects of display duration on recall accuracy for any of the test images. The reason is that participants rarely depend on the test image duration to streamline the recall, because: 1) there is a very short interval (less than 100ms) between the last gaze and when participants start to recall, so participants can accurately remember the position of their last gaze; 2) the test image and grid image are temporally adjacent, so when the grid image displays, the effects of persistence of vision can facilitate participants in still “seeing” the area where their last gaze was located.
Results of all gazes recall
All recalled gazes generate a multiple-point sequence that is similar to “scanpath” (sequence of fixations). Hence, we are inspired by scanpath comparison methods for comparing the scanpaths as measured by the eye tracker and the corresponding recalled points. Jarodzka et al [19] proposed five dimensions to measure scanpath similarity: shape, fixation position, length, direction and fixation duration. However, our participants’ recalled point sequence is not a true scanpath, as it contains no information about duration. Also, this method for calculating the similarity of two scanpaths, does not discriminate between different fixation points in the same area of interest (AOI). Hence, we refine this method, by replacing the AOI-oriented calculation with a pixel-oriented calculation, and then propose three new measures more appropriate for our study: point count ratio (CR), trajectory comparison distance (D) and recall order ratio (RR), with descriptions as follows:
458
Leveraging the Crowd CSCW 2015, March 14-18, 2015, Vancouver, BC, Canada
Point count ratio (CR): the fixation count and recalled point count are denoted as n (n>0, as we displayed the image long enough, so as to record one fixation at least) and m (m>0, as we asked the participants to click one point at least), respectively. The point count ratio CR=m/n, and the closer CR is to 1, the more accurate the recall.
Trajectory comparison distance [3] (D): we consider a trajectory as a sequence of positions over time, and define the fixation trajectory F ={(x1, y1),(x2, y2),...,(xn, yn)}, xi and yi are the coordinates of fixation i (i=1,2,...n); and the recalled point trajectory R ={(a1, b1),(a2, b2),...,(am, bm)}, aj and bj are the coordinates of recalled point j (j=1,2,...m). For each time step t (t=1, 2... k and k= minimum (n, m)), the distance between fixation and its recalled point positions is given bydt  (xt at )2 (yt bt )2 . We do not
know which fixation point a recalled point represents, so we assume it is the nearest one. Then we can use the mean of
D and CR indicate recall performance with regard to spatial distribution: CR indicates local/global similarity, and D detects scale and spatial-offset similarity. RR indicates recall performance with regard to temporal distribution, such as detecting offsets and reversed similarity. Ideally, we want recall results with low D, and both CR and RR close to 1, indicating a high similarity between trajectories F and R.
For all gaze recall trials without tasks: 1) RR ranged from 0% to 100% (M=64%, SD=30%), and with 57% of the instances being higher than 60%. 2) CR ranged from 11% to 400% (M=90%, SD=69%), and 68% of the instances being higher than 60%. 35% of the instances were higher than 100% (i.e., there were more recalled points than actual fixation points), and this was mainly because some participants recalled the visual objects in the test images using many clicks along the objects’ boundaries, instead of clicking one or two points representing the objects. 3) D ranged from 11 pixels to 433 pixels (M=119, SD=68), and 44% of instances were shorter than 100 pixels. Moreover, there was no significant difference of RR, CR, D, in display duration (1000ms and 2000ms conditions) for most of the test images. For two complex test images (e.g., advertisement poster with human face), CR was significantly smaller in the 2000ms condition than the 1000ms condition. In the 2000ms condition, the eye tracker recorded more fixations than in the 1000ms condition for these complex visual scenes, but most participants were unable to recall the increased number of fixation points.
For all gaze recall trials with detection and problem solving tasks: 1) RR ranged from 0% to 100% (M= 66%, SD=27%), and 60% of the instances being higher than 60%. 2) CR ranged from 1% to 171% (M=44%, SD=42%), and 30% of instances being higher than 60%, and 13.3% of them being higher than 100%. 3) D ranged from 23 to 202 pixels (M= 98, SD=46), and 60% of instances being shorter than 100 pixels. There were no significant differences in these metrics between the two conditions: with and without tasks, except for CR. CR was smaller in the with tasks condition, as most participants spent more time executing tasks (with tasks had no time limit), resulting in more fixations being recorded and less accuracy in point count recall.
From these results, we can conclude that, in general, participants can recall their gaze trajectories well, when compared against the gaze data from the eye tracker. RR (recall order ratio) was ~66% in both with task and without task conditions, CR (count ratio) was quite high (90%) without tasks, but much lower (44%) with tasks, due to the increased number of fixation points, and D (distance) was ~100 pixels for both conditions. The gaze trajectories typically contained between 3 and 8 recalled points, each corresponding to an AOI in the image. The recalled count (CR) was low for the with-tasks condition, as participants recalled AOIs, and not fixation points that were in the transitions between AOIs. While the recall order ratio (RR) was lower than we would like to see, the distance (D) had
 these distances 1 k Dk dt
t1 trajectory F and trajectory R.
to
indicate
the
similarity
of
 1
d1 2F 4
  1d3 1R22 (a)
d2F 1
12d2d3 R
3 3
4
 1F2(b)
d2 d d1
d1 2 2 1 1R 2R
(c) (d)
Figure 2. Examples of trajectory comparisons (D).
For example, as shown in Figure 2(a), F has four fixations, R has two recalled points (local similarity); in contrast, F and R in Figure 2(b) are more similar (both have four fixations and recalled points) with some offset. However, D in Figure 2(a) may be smaller than in 2(b). So we cannot make a conclusion about distance only based on D: R in Figure 2(a) is more accurate than in Figure 2(b), leading us to consider the point count ratio CR as well. CR=2/4 in Figure 2(a), but 4/4 in 2(b). Based on CR, we can say that the recall performance in Figure 2(b) is better than in 2(a).
Recall order ratio (RR): an additional factor to consider is the order of recalled points in the trajectory. To illustrate, the result of D and CR in Figures 2(c) and 2(d) are quite similar. However, the point order of R in Figure 2(c) (e.g., 1, 2) is the same as F (e.g., 1, 2), but the point order of R in Figure 2(d) (e.g., 2, 1) is reversed with F (e.g., 1, 2). So we denote the number of recalled points, which are in the correct order according to the real fixations, as r, and then the recall order ratio RR= r/m (as 0≤r≤m, 0≤RR≤1).
4 d4 1F2
   459
Leveraging the Crowd CSCW 2015, March 14-18, 2015, Vancouver, BC, Canada
about a 3 degree error in visual angle. Additionally the count ratio (CR) was quite good for the without-tasks case, and for the with-tasks case, the viewed AOIs were recalled, even if every fixation within each AOI was not recalled. Furthermore, we found that the more complex the test image, the harder it was for participants to recall all fixations. For example, for the without-tasks case, one image only showing some rectangles has higher RR (M=78%, SD=25%) than another involving many flowers and leaves (M=49%, SD=35%).
Comparison with webcam based eye tracker
As mentioned earlier, a webcam based eye tracker is an alternative option if a commercial eye tracker is unavailable, and commonly available to crowdsourcing workers as many are now built into laptops. Hence we compared our recall approach (just for first gaze) with a webcam based eye tracker to better understand the differences in accuracy. In practice, most users do not have infrared webcams or infrared lamps, which the great majority of the public eyegaze tracking software assume, such as ITU Gaze Tracker. Instead we used a regular webcam (Logitech Webcam Pro 9000 with Carl Zeiss glass lens and autofocus, resolution of 640×480) along with a combination of open source eye tracking systems: Webcam Gaze-Tracking (www.luiwenhao.com/project/gazetrack/), with a refined algorithm from Opengazer.
We conducted a pilot study with 6 participants (3 female, 3 male), with ages from 19 to 25 (M=22, SD=2.6). Each participant was shown 9 predefined points, one at a time, on the same screen described earlier. We used both the SMI eye tracker and our webcam based eye tracker to record participants’ gaze data when they view the points, and also asked participants recall these points. Our lab settings had good illumination conditions, and we asked participants to keep their heads stable. However, despite this, the results showed that webcam based eye tracker had much lower accuracy (M=18.1°, SD=8.8°) and higher data loss rate (29%) than both our recall approach (M=2.8 2.1
) and SMI eye tracker (M=0.7 0.3
). Subjective feedback from a post-test questionnaire showed that participants also believed that our approach was more accurate (M=4.2, SD=0.8) than the webcam eye
tracker (M=3.2, SD=1.5) on a 5-point scale.
Discussion of Validation Results
Based on the evaluation of the first and last gaze recall results, we found that our gaze recall approach had reasonable accuracy: 3.4%-5.6% of the trials were equivalent to using real commercial eye trackers whose minimal visual angles are 0.5 degrees; and between 73.4- 81.9% of the trials were equivalent to webcam based eye trackers for which the maximum visual angle error is 5 degrees. Our results also validated the feasibility of, and reasonable accuracy for, recalling all gazes and trajectory.
In terms of the duration of displaying test images, we found that participants had higher recall accuracy for the first gaze in the 500ms and 1000ms conditions than in the 250ms condition. For last gaze recall, we found no significant difference in accuracy for durations of 1000ms and 2000ms. The same was true for all gaze recall, except for visually complex images, where the 1000ms duration led to higher accuracy than the 2000ms duration. Based on these results, and to reduce the fatigue of using our system in an online setting (by having the minimal possible duration), we chose to display images for 500ms for the first gaze recall, and 1000ms for the last gaze and all gazes recall.
USER STUDY 2: GAZE RECALL OF ONLINE CROWDS
Based on User Study 1, we know that our gaze recall method has reasonable accuracy and is feasible when applied in lab settings to individuals. We now apply this approach in a crowdsourcing setting where we look to see if an aggregate of a large number of participants’ data is comparable to that of eye trackers and visual attention models. We conducted a followup online study without an eye tracker to validate our approach in a real crowdsourcing environment, and demonstrate its efficiency for collect gaze data to generate heat maps and trajectory maps.
Participants and Procedure
We recruited 227 participants (85 female and 142 male) from Amazon Mechanical Turk (AMT), which is the most widely used environment for crowdsourcing as it is easy to recruit worldwide participants, manage tasks and data, and make payments. They were aged between 18 and 67 years (M=30.0, SD=9.1), and none reported being color-blind or having visual impairments. We designed and developed a website called GazeCrowd to conduct an online study, and integrated it with AMT. When participants opened GazeCrowd, they were presented with a consent form, and upon giving consent, were shown the study page. The test images and sequences were the same as in study 1. Subjects were told that the test session would take 15 minutes (actual duration was M=621.6 seconds, SD=428.1), and each participant received $0.20 USD.
As opposed to traditional eye-tracking experiments, it was not possible to observe participants and detect any fake results. Hence, to filter out cheating (fake data) or noisy data from participants, we designed three mechanisms to insure the quality of test data: 1) each participant went through a training session similar to our previous study; 2) each participant received a code from GazeCrowd after finishing all test sessions, and had to provide this code to receive payment; 3) if the test session completion time was very short (e.g., much shorter than the minimum duration in our lab test), we did not use this participant’s data. In our study, we removed 13 of 227 (5.7%) participants who submitted invalid codes or had short test durations.
We also designed mechanisms to reduce noisy data generated by our system. For example, to limit the impacts of network delay, test images were pre-loaded to a
    °, SD=
°, 0%
   data loss
°, SD=
°, 0% data
 loss
460
Leveraging the Crowd CSCW 2015, March 14-18, 2015, Vancouver, BC, Canada
participant’s local drive while she read the instructions at the start of a session. To avoid issues with different screen resolutions, we dynamically generated the grid images to cover the test images, regardless of screen or browser used.
Results
For each image in the first and last gaze recall tasks, we visualized all the participants’ mouse clicks as heatmaps [11], which provides a clean depiction of aggregate gaze by combining gaze points from multiple viewers. Then we compared the heatmaps generated with our approach with 2 other approaches: eye tracker data and a salience-based visual model. Figure 3 shows 3 example images, on which the 3 approaches were applied. From left to right, the first column has the original images, the second column has heatmaps generated with the iLab Neuromorphic Vision C++ Toolkit (http://ilab.usc.edu/toolkit/home.shtml) based on a bottom-up saliency-based visual attention model (denoted as VHM) [17], and the third column has heatmaps generated from eye tracker fixation data from the first study (denoted as FHM), using the SMI BeGazeTM software. In the fourth column, heatmaps were generated from mouse clicks from the online crowdsourcing study (CHM).
The first row shows the results of the first gaze recall (with test image display duration of 500ms). All the heatmaps are similar, but the VHM has a greater number of hot spots than both the FHM and CHM. The second and third rows show results for the last gaze recall (image display durations are both 1000ms); although all the heatmaps are similar, VHM is not very sensitive to people in the images, especially faces (e.g., second column, third row). These results indicate that compared to FHM and VHM, CHM appears to be robust and accurate, in identifying salient aspects of these images.
We also found that our gaze recall approach can capture both bottom-up and top-down visual mechanisms. We compared the heatmaps from the all gaze recall without and
with specific tasks. VHM was not applied in this analysis because it only applies to the bottom-up mechanism. Figure 4(a) (CHM) and 4(c) (FHM) had no tasks assigned. Driven by the bottom-up visual cognitive mechanism, most participants only focused on the high intensity human face in the middle of the image. Figure 4(b) (CHM) and 4(d) (FHM) show the results given the task of counting how many people were in the image. Driven by the top-down visual cognitive mechanism, participants had more gazes on each person in the image. Similarly, as shown in Figure 4(e) to 4(h), when participants were given the task to find the Nike logo in the image (4f, 4h corresponding to CHM and FHM), they narrowed down the area for visual search, and allocated greater visual attention where the logo was located, compared to the without task case (4e and 4g). All figures indicate a high consistency between the CHM and FHM. This indicates that our gaze recall approach can identify both bottom-up and top-down visual mechanisms.
Participants were fairly consistent in what regions they looked at but not in what order they viewed them [26]. However, heatmaps do not depict gaze order. Instead we used a transition map [23] to illustrate the gaze trajectories from all participants. We divided each test image into a number of AOIs on the basis of the gestalt laws (e.g., “law of closure”, “law of similarity”). Then we recorded the count of fixations or mouse clicks located in each AOI, and the frequency of gazes (or clicks) that transitioned from one AOI to another. Note that we removed the fixations from the eye tracking data that did not correspond to any AOI (most were image backgrounds), as users rarely report these and they were not salient in our images.
Taking one test image as an example (Figure 5), we identified 3 AOIs in the test image (corresponding to the 3 objects), and recorded the count of fixations or mouse clicks located in each AOI, and the frequency of gazes (or clicks) that transitioned from one AOI to another.
                                                                                                                                                                                                                                                                                                    Figure 3. Examples of heatmap comparison without tasks for the first (row 1) and last gaze recall (rows 2 & 3). Ovals indicate the parts of the image that were the most visually salient (from a visual attention model, from eye gaze data, and from our recall data).
461
Leveraging the Crowd
CSCW 2015, March 14-18, 2015, Vancouver, BC, Canada
        (a)
(c)
(e)
(g)
(b)
(d)
(f)
(h)
GENERAL DISCUSSION
Our crowdsourced approach for gathering fixation data has comparable accuracy to the more standard device-based eye tracking methods, particularly for point-count ratio, trajectory distance and first and last gaze heatmaps. While participant recall of gaze cannot capture real eye movement data, and relies on short-term memory to achieve fast and accurate performance, it trades off this potential inaccuracy against the cost and convenience of being able to conduct a study online efficiently with no special equipment. In our crowdsourced study, we collected data from 227 participants from all over the world in 5 days, at a total cost of US$45.4. In contrast, a commercial eye tracker, while much more accurate (0.5° of visual angle error), costs tens of thousands of dollars, and can only be used with a single participant at a time. Our initial lab study also took 5 days, but for only 20 participants, at a cost of $200.
Our approach does not require new hardware, and does not have the limitations of webcam based eye trackers. It was interesting that some participants had much lower visual angle errors than others. The mean error was 3.7°, (range: 1.8 to 5.5°). Similarly, some images resulted in lower error than others. In the future, we will explore why some users and images performed better, to identify opportunities for refining our approach to improve recall accuracy overall. We will show participants the same image multiple times (interleaved with other images), and use an aggregate of their recall data, to reduce noise.
Our approach is appropriate for those who want to collect a lot of gaze data cheaply and quickly. It can be applied to identify visual distractors of interfaces or images. Visual distractors will likely be found using the first gaze recall approach. A second example is refining visual layout. Using the last gaze and all gaze results, both a heatmap and a scan path can be generated, respectively. Together, these can reveal the distribution of visual attention on the stimuli, and a designer can use these to refine the layout, e.g., by grouping relevant interactors to create a more logical layout.
Our approach also has some limitations: 1) with short term memory, participants cannot recall more than a few gaze points and have difficulty recalling gaze order, especially for complex scenes or tasks; 2) it does not obtain gaze duration, unlike eye trackers; 3) we did not explore the size of the grid cells which may influence recall performance. We used 9×9 grids in our current research, with the size of each grid cell being 77 pixels wide and high (1.5 to 2° of visual angle). A larger number of smaller cells would allow for finer-grained selection, but may also make it difficult for participants to identify cells that best reflect their gaze; 4) we did not test dynamic images, e.g., videos.
Our approach currently works well for static images (e.g., poster and webpage design), which do not include many independent AOIs, so that they can be recalled easily with short-term memory. Also, with the large grid size, we could not test very tiny AOIs, such as text. To reduce these
                       Figure 4. Examples of heatmap (generated from crowdsourced mouse click data (a, b, e, f) and eye tracker data (c, d, g, h) comparison for the all gaze recall, without (first column) and with specific tasks (second column). Arrows indicate the areas that received the most visual attention.
40.00% 26.09%
33.33% (a)
8.71%
(b) 78%
33.33% 20.00%
30.00% 51.95%
50.00% (b)
25.00%
(b)
06%
33.33% 52.22%
              79.62%
34.
11.67%
34.43%
35.
30.53%
  Figure 5. Transition map of gaze trajectory aggregated by a) eye tracking data b) crowdsourced click data.
As shown in Figure 5(a), from the eye tracking data, for the AOI on the left (microscope), 26% of gaze transitions moved to the red cross (top right), and 35% to the red test tube (bottom right). The recall data showed similar results: 52% to the red cross, and 35% to the test tube. The recall data was very accurate for half of the transitions: microscope to test tube, test tube to red cross, and red cross to microscope. However the transitions in the opposite direction were not as accurate. Future work is necessary to
understand the uni-directional nature of our approach.
 462
Leveraging the Crowd CSCW 2015, March 14-18, 2015, Vancouver, BC, Canada
limitations,wecould:1)dividedynamicimages(e.g.,video) into a number of static frames to be used in a recall task, and aggregate the recall data later to produce a scan path; 2) explore whether the grid size influences recall performances and the impact on users’ mental workload; 3) for complex images, ask users to review either their own or others’ recall results and allow them to correct or refine them.
CONCLUSION
We measured eye movement using crowdsourcing instead of a real eye tracker, by having online crowds recall their gazes. We validated this approach through a lab-based user study, and found that participants could recall their gazes with reasonable accuracy when compared to commercial and webcam eye trackers. We then developed a prototype system, GazeCrowd, which leverages the crowd to collect a large amount of eye gaze data. The aggregated data produced heatmaps and transition maps similar to those from eye trackers and visual attention models. Our approach is a good complement to traditional eye- tracking, and it is cheap and efficient in collecting gaze data and trades these features for accuracy when compared to more expensive commercial eye tracking systems, and is robust to head movement and imperfect lighting conditions, and data loss typical of web cameras. Next we will apply it to remote usability tests for assessing and refining visual designs, or social recommendation systems that share gaze visualization to improve decision- making [10].
ACKNOWLEDGMENTS
We thank our study volunteers, and reviewers who provided helpful comments. This work was supported by NSF grant SES-0968566 and NSFC grant 61272308.
REFERENCES
1. Andrienko,G.,etal.Visualanalyticsmethodologyfor eye movement studies. IEEE Trans. on Visualization and Computer Graphics 12, 18 (2012), 2889-2898.
2. Aznar,F.,Pujol,M.,Rizo,R.Generatingsaliencymaps using human based computation agents. Current Topics in Artificial Intelligence (2010), 252-260.
3. Baud,O.,etal.Trajectorycomparisonforcivilaircraft. Aerospace Conference (2007), 1-9.
4. Bednarik,R.,Tukiainen,M.Validatingtherestricted focus viewer: a study using eye-movement tracking. Behavior Research Methods 39, 2 (2007), 274-282.
5. Blackwell,A.F.,Jansen,A.R.,Marriott,K.Restricted focus viewer: a tool for tracking visual attention. Theory and application of diagrams (2000), 162-177.
6. Bojko,A.Eyetrackingtheuserexperience:apractical guide to research. New York: Rosenfeld (2013), 8-9.
7. Borji,A.,Itti,L.State-of-the-artinvisualattention modeling. IEEE Trans. on Pattern Analysis and Machine Intelligence 35, 1 (2013), 185- 207.
8. Borji,A.,Sihite,D.N.,Itti,L.Quantitativeanalysisof human-model agreement in visual saliency modeling: a comparative study. IEEE Trans. on Image Processing 22, 1 (2012), 55-69.
9. Chen,M.C.,Anderson,J.R.,Sohn,M.H.Whatcana mouse cursor tell us more?: correlation of eye/mouse movements on web browsing. CHI 2001, 281-282.
10.Cheng, S. Third eye: designing eye gaze visualizations for online shopping social recommendations. Extended Abstracts CSCW 2013, 125-128.
11.Duchowski, A. T., Price, M. M., Meyer, M., Orero, P. Aggregate gaze visualization with real-time heatmaps. ETRA 2012, 13-20.
12.Gegenfurtner, A., Lehtinen, E., Säljö, R. Expertise differences in the comprehension of visualizations. Educational Psych. Review 23, 4 (2011), 523-552.
13. Henderson, J.M. Human gaze control during real-world scene perception. TRENDS in Cognitive Sciences 7, 11, (2003), 498-504.
14.Henderson, J.M., Hollingworth, A. High level scene perception. Ann. Rev. Psychology 50, (1999), 243-271.
15. Holmes, T., Zanker, J. Eye on the prize: using overt visual attention to drive fitness for interactive evolutionary computation. GECCO 2008, 1531-1538.
16.Hornof, A.J., Halverson, T. Cognitive strategies and eye movements for searching hierarchical computer displays. CHI 2003, 249-256.
17.Itti, L., Koch, C. Computational modeling of visual attention. Nature Reviews Neuroscience 2, 3 (2001), 194-203.
18.Itti, L. Models of bottom-up attention and saliency. In Tsotsos (Eds.), Neurobiology of Attention, Elsevier (2005), 576-582.
19.Jarodzka, H., Holmqvist, K., and Nyström, M. A vector- based, multidimensional scanpath similarity measure. ETRA 2010, 211-218.
20.Johansen, S. A., San Agustin, J., Skovsgaard, H., Hansen, J. P., Tall, M. Low cost vs. high-end eye tracking for usability testing. CHI 2011, 1177-1182.
21. Jones, M. N., & Mewhort, D. J. K. Tracking attention with the focus-window technique: The information filter must be calibrated. Behavior Research Methods, Instruments, & Computers, 36 (2004), 270-276.
22.Klein, R.M. Inhibition of return. Trends in Cognitive Sciences 4, 4 (2000), 138-147.
23. Lessing, S., Linge, L. IICap: A new environment for eye tracking data analysis. Master’s thesis. University of Lund (2002), Sweden.
24.Marshall, S.P. Identifying cognitive state from eye metrics. Aviation, Space, and Environmental Medicine 78, 5(2007), 165-185.
25.Nothdurft, H.C. Salience of feature contrast. Neurobiology of attention. Academic Press, (2005).
26.Privitera, C. M. The scanpath theory: its definitions and later developments. SPIE 2006, 87-91.
27.Rodden, K., Fu, X., Aula, A., Spiro, I. Eye-mouse coordination patterns on web search results pages. Extended Abstracts CHI 2008, 2997-3002.
28.Rudoy, D., et al. Crowdsourcing gaze data collection. CI 2012, 1-8.
29.Yarbus, A.L. Eye-movements and vision. Plenum, 1967.
UBICOMP/ISWC '15 ADJUNCT, SEPTEMBER 7–11, 2015, OSAKA, JAPAN
 Riki Kudo
Osaka Prefecture University 1-1 Gakuen-cho, Naka, Sakai, Osaka , JAPAN kudo@m.cs.osakafu-u.ac.jp
Olivier Augereau
Osaka Prefecture University 1-1 Gakuen-cho, Naka, Sakai, Osaka , JAPAN augereau.o@gmail.com
Takuto Rou
Osaka Prefecture University 1-1 Gakuen-cho, Naka, Sakai, Osaka , JAPAN rou@m.cs.osakafu-u.ac.jp
Koichi Kise
Osaka Prefecture University 1-1 Gakuen-cho, Naka, Sakai, Osaka , JAPAN kise@cs.osakafu-u.ac.jp
Abstract
The eye movement is an important source of information for the reading analysis. We propose a method for computing a similarity measure between two fixation sequences. In or- der to estimate the effectiveness of the similarity measure, we investigate whether a high similarity is obtained when two subjects read the same document. A F1score of 0.92 is obtained for retrieving the same document based on the reading similarity.
Author Keywords
Reading similarity; Eye tracker
ACM Classification Keywords
H.5.2 [User Interfaces]: Input devices and strategies
Introduction
Nowadays, more and more research focus on sensing and recording our daily life. For example, the food-life log pro- vides information to the users about the calories contained in foods they eat every day [1]. The activity-life log recog- nizes human activities such as walking, climbing stairs, etc. for health monitoring [8]. In this paper, we will focus on the reading-life log, i.e. the analysis of reading.
Everyday, we read some documents or texts. Reading takes a very important role in our life, it is one of the ma-
Reading Similarity Measure Based on Comparison of Fixation Sequences
 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org.
UbiComp/ISWC ‘15 Adjunct, September 7-11, 2015, Osaka, Japan.
© 2015 ACM ISBN 978-1-4503-3575-1/15/09...$15.00.
DOI: http://dx.doi.org/10.1145/2800835.2807935
1221
jor ways to get knowledge. Some researches have been done in reading analysis. For example, there are studies about recognizing the document type from eye movement with a mobile eye tracker [7]. Another application consists of estimating the number of read words [6].
The reading contains information about the contents of read documents and the reader. For example, if the doc- uments have different layouts or contain a different number of words, the reading will be different. Furthermore, the reading also contains information about the reader such as his reading skills, comprehension or attention.
In this paper, we propose a method for computing the sim- ilarity between two reading recordings. This method does not use the document image, but only the reader’s eye gaze movements. The reading similarity measure can be used to find some relationships between the readers or the docu- ments. The similarity of reading might be correlated to the similarity of reader’s English skills or to the similarity of the content of the documents. In order to compute the similar- ity between two readings, we will compute the similarity of two sequences of eye gazes. Several methods based on the Levenstein distance have already been proposed [2], [5]. We chose to use the ScanMatch algorithm [4] based
on Needleman-Wunsch as it offer better performances than the ones based on the Levenstein distance. To prove the effectiveness of the similarity measure, we show in the ex- periment that if two readers read the same document, a high similarity measure is obtained. We show that for the best parameters, a precision of 100% and a recall of 83% are obtained.
The rest of this paper is organized as follows. First, we describe how to measure the reading similarity. Next, we present the experimental results. The similarity measure can be employed to identify if two subjects read the same
document. Finally, we conclude and propose some future work.
Measure of the reading similarity
In this section, we explain how to measure the reading sim- ilarity through the eye movements. While reading, the eye movement is decomposed as a sequence of fixations and saccades. A fixation is obtained when the eye stares at
the same place for more than a certain period of time. A saccade corresponds to a quick movement between two fixations. We use the ScanMatch method1, an algorithm for comparing fixation sequences.
The reading similarity is computed in two major steps. First, we record the eye movement positions by using an eye tracker and we extract the fixation information from the eye gaze. Next, we measure the reading similarity by using the ScanMatch method.
1) Fixation detection
The fixations and saccades are obtained by filtering the eye gaze [3]. The fixation detection is based on two following steps (illustrated in Fig. 1):
• First, when the eye gaze is gathered closely enough (within a 30-pixel square), we regard these points as a minimum fixation.
• If the following gaze is close enough (included in a 50-pixel square), it is added to the fixation. If it is too far, it is regarded as noise. If four consecutive gazes are noise, the fixation is stopped and a new fixation detection is started.
1http://seis.bris.ac.uk/~psidg/ScanMatch/
UBICOMP/ISWC '15 ADJUNCT, SEPTEMBER 7–11, 2015, OSAKA, JAPAN
 1222
WORKSHOP
  Figure 1: Fixation detection (based on Buscher et al. [3] method.)
2)ScanMatch
In this section, we explain the ScanMatch algorithm. It is used for measuring the similarity between two fixation se- quences. ScanMatch consists of two processes: creating
a sequence of characters from the fixations, and then com- paring two sequences of characters.
2.1)Creating a sequence of characters
First, the area of the monitor is divided in a grid and a char- acter is associated to each cell of the grid. Then, the fixa- tions are assigned to the corresponding cells, depending on their positions such as in Fig. 2. Next, for each fixation, we check which cell contains the fixation and add the corre- sponding character to the sequence.
A temporal binning is set up. If one fixation lasts more than a fixed threshold, it is represented as several times the same character. This process is also illustrated in the Fig. 2.
2.2)Global alignment of fixation sequences
The Needleman-Wunsch Algorithm [9] is used for determin- ing the global alignment of two sequences. This algorithm
Figure 2: Creating the character sequence from the fixations (based on Cristino et al. [4]).
expresses a score of similarity between two character se- quences.
2.3)Normalizing the score
The score of two sequences is difficult to compare, because it depends on the length of the sequences. So the score is normalized by using the following equation:
N= S (1) L·M
In this equation N is the normalized score, S is the score of two characters similarity, and L is the length of the longest sequence. M is the score of the two characters matching. It is a constant value fixed in Needleman-Wunsch Algorithm. The score is normalized between 0 and 1, where 1 is the highest similarity.
 1223
Experiment
The ScanMatch method was designed for visual tasks such as researching a symbol or a number of a defined color; but not specifically for a reading task. In order to prove that the reading similarity measure can be used to find if two readers read the same document, we examined how the parameters of the ScanMatch method could affect the read- ing similarity.
In this experiment, we asked 12 subjects to read 5 docu- ments, being 60 reading recordings. By using the similarity measure, we wanted to show that a high reading similarity was obtained when different subjects read the same docu- ment. The Tobii EyeX2 eye tracker is used for recording the eye gaze.
The eye tracker was attached under the monitor such as in Fig. 3. The experiment is carried out in the following pro- cess:
2 http://www.tobii.com/ja-JP/eye-experience/eyex/
UBICOMP/ISWC '15 ADJUNCT, SEPTEMBER 7–11, 2015, OSAKA, JAPAN
 Figure 3: Experiment setup.
1. We explain the experiment to a subject.
2. The eye tracker is calibrated for the subject. 3. The subject reads the document.
The subject had been asked to read each document from the beginning to the end without rereading. This constraint had been added in order to reduce the differences between the subjects because they come from different countries and have different English skills.
For each of the 60 recordings, we compared the combina- tions with all the other recordings by computing the nor- malized score based on the ScanMatch method. A total
of 1770 combinations is obtained. If the normalized score
is higher than the threshold T , we considered that the two subjects read the same document. Then, this decision is compared with the ground truth in order to compute the per- formances of the similarity measure. We analyze the recall and the precision while changing the following 3 parame- ters:
1. The threshold T.
A range of values from 0.3 to 0.7 is tested in order to obtain a large range of a recall and a precision.
2. The temporal binning.
We fixed the temporal binning at 50 ms. We also tested not to apply any temporal binning.
3. The grid.
The grid is defined as the number of columns times the number of lines. We tested the 3 different follow- inggrids: 24×12,18×9and12×6.
 1224
WORKSHOP
  Figure 4: Influence of temporal binning.
Experiment results
We show the experiment results about the threshold T and the temporal binning in Fig. 4. For the threshold, the preci- sion sharply decreases when it is less than 0.5 because the similarity measure of most of the same documents is more than 0.5. We also observe that the performances are better without using the temporal binning. In the reading context, the differences of English reading skills between the sub- jects have an impact on the duration of the fixations. So, if we use a temporal binning, when two subjects with different English skills read the same document, the similarity will be lower.
Next, we show the impact of the grid on the performances in Fig. 5. This figure shows the performances of the 24 × 12 and 18 × 9 grid are similar, but the 12 × 6 grid has a lower precision. If the cell size is too large, many fixations are merged in the same cell, which causes a low precision.
In order to obtain the best F1score, the optimal parameters are the following: T =0.55, a 24 × 12 grid and no tempo-
Figure 5: Influence of the grid.
ral binning. With this parameters, the precision is 100%, the recall is 85% and the F1score is 0.92. In other words, if the similarity measure is higher than 0.55, the two record- ings come from the same document. And, if the recordings came from the same document, the similarity is higher than 0.55 85% of the times.
Conclusion and Future Work
We explained how to compute the reading similarity be- tween two fixation sequences. Our experiment shows that 3 parameters have a great impact on the reading similar- ity. As a result, we show that the similarity measure can be used for recognizing if two readers read the same docu- ment with 100% precision and 85% recall.
We proved that the reading similarity measure proposed in this paper can be used to find some relationships between the documents. One of the next steps will be to use the reading similarity measure to find relationships between the readers.
1225
We fixed the constraints of reading all the text from the be- ginning to the end without rereading. This is just a first step towards computing a reading similarity. The next step will be to analyze the reading similarity on recordings with no constraint. However, in this case, some readers might skip or reread different parts of the document. So, a global align- ment is not suitable anymore, the algorithm must be local in order to align only some parts of the reading sequences.
REFERENCES
1. Kiyoharu Aizawa, Gamhewage C De Silva, Makoto Ogawa, and Yohei Sato. 2010. Food log by snapping and processing images. In Virtual Systems and Multimedia (VSMM), 2010 16th International Conference on. IEEE, 71–74.
2. Stephan A Brandt and Lawrence W Stark. 1997. Spontaneous eye movements during visual imagery reflect the content of the visual scene. Journal of cognitive neuroscience 9, 1 (1997), 27–38.
3. Georg Buscher, Andreas Dengel, and Ludger van Elst. 2008. Eye movements as implicit relevance feedback. In CHI’08 extended abstracts on Human factors in computing systems. ACM, 2991–2996.
4. Filipe Cristino, Sebastiaan Mathôt, Jan Theeuwes, and Iain D Gilchrist. 2010. ScanMatch: A novel method for comparing fixation sequences. Behavior research methods 42, 3 (2010), 692–700.
5. Tom Foulsham and Geoffrey Underwood. 2008. What can saliency models predict about eye movements? Spatial and sequential aspects of fixations during encoding and recognition. Journal of Vision 8, 2 (2008), 6.
6. Kai Kunze, Hitoshi Kawaichi, Kazuki Yoshimura, and Kenji Kise. 2013a. The wordometer–estimating the
number of words read using document image retrieval and mobile eye tracking. In Document Analysis and Recognition (ICDAR), 2013 12th International Conference on. IEEE, 25–29.
7. Kai Kunze, Yuzuko Utsumi, Yuki Shiga, Koichi Kise, and Andreas Bulling. 2013b. I know what you are reading: recognition of document types using mobile eye tracking. In Proceedings of the 2013 International Symposium on Wearable Computers. ACM, 113–116.
8. Myong-Woo Lee, Adil Mehmood Khan, and Tae-Seong Kim. 2011. A single tri-axial accelerometer-based real-time personal life log system capable of human activity recognition and exercise information generation. Personal and Ubiquitous Computing 15, 8 (2011), 887–898.
9. W John Wilbur and David J Lipman. 1983. Rapid similarity searches of nucleic acid and protein data banks. Proceedings of the National Academy of Sciences 80, 3 (1983), 726–730.
1226
UBICOMP/ISWC '15 ADJUNCT, SEPTEMBER 7–11, 2015, OSAKA, JAPAN
UBICOMP '15, SEPTEMBER 7–11, 2015, OSAKA, JAPAN
Quantifying Reading Habits – Counting How Many Words You Read
KaiKunze,Katsutoshi O ̈merSacakli,Marcus ShoyaIshimaru,KoichiKise
Masai, Masahiko Inami
Keio Media Design Yokohama, Japan Email:{kai/masai/inami} @kmd.keio.ac.jp
ABSTRACT
Reading is a very common learning activity, a lot of people perform it everyday even while standing in the subway or waiting in the doctors office. However, we know little about our everyday reading habits, quantifying them enables us to get more insights about better language skills, more effective learning and ultimately critical thinking. This paper presents a first contribution towards establishing a reading log, track- ing how much reading you are doing at what time. We present an approach capable of estimating the words read by a user, evaluate it in an user independent approach over 3 experi- ments with 24 users over 5 different devices (e-ink reader, smartphone, tablet, paper, computer screen).We achieve an error rate as low as 5% (using a medical electrooculogra- phy system) or 15% (based on eye movements captured by optical eye tracking) over a total of 30 hours of recording. Our method works for both an optical eye tracking and an Electrooculography system. We provide first indications that the method works also on soon commercially available smart glasses.
Author Keywords
Mobile Eye tracking; Electrooculography; Quantifying Reading; Eye Movement Analysis; Reading Behavior
ACM Classification Keywords
H.5.2 Information interfaces and presentation (e.g., HCI): Miscellaneous.
INTRODUCTION
Increased reading volume is associated with numerous cog- nitive benefits, including improved vocabulary skills, higher general knowledge and increased critical thinking [17]. Fur- thermore, reading is entertaining and has social value, higher reading volumes in adolescents are correlated with higher self-esteem and improved cognitive and emotional well- being[38]. Although there are these strong positive effects,
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org.
UbiComp ’15, September 7-11, 2015, Osaka, Japan.
Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3574-4/15/09 . . . $15.00 http://dx.doi.org/10.1145/2750858.2804278
Liwicki, Andreas Dengel
DFKI Kaiserslautern, Germany
Email:faruksacakli@gmail.com, liwicki@dfki.uni-kl.de, andreas.dengel@dfki.de
Osaka Prefecture University Sakai, Japan Email:shimaru@m.cs.osakafu- u.ac.jp, kise@cs.osakafu-u.ac.jp
87
only few previous works evaluated reading activities in situ and even fewer tried to quantify them[9, 35].
Despite the growing awareness on how important reading is for learning, it’s challenging to get people to read more, espe- cially as the amount easy digestible content in form of videos etc. increases. Automatically tracking physical activities can motivate users to more healthy lifestyles [6]. We believe this translates also to cognitive skills and tasks, as students can already boost their learning rate by keeping a manual record of their activities [27]. We want to investigate wether we can track reading habits similar to physical activity to give users tools to improve their mental fitness. Letters and words repre- sent ideas and concepts; tracking the volume, speed and time a user is reading them seems particularly valuable as it gives first insights into learning and provides us with a basic count- able measure of our performance [25]. For example, children suffering from reading disabilities can be earlier diagnosed, people can without trouble improve their reading speed and older adults have an easier way to fight dementia. Since re- search suggests that performance related to these situations is closely linked to reading volume[42, 26, 41].
We still have a hard time defining what healthy reading habits for adults are[17], as tools are missing to quantify reading in everyday situations and in long term studies. This paper provides the first steps towards assessing reading volume in realistic settings utilizing mobile eye tracking.
The contributions of this work are: (1) We present methods to quantify how much words a user reads working for both, optical eye tracking and electrooculography, (2) we achieve the lowest error rate between 5 -15% user-independent for our word count estimation over 3 data sets of in total 24 users with over 30 hours of eye gaze recordings, (3) we provide initial evidence that our methods to estimate word count can work on consumer smart glasses, performing a small user study on a first EOG glasses prototype.
RELATED WORK
Alternative Modalities
As we are interested in tracking reading habits, a collection of cognitive tasks, we first might try direct brain sensing. Yet, as related work shows most methods are too bulky or are quite noisy to get decent results related to recognizing reading [36,
16, 20, 18]. The pre-processing steps seem more computa- tionally complex compared to EOG or optical eye tracking.
The most interesting modalities for direct brain sensing seem to be electroencephalography (EEG) and near-infrared spec- troscopy (NIRS), as both can be used in mobile settings[48, 46, 22]. However, for both spacial and for NIRS temporal resolution are not so good. Their signal is also strongly af- fected by motion noise and usually requires more complex filtering/pre-processing steps compared to EOG/optical eye tracing [9, 35].
Eye tracking
However, the strong relationship between reading and eye movements is very well explored in cognitive science and psychology [45, 29]. For example, Kligel et al. investi- gate correlations of eye fixations with cognitive tasks re- lated to reading [32]. Rayner provides a good summary of eye tracking research [43]. Most of the reading research in psychology however emphasizes on older adults or dis- abled [20, 16].There are only a few research publications cen- tering around reading detection in mobile and stationary set- tings [10, 9]. Such reading detection algorithms can be used as a very simple word counting mechanism, as there’s a re- lation between time read and the read volume. Biedert et al. look into how people read text. They presented a method to discriminate skimming from reading using a novel set of eye movement features [5]. Their algorithm works in real-time, deals with distorted eye tracking data and provides robust classification accuracies of 86% accuracy. They also showed a method to recognize text comprehensibility with an accu- racy of 62% from gaze data recorded from multiple readers [4]. Buscher et al. proposed eye movements as automatic relevance feedback for information retrieval tasks[13].
Enhancing the Reading Experience
In a series of works, Biedert et al. studied ways to enhance the reading experience of the user. They presented EyeBook [2] and Text 2.0 [3] a reading interface that observe which part of the text is currently being read by the user and that generate appropriate effects (e.g. playing sounds). However, they don’t evaluate what suitable interventions are to increase users enjoyment, comprehension or attention. Xu et al. apply eye movement analyzes for document summaries, yet the en- vironment is very controlled, e.g. the users need to rest their chin on a support when performing the reading task [49].
Concerning reading habits, there are some questionnaire based evaluations giving advice about effective reading tech- niques to second language learners, as well as for children with reading disabilities and older adults struggling with de- mentia[21, 42, 41]. Hansen [26] reports on a series of studies on reading comprehension with rapid readers trained in the Evelyn Wood method. There are also a couple of other works exploring speed reading and comprehension[45, 19, 15, 24], giving advice about reading techniques to increase speed and understanding. Several mention rigorous practice and steady increase in reading volume as one of the key factors to suc- cess [31]. There are also a couple of papers exploring speed reading together with eye movement analysis[31, 29]. Busher
et al. discuss in general the feasibility of gaze based annota- tions for documents [14].
Cognitive Task Tracking
There are also some efforts to infer the users expertise, lan- guage skill and other higher level cognitive activities using eye tracking [34, 37, 11, 23, 7, 28]. Most of the research focusing on second language learners or infants as improve- ments can be easier tracked using indirect measurements (questionnaires etc.). Bulling et al. coin the term cognition- aware computing to describe computing able to understand and support our mental activities not focusing alone on eye tracking but brain sensing in general [11]. Orchard et al. try to assess other cognitive states, especially cognitive workload, while users read by analyzing blinking patterns[40]. Rud- mann et al. give a concise overview about cognitive state detection using visual behavior [44].
Toward Quantified Reading
The closest to our work is the Wordometer implemented by Kunze et al. [35, 33]. They introduce word counting algo- rithms also based on mobile eye tracking and EOG (how- ever both algorithms are separate). For the optical systems, their work relies on document image retrieval for filtering and mapping the eye gaze into the coordinate system of the read text. Therefore, they need to use the scene camera of the eye tracker (not only for document identification, but also for eye gaze filtering). We see our work complementary and as an extension of their method. As our comparison with their algorithm shows we improve the average error rate from 45 % to 8% for a complex dataset. Their mobile eye tracking method cannot cope with varying line lengths often found on different reading media (e.g. tablet versus news paper). In addition, all of their participants were Japanese and the ex- perimental setup was more constrained. We in turn present a dynamic line break detection as well as a novel reading de- tection and word count estimation based on saccade features only, making the whole method more versatile and portable to other eye tracking or eye movement analysis systems.
As far as we know, this is the only research work explor- ing technology support to quantify reading and presenting a word count estimation algorithm capable of dealing with varying device types, line lengths working for both common eye tracking techniques.
APPROACH
As seen from the related work section, using eye mo- tions seem to be a promising approach to quantify reading habits. There are two common techniques for tracking eye gaze/motion: electrooculography (EOG) and optical tracking. EOG uses electrodes to measure a change in potential when the eye moves, as the eye can be represented as a dipole be- tween the cornea and retina, This approach is cheap to imple- ment and requires little processing compared to optical track- ing. However, it just gives relative eye movements. Alterna- tively, we can track eye gaze using cameras and infrared light, providing potentially higher accuracy but requiring more pro- cessing power, inferring eye position, motion and gaze based on iris shape.
88
UBICOMP '15, SEPTEMBER 7–11, 2015, OSAKA, JAPAN
SESSION: QUANTIFYING AND COMMUNICATING THROUGH WEARABLES
 Optical tracking EOG-based
  raw eye gaze
  filtering
EOG V H
de-noise
preprocessing
 Saccade Detection
  feature calc
feature calc
reading detection
 classification
  pre-process saccades
if reading detected
 line break estimation
word count algorithms (basic + SVR based)
line break detection
  word count estimation
 saccade related features
average length of saccades minimum length of saccades horizontal element of saccades vertical element of saccades
saccade direction mean and variance saccade slope
         Figure 1: Approach overview, on the left for optical and right EOG systems.
Our method works in principle with both most of these tech- niques with slight adaptations. Our approach is divided into 4 discrete steps: preprocessing, reading detection, line-break detection, word count estimation. However, some of the steps are specific to the given technique also highlighted in the method overview as seen in Figure 1(left optical eye track- ing, right EOG based eye movement analysis).
We get either the raw eye gaze data from the (mobile) optical eye tracker (fixation and saccade information), or horizontal/ vertical component from the Electrooculography. We use an EOG setup that can be integrated in smart glasses shown later Figure 7, similar to Kunze et al. [33].
Preprocessing
Optical Eye Tracking –We combine several small, close-by fixations into larger duration fixations using the method of Busher et al. [12].
EOG – We apply a Median filter using a sliding window of size w to filter noise. For saccade detection we use the Con- tinuous Wavelet TransformSaccade Detection (CWT-SD) de- scribed Bulling et. al.[10]. We don’t perform the letter en- coding of CWT-SD, just use it to get saccade direction and amplitude.
Reading Detection
The method is straight forward. We calculate the common features given in Table 1 over a 3 sec. frame sliding window and apply a Support Vector Machine classifier with a radial basis function on the resulting feature vector.
For the optical system we additionally calculate mean fixation duration and variance of the fixation count over the sliding window. For EOG we add blinking duration and frequency
Table 1: Common features for reading detection
Figure 2: Preprocessing for Line break detection. Top: un- processed eye gaze while reading. Bottom: Processed Eye gaze, saccades against the reading direction are combined.
(the blink detection algorithm is similar to Bulling et. al. [10]) as features.
As already mentioned in related work, methods for reading detection are not new [5, 8]. However, some of them work using different sensing modalities and eye tracking hardware. We wanted to present a whole system. Additionally, the read- ing detection method can be seen as a very simple word count method based on time. The longer a user reads the more words he reads.
Line-Break Detection
Our approach uses a dynamic line break detection using the distributions of the horizontal component of the saccades. We combine the saccade amplitude sa with the horizontal direc- tion component sdh of the saccade (-1 for a completely hor- izontal saccade against main reading direction and +1 for a completely horizontal saccade in reading direction). We re- fer to it as horizontal saccade direction component (HSD). HSD = sdh ⇤ sa. Another way to define HSD is a projec- tion of each saccade on the horizontal axis.
We assume that reading is dominated by two types of sac- cades, short ones in reading direction (indicating reading words) and longer saccades against the reading direction (in- dicating the line breaks). Now, if we plot the HSD histogram for some sample reading recording, we should see two max- ima (see also Figure 8 for histogram examples), the larger one is the average reading saccade direction * amplitude (forward motion) and the second maximum (smaller as there are fewer line break saccades) in the place of the average line break sac- cade direction. The text width in coordinates or the so-called line break distance is the distance between theses maxima. Using half of this length as a threshold works well for recog- nizing a line break (experimentally determined).
Before, we can calculate the HSD histogram on optical eye tracking raw eye gaze data we have to do an additional step,
89
UBICOMP '15, SEPTEMBER 7–11, 2015, OSAKA, JAPAN
   Figure 3: Determining the Line Break Saccade Threshold: Distance calculation between the two largest maxima of a Gaussian mixture fit on the saccade length histogram.
combining consecutive saccades against the reading direction (line break saccades). Sometimes backward saccades of the optical system are separated by small fixations (this might be an artifact of the optical system we use for our recordings). Figure 2 illustrates the effect of this pre-processing step. The top figure shows the eye gaze with 3 backward saccades, the bottom shows the saccades combined into one.
To determine a line break we take all saccades for the segment detected as reading, we calculate the HSDs for each saccade, combine them in the HSD histogram, fit a mixture of 2 Gaus- sians to it and take the distance between the two maxima (see Figure 3). Empirically, we found that taking half of this dis- tance as threshold for a line break works best.
For our experiments and analysis we assume that the main reading direction is horizontal (right to left or left to right). However, the algorithm is easily adjustable for other reading directions (e.g. for some Japanese/Chinese texts).
Word Count Estimation
We use two methods to derive the amount of words read.
Basic Word Count-
This simple estimator uses the average word count of the doc- ument read times the line numbers estimated from the previ- ous step. For this estimator to work we need to get the average words per line for the document. This can be easy achieved if reading on an electronic device. For reading from paper with the optical system, we use a document image retrieval tech- nique called “Locally Likely Arrangement Hashing” (LLAH) to associate the paper with the digital document [39]. We use the video feed from the eye tracker as input to LLAH. LLAH retrieves the corresponding page from a document im- age database by comparing feature points. This comes with the limitation that the document needs to be registered first in a database, yet as previous research shows the retrieval al- gorithm is very fast. For example, retrieval from a database with 100 Million pages (around 440 thousand books) takes around 178 ms on a single server core (for performance de- tails please see [30]). Even a corpus as large as Google books can be handled.
Figure 6: Baseline Experimental Setup. Top picture shows a participant starting to read wearing the mobile eyetracker; the bottom picture shows the recording software with a scene image and the eye fixation depicted as green dot. Bottom shows a photo from the recordings a user reading on a kindle.
Support Vector Regression Count-
We calculate features over the complete reading segment rec- ognized by the reading detection method. After evaluating over 25 standard features, we use 5: total time read, sum of all saccades distance, sum of the line break saccade distances, number of line breaks and sum of the reading saccade dis- tances. We train a Support Vector Regression with a Radial Basis Function using these features.
EXPERIMENTAL SETUPS
Mobile Optical Eye Tracking
For both optical eye tracking experiments we use the SMI mobile eye tracker 2.0. The glasses have binocular gaze esti- mates at a joint sampling frequency of 30Hz as well as a scene video with a resolution of 1280x960 pixels. At this sampling frequency, only saccades of about 33ms and slower can be de- tected. The scene videos are recorded solely for ground truth and documentation purposes. Following the recordings on a laptop, the data is exported by BeGaze 3.4.52, an eye track- ing analysis software. The data analysis is done using python scripts, we will make data as well as source code publicly available for other researchers to use.
Baseline Experiment
We record the documents used by our previous experiment to extend their validity to international participants with varying English skills [35]. The subject reads a paper in an office scenario. We calibrated the eye tracker using a standard 3- point calibration prior to each recording. Each subject reads 14 documents, the document order is assigned using the Latin Square method.
The documents consist of 10 preliminary English texts (PET) and 4 difficult English Scholastic Assessment Test (SAT) texts. The document size ranges from 135 to 414 words (mean of 245 words). We recruited 9 subjects with an in- ternational background with following national background: French, German, Luxembourgian, Beninese, Turkish, Viet- namese (3 female average age 28 std 7). We record the eye gaze and scene camera using the SMI glasses for all partic- ipants while reading the documents. The participants don’t get any special instructions except of reading naturally. Af- ter finishing each document, we ask several comprehension questions to assess the participant in terms of understanding.
90
 SESSION: QUANTIFYING AND COMMUNICATING THROUGH WEARABLES
(a) (b) (c) (d) (e)
Figure 4: Photos from the Device Experimental Setup with the users reading from different devices: an e-ink reader (a), computer screen (b), tablet computer (c), smartphone (d) and a sheet of paper (e).
(a) (b) (c) (d) (e)
Figure 5: Sample, filtered eye gaze recorded by the SMI mobile eye tracking glasses while the user reads from different devices: an e-ink reader (a), computer screen (b), tablet computer (c), smartphone (d) and a sheet of paper (e).
Devices Experiment
In this experiment we want to evaluate the effects of docu- ment length and device types (line length) on the word count accuracy. The subjects read 5 documents with 115, 253, 519, 679 and 881 words respectively. The document are read from different devices. The devices have different width and height sizes, but their font size -12pt- is constant. In the follow- ing we list the devices used and their sizes: Paper (169mm width,117mm height), E-ink reader (90mm width, 122mm height), Smartphone (56mm width, 80mm height), Com- puter Screen (516mm width, 325mm height), Tablet (217mm width, 118mm height). The line breaks differ for each docu- ment depending on the device.
We recruited again 10 subjects with an international back- ground (German, Japanese, French, Luxembourgian, Beni- nese, Australian, Chinese) (4 female, average age 27 std 7). Two subjects have native English skills. They read from dif- ferent device types (paper, screen, smart phone, tablet, e-ink reader) with varying line lengths and in addition they per- form the following activities: solving a Sudoku puzzle on a printed paper, talking to a person, playing Angry Birds on the smart phone, performing a visual search task finding x in the web browser on the computer screen, watching a short movie on the tablet. We again calibrated the eye tracker using a standard 3-point calibration prior to each recording. The document, device and activity assignments are again deter- mined using the Latin Square method. The participants are in- structed to perform each task naturally, we don’t present them with any specific restrictions regarding the tasks. As with the previous experiment, we ask questions after each document reading task to assess comprehension.
Electrooculography
Figure 7: Electrooculography Setup: top shows the electrode placement, we use the potential between electrodes left and right from the nose for the horizontal EOG component and the potential between right/left electrode and top electrode for the vertical EOG compoenent. R is the reference electrode.
91
UBICOMP '15, SEPTEMBER 7–11, 2015, OSAKA, JAPAN
   3"Point Electrography Ba;ery
Mo4on Sensors Bluetooth LE etc.
Figure 9: J!NS MEME Prototype and user wearing MEME reading on a iPad outside.
For the EOG experiment, we follow the script of the Devices Experiment given above. We use the data from 6 participants from Kunze et. al. [33] as well as 2 additional users in total 8 (4 female, average age 24 std 8). As EOG Device we use the Polymate mini with active electrodes (sampling rate 1k Hz). The Figure 7 shows our electrode setup. The Polymate is con- nected via Bluetooth to a laptop. Otherwise the experimental conduction and conditions resemble the previous device ex- periment.
MEME Case Study
In addition we use a small case study with an early proto- type of commercial smart eyewear, J!NS MEME 1 to show the potential of our approach. J!NS MEME are smart glasses glasses including 3 electrodes to detect eye movements and inertial motion sensors (accelerometer/gyroscope) for head motions (see Figure 9) [1]. MEME streams the sensor data over bluetooth LE to a laptop or smartphone with a sampling rate of over 100 Hz for the EOG data. Battery runtime for the current prototype is around 8 hours. They weight 32 grams and can be easily confused with normal eye glasses.
We record 4 participants (2 female, mean age 34, std. 15) for 3 days (each 2 x 3 hours per day) with MEME. For sta- bility reasons the recording is done on a small laptop (11”).
1 https://www.jins-jp.com/jinsmeme/en/
The users wear the device. The experiment conductor record when their are reading and the word amount using a custom labeling software on the laptop (start time/end time, word count). The experiment conductor videotapes each run. The users are free to choose texts, reading devices, location and other activities recorded. We have however 2 limitations: (1) the material read must consist mostly of text (no comics etc.) (2) for the free activities we exclude text processing (writ- ing text on paper or computer). We believe we can deal with more free reading material and text processing to some ex- tend, yet it would complicate the word counting (especially ground truth recordings) and is left for future work. Activities done by users include: cooking a meal, making coffee, pho- tocopying, ironing clothes. Reading done by users include: novel on iPad outside, newspaper in coffeeshop, textbook in car etc.
RESULTS AND DISCUSSION
In the following, we present the analysis results for the dif- ferent data sets. All evaluations are done using the leave-one- out strategy, training on n-1 users, evaluating on 1 user this n times, presenting the average over all as results.
The reading detection performs very well (close to 100 % for all data sets except the MEME Use Case see later). Therefore, we will not discuss it further.
Optical Eye Tracking
Figure 8 depicts the HDS histograms for the different reading devices of the device experiment. Our line break detection method decreases the error from 15% to 6% for the Baseline Dataset and from 62% to 8% for the Devices Dataset compar- ing it with the algorithm of the previous work.
     Word Count Estimation
Method
Time
Previous Method[35]
Basic Word Count (average words * lines) Support Vector Regression Word Count
Baseline Exp
22% 11% 9% 8%
Devices Exp
32% 45% 8% 17%
   Table 2: Overview about the word count estimation error for different methods
Table 3 summarizes the results for estimating the words read for the different data sets. We see that the most basic estima- tion using Reading detection and Time has an error of 22 % or 32%. The previous method by Kunze et al. performs better in the Baseline Experiment, yet worse in the Devices Experi- ment, as it cannot cope well with changes in line length. The Basic Word Count using average words per line performs best overall with 9% and 8%. Support Vector Regression has trou- ble with the Devices Experiment with a 17% error rate. This is mostly due to the very diverse English skills of the subjects participating in this experiment.
The standard deviation of the error rate is 3% for baseline 4% for devices experiment. Compared to a std of 15 % and 35% of the previous method and 22% and 45% for using time only. The improvement is also significant shown by an F-Test between the error rates of the Kunze et al. method and the current implementation (p<0.05, F(4,6)= 5.14).
 92
SESSION: QUANTIFYING AND COMMUNICATING THROUGH WEARABLES
     (a) (b) (c) (d) (e)
Figure 8: Horizontal Saccade Direction (HSD) Histograms for different reading devices: an e-ink reader (a), computer screen (b), tablet computer (c), smartphone (d) and a sheet of paper (e).
Reading on the Computer Screen has by far the highest error. This is reasonable, as there are very few, long lines (miss- ing one line or detecting an additional line increases the error significantly) and the reading is very unnatural. Users tend to move their head a lot while reading. The second worst device is the smartphone, as in this case the users also moved a lot their head and the device.
Electrooculography
For the EOG, we compare our inference to a word count es- timate derived from a perfect reading detection system for baseline (most research in the related work focuses on it). We estimate the number of words a person was reading just based on time and compare this to our system.
3 hours of the total recordings. Removing these 3 hours we get to 89% precision. The SVR word count is at 20 % error rate comparing it to 30 % on the data using perfect reading detection and time only.
Improvements and Limitations
A major problem not addressed in previous work are chang- ing line lengths. Therefore we see the dynamic line-break detection as one of our major contributions for this work, ap- plicable to wide variety of eye movement data. We got also rid of the more direction criterion introduced by Kunze et. al (the direction of the saccade must be slightly downwards). This makes the line break detection is robust against ”swiping while reading” e.g. on a tablet. Rereading some words can be also tolerated to a certain extend (as long as it is only half the width). However, if there are short lines (again smaller than half the width of a regular line), our algorithm fails to detect them. This trade-off can be adjusted by the line break thresh- old, if we decrease it smaller to half the size of a normal line break saccade, the effect of re-reading on the word count is stronger yet shorter lines can still be detected.
Given the respective 17% or 8% of total error from our exper- iment, we try to assess the question: Is our method accurate enough to quantify reading habits in a sensible way for users? This is difficult to address without a system implementation and long term study. Yet, when we compare the performance to physical activity tracking we can find an answer. We as- sume that the tracking accuracy for inducing physical and mental behavior changes are equivalent. In most controlled experimental setups, step counters show an error rate of 5- 10%, the error can increase rapidly in real-life scenarios to up to 20 - 35%[47]. Still they are regarded as a effective measure to log activity and motivate people to become more active. Therefore, we believe the perceived error of 8% to 17% is enough to motivate users to read more. Comparing our method to step counting might be controversial, as we are dealing with a cognitive task and not physical activity. How- ever, studies in learning and reading have shown that learners who track their work manually feel encouraged and acquire faster and better reading skills [27].
Of course, larger scale studies are needed to validate the ac- curacy of our method for more real-life datasets (e.g. includ- ing reading in different situations, not only an office envi- ronment). Also, long term studies have to show if cognitive
 Method
Time Baseline Static Word Count SVR Word Count
Error Rate STD
31% 9% 13% 3% 5% 0.2%
  Table 3: Overview of the word count estimation error and standard deviation of the error for different methods. First the baseline just using the time a user read a text, second is a static word number times the detected line-breaks, third a SVR based on Line Break features alone, and last a SVR based on Line Break and Line features.
For the Line break detection we have an error of 5%, std 1.2%. The summary of the results can be found in Table3. The static word count method already performs with around half the error of the time baseline (13%).
Comparing optical tracking with EOG, it the EOG system performs better than the mobile eye tracker. Yet, this is ex- pected. Our word count method focuses on line break detec- tion saccades and the EOG is usually preferred for saccade detection as it can sample higher, in our case 1k Hz compared to 60 Hz for the SMI eye tracker. Additionally, the advantage of the optical system (gaze coordinates not only eye move- ment data) are not relevant for our application case.
MEME Case Study
For reading detection, recall is 87% precision is only at 79 %, yet looking into the data, one particular prototype shows a very noisy EOG signal (additional frequencies around 20 - 50 Hz). The errors happen exactly with this prototype for
93
activity tracking is as motivating as physical activity tracking given the same level of accuracy.
Currently, the word count estimation is done in batch process- ing on previously recorded eye gaze data. In principle, all al- gorithm steps work online (given the 3 second delay for the reading detection). The dynamic line break detection is more problematic, as it requires a couple of line breaks to work and with fewer line breaks the results might not be robust enough. This should be investigated in future work. The computa- tional complexity of the methods is quite low. The hardest is the SVR training phase, gaze filtering, line break detection and classification can be done in polynomial time. Taking the line break detection into account, a robust word count esti- mation could be done in a least minute interval maybe even shorter.
CONCLUSIONS AND FUTURE WORK
Figure 10: Mockup of a reading service using our approach and a user wearing J!NS MEME prototype, smart glasses with EOG.
We presented our work towards tracking how much a user reads, enabling quantified feedback about reading volume. We show an word count estimation algorithm that works with 8% (using some kind of document identification) or 17% just on eye gaze only, user-independent evaluated on 2 experi- ments with a total of 19 users. In addition we evaluate our method also on an EOG recording.
Larger Scale Experiments-
An obvious extension to our current work is to include more participants, increase the device diversity and also change font sizes. We expect that the dynamic line break detection algorithm should also work in these situations. Yet, espe- cially we need to record more data with native speakers as the results show the estimates for them are not so good. This indicates that reading skill level impacts the word estimation method strongly. This raises the question if we can use this to our advantage and estimate the reading skills using eye gaze features.
General Cognitive Activity Tracking using Visual Behavior-
As mentioned, just the saccade features used for reading de- tection are enough to separate the different other activities we recorded (e.g. watching video, playing games, ...) reasonably well (71%). We should explore how far visual behavior can be used to distinguish and detect various cognitive activities.
Extending over Document Types-
So far we only use texts from reading comprehension sections of standardized tests. This gives us so far the ability to assess the reading skill of the user (and ensure the user actually read the text and is not only faking reading). Other document types could also be recognized using information about text layout, this in turn can help to improve the word count estimation.
Word Count Estimation for Everybody-
To enable word count estimation for a larger population, we need to port the system to one of the hopefully soon avail- able commercial EOG devices or cheap DIY eye trackers. Smart eye glasses seem to be the best platform to achieve this goal [1]. We plan on implementing our system on J!NS MEME (see Figure 10). We expect that the algorithm can work with slight adjustments, given a good EOG signal qual- ity.
ACKNOWLEDGEMENTS
This work is supported in part by the JST CREST project ”Reading Life Log”. Part of this work was envisioned at the Dagstuhl Seminar ”Augmenting Human Memory” No. 14362.
REFERENCES
1. Amft, O., Wahl, F., Ishimaru, S., and Kunze, K. Making regular eyeglasses smart. Pervasive Computing, IEEE 14, 3 (2015), 32–43.
2. Biedert, R., Buscher, G., and Dengel, A. The eyebook–using eye tracking to enhance the reading experience. Informatik-Spektrum 33, 3 (2010), 272–281.
3. Biedert, R., Buscher, G., Schwarz, S., Hees, J., and Dengel, A. Text 2.0. In Ext. Abs. CHI 2010 (2010), 4003–4008.
4. Biedert, R., Dengel, A., Elshamy, M., and Buscher, G. Towards robust gaze-based objective quality measures for text. In Proc. ETRA 2012 (2012), 201–204.
5. Biedert, R., Hees, J., Dengel, A., and Buscher, G. A robust realtime reading-skimming classifier. In Proc. ETRA 2012 (2012), 123–130.
UBICOMP '15, SEPTEMBER 7–11, 2015, OSAKA, JAPAN
  94
SESSION: QUANTIFYING AND COMMUNICATING THROUGH WEARABLES
6. Bravata, D. M., and Smith-Spangler. Using pedometers to increase physical activity and improve health. The journal of the American Medical Association 298, 19 (2007), 2296–2304.
7. Bulling, A., and Roggen, D. Recognition of visual memory recall processes using eye movement analysis. In Proceedings of the 13th international conference on Ubiquitous computing, ACM (2011), 455–464.
8. Bulling, A., Ward, J., Gellersen, H., and Troster, G. Eye movement analysis for activity recognition using electrooculography. Pattern Analysis and Machine Intelligence, IEEE Transactions on 33, 4 (2011), 741–753.
9. Bulling, A., Ward, J. A., and Gellersen, H. Multimodal Recognition of Reading Activity in Transit Using Body-Worn Sensors. ACM Trans. on Applied Perception 9, 1 (2012), 2:1–2:21.
10. Bulling, A., Ward, J. A., Gellersen, H., and Tro ̈ster, G. Eye Movement Analysis for Activity Recognition Using Electrooculography. IEEE Trans. on Pattern Analysis and Machine Intelligence 33, 4 (Apr. 2011), 741–753.
11. Bulling, A., and Zander, T. O. Cognition-aware computing. Pervasive Computing, IEEE 13, 3 (2014), 80–83.
12. Buscher, G., and Dengel, A. Gaze-based filtering of relevant document segments. In WSRSP Workshop, vol. 9 (2009), 20–24.
13. Buscher, G., Dengel, A., and van Elst, L. Eye movements as implicit relevance feedback. In Ext. Abs. CHI 2008 (2008), 2991–2996.
14. Buscher, G., Dengel, A., van Elst, L., and Mittag, F. Generating and using gaze-based document annotations. In CHI ’08 Extended Abstracts on Human Factors in Computing Systems, CHI EA ’08, ACM (New York, NY, USA, 2008), 3045–3050.
15. Carver, R. P. Sense and nonsense in speed reading. Revrac Publications, 1971.
16. Clarke, A. R., Barry, R. J., McCarthy, R., and Selikowitz, M. Eeg analysis of children with attention-deficit/hyperactivity disorder and comorbid reading disabilities. Journal of Learning Disabilities (2002), 276–285.
17. Cunningham, A., and Stanovich, K. What reading does for the mind. Journal of Direct Instruction 1, 2 (2001), 137–149.
18. Dimigen, O., Sommer, W., Hohlfeld, A., Jacobs, A., and Kliegl, R. Coregistration of eye movements and eeg in natural reading: analyses and review. Journal of Experimental Psychology: General 140, 4 (2011), 552.
19. Ehrlich, E. Opinions differ on speed reading. NEA Journal 52 (1963), 43–44.
20. Ferstl, E. C., Neumann, J., Bogler, C., and von Cramon, D. Y. The extended language network: A meta-analysis of neuroimaging studies on text comprehension. Human Brain Mapping (2008), 581–593.
21. Genesee, F., Lindholm-Leary, K., Saunders, W., and Christian, D. English language learners in us schools: An overview of research findings. Journal of Education for Students Placed at Risk 10, 4 (2005), 363–385.
22. Gevins, A., Smith, M. E., Leong, H., McEvoy, L., Whitfield, S., Du, R., and Rush, G. Monitoring working memory load during computer-based tasks with eeg pattern recognition methods. Human Factors: The Journal of the Human Factors and Ergonomics Society 40, 1 (1998), 79–91.
23. Gog, T. v., Kester, L., Nievelstein, F., Giesbers, B., and Paas, F. Uncovering cognitive processes: Different techniques that can contribute to cognitive load research and instruction. Computers in Human Behavior 25, 2 (2009), 325–331.
24. Graf, R. G. Speed reading: Remember the tortoise. Psychology Today 7, 7 (1973), 112–13.
25. Halpern, D. F. Teaching for critical thinking: Helping college students develop the skills and dispositions of a critical thinker. New directions for teaching and learning 1999, 80 (1999), 69–74.
26. Hansen, D. M. A discourse structure analysis of the comprehension of rapid readers.
27. Hattie, J. Visible learning: A synthesis of over 800 meta-analyses relating to achievement. Routledge, 2013.
28. Iqbal, S. T., Zheng, X. S., and Bailey, B. P. Task-evoked pupillary response to mental workload in human-computer interaction. In CHI’04 extended abstracts on Human factors in computing systems, ACM (2004), 1477–1480.
29. Irwin, D. E. Fixation location and fixation duration as indices of cognitive processing. The interface of language, vision, and action: Eye movements and the visual world (2004), 105–134.
30. Iwamura, M., Nakai, T., and Kise, K. Improvement of retrieval speed and required amount of memory for geometric hashing by combining local invariants. Proc. of BMVC2007 (2007), 1010–1019.
31. Just, M. A., and Carpenter, P. A. The psychology of reading and language comprehension. Allyn & Bacon, 1987.
32. Kliegl, R., Nuthmann, A., and Engbert, R. Tracking the mind during reading: the influence of past, present, and future words on fixation durations. Journal of Experimental Psychology: General; Journal of Experimental Psychology: General 135, 1 (2006), 12.
33. Kunze, K., Katsutoshi, M., Uema, Y., and Inami, M. How much do you read? counting the number of words a user reads using electrooculography. In Proceedings of
95
the 5th Augmented Human International Conference, ACM (2015), 150–153.
34. Kunze, K., Kawaichi, H., Yoshimura, K., and Kise, K. Towards inferring language expertise using eye tracking. In Ext. Abs. CHI 2013 (2013), 4015–4021.
35. Kunze, K., Kawaichi, H., Yoshimura, K., and Kise, K. The wordmeter – estimating the number of words read using document image retrieval and mobile eye tracking. In Proc. ICDAR 2013 (2013).
36. Kunze, K., Yuki, S., Ishimaru, S., and Kise, K. Reading activity recognition using an off-the-shelf eeg. In Proc. ICDAR 2013 (2013).
37. Mart ́ınez-Go ́mez, P., and Aizawa, A. Recognition of understanding level and language skill using measurements of reading behavior. In Proceedings of the 19th international conference on Intelligent User Interfaces, ACM (2014), 95–104.
38. Moore, D. W., Alvermann, D. E., and Hinchman, K. A.
Struggling Adolescent Readers: A Collection of Teaching Strategies. ERIC, 2000.
39. Nakai, T., Kise, K., and Iwamura, M. Use of affine invariants in locally likely arrangement hashing for camera-based document image retrieval. In Proc. of DAS 2006 3872 (Feb. 2006), 541–552.
40. Orchard, L. N., and Stern, J. A. Blinks as an index of cognitive activity during reading. Integrative Physiological and Behavioral Science 26, 2 (1991), 108–116.
41. Patterson, K. E., Graham, N., and Hodges, J. R. Reading in dementia of the alzheimer type: A preserved ability? Neuropsychology 8, 3 (1994), 395.
42. Rashotte, C. A., and Torgesen, J. K. Repeated reading and reading fluency in learning disabled children. Reading Research Quarterly (1985), 180–188.
43. Rayner, K. Eye movements in reading and information processing: 20 years of research. Psychological bulletin 124, 3 (1998), 372.
44. Rudmann, D. S., McConkie, G. W., and Zheng, X. S. Eyetracking in cognitive state detection for hci. In Proceedings of the 5th International Conference on Multimodal Interfaces, ICMI ’03, ACM (New York, NY, USA, 2003), 159–163.
45. Spache, G. D. Is this a breakthrough in reading? The Reading Teacher 15, 4 (1962), 258–263.
46. Strait, M., Canning, C., and Scheutz, M. Reliability of nirs-based bcis: a placebo-controlled replication and reanalysis of brainput. In CHI’14 Extended Abstracts on Human Factors in Computing Systems, ACM (2014), 619–630.
47. Swartz, A., Bassett Jr, D., Moore, J., Thompson, D., and Strath, S. Effects of body mass index on the accuracy of an electronic pedometer. International journal of sports medicine 24, 08 (2003), 588–592.
48. Tsunashima, H., Yanagisawa, K., and Iwadate, M. Measurement of brain function using near-infrared spectroscopy (nirs), 2012.
49. Xu, S., Jiang, H., and Lau, F. C. User-oriented document summarization through vision-based eye-tracking. In Proc of IUI, IUI ’09 (2009), 7–16.
96
UBICOMP '15, SEPTEMBER 7–11, 2015, OSAKA, JAPAN
Reading and Estimating Gaze on Smart Phones
Ralf Biedert∗ Georg Buscher‡ Andreas Dengel† Microsoft Bing
German Research Center for Artificial Intelligence (DFKI)
Arman Vartan§ Technical University Kaiserslautern
 Abstract
While lots of reading happens on mobile devices, little research has been performed on how the reading-interaction actually takes place. Therefore we describe our findings on a study conducted with 18 users which were asked to read a number of texts while their touch and gaze data was being recorded. We found three reader types and identified their preferred alignment of text on the screen. Based on our findings we are able to computationally estimate the reading area with an approximate .81 precision and .89 recall. Our computed reading speed estimate has an average 10.9% wpm error in contrast to the measured speed, and combining both techniques we can pinpoint the reading location at a given time with an overall word error of 9.26 words, or about three lines of text on our device.
CR Categories: H.5.2 [Information Systems]: User Interfaces— Evaluation/methodology;
Keywords: touch,smartphone,reading,eyetracking 1 Introduction
Recently the sales of digital books began to surpass the number of paper sales [Miller and Bosman 2011] and probably this trend will not reverse. While reading and interacting with web sites or PDFs on desktop PCs has become very common, the trend to mo- bile reading devices is quite new. Tablets, cell phones and dedi- cated ebook readers appear to become the device-of-choice, and a variety of them manifested during the last months and years. With them arrive new interaction paradigms [Shneiderman 1991], and traditional keyboard-mouse interaction is more and more replaced by small, touch-sensitive fullscreen reading devices. At the same time, traditional eye tracking and reading research [Rayner 1998] has shown that by taking into account gaze and interaction data and putting them into relation with the displayed content, there is a considerable potential for improving human-computer interaction techniques [Biedert et al. 2010b][Buscher 2010]. There have been interaction [Drewes et al. 2007] and reading [Oquist and Lundin 2007] studies on traditional cell phones and we believe on top of that touch interface with smoothly scrollable screens offer some unique insights and interaction possibilities. For this paper our goals are therefore twofold. First we want to explore how the read- ing interaction usually takes place on these devices, especially in terms of eye movements and touch behavior. Second we want to investigate to what extent we can approximate the reader’s current focus of attention (i.e., the read text) by analyzing the available in- puts. Such an approximation can enable us to provide eyeBook
∗e-mail: ralf.biedert@dfki.de †e-mail: andreas.dengel@dfki.de ‡e-mail: georgbu@microsoft.com §e-mail: arman.vartan@gmail.com
Copyright © 2012 by the Association for Computing Machinery, Inc.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1 (212) 869-0481 or e-mail permissions@acm.org.
ETRA 2012, Santa Barbara, CA, March 28 – 30, 2012. © 2012 ACM 978-1-4503-1225-7/12/0003 $10.00
Figure 1: A user reading texts on a mobile phone while being eye tracked. While he reads, the current touch and gaze information is being recorded as well as the screen’s content for a later analy- sis. Notice that the tracking device is flipped and all calibration is performed directly on the smart phone.
[Biedert et al. 2010a] like ambient reading effects on existing hard- ware without the need for expensive eye tracking equipment. If properly estimated interaction data is being considered for many users, it also could give valuable insights on possible problematic passages within the text [Biedert et al. 2012].
2 Setup
In order to correlate eye tracking data with scroll and touch infor- mation we need to integrate an eye tracking device into our interac- tion scenario. For the purpose of our study we use a Nexus One as the actual presentation device, which contains a 480x800px display with a screen diagonal of 94mm. It also has a capacitive touch sen- sor, a built-in HTML rendering engine and WiFi networking facili- ties, and we rely on all of them for the creation of our experimental application. For the purpose of our study the devices is fixed on a table and used in portrait mode.
As the tracking device we use a Tobii X120 unit which we inte- grated in a novel setup, allowing us to calibrate and record eye tracking data without the need of external cameras. For that we mount the unit head-down (compare Figure 1) to ensure that it can properly track the user’s eye which when looking at the device which is placed below the tracker. Furthermore, the incoming gaze data needs then to be post-processed to match the flipped order of axes.
3 Experiment
Using the setup described in the previous section we conducted a user study to investigate how people actually interact with mobile devices. We drafted 18 students from the local university, 13 male, 5 female, all aged between 18 and 28 years, most of them were
  385
 computer science students and all of them had German as their na- tive language. We also surveyed the user for previous smartphone experience and 8 users reported they had used or do use such de- vices, and all were given time to familiarize with the device.
The participants were told to participate in a reading comprehen- sion study. We asked them to read three HTML documents on the topics of cosmetics (D1), biology (D2) and gardening (D3), excerpts from ’excellent’ German Wikipedia articles, and answer a number of questions afterwards. The texts were about 530 words each, with different paragraph lengths on average (9, 12 and 15 screen lines per paragraph, respectively). The whole device could display approxi- mately 15 lines of text and a single line contained approximately 5 words.
After the instructions were given the users familiarized themselves with the device. Eventually the eye tracker was calibrated and the actual experiment started with the documents presented in random order.
4 Evaluation
Using the data acquired in the experiment we start by analyzing a number of principal questions and eventually investigate how well the true reading position can be estimated by heuristics based on touching and scrolling behavior.
4.1 General Definitions
On the display of the device, a document Di is presented to the reader. More specifically, at each moment t ∈ T during the docu- ment interaction time T only a part of the document p(t) is visible on the screen. There are two major ways to represent the viewport (the visible area of the document): a pixel-based variant that maps to two y-coordinates (y and y + 800), and a character-based variant that maps to two character offsets (o1 and o2, the offsets that were fully visible in the upper left and lower right part of the screen at t), counted from the start of the text. With the help of p we segment our recordings into two classes, namely reading phases and a scrolling phases. As reading we consider phases ri ⊆ T with p′(ri) = {0} (i.e., the first derivation of p) and ∆ri > 2s, i.e., where the content of the screen stood still for more than 2 seconds (a value empirically found when analyzing the recordings). We consider each block of time that does not form a reading phase as a scrolling phase si, and for each document interaction timeline we now have a partition into the set R of all reading phases and a set S of all scrolling phases. In addition to the scroll movements we also recorded gaze data g(t) and touch data f (t), and similar to p their values can be interpreted as raw screen position or character offsets in the document.
4.2 General Behavior and Gaze Distribution
We start by describing the overall distribution of reading areas. Since previous research has indicated that favored reading areas might exist on desktop screens [Buscher et al. 2010] the main ques- tion in this part is whether they also exist on small screens where only a very limited number of lines can be displayed at a time.
Visually inspecting the overall scrolling behavior (compare Figure 2 for a general overview) we noticed three general classes of read- ers. Four of our readers employed a reading pattern that is mostly page-wise, i.e., they read one page more or less completely and then scroll so that all the screen’s content is replaced with new text. In contrast, four others exhibited mainly line by line reading behav- ior in which they tend to focus on a single or very few lines on the screen. They scroll almost constantly to keep new information flow- ing into that preferred area. The majority of our users (10) however preferred mostly blockwise scrolling, in which they changed only parts of the screen with each scrolling phase.
0
100
200
300
400
500
600
700
800
0 100
200 300 400
X Position
Y Position
386
Figure 2: Heat map with the general distribution of gaze data for all users and all documents. Although no top or bottom bar were present not the full height of the screen was generally being used.
It should be noted that these modes are not strict. Individually also a mixing of different behaviors can be observed, such as when a para- graph does not fit into the entire screen it is read on a line-by-line basis, and when another can be fit again, the reading pattern changes back to a blockwise mode. When looking only at the mostly non- fullscreen readers and investigating their average gaze distribution over the whole document, preferred reading regions can also be ob- served here. While the average upper placement position was rang- ing between 8% and 15% screen height we could notice that with an increase of average paragraph length the reading bounds shifted outwards. It appears that this is caused by the general preference of these readers to align paragraphs so that they can be read in whole.
4.3 Correlation of Touch and Gaze
We also investigate to what extent gaze and touch behavior cor- relate. The main question is whether the touch-down or touch-up position, i.e., the vertical position where the finger touched or left the screen for a swipe, related directly to the reading bounds. From a coarse view the test group can be separated in a small group of 3 people out of 18 which used a dedicated portion of the screen for short and more frequent scrolling. The remaining users usually used the full length of the screen. In addition there is the obvi- ous finding that all right handed persons used the right side of the screen, while all left handed persons used the left side of the screen. Overall each participant had an individual swipe pattern which was uniquely distinguishable from the other patterns.
The first analysis indicates that the majority performed their touch movements within the middle 80% of their individual reading area with a noticeable scatter increasing from the touch-up (μ = 448px) and touch-down (μ = 152px) region. For further investigation the Pearson correlation coefficient between the measurement parame- ters were calculated resulting in a very low correlation of r = .094 for the touch-down position and the bottom bound of the reading area. The same applied to the touch-up position and the top bound of the reading area r = .164.
4.4 Locating the Reading Area
In order to estimate the actual reading position we first have to de- termine which part of the screen we actually consider for its com- putation. We base our algorithm on two key observations: First, the non-fullscreen users mostly seem to ignore the upper and lower parts of the screen in general. Second, users instead tend to align paragraphs so that they can be read in total. We combine both ob- servations into our area estimator a(ri). Given a reading slice ri we analyze the document content within p(ri) and locate the paragraph with the start location most proximate to y = 120px (which is equal to 15% screen height). If the entire paragraph fits onto the screen, we assume its bounds to be the reading area, otherwise we assume the remaining part of the screen to be the reading area.
In order to evaluate this heuristic, we excluded the data of full screen readers (for which the bounds are obviously known, and which can be detected automatically since their p(ri) results are non-overlapping) and line-wise readers (which we deemed to be too difficult and which can probably be detected automatically by an analysis of their ∆ri : ∆si+1 ratios) we considered the remaining blockwise readers that had sufficiently good eye tracking (7 total1) data. For these we compared the computed regions with the ac- tually measured gaze regions as defined by the highest and lowest fixation in g(ri). The results of this analysis can be seen in Figure 3 in which we plotted the precision and recall values of the area of the true reading regions compared against the estimated reading regions. Overall we achieve a precision of .81 and a recall of .89 for the true reading regions. A precision of 1.0 means that all of the estimated reading area was actually read, a recall of 1.0 that all of the area that really was used for reading was also being detected as such.
4.5 Estimating the Reading Speed
The main assumption in the computation of the reading speed is that it is approximately constant within one ri slice. While in reality the fixation and saccade pattern is influenced by many factors, the actual slices are our atomic observation unit and therefore we take the a priori assumption that the time spent in each part of it is evenly distributed. Thus, for the estimation of the reading speed of given slice ri we considered the amount of text available within a(ri) and the time ∆ri this text was available. This is simply the number of words within that area divided by the time taken for the area, v∗(ri) = a(ri) : ∆ri
For estimating reading speed, we measured how many words were presented between the first and last fixations of a given reading slice and also the time the slice was displayed. Comparing both readings we can observe an average error rate of about 17.6% in terms of words per minute. Overall we measured real average read- ing speeds in the range from 174 wpm to 272 wpm per session2, and the errors reported for the individual users varied quite widely. However, when comparing the relative errors our algorithm pro- duced between the three documents, we could not find a significant difference (p = .55) on a Kruskal-Wallis test. In general the com- putation appears to have the slight tendency to underestimate the true reading speed.
 0.6
0.5
0.4
0.3
0.2
0.1
0.0
1234567
User
                                                                1.0 0.9 0.8 0.7 0.6 0.5 0.4
0.3
0.0 0.2
0.4 0.6 Precision
0.8 1.0
                                                                                                                  Figure 3: Precision-recall map of the computed reading areas, in contrast to the real reading areas for all users and all documents.
1Apparently our number of dropouts (3 of effectively 10) is approxi- mately in the range of the numbers reported in [Oquist and Lundin 2007], which was 4 of 16. We considered data to be good when it could be matched to the text in most cases.
Figure 4: Relative errors when estimating the current speed per user. While the inter-document differences were not significant, the differences between users vary quite widely.
In terms of realtime computation the presented numbers can only be computed for the previous slice since they require knowledge for how long the slice has been presented. Thus we estimate the most likely current speed to be the moving average of all previously observed slices, hence vi =  { v∗(rj) } for all j < i . Using vi instead of v∗(ri) for a comparison against the actual reading speed yields an even better estimate and achieved a 10.9% total average wpm error, compare Figure 4.
4.6 Pinpointing the Reading Position
Based on the estimation of the reading bounds and the approx- imation of the current reading speed, we eventually try to esti- mate the most likely reading location l for a given time. Taking the currently displayed reading segment ri we measure the time ∆t elapsed since its start and compute an offset by multiplying it
2Our recorded reading speeds are above the reported speeds of 178 wpm mentioned in [Oquist and Lundin 2007] for scrolling interfaces which can probably be attributed to the differences in content (sci-fi prose vs. encyclo- pedic articles), the language (Swedish vs. German) and the way the inter- faces are operated and sized (small key-operated cell phone vs. relatively large touch operated smart phone)
  387
Recall
Relative Error
 50 40 30 20 10
0
D1 D2 D3
Estimation Error [words]
0 200
400 600
800 1000
Time [fixation]
1200 1400
1600
Figure 5: Visualization of the data on all documents and all block-wise readers we were able to extract proper reading positions from. The x-axis reflects the nth fixation, i.e., the time. The y-axis shows how many words the computed position was away from the real position. The black line reflects the average error over all users and documents. It can be seen that, except for some outlines within two exceptionally long document passes, there is no obvious trend for the accuracy to degrade over time. If there were a general user drift, the average should increase within the latter parts.
with the average speed as described in the previous chapter, i.e., l(ri + ∆t) = a(ri) + vi∆t.
To evaluate the quality of this approach we compare the computed position l with our actual eye tracking data g. We consider all fix- ation events that occurred during reading slices ri and for each fix- ation we compare the computed word offset against the true word offset in the document. The overall average word error we achieve is 9.26 words, with a standard deviation of 8.90 words. Investigat- ing the overall error over time, compare Figure 5, there appears to be no obvious trend for the algorithm to degrade in performance over time. While individual measures can be far off (which can be caused, for example, by accidental saccades to the other end of the text) the average error after the nth fixation of runtime remains at around 10 words. We also investigated the individual users and doc- uments. The lowest average error we observed was 6.49 words for one user and one document, the highest average error 16.02 words, with standard deviations ranging from 4.39 to 10.67 words. Ex- pressed in lines, our prediction was, on average, about 3 lines off from the true reading position.
5 Conclusion & Outlook
We implemented a novel setup and investigated how text interaction and reading are performed on a mobile touch screen device. Invit- ing 18 users we had them read a number of documents and recorded their gaze and touch behavior. We categorize our readers into three types (full screen, linewise and blockwise) and find that blockwise readers tend to align the read paragraph so that it fits on screen in its entirety. Measuring their scrolling speed and modeling our find- ings in an algorithm we are able to pinpoint their reading position with an average accuracy of 10 words. There are also a number of open questions. For example it is unclear how stable our clas- sification into three reader groups actually is, whether it changes over sessions, days or weeks, and if it changes or converges with smartphone or touchscreen experience. Likewise the fit-to-screen strategy could be verified in an independent experiment. Lastly we can imagine that, although we could not find any obvious relation in reading and touch positions in the scope of our experiment (be- yond as a means for scrolling), there are some relations waiting to be uncovered.
References
Biedert, R., Buscher, G., and Dengel, A. 2010. The eyeBook - Us- ing eye tracking to enhance the reading experience. Informatik- Spektrum 33, 3 (June), 272–281.
Biedert, R., Buscher, G., Schwarz, S., Hees, J., and Dengel, A. 2010. Text 2.0. CHI EA ’10: Proceedings of the 28th of the international conference extended abstracts on Human factors in computing systems (Apr.), 4003–4008.
Biedert, R., Hosseiny, M., Georg, B., and Dengel, A. 2012. To- wards Robust Gaze-Based Objective Quality Measures for Text. In Seventh ACM Symposium on Eye Tracking Research & Appli- cations (ETRA), German Research Center for Artificial Intelli- gence.
Buscher, G., Biedert, R., Heinesch, D., and Dengel, A. 2010. Eye tracking analysis of preferred reading regions on the screen. In CHI EA ’10: Proceedings of the 28th of the international confer- ence extended abstracts on Human factors in computing systems, New York, NY, USA, 3307–3312.
Buscher, G. 2010. Attention-Based Information Retrieval. PhD thesis, University Kaiserslautern, Kaiserslautern.
Drewes, H., De Luca, A., and Schmidt, A. 2007. Eye-gaze inter- action for mobile phones. Proceedings of the 4th international conference on mobile technology, applications, and systems and the 1st international symposium on Computer human interaction in mobile technology, 364–371.
Miller, C. C., and Bosman, J. 2011. E-Books Outsell Print Books at Amazon. New York Times (May), B2.
Oquist, G., and Lundin, K. 2007. Eye movement study of read- ing text on a mobile phone using paging, scrolling, leading, and RSVP. Proceedings of the 6th international conference on Mo- bile and ubiquitous multimedia, 176–183.
Rayner, K. 1998. Eye movements in reading and information pro- cessing: 20 years of research. Psychological Bulletin 124, 3, 372–422.
Shneiderman, B. 1991. Touch screens now offer compelling uses. Software, IEEE 8, 2, 93–94.
IUI 2014 • Learning and Skills February 24-27, 2014, Haifa, Israel
Recognition of Understanding Level and Language Skill using Measurements of Reading Behavior
Pascual Mart ́ınez-Go ́mez
The University of Tokyo National Institute of Informatics pascual@nii.ac.jp
ABSTRACT
The reading act is an intimate and elusive process that is important to understand. Psycholinguists have long studied the effects of task, personal or document characteristics on reading behavior. An essential factor in the success of those studies lies in the capability of analyzing eye-movements. These studies aim to recognize causal effects on patterns of eye-movements, by contriving variations in task, personal or document characteristics. In this work, we follow the oppo- site direction. We present a formal framework to recognize reader’s level of understanding and language skill given mea- surements of reading behavior via eye-gaze data. We show significant error reductions to recognize these attributes and provide a detailed study of the most discriminative features.
Author Keywords
Reading behavior; eye-tracking; cognitive and user profiling
ACM Classification Keywords
H.1.2 Information Systems: User/Machine Systems
INTRODUCTION
Reading is a fundamental activity that is not well understood, since it involves complex and unobservable cognitive pro- cesses. The mechanism behind those cognitive processes is determined by personal attributes of the reader such as the native language or background knowledge, and influences reading behavior. The analysis of eye-movements to mea- sure reading behavior has been extensively studied in the psy- cholinguistic literature [48] and has been shown to reflect on- line cognitive processing [30, 39] when reading text. Current eye-trackers allow to record areas of attention unobtrusively, which may reveal personal interests on the content in that text area or individual difficulties to integrate information.
Document characteristics such as lexical difficulty [39], syn- tactic complexity [17] or semantic ambiguity [49] also proved
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
IUI’14, February 24–27, 2014, Haifa, Israel.
Copyright c 2014ACM978-1-4503-2184-6/14/02..$15.00. http://dx.doi.org/10.1145/2557500.2557546
Akiko Aizawa
The University of Tokyo National Institute of Informatics aizawa@nii.ac.jp
to influence reading behavior and their effects were success- fully observed analyzing eye-movements. The common strat- egy was to carefully contrive words or sentences that mani- fest the linguistic variation under study and to observe the effect on fixation times or regression events of readers. Ana- lyzing eye-movements is, however, a challenging task due to variable and systematic errors introduced by eye-trackers [25] and the difficulty to align fixation events to words in a text. For this reason, most studies investigating the influence of linguistic features on reading behavior were limited to iso- lated words or sentences. The Dundee corpus [31] was one of the earliest attempts to enable quantitative analysis of eye- movements in natural reading tasks, where subjects read doc- uments splitted into screens containing five lines of text.
The objectives of these analyses typically follow two direc- tions. The first category consists in integrating measurements of eye-movements across subjects to obtain a general model of reading behavior. Examples of this category are the esti- mation of cognitive cost models [51], recognition of machine translation and text quality [18, 5]. Contrarily, works in the second category aim to model individual differences in read- ing behavior among subjects to discriminate them according to certain personal attributes. Using eye-movements to build individual models contributed to create personalized applica- tions without the need of explicit user feedback, such as rec- ommendation and summarization systems [54, 55], document filtering [12] and query expansion [13].
Our work is motivated by the latter objective, where we aim to recognize task or personal attributes such as language skill or reader’s level of understanding given measurements of eye- movement and characteristics of the document. Thus, we can state our hypothesis as:
Hypothesis: A combination of eye-movement features and document characteristics are predictive of language skill and level of understanding.
The contribution of this work is to propose a formal and gen- eral framework to build recognizers (predictors) of personal and task attributes given measurements of reading behavior via eye-movements and document characteristics. We also provide a detailed study of the most discriminative features to recognize target attributes and characterize readers accord- ing to their patterns of reading behavior.
In the next section, we review similar efforts to recognize per- sonal and task attributes in behavioral disciplines. In Method- ology section, we describe the rationale behind our features
95
IUI 2014 • Learning and Skills
February 24-27, 2014, Haifa, Israel
of eye-movement and linguistic characteristics of documents. Then, we introduce our framework to combine these fea- tures. In Experiments section, we assess the recognition per- formance of our systems, study feature importance and char- acterize readers according to their reading behaviors. We ded- icate Discussion section to discuss the limitations and degree of success achieved by our feature set in this recognition task and the implications of our findings. Finally, a summary of our conclusions can be found in the last section.
RELATED WORK
In human behavioral sciences, researchers strive to under- stand what influences human behavior. Systematic analyses are typically performed via controlled environmental condi- tions or via naturalistic observations. Recent advances in sensor technology allow to measure changes in body tem- perature, galvanic skin fluctuations or superficial brain elec- trical activity, which are signals that help to describe and quantify human behavior. From a computational perspective, there have been recent investigations in integrating these mea- surements to analyze their relationships to external stimuli. In [35], researchers attempted to recognize an intimate human quality such as “affect” using a combination of facial images with prosodic and spectral features from the voice. In [3], they also tried to recognize emotions using other behavior measurements such as brainwaves or mouse movements.
In sociolinguistics, much has been investigated about ob- servable effects of author’s characteristics in written produc- tions [34, 4]. The electronic availability of large collections of text in blogs on internet has also inspired practitioners of computational linguistics to investigate effects of author’s at- tributes in their written productions [50, 1, 11]. They showed that age and gender are reflected as variations in language style and content, and they had different degrees of success in recognizing those author’s attributes. These works were followed by others that used more sophisticated features such as patterns in sequences of Part of Speech (POS) tags [43], or that aimed to recognize other personal attributes such as regional origin or political orientations from Twitter [47]. To recognize age, gender and native language, other forms of modern informal communications were successfully used such as conversation transcripts and e-mail [22] or transcribed telephone conversations [8]. As has been shown, sponta- neous or personal expressions can potentially reflect personal characteristics that would otherwise be difficult to recognize. In [40], researchers attempted to recognize reading perfor- mance, which is a slightly different target attribute in nature since it depends both on the person and the task. They exam- ined atypical pauses in transcriptions of temporally aligned prosodic speech to recognize reading skill. While individual differences may reflect reading performance, reading hesita- tions of a sufficiently large population would reveal actual difficulties associated to the text being read.
Our work shares the objectives described above, in that we aim to recognize a personal attribute such as English language skill, and a task attribute such as level of understanding. We described how human behavior expressions such as written text or transcribed speech has been used to recognize personal
and task attributes. In this work we use measurements on eye-movements when reading, which are a more subtle and unconscious modality of human behavior expression.
There are several factors that influence eye-movements and that have been traditionally investigated. The first factor re- lates to the characteristics of the stimuli, such as linguistic features of the text or objects in an image. The second factor involves task characteristics such as objective (e.g. memo- rize, understand, etc.) or time constraints. Refer to [48] for a careful review. Personal attributes have also proved to in- fluence eye-movements. In [28], subjects from different ages were exposed to emotional stimuli and the researchers ana- lyzed the patterns of eye-movements to discover attentional preference. Differences in patterns of eye-movements have also been reported in the context of autism studies [7].
Eye-movements were also used as implicit feedback in us- ability studies and information foraging [23, 16]. There is a growing interest in the community of information retrieval to understand what personal and task characteristics influ- ence user behavior as reflected by eye-movements [44, 36, 2]. The common approach is to consider eye-movements as a dependent variable of personal or task characteristics and test for significance in eye-movement variability between dif- ferent personal or task conditions. This approach follows a causal reasoning since eye-movements are the result of an in- teraction between the subject and the stimuli. In our work, however, our independent variables are measurements on eye- movements, and we aim to recognize unobservable personal and task latent attributes given these measurements. Although we do not attempt to model the causal relationship between personal and task attributes to eye-movements, we will be able to provide an idea of what are the features of reading behavior that best characterize different personal or task at- tributes from a computational perspective.
Although regularities in eye-movements across subjects have been reported in eye-tracking research [24], we aim to infer individualities from the observation of reading behavior. To our knowledge, there are only two works that attempted to recognize personal or task attributes from measurements of eye-movements in natural reading tasks. In [38], every read- ing session was represented using a fixed-size feature vector where each component contained the fixation time on a cer- tain linguistic feature. They found that a projection of these fixed-size feature vectors onto two or three principal compo- nents led to a separation of subjects according to their docu- ment understanding. Although the idea was promising, they did not provide a formal evaluation nor a hypothesis test to complement their results, due to the small number of subjects (9) in their experimentation. The work in [33] proposed to compute the distribution of fixation time on every word with the intention to infer language ability. However, no evaluation was carried out to measure the success of the approach.
In our work, we aim to recognize document understand- ing and language ability, as we think those attributes might be important for user profiling given measurements of eye- movements in a naturalistic reading environment. To that end, we use a wide array of gaze and linguistic features. The work
96
IUI 2014 • Learning and Skills
February 24-27, 2014, Haifa, Israel
in [29] provides an extensive summary of the most relevant gaze features traditionally used in the eye-tracking literature. We borrow features that are suitable for reading tasks and de- velop some other specific features for this occasion. We pro- vide a formal evaluation of the recognition capabilities to in- fer personal and task latent attributes given measurements of eye-movements from 39 subjects. Then, we study the most relevant features of reading behavior that contribute to the recognition task and characterize readers with different lev- els of understanding using their patterns of reading behavior.
METHODOLOGY
We proceed to describe the latent personal and task attributes that we aim to recognize, and motivate the features that were used as our predictors. Then, we describe our recognition systems and the method to select the most relevant features.
Personal and task latent attributes
We work under the assumption that personal and task at- tributes are reflected in reading behavior. We define personal and task latent attributes as those attributes of the user or the task that cannot be directly observed by our system. In this work, our personal attributes are a subjective and objective measurement of English language skill:
English: Self-reported language ability in English measured in three levels: beginner, intermediate or advanced.
ToE: English language ability as measured by a normalized score in either the Test of English for International Commu- nication (TOEIC) or Test of English as a Foreign Language (TOEFL), scaled to mean = 0 and standard deviation = 1.
The task attribute that we aim to recognize relates to “under- standing”, which depends on the reader and the text:
Understanding: A quantification of document understanding using an exhaustive questionnaire after reading every docu- ment, scaled to mean = 0 and standard deviation = 1.
Binarized understanding: Reading sessions where subjects’ understandings are below the first quartile or above the third quartile are labeled as “low” or “high” level of understanding.
Feature extraction
We divide our features into two groups. The first group con- tains features that are computed solely from gaze data, such as sequence of fixations, saccades or changes in pupil diame- ters. The second group consists of features that combine gaze data and linguistic information, such as the proportion of fix- ation time on prepositions or long words.
Gaze features
Our gaze features are related to fixations, saccades and pupil dilations. Fixations are short periods of time (between 200 and 300 milliseconds), where eyes gaze to a still location and are typically associated to lexical decoding. Saccades are rapid eye-movements used to change the fixation location. Although lexical processes may also occur during saccadic eye-movements [27], saccade length may also be an effective signal to recognize skilled readers in syntactic integration. Pupil dilation has proved to reflect cognitive effort in certain
tasks [45]. However, pupil response may have some delays and obey to conditions other than textual content, such as luminosity or contrast in different screen locations, but they are included in this study due to their potential discrimination power. We collected several statistics of this features.
Number of fixations: Skilled readers have an increased abil- ity of identifying words in the parafoveal region of the gaze point. Thus, the number of fixations per 100 words may re- flect reading skill or education level.
Reading time: Total reading time per 100 words is a rough measure of information processing speed. A long reading time may reflect difficulties in understanding or a careful in- terpretation of the content which may lead to greater compre- hension. Although this feature might be an ambiguous pre- dictor of how well a subject understands a document, it may reflect how familiar the subject is to the topic of the text.
Average fixation time and standard deviation of fixation time: Cognitive effort in early stages of reading such as lexi- cal decoding might be reflected by larger fixation times. Since fixation time has a large variability during a reading session, we compute the average and the standard deviation to obtain some information about the distribution.
Maximum fixation time: Some mental processes might be blocking and prevent the reader to proceed until they are re- solved. We extract the maximum fixation time from every session to account for these potentially blocking situations.
Fixation mean velocity and fixation mean acceleration: Confident readers may produce longer saccades to advance faster along the text. We compute the mean of the first and second derivatives of fixation locations with respect to x and y coordinates, obtaining the fixation mean velocity and accel- eration in x and y coordinates (four features).
Saccade median length: There are different types of sac- cadic eye-movements. Forward saccades are the most com- mon type in sequential reading; regressions are backward sac- cades to previously read content; and return sweeps are used to proceed to the beginning of the next line. The length of saccades may give a sense of reader skill in navigating across the text, but the average of saccade length is dominated by few long return sweeps. Thus, we compute the median sac- cade length to exclude return sweeps from this feature.
Number of regressions: There are multiple possible causes of regressions in natural reading, such as resolving corefer- ences or semantic contradictions. The number of regressions per 100 words may reflect hesitations in integrating new in- formation or other reading difficulties.
Pupil max-min diff: To quantify cognitive effort, we mea- sure the difference between the maximum and minimum di- ameter of the pupil during the reading of a document [45].
Pupil standard deviation: Variations in pupil response to visual stimuli may reveal genuine intention of understanding. Thus, we compute the standard deviation of pupil dilations.
Pupil mean velocity and pupil mean acceleration: The ve- locity and acceleration of changes in the pupil diameter may
97
IUI 2014 • Learning and Skills
February 24-27, 2014, Haifa, Israel
Short description
All uppercase       Tokens with all letters capitalized
Type Binary Lexical     Yes Lexical Yes Lexical Yes Lexical No Lexical/Syntactic No Syntactic Yes Syntactic Yes Syntactic Yes Syntactic No Syntactic No Syntactic No Syntactic No Syntactic No Syntactic Yes Semantic Yes Semantic No Semantic Yes Semantic Yes Semantic Yes Discourse No Discourse No Discourse Yes Physical Yes
Linguistic feature
  Contains uppercase Numbers
Word length Perplexity Prepositions
Verbs
Nouns
Terminal nodes Dependency distance Dependency density FoM
Sentence length Passive
Named entity
Height hypernym General
Academic
Out of Vocabulary Coreference distance Lexical chains Discourse connectors Line feed
Tokens with at least one letter capitalized
Token with at least one digit (e.g. protein name)
Number of syllables
Lexical surprise as given by an N-gram language model
Whether the token is a preposition
Whether the token is a verb
Whether the token is a noun
Ratio of terminal nodes to non-terminal nodes in HPSG parse tree Maximum syntactic dependency distance spanning current token Number of dependencies spanning current token
Figure of merit in building HPSG parse tree for sentence of current token Number of words in sentence of current token
Verbs or verb phrases in passive form
Names of person, location or organization
Quantification of concreteness of term
Tokens appearing in list of general and common words
Tokens appearing in list of academic words
Tokens not appearing in any list of words
Maximum coreference distance spanning current token
Number of active lexical chains at current token
Whether the token is (part of) a discourse connector
Token at the beginning or the end of a line (not sentence)
                                                                  Table 1: Linguistic features used to compute fixation time distribution. Linguistic features were mainly of lexical, syntactic, semantic and discourse nature. About half of the features were binary, while the rest were normalized in the interval [0, 1].
also reflect the intensity of cognitive effort. We compute these two features as the first and second derivative of the pupil di- ameter from the sequence of fixations.
Linguistic features
The assumption behind this type of features is that the pro- portion of fixation duration on certain linguistic features is different among different types of readers. For example, fix- ating 200 milliseconds on a verb might be the typical read- ing behavior of a native reader, but fixating 200 milliseconds on a preposition may reveal language difficulties. There are multiple gaze features that could be measured on different text locations, such as fixation time, number of regressions or pupil dilation. However, gaze features related to regressions or pupil dilations on text locations are difficult to interpret. In the case of regressions, they may occur several words or sentences after their actual cause, when the reader does not expect to recover from processing errors anymore. In the case of pupil dilations, they might be caused by changes in bright- ness in different physical locations. For this reason, our only gaze feature on text locations will be the duration of fixations.
We characterize text locations by the linguistic features they contain. Some linguistic features are binary, such as whether a word is a “verb” or a “preposition”. Other features are quan- tified using real numbers that ultimately can be normalized to be in the range [0, 1], such as “sentence length”. Intuitively, linguistic features that span more than one word or that are very frequent are more likely to accumulate longer fixation times. To account for frequent features and to preserve sim- plicity, all linguistic features are defined at word level, such
that every word in the span of a certain feature is quantified as the whole span1. Let tj be the total amount of fixation time on word wj , and let fi,j be the quantification of linguistic feature i at word wj2. Then, we can estimate the normalized fixation time on linguistic feature i, Ti as:
Pt·f jj i,j
Ti = (Pj tj) · (Pj fi,j) (1)
 Ti will account for the proportion of the total amount of fix- ation time that every linguistic feature i attracts from the reader, and they will be used as predictive signals to recog- nize personal and task latent attributes. The list of linguis- tic features can be found in Table 1. A graphical example of proportions of fixation times on linguistic feature “lexical chains” can be found in Figure 1.
Latent attribute recognition
We can formulate the problem of recognizing personal and task latent attributes as a pattern recognition problem where, given measurements of reading behavior, our objective is to predict the value of the target attributes. Some latent at- tributes such as “ToE” scores or the quantification of “under- standing” are real numbers, and we will perform the recogni- tion in the form of a regression. Other latent attributes such as
1 In the case of the feature “sentence length”, every word has its feature value equal to the length of its sentence.
2 For feature “is noun”, fnoun,j = 1 if wj is a noun.
 98
IUI 2014 • Learning and Skills
February 24-27, 2014, Haifa, Israel
 Figure 1:
lexical chains (green) of a single reader. Larger fixation times are displayed as intense red points. Density of active lexical chains at every token (rectangular shapes) is proportional to intensity of green color. Fixation and text overlaps in yellow.
self-reported English skill (three levels) or “binarized under- standing” take values within a limited set of classes. In this case, the recognition will take the form of a classification.
Formally, let x1 , . . . , xp be the feature measurements of read- ingbehaviordescribedabove,andlety1,...,yq bethevalues of personal and task latent attributes of a subject reading a cer- tain document. The objective is to construct mechanisms fi that are capable to predict attributes yi:
yˆi =fi(x1,...,xp) (2) We measure the degree of success in recognizing latent at-
tribute yi by quantifying errors of its prediction mechanism:
errori = di↵erence(yi, yˆi) (3)
where the difference between the predicted attribute value yˆi and the actual attribute value yi is the arithmetic difference between their numerical values in a regression task, or a 0   1 error if their classes differ in a classification task.
We use two models to build recognition mechanisms of latent attributes, namely Support Vector Machines (SVM) [14, 19] and Random Forests [10]. Both models are capable of per- forming classification and regression, but they are based in substantially different ideas.
Support Vector Machines build a hyper-tube for regression or a separating hyper-plane for classification and apply non- linear transformations to input feature values. Parameters for regression and classification are estimated with the objective of reducing the error on the training set. When estimating the regression or classification parameters, a certain degree C of mispredicted training observations might be allowed,
which controls overfitting and increases performance on un- seen data. Another aspect of Support Vector Machines that has to be decided is the non-linear transformation function of the input feature values. The parameter C an the non-linear transformation function are usually application-dependent.
Random Forests are built by growing a multitude of classifi- cation or regression decision trees with a controlled variance. Variability in the trees is introduced by two techniques that complement each other. The first one consists in growing each tree with a different random subsample of the training set. The second technique consists in using a different num- ber of m randomly selected features to split decision nodes at each tree. Predictions of Random Forests are computed as the mode or average in classification or regression, respectively. The error rate in random forests depends on the correlation between trees and the predictive strength of individual trees. Both factors are controlled by m, which can be estimated well from a disjoint set of training samples at each tree.
In the recognition of latent attributes such as level of under- standing or language skill, some features may contain infor- mation that is useful to recognize the latent attribute, but other features may not be discriminative and simply add noise to the model. In order to estimate the usefulness or importance of each feature j to recognize each attribute yi, we used the permutation test technique of Random Forests. For every tree, we randomly exclude about 1/3 of the training data into a separate subset called out-of-bag samples, and we grow the tree with the remaining 2/3 of the data. For every tree, we compute classification or regression for the out-of-bag sam- ples. Then, we randomly permute the value of feature j for these samples, and compute the new decision at every tree. Importance of feature j can be computed as the difference between the number of correct votes for the original out-of- bag sample and the number of correct votes for the out-of-bag sample with perturbed feature j, normalized by the number of trees. The feature set in our decision functions fi will consist of features with positive importance.
EXPERIMENTS Data
Document characteristics
There were 2 documents on 3 topics (6 in total), about eco- nomics, nutrition and astronomy written in English. Docu- ments contained 22.5 sentences and 469 words on average. The text of each document was displayed entirely on a single 23” screen with a resolution of 1920 ⇥ 1080 px. The font fam- ily of the text was Courier New with a font size of 16px and line height of 30px. The upper left corner of the text was lo- cated at pixels (500, 24) and the bottom right corner at (1400, 980), displayed at a distance of 70 cm from the reader.
We computed proportions of fixation times on linguistic fea- tures from Table 1. Lexical and Part of Speech (POS) fea- tures were averaged at word level to obtain a single estima- tion for every document. As an example, feature “Numbers” denotes the percentage of tokens that contain digits. Named
Fixation times (red) on a quantification of active
99
IUI 2014 • Learning and Skills
February 24-27, 2014, Haifa, Israel
entities, percentage of prepositions, nouns and verbs were ex- tracted using the NLTK toolkit [6]. Word lengths (in sylla- bles) were computed using the CMU pronunciation dictio- nary [52], and word perplexity was computed using Google 5-grams [9] with deleted interpolation. To estimate syntac- tic difficulty [26], we computed maximum dependency den- sities and average distance between dependents, using a de- pendency parser [32]. To estimate phrase-based syntactic dif- ficulty, we computed terminal node to non-terminal node ra- tio, figure of merit of parsing surprise and average number of passive clauses using an HPSG parser [42]. We obtained the height of hypernyms, as a measure of term concreteness, by computing the average distance between lemmas to the most abstract term in WordNet [20]. Features “General”, “Aca- demic” and “Out of Vocabulary” denote the average num- ber of words in the General Word Service List [53], in the Academic Word List [15], or in none of them. Linguistic features related to discourse were the average distance be- tween mentions and their referents, maximum number of ac- tive lexical chains [21] (both computed using a coreference resolver [46]), and the percentage of tokens that are discourse connectors from a hand-crafted list of 279 connectors.
Documents had different text lengths and linguistic charac- teristics. For that purpose, we normalized our measurements, first by document and then across all reading sessions. Thus, all feature values of pure gaze and fixation proportions on lin- guistic features were centered and scaled, to have mean = 0 and standard deviation = 0.
Eye-tracking sessions
We collected fixation times on every word using the eye- tracker Tobii TX300, and used a text-gaze aligner [37] to cor- rect systematic errors introduced by the eye-tracker. No chin- rest was used, to favor naturalistic reading behavior. There were 40 subjects participating in our study, but one of them was discarded due to unrecoverable calibration errors. From a manual inspection of eye-data quality, we found that 20% of sessions displayed an excellent tracking signal and perfect text-gaze alignment. About 70% of sessions had good eye- data quality but displayed text-gaze misalignments in some parts of the screen. The remaining sessions displayed poor tracking signal and strong text-gaze misalignments. Although eye-tracking errors and text-gaze misalignments may influ- ence negatively on the recognition accuracy, we decided to include all data in our experiments since errors are a common factor in naturalistic applications of eye-tracking.
The two documents of each topic were randomly flipped for each reader, and the three topics were also randomly per- muted. A questionnaire was presented in the web browser after reading each document, and subjects answered 8 single and multiple choice questions related to the document. The average duration of a reading and question-answering session was 1 hour, and subjects were compensated with the equiva- lent to 20 US dollars in cash. We collected data from 234 eye- tracking sessions (39 subjects ⇥ 6 documents), which was later splitted into training and test sets following a leaving- one-out cross-validation to evaluate our recognition systems.
There were 12 female and 27 male subjects, with ages ranging from 22 to 65 years (avg. 30 years, std. 8.4). Most subjects were non-native English speakers linked to academia, with varying language skills and background knowledge. All read- ers self-reported on their own English language skill. Thus, our data set for this personal attribute consisted in 234 ob- servations. We had 22 readers reporting on their TOEFL or TOEIC scores, and thus we had 132 reading sessions anno- tated with a normalized score of English language skill. For the recognition of quantified understanding score, all readers were tested with questionnaires after reading each text. Thus, all 234 reading sessions were used as data points for this at- tribute. The binarized version of understanding (“understand- ing bin”) consisted in the first and fourth quartile of quantified understanding scores, corresponding to 127 reading sessions from 38 different readers.
To inform our baseline, we used the prior on the training data for the target attribute. That is, for classification, we obtain the most common class in the training data and use it to pre- dict the class of the test sample. For regression, the base- line computes the average value of the target attribute on the training data (e.g. average understanding score), and uses it to assign such score to the test sample. SVMs used an RBF kernel and the parameter C was estimated from the training data. Random Forests were estimated using 500 trees.
Results
Figure 2 compares system performance to our baseline in a leaving-one-out cross-validation. That is, at every iteration, the reading session of one subject is selected as a test sample and all remaining reading sessions are used as a training set. To avoid overfitting and other biases, all other reading ses- sions of the test subject are removed from the training data.
Random Forests recognized English language skill (ToE) with a statistically significant (p-value = 0.015) lower error than baseline. However, neither SVMs nor Random Forests found patterns in reading behavior that are discriminative of self-reported English skill. To our surprise, the correlation between self-reported English skill and the normalized ToE score is  0.39, which shows considerable differences in our subject’s perceptions of their own English language ability. In the regression task of recognizing the normalized text un- derstanding score, our algorithms did not capture any mean- ingful pattern that helps to predict this task attribute. In the binary classification task of recognizing text understanding level, the systems obtained significant (p-values < 10 6) er- ror reductions when discriminating between readers with high and low levels of understanding. We also computed the t-test significance of error reductions in this task, when comparing to a baseline with 50% of error, and obtained p-values below 4 · 10 4 for both SVM and Random Forest methods. This suggests that recognizing a binarized understanding level is a substantially easier task than its regression counterpart.
Variable importance
Table 2 displays the top 10 most discriminative features for at- tributes that we succeeded to recognize. In general, pure gaze features proved to be more discriminative than fixation time
100
IUI 2014 • Learning and Skills February 24-27, 2014, Haifa, Israel
 Figure 2: Average error in recognizing latent attributes using Baseline, SVMs and Random Forests. Statistically significant error reductions are labeled with their p-values computed using Wilcoxon paired t-test. Our systems had a considerable success when recognizing the binary classification of understanding level (understanding bin), and mild but significant error reduction in recognizing English language skill as measured by a test of English (ToE).
No. ToE 1
2
3
4 5 6 7 8 9 10
understanding bin
fixation average
saccade median length fixation mean acceleration x discourse connectors fixation std
fixation mean velocity x num fixations
reading time
pupil mean acceleration num regressions
In the binary classification task of recognizing the level of understanding, the most discriminative features were “fixa- tion average”, “saccade median length” and “fixation mean acceleration x”, which relate to fixation durations and the dis- tance between consecutive fixations. A discriminative feature of different nature was the proportion of fixation time on “dis- course connectors”, whose importance in text comprehension was already suggested in [41]. The ninth most discrimina- tive feature is related to pupil dilations, which was found to correlate with cognitive effort in memory tasks [45].
Characterization of readers
Studies on variable importance help us to discover what dis- criminative features play an important role in the recognition of reader’s latent attributes. However, such studies fail at giv- ing us information on the actual relationship between each feature and the latent attribute of interest. For brevity and simplicity, we computed the Wilcoxon t-test of differences in feature values between readers with a low and high level of understanding. Results can be observed in Table 3.
Readers that displayed lower level of understanding were characterized by having significantly larger average fixation durations and larger variance on this duration, which might be related to slower word recognition or lexical processing time. Readers with lower level of understanding were also characterized by smaller saccade median length and fixation mean acceleration on the x-coordinates, which may be related to difficulties in syntactic integration and also reflect limited lexical processing in the parafovea. They also fixated longer discourse connectors and had longer overall reading times than subjects with higher level of understanding. Although
  reading time
saccade median length fixation mean acceleration x fixation std
fixation mean velocity y fixation average
num regressions
pupil mean velocity
fixation mean acceleration y num fixations
                      Table 2:
English language skill (ToE) and level of understanding (un- derstanding bin), sorted in descending order of discriminative power. Pure gaze features are in general more discriminative than proportions of fixation time on linguistic features.
on quantifications of linguistic features. In the task of recog- nizing English language skill (ToE), reading time proved to be a very discriminant feature in our laboratory conditions, since subjects were probably well motivated to complete the reading task. However, these conditions may not hold when users can be driven by different reading objectives. Other very discriminative features were “saccade median length” and “fixation mean acceleration x”, which are two similar features that measure the distance between two or three con- secutive fixations. The fourth and sixth most discriminative features in recognizing ToE were “fixation std” and “fixation average”, which relate to the duration of fixations and their variability during the reading session. These features have been reported to correlate with speed in lexical processing in the literature on psycholinguistics [48].
Top 10 most discriminative features to recognize
101
IUI 2014 • Learning and Skills
February 24-27, 2014, Haifa, Israel
No. Feature
1       fixation average
2 saccade median length
3 fixation mean acceleration x
4 discourse connectors
5 fixation std
6 fixation mean velocity x
7 num fixations
8 reading time
9 pupil mean acceleration
10 num regressions
low vs. high p-value
> 2·10 4 < 3·10 6 < 3·10 4 > 6·10 4 > 2·10 5 < 2·10 4 > 0.06 > 4·10 3 > 0.17 < 0.44
In Table 3 we displayed the differences in feature values between readers with low and high level of understanding. Some features did not show strong statistical significance in their values between the two reader populations. However, the optimal predictive mechanism may consist of a non-linear function of those features, and Random Forest were capable of discovering those non-linear relationships.
Our experiments could be extended by including a larger number of subjects, possibly to recognize a wider set of at- tributes. Quality of eye-tracking data and text-gaze alignment are affected by experimental conditions such as the use of a chin-rest to prevent head movements, or displaying less lines per screen to reduce text-gaze misalignments. We favored naturalistic reading conditions to preserve validity in general applications, but recent advances in eye-tracking error cor- rection would benefit similar applications. Other extensions could consist in investigating gaze patterns on non-textual re- gions or combining other modalities such as face micromove- ments or body pose to recognize latent personal attributes.
We found evidence that eye-movements contain patterns that are useful to recognize reader’s level of understanding and language skill, but their direct application to information rec- ommendation and adaptive systems may still be challenging. Our work could be extended by making actual use of the pos- itive gaze patterns, to recommend easier-to-read documents for users with low language skill, or activate reading assis- tance when users display gaze patterns of low understanding. Recognizing understanding on a certain topic would also help us to discover topic familiarity, useful to build refined user models of the World. We believe in the importance of gain- ing capabilities to recognize personal and task attributes given observations of reading behavior, and we hope this work can initiate us in that research direction.
CONCLUSIONS
We presented a framework to recognize English language skill and level of understanding given measurements of read- ing behavior. We achieved a small but significant error re- duction in recognizing English language skill as measured by TOEFL or TOEIC, but failed at recognizing self-reported English skill, probably due to inconsistencies between self- assessment of English ability by our readers.
We did not achieve error reductions in the recognition of the exact quantification of reader’s understanding with respect to our baseline based on priors. However, we obtained signif- icant recognition performance in the task of discriminating between readers with low and high level of understanding, which is unprecedented given measurements of reading be- havior. We believe our findings have potential applications in adaptive and recommendation systems.
We analyzed the most discriminative features of reading be- havior in recognizing our target latent attributes. We found that pure gaze features are more discriminative than propor- tions of fixation times on different linguistic features, and we showed a characterization of the readers in our study that showed low and high levels of understanding.
                               Table3: Characterizationofreaderswithalowandhighlevel of understanding. Column “low vs. high” indicates whether each feature has a larger value (>) for readers with a low level of understanding or vice versa (<). P-values are computed using the Wilcoxon t-test.
the variable importance method ranked “pupil mean accelera- tion” and “num regressions” as important features, there were no statistically significant value differences of those features for different levels of understanding. This may suggest that these features might be effective to discriminate readers only when they are combined with other features.
DISCUSSION AND FUTURE WORK
As presented in Figure 2, there was a small but significant reduction in recognition error of English skill as measured by a normalized test of English (ToE), but no error reduc- tions were achieved when recognizing self-reported English skill (english). Surprisingly, there was no positive correlation between self-reported English skill and English skill as mea- sured by tests. Under the assumption that tests of English (as given by TOEFL and TOEIC) are consistent across subjects, a possible explanation for a lack of positive correlation is that our readers were not consistent in their self-assessment of lan- guage ability. For that reason, our machine learning methods may have had difficulties in finding characterizing patterns to discriminate different levels of self-reported English skill.
We did not achieve error reductions in the regression task of recognizing reader’s understanding score, but we achieved significant error reductions when discriminating readers with low and high level of understanding. While personal at- tributes might be available in personalized reading environ- ments, the level of understanding remains latent, and gaining capabilities to recognize it has advantages in adaptive appli- cations and information recommenders.
In Table 2 we showed that pure gaze features are more dis- criminative than proportions of fixation times on linguistic features. The reason could be that small calibration drifts have a strong negative effect on the correct alignment be- tween fixation locations to actual fixated words. Most pure gaze features are not strongly affected by small to moderate calibration drifts, making them more robust features for these recognition tasks. However, personal differences in process- ing text with certain characteristics have been studied in the literature, and they might be potentially informative features if accurate eye-tracking data is available.
102
IUI 2014 • Learning and Skills
February 24-27, 2014, Haifa, Israel
REFERENCES
1. Argamon, S., Koppel, M., Pennebaker, J., and Schler, J. Mining the blogosphere: age, gender, and the varieties of self-expression. First Monday 12, 9 (2007).
2. Aula, A., Majaranta, P., and Ra ̈iha ̈, K.-J. Eye-tracking reveals the personal styles for search result evaluation. In Human-Computer Interaction-INTERACT 2005. Springer, 2005, 1058–1061.
3. Azcarraga, J., and Suarez, M. T. Predicting academic emotions based on brainwaves, mouse behaviour and personality profile. In PRICAI 2012: Trends in Artificial Intelligence. Springer, 2012, 728–733.
4. Biber, D., and Finegan, E. Sociolinguistic perspectives on register. Oxford University Press Oxford, 1994.
5. Biedert, R., Dengel, A., Elshamy, M., and Buscher, G. Towards robust gaze-based objective quality measures for text. In Proceedings of the Symposium on Eye Tracking Research and Applications, Association for Computational Linguistics (2012), 201–204.
6. Bird, Steven, E. L., and Klein, E. Natural Language Processing with Python. O’Reilly Media Inc, 2009.
7. Boraston, Z., and Blakemore, S.-J. The application of eye-tracking technology in the study of autism. The Journal of Physiology 581, 3 (2007), 893–898.
8. Boulis, C., and Ostendorf, M. A quantitative analysis of lexical differences between genders in telephone conversations. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, Association for Computational Linguistics (Stroudsburg, PA, USA, 2005), 435–442.
9. Brants, T., and Franz, A. Web 1T 5-gram version 1, 2006.
10. Breiman, L. Random forests. Machine learning 45, 1 (2001), 5–32.
11. Burger, J. D., and Henderson, J. C. An exploration of observable features related to blogger age. In Computational Approaches to Analyzing Weblogs: Papers from the 2006 AAAI Spring Symposium (2006), 15–20.
12. Buscher, G., and Dengel, A. Gaze-based filtering of relevant document segments. In International World Wide Web Conference (WWW) (2009).
13. Buscher, G., Dengel, A., and van Elst, L. Query expansion using gaze-based feedback on the subdocument level. In ACM Special Interest Group on Information Retrieval (SIGIR) (2008).
14. Cortes, C., and Vapnik, V. Support-vector networks. Machine learning 20, 3 (1995), 273–297.
15. Coxhead, A. An academic word list, vol. 18. School of Linguistics and Applied Language Studies, Victoria University of Wellington, 1998.
16. Cutrell, E., and Guan, Z. What are you looking for?: an eye-tracking study of information usage in web search. In Proceedings of the SIGCHI conference on Human factors in computing systems, ACM (2007), 407–416.
17. Demberg, V., and Keller, F. Data from eye-tracking corpora as evidence for theories of syntactic processing complexity. Cognition 109, 2 (2008), 193 – 210.
18. Doherty, S., O’Brien, S., and Carl, M. Eye tracking as an MT evaluation technique. Machine Translation 24 (2010), 1–13. 10.1007/s10590-010-9070-9.
19. Drucker, H., Burges, C. J., Kaufman, L., Smola, A., and Vapnik, V. Support vector regression machines. Advances in neural information processing systems (1997), 155–161.
20. Fellbaum, C. WordNet. Theory and Applications of Ontology: Computer Applications (2010), 231–243.
21. Feng, L., Jansche, M., Huenerfauth, M., and Elhadad, N. A comparison of features for automatic readability assessment. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, Association for Computational Linguistics (2010), 276–284.
22. Garera, N., and Yarowsky, D. Modeling latent biographic attributes in conversational genres. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, ACL ’09, Association for Computational Linguistics (Stroudsburg, PA, USA, 2009), 710–718.
23. Granka, L. A., Joachims, T., and Gay, G. Eye-tracking analysis of user behavior in WWW search. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’04, ACM (New York, NY, USA, 2004), 478–479.
24. Hayhoe, M. M., Ballard, D. H., Triesch, J., Shinoda, H., Aivar, P., and Sullivan, B. Vision in natural and virtual environments. In Proceedings of the 2002 symposium on Eye tracking research & applications, ETRA ’02, ACM (New York, NY, USA, 2002), 7–13.
25. Hornof, A., and Halverson, T. Cleaning up systematic error in eye-tracking data by using required fixation locations. Behavior Research Methods 34 (2002), 592–604. 10.3758/BF03195487.
26. Hudson, R. Measuring syntactic difficulty. Manuscript, University College, London (1995).
27. Irwin, D. E. Lexical processing during saccadic eye movements. Cognitive Psychology 36, 1 (1998), 1–27.
28. Isaacowitz, D. M., Wadlinger, H. A., Goren, D., and Wilson, H. R. Selective preference in visual fixation away from negative images in old age? an eye-tracking study. Psychology and aging 21, 1 (2006), 40.
103
IUI 2014 • Learning and Skills
February 24-27, 2014, Haifa, Israel
29. Jacob, R. J., and Karn, K. S. Eye tracking in human-computer interaction and usability research: Ready to deliver the promises. Mind 2, 3 (2003), 4.
30. Just, M. A., and Carpenter, P. A. Eye fixations and cognitive processes. Cognitive Psychology 8, 4 (1976), 441–480.
31. Kennedy, A., and Pynte, J. Parafoveal-on-foveal effects in normal reading. Vision Research 45 (2005), 153–168.
32. Klein, D., and Manning, C. D. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, Association for Computational Linguistics (2003), 423–430.
33. Kunze, K., Kawaichi, H., Yoshimura, K., and Kise, K. Towards inferring language expertise using eye tracking. In CHI’13 Extended Abstracts on Human Factors in Computing Systems, ACM (2013), 217–222.
34. Labov, W. Sociolinguistic patterns, vol. 4. Philadelphia: University of Pennsylvania Press, 1972.
35. Lee, N. N., Cu, J., and Suarez, M. T. A real-time, multimodal, and dimensional affect recognition system. In PRICAI 2012: Trends in Artificial Intelligence,
P. Anthony, M. Ishizuka, and D. Lukose, Eds., vol. 7458 of Lecture Notes in Computer Science. Springer Berlin Heidelberg, 2012, 241–249.
36. Lorigo, L., Pan, B., Hembrooke, H., Joachims, T., Granka, L., and Gay, G. The influence of task and gender on search and evaluation behavior using Google. Information Processing & Management 42, 4 (2006), 1123–1131.
37. Mart ́ınez-Go ́mez, P., Chen, C., Hara, T., Kano, Y., and Aizawa, A. Image registration for text-gaze alignment. In Proceedings of the 2012 ACM international conference on Intelligent User Interfaces, IUI ’12, ACM (New York, NY, USA, 2012), 257–260.
38. Mart ́ınez-Go ́mez, P., Hara, T., and Aizawa, A. Recognizing personal characteristics of readers using eye-movements and text features. In Proceedings of COLING (Mumbai, India, December 2012), 1747–1762.
39. McDonald, S. A., and Shillcock, R. C. Eye movements reveal the on-line computation of lexical probabilities during reading. Psychological Science 14, 6 (2003), 648–652.
40. Medero, J., and Ostendorf, M. Atypical prosodic structure as an indicator of reading level and text difficulty. In Proceedings of NAACL-HLT (2013), 715–720.
41. Millis, K. K., and Just, M. A. The influence of connectives on sentence comprehension. Journal of Memory and Language 33, 1 (1994), 128–147.
42. Miyao, Y., and Tsujii, J. Feature forest models for probabilistic HPSG parsing. Computational Linguistics 34 (March 2008), 35–80.
43. Mukherjee, A., and Liu, B. Improving gender classification of blog authors. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language
Processing, EMNLP ’10, Association for Computational Linguistics (Stroudsburg, PA, USA, 2010), 207–217.
44. Pan, B., Hembrooke, H. A., Gay, G. K., Granka, L. A., Feusner, M. K., and Newman, J. K. The determinants of web page viewing behavior: an eye-tracking study. In Proceedings of the 2004 symposium on Eye tracking research & applications, ACM (2004), 147–154.
45. Piquado, T., Isaacowitz, D., and Wingfield, A. Pupillometry as a measure of cognitive effort in younger and older adults. Psychophysiology 47, 3 (2010), 560–569.
46. Raghunathan, K., Lee, H., Rangarajan, S., Chambers, N., Surdeanu, M., Jurafsky, D., and Manning, C. A multi-pass sieve for coreference resolution. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics (2010), 492–501.
47. Rao, D., Yarowsky, D., Shreevats, A., and Gupta, M. Classifying latent user attributes in twitter. In Proceedings of the 2nd international workshop on Search and mining user-generated contents, SMUC ’10, ACM (New York, NY, USA, 2010), 37–44.
48. Rayner, K. Eye movements in reading and information processing: 20 years of research. Psychological Bulletin 124 (Nov. 1998), 372–422.
49. Rayner, K., Carlson, M., and Frazier, L. The interaction of syntax and semantics during sentence processing: eye movements in the analysis of semantically biased sentences. Journal of Verbal Learning and Verbal Behavior 22, 3 (1983), 358 – 374.
50. Schler, J., Koppel, M., Argamon, S., and Pennebaker, J. Effects of age and gender on blogging. In Proceedings of 2006 AAAI Spring Symposium on Computational Approaches for Analyzing Weblogs (2006), 199–205.
51. Tomanek, K., Hahn, U., Lohmann, S., and Ziegler, J. A cognitive cost model of annotations based on eye-tracking data. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, Association for Computational Linguistics (Stroudsburg, PA, USA, 2010), 1158–1167.
52. Weide, R. The CMU pronunciation dictionary, release 0.6, 1998.
53. West, M., and Jeffery, G. B. A general service list of English words: with semantic frequencies and a supplementary word-list for the writing of popular science and technology. Longmans, Green London, 1953.
54. Xu, S., Jiang, H., and Lau, F. C. Personalized online document, image and video recommendation via commodity eye-tracking. In ACM Recommender Systems (RecSys) (2008).
55. Xu, S., Jiang, H., and Lau, F. C. User-oriented document summarization through vision-based eye-tracking. In International Conference on Intelligent User Interfaces (IUI) (2009).
104
Eye Movement Plots
Michael Burch
VISUS, University of Stuttgart Allmandring 19
Stuttgart, Germany 70569 michael.burch@visus.uni-stuttgart.de
Figure 1: Several appended eye movement scan paths from an eye tracking experiment investigating route finding tasks (Burch et al. 2014a,b; Netzel et al. 2016b) in which the point sequence is first transformed into a line sequence: A density field is computed and a box reconstruction filter is applied 15 times. Additionally, contour lines enhance the diagram.
 ABSTRACT
The visual analysis of eye movement data is a challenging task since the data has an inherent spatio-temporal nature and is typically recorded by measuring the eye movements of various study participants and sometimes, over long time periods. Finding similar strategies among the participants while applying a visual scanning strategy is one of the most important but also most difficult tasks. In this paper we describe the eye movement plots with which we are able to visually encode the eye movement fixation sequences of sev- eral study participants as a stacked line-based representation. The two-dimensional fixation points are mapped to a time- dependent sequence of lines while edge splatting is applied to visually augment the otherwise cluttered diagrams. With this visualization concept, time-varying eye movement patterns become visible, comparable, and generate an overview rep- resentation for a set of eye movements allowing comparison tasks. We illustrate the usefulness of the technique by means of applying it to formerly recorded eye tracking data by in- vestigating route finding tasks in public transport systems. Moreover, we describe challenges, limitations, and scalability issues of the approach.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
VINCI ’17, Bangkok, Thailand
© 2017 ACM. 978-1-4503-5292-5/17/08. . . $15.00 DOI: 10.1145/3105971.3105973
CCS CONCEPTS
  Human-centered computing   Information visual- ization;
KEYWORDS
Eye movement data, splatting, time-dependent data, spatio- temporal data
ACM Reference format:
Michael Burch. 2017. Eye Movement Plots. In Proceedings of VINCI ’17, Bangkok, Thailand, August 14-16, 2017, 8 pages.
DOI: 10.1145/3105971.3105973
1 INTRODUCTION
As researchers in the field of eye tracking (Duchowski 2003; Holmqvist et al. 2011), visualization, and visual analytics we are oftentimes confronted with the challenging problem of how to visually explore eye movement data with the goal to detect common visual task solution strategies among the participants (Burch et al. 2013). Traditional visualization concepts like visual attention maps (Bojko 2009; Burch 2016b; Spakov and Miniotas 2007) or gaze plots (Scinto et al. 1986) are either time-, space-, and participant-aggregated diagrams or they suffer from visual clutter (Rosenholtz et al. 2005) making exploration tasks difficult to solve. This problem gets even worse if we have to deal with many study participants and long-durating events making eye tracking data to an application example for big data visualization and analy- sis (Blascheck et al. 2015; Kurzhals et al. 2014). Moreover, long-durating tasks in eye tracking experiments are often- times a challenging problem for existing work on visualizing

VINCI ’17, August 14-16, 2017, Bangkok, Thailand
Michael Burch
eye tracking data and hence, the computation of eye tracking metrics or data aggregations become inevitable concepts.
Although many visualization techniques for eye movement data already exist (Blascheck et al. 2014), not many of them can reliably work with growing eye movement datasets and particularly to visually compare the participants’ eye move- ments. In this paper we describe the eye movement plots that first transform eye movements from a sequence of two- dimensional points with a duration into a sequence of lines preserving this duration information. By such a mapping from points to lines we can linearize the corresponding vi- sualization allowing a stacking of the individual plots. As a drawback of the line-based diagrams we can identify an increased visual clutter effect. This negative consequence of the linearization process is mitigated by applying an edge splatting approach (Burch et al. 2011) that has perceptual benefits if also enhanced by contour lines (Burch 2016a).
In this paper we describe the visual patterns that can be derived from the eye movement plots, illustrate an applica- tion to formerly recorded eye movement data, explain the applicability of interaction techniques, and investigate the effect of clustering the eye movement patterns for similar visual scanning strategies. The visualization is not designed in a way to graphically depict every little detail in the data, but it merely serves as an overview representation with the goal to initiate further data exploration processes.
Abstracting from the tiny visual patterns caused by lines and line crossings, we apply line splatting as a concept to generate more region-based visual patterns that are easier to perceptually compare. This visual effect is due to the weak- nesses of the human’s short term memory (Ware 2004, 2008) and problems that are denoted by change blindness (Healey and Enns 2012). We base our idea on the concept of reducing the amount of visual features supporting comparison tasks in an overview-based diagram for time-varying data (Aigner et al. 2011). Finally, we discuss limitations and scalability issues in this work focussing on algorithmic/computational, visual, and perceptual scalability.
2 RELATED WORK
There are various techniques investigating the visual analysis of eye tracking data as surveyed in a recent state-of-the-art report (Blascheck et al. 2014). Many of the techniques are either based on points of interest (POIs) or areas of interest (AOIs), but even if they do not aggregate the eye movement data as typically done in AOI approaches, they do not allow to visually compare the time-varying data (Aigner et al. 2011). Moreover, if space, time, and participants should be incorporated into a visual design, a trade-off can be observed, focusing on highlighting one or two aspects in the data.
For example, gaze plots (Scinto et al. 1986) focus on show- ing the time-dependent eye movements for many study par- ticipants by directly overplotting the scan paths in a corre- sponding stimulus. This strategy is a straightforward and natural way to visually encode eye movements, but due to the overdraw, a lot of visual clutter (Rosenholtz et al. 2005)
occurs. Visual attention maps (Bojko 2009; Burch 2016b; Spakov and Miniotas 2007), on the other hand, show the distribution of the fixations by mapping them to density fields that are color coded. The generated hot spots reflect the areas of frequent attention in a static or also in a dy- namic diagram. But, although this strategy provides a good overview, it has some drawbacks caused by aggregating over space, time, and participants (Andrienko et al. 2012).
The AOI rivers (Burch et al. 2013) illustrate a way to visually depict the dynamics of AOI fixation frequencies. But negatively, individual scanning strategies can only be compared by applying interaction concepts. Moreover, the analyst is not provided by a participant-separated overview to start further data exploration processes (Keim 2012). In our technique, all of the participants’ scanning strategies are displayed in a time-compressed overview-based visualization following the principle: overview first, zoom and filter, then details on demand (Shneiderman 1996). This frees the viewer from the challenging decision of how to interact with the diagram to find a certain specific study participant with a characteristic scanning feature.
Also approaches like gaze stripes (Kurzhals et al. 2016b) or fixation image charts (Kurzhals et al. 2016a) are useful techniques to illustrate the scanning behavior and as a benefit, they show the individual visual scanning strategies of the participants. As a negative consequence, the plots are based on image thumbnails extracted from the original stimulus which makes them suffer from visual scalability problems. In our approach, we do not directly show the stimulus content in the eye movement plots, but we base our technique more on providing a comparable overview about the scanning strategies of individual study participants. For this reason, we linearize the sequence of two-dimensional points, map them to a sequence of lines, and finally, apply a splatting concept (Burch et al. 2012, 2011; Heinrich and Weiskopf 2009) augmented with contour lines (Burch 2016a) to allow better comparisons of the generated visual patterns.
In this paper we try to cirumwent some of the drawbacks that other approaches have, but we also produce new ones, that we discuss later on in Section 6.
3 DATA MODEL
In this section we mathematically model eye movement data (Section 3.1) and how it can be transformed from a sequence of points to a sequence of lines (Section 3.2) while preserving the duration information. Moreover, we describe how the individual eye movements can be structured, ordered, and clustered (Section 3.3) to better derive common visual task
solution strategies among the eye tracked people.
3.1 Eye Movement Data
We model eye movement data in form of scan paths, i.e., each study participant generates a sequence of two-dimensional points attached with a duration each. This results in
Si := (pi,1,...,pi,ni)
Eye Movement Plots
VINCI ’17, August 14-16, 2017, Bangkok, Thailand
 which expresses that each participant can have a differently longeyemovementtrajectory.Eachpi,j ∈N×N×Nconsists of three components which are x- and y-coordinates and a corresponding time stamp that models the point in time someone is fixating a certain point in a two-dimensional visual stimulus. We assume that no time is wasted in a saccade, i.e., when the eye moves from pi,j to pi,j+1.
3.2 Data Transformations
To obtain a linear mapping of the point sequence we first transform
into a sequence
Si = (pi,1,...,pi,ni) Si = (li,1,...,li,ni)
Figure 2: An eye movement plot for 3 study participants showing the transformation of scan paths into sequences of lines plotted on top of each other. x- and y-coordinates as well as the time stamp t is taken into account for generating the plot. Moreover, a user-defined horizontal offset is used to build the horizontal basis for a corresponding line.
4 EYE MOVEMENT PLOTS
We describe our novel visually scalable eye movement plots fo- cusing on providing an overview representation for individual scanning strategies among eye tracking study participants. To reach this goal we apply a transformation from the sequence of fixation points to a sequence of lines, vertically stack the individual line plots, compute a density field, and finally, apply a reconstruction filter to obtain smoother color coded regions that allow to do comparison tasks since they do not contain that many tiny visual features as the non-splatted diagrams. Moreover, interaction techniques are applicable to filter the data in the time and participant data dimension. Also data aggregation over time and participants, as well as participant clustering is possible based on an eye movement sequence similarity measure.
4.1 Visual Design
Each eye movement scan path is first transformed from a point sequence to a line sequence. Then those resulting lines are visually mapped to horizontal stripes by also taking into account the time stamps as well as a user-defined horizontal offset. The transformation process is already described in Section 3.2. All transformed scan paths are stacked on top of each other, finally depicted in the eye movement plots.
Following this approach leads to vast amounts of visual clutter as a negative consequence due to lots of line crossings and overplotting. The density of the eye movement data in both dimensions, time and participants, is to blame for this issue. To mitigate that, we apply a line splatting, i.e., we compute density fields and apply a reconstruction filter several times to obtain smoother density regions.
Finally, those regions are color coded and can additionally be visually enhanced by contour lines as already described in a work by Burch on dynamic graph visualization (Burch
 of lines. Each individual line is composed of a start and an end point based on the x- and y-coordinates of the points, the time stamps, and a fixed horizontal offset. This means each
pi,j :=(xi,j,yi,j,ti,j) with a time stamp ti,j becomes a line
li,j :=[(ti,j,xi,j)(ti,j +δ,yi,j)] with a user-defined horizontal offset δ.
3.3 Structuring, Ordering, and Clustering
To better visually structure the set of time-varying data we apply a hierarchical clustering (Defays 1977; Everitt et al. 2001; Kaufman and Rousseeuw 1990; Schaeffer 2007). This is based on a difference measure expressing a pairwise com- parison of the eye movements.
For the difference of two sequences Si and Sj (modeled as fixation point sets) we first compute the Jaccard coefficient
Jacc(Si,Sj) := | Si ∩ Sj |. | Si ∪ Sj |
This results in a similarity value
Jacc(Si,Sj) := vSi,Sj ∈ [0,1] ⊂ R.
To do this, we define the union of two eye movement se- quences as the number of all points occurring in both of the sequences while the intersection of two eye movement sequences is defined by only those points that occur in both sequences together. This measure gives a hint about eye movement sequence similarity, but more complex (but also time-consuming) measures might be incorporated in this data preprocessing step.
We take the hierarchical clustering structure to compute a linear order among the eye movement sequences that is applied to the eye movement plots for the ordering of the stacking. It may be noted that the traversal of the generated eye movement sequence hierarchy might be further improved, e.g., by reducing the sum of the linear differences, an NP- hard problem known as the Optimal Linear Arrangement
(OLA) (Garey and Johnson 1979) or Minimum Linear Ar- rangement (MinLA).

VINCI ’17, August 14-16, 2017, Bangkok, Thailand
Michael Burch
2016a). Those contour lines are particularly useful to better separate the regions.
Figure 2 illustrates how the mapping to the eye movement plots looks like if only the transformed lines were drawn. Without line splatting vast amounts of visual clutter will be the result making the diagram unreadable. For this reason, we follow the concept already successfully applied by Burch et al. (Burch et al. 2011) for displaying dynamic graphs. In this figure we show eye movement scan paths for three study participants and annotate the very first fixation for the first participant P1 by x- and y-coordinates, a time stamp t, and also a horizontal offset. This concept is applied for each fixation point in each scan path.
4.2 Benefits of Splatting
Splatting the line-based diagrams has some benefits that will be described in the following:
• Independency of rendering order: Computing a density field from line-based diagrams means that the rendering order of the lines gets irrelevant. This is useful when the lines carry extra quantitative attributes like edge weights in graph visualization or the importance or relevance of a fixation, the spatial placement in a stimulus, or a semantic meaning of a fixation in an eye movement study.
• Visibility of structures: If we only take into ac- count the lines without splatting, the visual clutter effect has a bad impact on the readability of the diagrams. Hairball-like representations are the re- sult which do not allow to use the visualization for data exploration tasks. When splatting is applied, some structures become visible again that can be recognized by the color coding currently applied.
• Removing many tiny visual features: If compar- ison tasks have to be done by an analyst, many tiny visual features can cause challenging problems which is due to change blindness effects. Consequently, a splatted line-based diagram only consists of color coded regions which are easier to identify and to compare. If the user wishes to see the details again, interaction techniques can be applied.
• Visual scalability: The splatting effect guarantees that we obtain a visually scalable diagram. This benefit can be found in both data dimensions, i.e., time and participants. Aggregating the data makes the eye movement plots even more visually scalable although intermediate data elements are typically lost by aggregation techniques.
• Overview representation: Since the visualization is not based on showing every little detail in the data, but more on providing an overview about the time- varying eye movements, splatting can be understood as beneficial concept. But, we also support a more detailed version of the splatted diagrams. The default view serves as an overview which can be a starting point for further data exploration tasks.
• Faster comparisons: Data comparisons can be done in a faster way because we do not have to com- pare that many visual patterns as in a non-splatted representation. Once a similar structure is found, the analyst might dig deeper in the data, i.e., he might build a hypothesis about the data (by means of visualization) that he can confirm, refine, or reject.
Figure 3: The public transport map of Antwerp in Belgium is used as a stimulus in the eye tracking study. The partici- pants had to find a route from the station annotated with a green hand to the station annotated with a red dart board.
4.3 Interaction Techniques
Although we start with a splatted and static overview-based representation we further support an eye movement data analyst by interaction techniques.
• Participant clustering: Similar scan paths can be identified by first computing a similarity measure based on the Jaccard coefficient. Then a hierarchical clustering can be applied. This process can be done globally, i.e., based on the entire sequences, or locally, based on a certain time period.
• Aggregation: If the data gets too large in any di- mension, it can be aggregated. Eye movements of study participants can be aggregated as well as user- selected time periods into single time snap shots. This frees some display space for showing the remaining data.
• Filtering: The data can be filtered spatially, i.e., only those fixations are taken into account that are located in user-selected regions. The same holds for user-selected time periods.
• Linking to stimulus: Hovering over a certain eye movement plot directly shows the corresponding scan path on the stimulus. This helps to additionally compare the eye movement plot with the spatial content of the stimulus which is one of the drawbacks of our approach.

Eye Movement Plots
VINCI ’17, August 14-16, 2017, Bangkok, Thailand
 (a)
(b)
Figure 4: Eye movement plots for 27 participants of the public transport maps eye tracking study. We can see that the scan paths have different lengths. (a) Only the color coded densities are visible. (b) An enhancement with contour lines makes the plots more readable and the color coded regions more visually separable.
 • Details on demand: Hovering over the eye move- ment plots provides details about each participant like the participant identification number, the gen- der, personal details, the task completion time, if the task was answered correctly or not and the like.
5 APPLICATION SCENARIO
We investigate scan paths from 27 study participants from a formerly conducted eye tracking experiment. The task in this study was to find routes by visually inspecting public transport maps (Burch et al. 2014a,b; Netzel et al. 2016b). The datasets were recorded by a Tobii T60 eye tracking device and are publicly available. We stored this data locally and preprocessed it to first bring it in our data format.
Figure 3 ilustrates one of the stimuli shown in the study which is the map of Antwerp in Belgium. Start and desti- nation stations are highlighted by a green hand and a red dart board, respectively. The task was to find a way from the start station to the destination station and to tell the station names of the interchange points. Several visual task solution strategies might be applied to answer this task as it was already found out in the work of Burch et al. (Burch et al. 2014a,b) and also Netzel et al. (Netzel et al. 2016b).
Although the authors of the metro map study found various visual scanning strategies they did not design a visualization technique that is visually scalable which is in focus of this work. Netzel et al. (Netzel et al. 2016a) relied on manual annotations of the recorded eye movement data to reliably judge the applied scanning strategies which is a useful but also time-consuming process. We do not preprocess the data
VINCI ’17, August 14-16, 2017, Bangkok, Thailand Michael Burch
 Figure 6: An individual scanpath can be selected to inspect it in more detail since it can be depicted on a larger display space which better reflects the visual patterns in it.
 Figure 5: Selecting a certain time period of the scan paths and displaying only this period as eye movement plots. A topographic color coding is used.
in this direction, but we transform the fixation sequences into line sequences to have a linearization of the data that can be stacked on top of each other to allow comparison tasks.
Figure 4 shows a visualization by the eye movement plots that take the raw eye movement scan paths as parameters without further interacting with the data. Already in this plot we can see several time-varying patterns of the 27 explored study participants. In (a) the eye movement plots show the color coded density fields without further augmenting them with contour lines as in scenario (b). We can see for example, that the scan paths are of different lengths which is a natural phenomenon in eye movement studies in which people have as much time as they like to answer a given task. Moreover, we can see that the visual scanning strategies are not absolutely similar but show different behaviors over time. To further investigate the eye movement data in specific time intervals, a time-based filtering technique might be applied.
For example, in Figure 4 we can see different visual patterns that are enclosed by rectangles and annotated with numbers. Pattern 1 indicates a dynamic eye movement pattern that occurs several times in many eye movement scanpaths. This sequence of diagonal parallel lines reflects that the fixation points remain at nearly the same positions in the stimulus which is a high x-value but a low y-value, i.e., the study participant looks at a region in the upper right corner where
the start station is located in the metro map. Pattern 2 shows a more or less smoothly changing pattern, i.e., the eyes move more slowly from one fixation point to another one. Both of them are located close to the diagonal in the stimulus since the visual pattern consists of many horizontal lines indicating that x- and y-values must be very similar. Also this pattern occurs many times in all of the participants’ scanpaths. The pattern number 3 shows an outlier, i.e., one study participant takes much longer to answer the route finding task than all the others. This scanpath is also displayed in Figure 6 as a details on demand view showing only an individual scanpath which is useful to better detect the fine-granular eye movement patterns. There are many more visual patterns worth identifiable but this would go beyond the scope of this paper, for example, the differently long temporal gaps, or the many different orientations of the lines while many orientations are not detectable due to the fact that certain regions on screen are not inspected at all.
Although the visualization is visually scalable in the time dimension we further select a certain time period after get- ting a first impression about the data which serves as an overview. The result of such a selection can be seen in Fig- ure 5, color coded in a topographic color scale. Vertically ordering the eye movement sequences can be achieved by first pairwisely comparing them algorithmically. To reach this goal, we compute the Jaccard coefficients for all those pairs, apply a hierarchical clustering based on these similarity measure values, and visually represent the eye movements again in a vertically ordered diagram. In the figure we can see Pattern 1 that indicates a sequence of fixation points at similar locations with a lower x- than y-value while it is nearly equidistantly distributed over time. A similar finding holds for Pattern 2 but there the fixation point is to a larger x- than y-value, probably again in the upper right corner. A longer fixation pattern can be seen in Pattern 3 while the temporal distances are varying and are getting longer. There are also various gap patterns reflecting that a lot of time passes between subsequent fixation points (Pattern 4).
An individual scanpath is illustrated in Figure 6 that shows the eye movement behavior in more detail. For example, we can see that the fixation sequence seems to be subdividable into 5 temporal periods. In Period A a more chaotic scanning behavior is observable, while in Period B the study partic- ipant makes a more structured, but recurring, sequence of
Eye Movement Plots
VINCI ’17, August 14-16, 2017, Bangkok, Thailand
subpatterns, probably he tries to find a solution to a subprob- lem and has to jump to and fro between different points in the stimulus. In Period C a sequence of fixations with large x- but low y-values is observable in a short period containing many small temporal gaps. Period D shows that the eyes are closer to the diagonal indicated by the shorter diagonal patterns or even that horizontal lines are visible (same x- and y-values) that indicates that he follows a route along the diagonal leading to the destination station. Finally, the study participant moves back to the upper right corner, probably inspecting the start station again (Period 5). It is a little bit abnormal that the lower left corner is not fixated over time. Either the information is acquired by peripheral vision or the study participant had problems to answer the route finding task which reflects his unnaturally long scanpath compared to all the others. Looking into the recorded correctness data we can find out that this study participant did not answer this task correctly, i.e., he mentioned wrong interchange points in the metro map system.
Finally, the analyst can look for details on demand and in particular, individual filtered eye movement plots can be regarded separately or also appended one after the other (see Figure 1). Focusing on an individual plot is beneficial since a lot of display space can be used to represent it and conse- quently, many more details about the data can be observed again compared to the compressed overview.
6 LIMITATIONS AND SCALABILITY
In this section we describe limitations of our approach and scalability issues that can be classified into algorithmic/compu- tational, visual, and perceptual problems.
6.1 Algorithmic/Computational Scalability
There are some algorithmic concepts for generating this visu- alization technique which are worth discussing due to the fact that interaction techniques have to rely on computationally efficient algorithms. This is in particular the case if we have to deal with eye movement data recorded in long-durating tasks. Lots of fixations are contained in each of the eye move- ment sequences and a growing number of study participants can have a bad influence on the performance of the applied clustering technique.
First of all, the transformation of point sequences into line sequences runs in linear time, i.e., the (x, y)-coordinates and the duration (or time stamp) have to be mapped to [(x1,y1)(x2,y2)]-sequences of finite lines.
More time is required for the splatting concept, i.e., com- puting density fields and applying the reconstruction filter. The splatting runs in linear time with respect to the number of points (or lines if transformed). The filter is dependent on the number of pixels, i.e., the horizontal and vertical resolution of the display as well as the number of iterations.
The integration of contour lines can be computed as a final enhancement step. Here, only one iteration over the density field is required in order to derive the contour structure which is then internally stored. Interacting with the eye movement
plots initiates another round of all these algorithms, i.e., depending on the type of interaction, one or all of them have to be run again.
6.2 Visual Scalability
Particularly, the number of fixations and the maximal time step are worth considering in this scenario. Those are re- sponsible for the generated visual clutter since a growing amount of fixations means a growing amount of lines and hence, a larger probability of line crossings. Moreover, on the other hand, distributing the fixations over shorter time periods makes the corresponding visual representation more compressed and compact and consequently, also this effect has a bad impact on the visual scalability. Splatting is a useful concept to make line-based structures visible again.
The growing number of study participants that have to be compared for common visual scanning strategies demands for a visual design that can handle several eye movement plots stacked on top of each other. More participants means a smaller horizontal region to map the eye movements on.
Generally, if the display space is limited, less space remains for the color coded regions. But positively, for each of the study participants the horizontal regions are treated similarly meaning no pattern distortion is incorporated resulting in a lie factor (Tufte 1992) for comparison tasks. Moreover, aggregation is possible, over time and over participant groups. If similar scanning strategies are identified those might be aggregated to achieve a more visually scalable visualization, but negatively, details can be hidden.
6.3 Perceptual Scalability
The applied color coding to the density region is important to visually analyze the eye movement data. In our visualization we provide several prominent color schemes that can be chosen on user’s demand. Moreover, if the density values are extremely differing in size, a logarithmic color coding might be applied but although small value differences might be recognizable, it can lead to misinterpretations of the density values. Hence, this option should be treated with care.
The eye movement plots benefit from their visual scala- bility, i.e., eye movement data of long-durating tasks and also with many study participants can be displayed. This effect is achieved by abstracting from the tiny visual fea- tures occurring in line-based diagrams that we transform into splatted density fields. This visual encoding strategy has the benefit that a region-based visual comparison has to be done instead of a line-based one. Consequently, the change blindness effect (Healey and Enns 2012) is not that strong as in diagrams consisting of various small sized graphical prim- itives. Positively, if the user detects something interesting in the time-varying data visualization, i.e., a visual pattern catches one’s attention, he can dig deeper into details by filtering for time periods and study participants.
VINCI ’17, August 14-16, 2017, Bangkok, Thailand
Michael Burch
7 CONCLUSION AND FUTURE WORK
In this paper we illustrated an idea for showing eye move- ments of several study participants in a stacked line-based and splatted diagram called the eye movement plots. The benefit compared to existing work in eye tracking data visu- alization is that comparison tasks can be answered by this overview visualization even if we have to deal with many study participants confronted by long-durating tasks. The splatted line-based diagram can reveal different shapes and color coded regions, i.e., it abstracts from diagrams with many tiny visual features being problematic for comparison tasks due to issues caused by the short term memory and change blindness effects. We implemented several interaction techniques and showed the usefulness of the technique by means of exploring data from a real eye tracking experiment focusing on route finding tasks in public transport systems. For future work, we plan to explore different kinds of eye movement data, i.e., also from dynamic stimuli like videos or interactive visualization systems. Moreover, we plan to evaluate the visualization technique in comparison to other techniques being able to display the same kind of data.
ACKNOWLEDGEMENTS
This work was supported by DFG under grant WE 2836/6- 1. We thank Robin Woods from Communicarta Ltd. for providing the metro maps.
REFERENCES
Wolfgang Aigner, Silvia Miksch, Heidrun Schumann, and Christian Tominski. 2011. Visualization of Time-Oriented Data. Springer.
Gennady Andrienko, Natalia Andrienko, Michael Burch, and Daniel Weiskopf. 2012. Visual Analytics Methodology for Eye Movement Studies. IEEE Transactions on Visualization and Computer Graphics 18, 12 (2012), 2889–2898.
Tanja Blascheck, Michael Burch, Michael Raschke, and Daniel Weiskopf. 2015. Challenges and Perspectives in Big Eye-Movement Data Visual Analytics. In Proceedings of the 1st International Symposium on Big Data Visual Analytics. 17–24.
Tanja Blascheck, Kuno Kurzhals, Michael Raschke, Michael Burch, Daniel Weiskopf, and Thomas Ertl. 2014. State-of-the-Art of Visu- alization for Eye Tracking Data. In EuroVis - STARs. 63–82.
Agnieszka Bojko. 2009. Informative or Misleading? Heatmaps Decon- structed. In Proceedings of Human-Computer Interaction. Springer, 30–39.
Michael Burch. 2016a. Isoline-Enhanced Dynamic Graph Visualiza- tion. In Proceedings of International Conference on Information
Visualisation. 1–8.
Michael Burch. 2016b. Time-Preserving Visual Attention Maps. In
Proceedings of Conference on Intelligent Decision Technologies.
273–283.
Michael Burch, Gennady L. Andrienko, Natalia V. Andrienko, Markus
Ho ̈ferlin, Michael Raschke, and Daniel Weiskopf. 2013. Visual Task Solution Strategies in Tree Diagrams. In Proceedings of IEEE Pacific Visualization Symposium. 169–176.
Michael Burch, Fabian Beck, and Daniel Weiskopf. 2012. Radial Edge Splatting for Visualizing Dynamic Directed Graphs. In GRAPP & IVAPP 2012: Proceedings of the International Conference on Computer Graphics Theory and Applications and International Conference on Information Visualization Theory and Applications. 603–612.
Michael Burch, Andreas Kull, and Daniel Weiskopf. 2013. AOI Rivers for Visualizing Dynamic Eye Gaze Frequencies. Computer Graphics Forum 32, 3 (2013), 281–290.
Michael Burch, Kuno Kurzhals, and Daniel Weiskopf. 2014a. Visual Task Solution Strategies in Public Transport Maps. In Proceedings of ET4S@GISCIENCE. 32–36.
Michael Burch, Michael Raschke, Tanja Blascheck, Kuno Kurzhals, and Daniel Weiskopf. 2014b. How Do People Read Metro Maps? An Eye Tracking Study. In Proceedings of the 1st International
Workshop on Schematic Mapping (Schematics).
Michael Burch, Corinna Vehlow, Fabian Beck, Stephan Diehl, and Daniel Weiskopf. 2011. Parallel Edge Splatting for Scalable Dynamic Graph Visualization. IEEE Transactions on Visualization and
Computer Graphics 17, 12 (2011), 2344–2353.
Daniel Defays. 1977. An Efficient Algorithm for a Complete Link
Method. The Computer Journal (British Computer Society) 20, 4 (1977), 364–366.
Andrew T. Duchowski. 2003. Eye Tracking Methodology - Theory and Practice. Springer.
Brian Everitt, Sabine Landau, and Morven Leese. 2001. Cluster Analysis.
Michael R. Garey and David S. Johnson. 1979. Computers and In- tractability: A Guide to the Theory of NP-Completeness. W. H. Freeman.
Christopher G. Healey and James T. Enns. 2012. Attention and Visual Memory in Visualization and Computer Graphics. IEEE Transactions on Visualization and Computer Graphics 18, 7 (2012), 1170–1188.
Julian Heinrich and Daniel Weiskopf. 2009. Continuous Parallel Co- ordinates. IEEE Transactions on Visualization and Computer Graphics 15, 6 (2009), 1531–1538.
Kenneth Holmqvist, Marcus Nystr ̈om, Richard Andersson, Richard Dewhurst, Halszka Jarodzka, and Joost van de Weijer. 2011. Eye Tracking: A Comprehensive Guide to Methods and Measures. Ox-
ford University Press.
L. Kaufman and P.J. Rousseeuw. 1990. Finding Groups in Data: An
Introduction to Cluster Analysis.
Daniel A. Keim. 2012. Solving Problems with Visual Analytics: Chal-
lenges and Applications. In Proceedings of Machine Learning and
Knowledge Discovery in Databases - European Conference. 5–6. Kuno Kurzhals, Brian D. Fisher, Michael Burch, and Daniel Weiskopf. 2014. Evaluating Visual Analytics with Eye Tracking. In Proceed-
ings of the Fifth Workshop on Beyond Time and Errors: Novel
Evaluation Methods for Visualization, BELIV. 61–69.
Kuno Kurzhals, Marcel Hlawatsch, Michael Burch, and Daniel Weiskopf. 2016a. Fixation-Image Charts. In Proceedings of the Ninth Bien- nial ACM Symposium on Eye Tracking Research & Applications,
ETRA. 11–18.
Kuno Kurzhals, Marcel Hlawatsch, Florian Heimerl, Michael Burch,
Thomas Ertl, and Daniel Weiskopf. 2016b. Gaze Stripes: Image- Based Visualization of Eye Tracking Data. IEEE Transactions on
Visualization and Computer Graphics 22, 1 (2016), 1005–1014. Rudolf Netzel, Michael Burch, Bettina Ohlhausen, Robin Woods, and Daniel Weiskopf. 2016b. User Performance and Reading Strategies for Metro Maps: An Eye Tracking Study. Special Issue on Eye Tracking for Spatial Research in Spatial Cognition and Computa-
tion: An Interdisciplinary Journal (2016), 39–64.
Rudolf Netzel, Michael Burch, and Daniel Weiskopf. 2016a. Interactive Scanpath-Oriented Annotation of Fixations. In Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research &
Applications, ETRA. 183–187.
Ruth Rosenholtz, Yuanzhen Li, Jonathan Mansfield, and Zhenlan Jin.
2005. Feature Congestion: A Measure of Display Clutter. In Pro- ceedings of Conference on Human Factors in Computing Systems. 761–770.
Satu Elisa Schaeffer. 2007. Graph Clustering. Computer Science Review 1, 1 (2007), 27–64.
Leonard F. Scinto, Ramakrishna Pillalamarri, and Robert Karsh. 1986. Cognitive Strategies for Visual Search. Acta Psychologica 62, 3 (1986), 263–292.
Ben Shneiderman. 1996. The Eyes Have It: A Task by Data Type Taxonomy for Information Visualizations. In Proceedings of Visual Languages. 336–343.
Oleg Spakov and Darius Miniotas. 2007. Visualization of Eye Gaze Data Using Heat Maps. Electronics and Electrical Engineering 2, 74 (2007), 55–58.
Edward Rolf Tufte. 1992. The Visual Display of Quantitative Infor- mation. Graphics Press.
Colin Ware. 2004. Information Visualization: Perception for Design. Morgan Kaufmann.
Colin Ware. 2008. Visual Thinking: for Design. Morgan Kaufmann Series in Interactive Technologies, Paperback.
Science of Computer Programming 79 (2014) 260–278
   Contents lists available at ScienceDirect Science of Computer Programming journal homepage: www.elsevier.com/locate/scico
  Taupe: Visualizing and analyzing eye-tracking data✩
Benoît De Smet a, Lorent Lempereur a, Zohreh Sharafi b, Yann-Gaël Guéhéneuc b,∗,
Giuliano Antoniolc, Naji Habraa
a Research Center in Information Systems Engineering, FUNDP, Namur, Belgium b Ptidej Team, École Polytechnique de Montréal, Québec, Canada
c Soccer Lab., École Polytechnique de Montréal, Québec, Canada
 article info
Article history:
Received 12 February 2011
Received in revised form 19 January 2012 Accepted 25 January 2012
Available online 11 February 2012
Keywords:
Eye-tracking Visualization Analysis Compatibility Extensibility
abstract
Program comprehension is an essential part of any maintenance activity. It allows developers to build mental models of the program before undertaking any change. It has been studied by the research community for many years with the aim to devise models and tools to understand and ease this activity. Recently, researchers have introduced the use of eye-tracking devices to gather and analyze data about the developers’ cognitive processes during program comprehension. However, eye-tracking devices are not completely reliable and, thus, recorded data sometimes must be processed, filtered, or corrected. Moreover, the analysis software tools packaged with eye-tracking devices are not open-source and do not always provide extension points to seamlessly integrate new sophisticated analyses. Consequently, we develop the Taupe software system to help researchers visualize, analyze, and edit the data recorded by eye-tracking devices. The two main objectives of Taupe are compatibility and extensibility so that researchers can easily: (1) apply the system on any eye-tracking data and (2) extend the system with their own analyses. To meet our objectives, we base the development of Taupe: (1) on well-known good practices, such as design patterns and a plug-in architecture using reflection, (2) on a thorough documentation, validation, and verification process, and (3) on lessons learned from existing analysis software systems. This paper describes the context of development of Taupe, the architectural and design choices made during its development, and its documentation, validation and verification process. It also illustrates the application of Taupe in three experiments on the use of design patterns by developers during program comprehension.
© 2012 Elsevier B.V. All rights reserved.
     1. Introduction
The software life-cycle [51] is traditionally divided into several macro phases: from inception to implementation, maintenance, and retirement. The most expensive phase of any software project is maintenance [6]. A software life- cycle often spans decades [33] and thus maintenance is rarely performed by the developers that first developed the program. Therefore, maintainers must first understand the program before implementing any change. During program
✩ Source code, examples, and documentation are available at http://www.ptidej.net/research/taupe/. This work has been partially funded by the Guéhéneuc’s Canada Research Chair on Software Patterns and Patterns of Software and NSERC Discovery Grant.
∗ Corresponding author.
E-mail address: yann-gael.gueheneuc@polymtl.ca (Y.-G. Guéhéneuc).
0167-6423/$ – see front matter © 2012 Elsevier B.V. All rights reserved. doi:10.1016/j.scico.2012.01.004

B. De Smet et al. / Science of Computer Programming 79 (2014) 260–278 261
comprehension activities, developers build mental models of the program, which should allow them to perform changes without introducing bugs [50] or degrading the program design [38].
The activity of program comprehension has been studied by many researchers, for example to devise models of program comprehension, e.g., [7,61,54], or tools to ease this activity, e.g., [3]. Recently, researchers have introduced the use of eye- tracking devices to further improve our understanding of the developers’ cognitive processes taking place during program comprehension [20,21,46,48,63] by studying their use of visualized data, such as source code and identifiers [47], diagrams [55], and others. In particular, we proposed a theory [21] to unify previous theories of program comprehension with current theories in vision science [37]. The proposed theory aims to model and explain in detail the developers’ process of acquiring the necessary data using their vision to understand a program.
Eye-tracking devices have been used for nearly 30 years in cognitive science for the study of human–computer interfaces, for marketing, medical research, and so on. An eye-tracker records the coordinates of a subject’s gaze when looking at a computer screen. It provides a new perspective on a subject’s comprehension process because it shows the areas attracting the subject’s attention as well as the visual path of her gaze on the screen [11]. The subject’s attention and visual path together form a window on her cognitive processes [43]. Thus, analyzing the data recorded using an eye-tracking device allows understanding in detail a subject’s process of acquiring data, for example during program comprehension.
However, there are still some major obstacles to the widespread adoption of eye-tracking devices. Besides the costs of such devices, the accompanying analysis software systems are not open-source and often not extensible, preventing the development and seamless integration of new sophisticated analyses [27]. Consequently, we undertook the development of the Taupe system1 (Thoroughly Analyzing the Understanding of Programs through Eyesight) to visualize, analyze, and edit eye-tracking data. The two main objectives of Taupe are compatibility and extensibility so that researchers can easily: (1) apply the system on any eye-tracking data and (2) extend the system with their own analyses. To meet our objectives, we base the development of Taupe: (1) on well-known good practices, such as design patterns and a plug-in architecture using reflection, (2) on a thorough documentation, validation, and verification process, and (3) on lessons learned from existing analysis software systems.
In Section 2, this paper describes the context of development of Taupe. In Section 3, we discuss related works. In Section 4, it details the architectural and design choices made during its development, and its documentation, validation, and verification process. In Section 5, it illustrates the application of Taupe in three experiments on the use of design patterns by developers during program comprehension. Finally, in Section 6, it summarizes the contributions of Taupe and concludes with future work.
2. Context
In this section, we first introduce definitions needed to understand the rest of the paper. We then discuss previous work on program comprehension and on using eye-tracking data. Finally, we describe two eye-trackers to show their usage and describe the recorded data that they provide.
2.1. Definitions
In vision science, a fixation is the position of the eye during a gaze and a saccade is a movement of the eye between two fixations [37]. In the field of empirical software engineering [24] and in particular in this paper, an experiment is a set of subjects who answer a set of questions (e.g., comprehension questions) using some visual data (e.g., a UML class diagram).
An area of interest (AOI) is an area of some visual data with a specified relevance. An area can thus be relevant to the subject to answer a question, irrelevant, or ignorable. The fixations in an ignorable area of interest should not be taken into account in any subsequent analyses.
For example, if we are interested to observe how subjects read a class diagram and relate relevant parts to answer a question that is displayed on top of the diagram, the area in which the question is written does not provide any interesting data about the subjects’ cognitive process. We can consider this area as an ignorable area of interest.
A visual path is a series of visited areas of interest, sorted by chronological order. An area is visited if there is a fixation in it. The same area of interest that is visited twice consecutively is considered only once. For example, Fig. 1 shows two distinct visual paths on a same diagram, with four areas of interest: A, B, C, and D. One visual path, on the sub-figure of the left, is ABDCA while the other, on the sub-figure of the right, is DBCAB.
2.2. State of the art
The Taupe system is used to edit, visualize, and analyze data that are gathered using eye-trackers during empirical studies on program comprehension activities. We summarize the theories in vision science, report some works on empirical studies on software engineering, and detail studies in program comprehension, emphasizing on studies of the developers’ use of UML class diagrams, which form the main bulk of the eye-tracking studies in software engineering.
 1 ‘‘Taupe’’means‘‘mole’’inFrenchandispronounced’tOp.
262 B. De Smet et al. / Science of Computer Programming 79 (2014) 260–278
  2.2.1. Theories in vision science
(a) ABDCA.
(b) DBCAB.
Fig. 1. Two distinct visual paths.
We summarize theories in vision science to explain the theoretical background behind usability of eye-tracking systems. Vision science is an interdisciplinary domain of cognitive science interested in the understanding of people’s vision system. Vision science collects facts on vision, formulates laws from these facts, and devises theories explaining these laws and facts. With these theories, vision scientists have been able to predict new facts successfully and to refine their theories.
Vision science possesses many theories to explain color vision, spatial vision, perception of motion and events, as well as eye movements, visual memory, and visual awareness. To the best of our knowledge, Palmer’s book presents a complete and in-depth coverage of vision theories, cast in the information processing paradigm [37].
2.2.2. Theories in program comprehension
The domain of software engineering possesses theories, which are an invaluable help in setting up experiments to observe facts (dis)proving laws and theories [13]. A theory helps in understanding a domain by explaining ‘‘Why is it so?’’ questions [13, p. 7 ] [36].
Empirical studies are essential to understand phenomena with which software engineering deals, thanks to the work of precursors, such as Basili [5]. Empirical studies are based on the classical cycle: observations (facts), laws, theories. Many facts have been recorded through empirical studies and some laws have been proposed [33].
Few theories of program comprehension have been proposed in the literature. One of the first theory of program comprehension, proposed by Brooks [8], describes program comprehension as a process of building a sequence of knowledge domains, bridging the gap between problem domain and program execution. A succession of knowledge domains describes a software engineer’s comprehension of a program.
Another theory, developed by von Mayrhauser [34], is an integrated theory describing the processes taking place in a software engineer’s mind during program comprehension, as a combination of top-down and bottom-up comprehension processes, working with a common knowledge base. This integrated theory accounts for the dynamics of forming and of abstracting a mental representation of a program.
Existing theories of program comprehension describe different processes deployed by software engineers to understand a program and to analyze available information. However, existing theories do not explain well how software engineers acquire this information. Guéhéneuc [21] proposed a theoretical framework by joining vision science and program comprehension. This framework could help explaining the processes of program comprehension, describing known facts, extracting new facts, and designing new experiments in program comprehension.
2.2.3. Studies of program comprehension
The rich literature on program comprehension focused on the problems of obtaining data from software artifacts (static and dynamic data, features, documentation, and other repositories), see for example [1,52]. It also tackles the means to represent and to communicate this data, using various techniques from text-based editors to 3D interactive dynamic environments, such as [49]. This literature is essential to understand what kind of data software engineers have at their disposal to understand programs.
Some works also studied the contexts in which program comprehension takes place. Murphy et al. [35] distinguished, described, and identified recurring patterns in software engineers’ daily activities. Although not related to program comprehension explicitly, their work brings insight in the program comprehension activity, because this activity is part of all but the most basic software engineering activities. Thus, this line of research is important to generalize claims in program comprehension.
All software engineers use diagrams as means to convey information to other developers or to better understand programs. Diagrams reduce comprehension and learning effort by omitting irrelevant details and highlighting pertinent information. The closer the information presented on diagrams is to a developer’s mental representation, the easier it is to understand [9].
(a) Headband.
(b) Setting.
B. De Smet et al. / Science of Computer Programming 79 (2014) 260–278 263
  Fig. 2. Eye-link II [53].
Class diagrams have been extensively studied in the literature on program comprehension. They represent the structure and global behavior [26] of programs, showing classes, interfaces, and their relationships. They are often used by software engineers during development and maintenance to abstract implementation details and to present an easier-to-grasp clustered view of the program source code [19,42].
2.3. Devices
There are two main classes of eye-tracking devices: intrusive ones, which requires subjects to wear some gears and non- intrusive ones. We now present three eye-tracking devices (one intrusive device and two non-intrusive devices) and the data visualization software that they use for visualizing and exporting their data.
2.3.1. Intrusive eye-tracking device
1. Eye-linkII:thissystemconsistsofthreeminiaturecamerasmountedonapaddedheadband(seeFig.2a).Twoeyecameras are used to track the eyes’ movements. An optical head-tracking camera integrated into the headband allows an accurate tracking of the subject’s point of gaze.
The Eye-link II has the highest resolution (noise limited at < 0.01◦) and fastest data rate (500 samples per second) of any head mounted eye-tracker.
Fig. 2b from SR-Research2 [53] describes the configuration of this eye-tracking system, which uses two separate computers: the Host PC and the Display PC. The Host PC performs real-time eye-tracking at 250 or 500 samples per second and also computes the subject’s true gaze position on the display. The Desktop PC provides displays for experiment and calibrating targets during eye-tracker calibration.
The major problem of this system is its instability. If the subject moves too far away from the calibration point, the calibration step must be done again. An offset (a difference between the real place of the fixation and the fixation recorded by the eye-tracker device) is often present.
The Eye-link II system comes with the Eye-link II Windows Developer Kit that provides sample display programs, C source code, and instructions for creating experimental programs. It uses Data Viewer3 to display eye-fixation and visualization path on top of the presented stimulus. It also provides the output files that are stored in the Host PC. One output file contains eye fixations, saccades and the information related to the interest area. The Data Viewer is neither open-source nor free and cannot be easily extended with new analyses.
2.3.2. Non-intrusive eye-tracking device
1. FaceLAB from Seeing Machine4: it is a more recent eye-tracking device than the Eye-link II system. It consists of one
computer (either a desktop PC or a laptop) and two miniature cameras, as shown in Fig. 3. It tracks first the position of the subject’s head using her eye-brows, nose, and lips and, then, uses this data to track the subject’s gaze. It works in pair with GazeTracker from EyeResponse,5 which is a more recent [15] data visualization software than the Eye-link II Data Viewer but which is also neither open source nor free. Multiple eye-trackers can interact with one another: such a configuration is used in cockpits or in cars to track the eyes of the subject even when she turns her head. It is also possible
2 http://www.sr-research.com/accessories_ELII_dv.html.
3 http://www.sr-research.com/accessories_EL1000_dv.html.
4 http://www.seeingmachines.com/.
5 http://www.eyeresponse.com/.

264 B. De Smet et al. / Science of Computer Programming 79 (2014) 260–278
 Fig. 3. The FaceLAB eye-tracker [45].
to run FaceLAB and GazeTracker on a single computer with a scene camera that records a video of the whole scene. FaceLAB interacts with the eye-tracking device and transmits the data to GazeTracker via network. GazeTracker simply saves the data associated with the displayed image. GazeTracker provides a more advanced interface and more visualization tools than Eye-link II Data Viewer but it cannot be readily extended with new analyses. A screen cast of the use of FaceLAB and GazeTracker is available on-line.6
2. ITU Gaze Tracker : it is a non-commercial eye-tracking device by the university of Copenhagen(ITU).7 The ITU Gaze Tracker is an open-source, low-cost software that allows the use of a web-cam as an eye-tracker [44].
Taupe can import and analyze eye-tracking data provided by Eye-link II’s Data Viewer and FaceLAB’s GazeTracker from Seeing Machine.
3. Related work
We only recall here some of the main lines of research on program comprehension using class diagrams.
Purchase et al. [10] reported the results of experiments on the effect of aesthetics criteria on the preferences of users of UML class diagrams. They performed several experiments with subjects to assess the subjects’ preferences over several pairs of class diagrams, each diagram in a pair conforming to different (but related) aesthetics criteria. They collected quantitative data in the form of percentages of subjects preferring one diagram over another in each pair and qualitative assessments of each diagram. They reported the most important aesthetic criteria for UML class diagrams: joined inheritance arcs or directional indicators.
Eichelberger [12] studied the relation between the UML notation for class diagrams, principles of human–computer interactions, and principles of object-oriented design and programming. The author then suggested changes to the UML notation and aesthetics criteria to lay out class diagrams. These changes and aesthetic criteria are implemented in a tool, Sugibib, to lay out UML-like class diagrams. The author claimed that laying out class diagrams conforming to aesthetics criteria improve the readability of the diagrams but only qualitative arguments were provided.
Hadar and Hazzan [23] presented results from a study on the strategies applied by software engineers in the process of comprehending visual models of programs. They use visual models that were described using the UML notation and included use case, activity, class, sequence, collaboration, state chart, object, package, and deployment diagrams. The subjects were senior students majoring in computer science from several universities. The subjects were divided in two groups to collect both qualitative and quantitative data. The authors concluded on the usefulness of multifaceted descriptions of programs provided by the UML and that no one type of diagram was more important than the other. However, further studies should be performed to confirm these findings.
Sun and Wong [56] evaluated the layout algorithms for class diagrams of two industrial tools, Rational Rose and Borland Together, according to criteria from previous work, including the cited works by Purchase [10] and Eichelberger [12]. Using laws from the Gestalt theory of visual perception [37, p. 50–53], they retained and justified 14 criteria to assess the visual quality of the layouts of class diagrams. They applied these criteria on a Thermometer program and on JUnit [17]. They concluded on the good quality of both industrial tools, on the relevance of their criteria, and on the difficulty of satisfying all criteria.
Guéhéneuc conducted an experiment with eye-trackers to study how software engineers acquire and use information from UML class diagrams [20]. He concluded on the importance of classes and interfaces and reported that developers seem to barely use binary class relationships, such as inheritance or composition. Yusuf et al. [63] conducted a similar study to analyze the utilization of specific characteristics of UML class diagrams (e.g., layout, color, and stereotypes) during program comprehension. They concluded on the efficiency of layouts with additional information as colors or stereotypes to improve program comprehension.
Sharif et al. [46] performed a controlled experiment with eye-trackers to assess the effect of different layouts on the comprehension of UML class diagrams. They reported that the multi-cluster layout obtains higher level of accuracy and
6 http://www.ptidej.net/research/taupe/videos/.
7 http://www.gazegroup.org/downloads/23-gazetracker.

B. De Smet et al. / Science of Computer Programming 79 (2014) 260–278 265
takes less time than the orthogonal layout. In another work, Sharif et al. [48] also conducted an eye-tracking experiment to investigate the impact of layout on the comprehension of four design patterns (Strategy, Observer, Composite, and Singleton) in UML class diagrams. They reported the positive impact of multi-cluster layout on speed for all four design patterns. They also concluded that the multi-layer layout has positive impact on accuracy and visual effort for Strategy and Observer design patterns.
Bednarik and Tukiainen [2] proposed an approach to study trends on repeated measures of sparse data over a small data set of program comprehension activities captured with eye-trackers. Using this approach, they characterized program comprehension strategies using different program representations (code lecture and program execution). Sharif et al. [47] similarly studied whether the Camel Case convention of creating identifiers was more efficient than using underscores. They replicated a previous study by Binkley et al. [4] that showed that the Camel Case convention leads to a higher accuracy in reading. Interestingly, they found opposite results: although the data indicated no difference in accuracy between the Camel Case and underscore conventions, subjects recognized identifiers in the underscores style more efficiently.
Uwano et al. [59] studied the code-reading habits of developers and identified typical ‘‘patterns’’ that distinguishes ‘‘efficient’’ readers from others. They first implemented an integrated environment to measure and record code reviewers’ eye movements and, based on their fixations, identify the lines of the source code that the reviewers are reading. They then conducted an empirical study of 30 review processes of six programs by five reviewers. They reported that all reviewer ‘‘scanned’’ the source code following a similar pattern and that reviewers who did not spend enough time during the ‘‘scan’’ tended to take more time for finding defects in the code.
4. Taupe
The Taupe system is a software program designed by the Ptidej Team8 to import data from eye-trackers and to enable the execution of various algorithms on the imported data. Its first version has been developed since 2005 with the contribution of many students. Its current version, v2.0, was developed by the first three authors by reusing some code from the first version but revising entirely the program architecture and design. The users’ and developers’ guides9 provide more details about versions of Taupe.
4.1. Motivation
Taupe was originally designed to compare the ways subjects’ read and understand UML class diagrams. A subject was asked different questions about her understanding of some diagrams when performing some maintenance tasks. The subjects’ eye movements were recorded using an eye-tracking device and then analyzed using Taupe.
Taupe is an open-source and free software system that can parse, process, and analyze eye-tracker’s data stored in the output files provided by eye-trackers software systems. Taupe not only visualizes eye fixations and the areas of interest on top of the stimulus but also provides a set of algorithms to analyze the data.
Taupe is compatible so it can handle eye-tracking data from different devices. The first version of Taupe worked with the Eye-link II Data Viewer’s output file. Then, we extended Taupe to support GazeTracker’s data. Unlike GazeTracker and Eye-link II Data Viewer, Taupe is not specific to any eye-tracking system. Moreover, Taupe is extensible and therefore it can be extended with new parsers and analyses for eye-tracking data.
Proprietary systems such as Eye-link II Data Viewer and GazeTracker are provided by the manufacturers of the corresponding eye-tracking devices and packaged with their eye-trackers. These systems are neither open-source nor free and they only analyze the eye-tracking data that are provided by their own integrated eye-tracking devices, offering a limited, not extensible set of analyses.
4.2. Architecture
Fig. 4 shows the architecture of Taupe v2.0. The core of Taupe consists of the data collected about an experiment: fixations, saccades, questions, and their answers. Some of this data is provided by eye-tracking devices. Other data is collected by the experimenters through questionnaires or other means.
The data is organized depending on the answers to the questions. The questions are represented by a set of images and their corresponding area of interest. Each file that contains the areas of interest related to a question can be manually written and Taupe also allows the user to create this kind of files using a graphical user interface. Different types of parsers are used to extract the information from the files provided by the eye-tracking devices.
The possibility to link a subject to other subjects is also available in Taupe, through the concept of group. A group is a set of subjects who have an attribute in common. For example, such an attribute could be the level of study (B.Sc., M.Sc., Ph.D., and so on), their gender (male or female), their UML knowledge (low, average, high, and so on). All these variables can be measured by some external means (questionnaires, interviews, and so on).
8 http://www.ptidej.net/.
9 http://www.ptidej.net/research/taupe/downloads/.

266 B. De Smet et al. / Science of Computer Programming 79 (2014) 260–278
 Fig. 4. Architecture.
Taupe handles one experiment at a time. The system’s main features are implemented as commands. For example, the AOI Maker command helps to graphically designate and create areas of interest for a given image. Other commands provide other essential features of the system, for example the set of algorithms that produce analysis results. These results may be written in a set of files using some printers selected by the user and they can be subsequently studied or analyzed using any data mining software. Taupe also implements a graphical user interface to visualize data and their characteristics.
4.3. Inspirations
The development of Taupe has been inspired by many sources, such as our experiences from our previous experiments using different eye-tracker devices. In this section, we explain the necessary requirements of the system that has been developed to visualize and analyze eye-tracking data. In addition, we explain how Taupe satisfies these requirements.
4.3.1. From the eye-trackers
The three main features of Taupe result from our past uses of eye-tracker devices and their analysis software systems. First, Taupe must implement some parsers to use the data from multiple eye-tracking devices. Second, Taupe must graphically display the fixations, saccades, and visual path over a diagram. Third, Taupe must allow its users to correct the data recorded by the devices using some offsets, as shown in Fig. 5, which is an example of a diagram with a clearly visible static offset. Every cloud of points must be moved to the same direction and the same distance to match with the diagram.
Three types of offset were encountered during our previous experiments: static offsets, non-static offsets, and chaotic offsets. Static offset can be easily seen and a constant translation can be applied to all the fixations to fix the offset. Non- static offsets are less easily seen and the translations are different for each group of fixations according to their position on the screen. Chaotic offsets are random offsets due to vagaries from the eye-tracking devices and–or the subject and must be corrected manually for each fixation.
4.3.2. From the field of study
The field of study using eye-tracking devices led to several useful metrics to assess a subject’s browsing effort, for example. Metrics based on fixations are the most common; for example, the total number of fixations is pointed out by Goldberg et al. [18]: ‘‘The number of fixations overall is thought to be negatively correlated with search efficiency’’. The number of fixations per areas of interest indicate that certain areas are more noticeable or more important than other areas [39]. According to Just et al. [31], the duration of the fixations on a specific area can have two different meanings:
1. The subject has a hard time to extract the information [16].
2. The subject is ‘‘more engag[ed] in some way’’ by the object [40].
B. De Smet et al. / Science of Computer Programming 79 (2014) 260–278 267
 Fig. 5. An example of a static offset.
Fig. 6. Example of spatial density of 8%.
The spatial density of fixations is a widely used metric: the ‘‘coverage of an area’’ due to search and processing may be captured by the spatial distribution of gazepoint samples. [. . . ] The area can be divided into grid areas either representing specific objects or physical screen area. [. . . ] The spatial density index was equal to the number of cells containing at least one sample, divided by the total number of grid cells. [. . . ] A smaller spatial density indicated more directed search, regardless of the temporal gazepoint sampling order’’ [18]. The spatial density of a visual path is presented as an example in Fig. 6. In this example, we have 10 × 10 = 100 cells while eight distinct grid cells are traversed visually so the spatial density is 8%. The spatial density is identified by the variable SD and it is formulated as follows:
n ci
SD = i=1 n
where n is the number of fixation in the specific area (one cell) and ci is equal to 1 if the area number i visited, otherwise it is equal to 0.
Some metrics use saccades. However, some eye-tracking devices do not provide the required raw data. For example, FaceLAB does not provide the amplitude of a subject’s saccades. Yet, other saccade-based metrics are implemented in Taupe, for example using a transitional matrix and its density: ‘‘also known as link analysis, frequent transitions from one region of a display to another indicates inefficient scanning with extensive search. The transition matrix is a tabular representation of the number of transitions to and from each defined area’’ [18]. A cell is filled if a saccade starts or ends in its area. The transitional matrix is computed as follows:
nn ci,j i=1 j=1
TM =
   n.n
268 B. De Smet et al. / Science of Computer Programming 79 (2014) 260–278
 Fig. 7. Example of the visual path and the corresponding transition matrix.
where n is the number of fixation in the specific area (one cell) and ci,j is equal to 1, if there is a saccade from cell i to cell j, otherwise it is equal to 0.
The density of the transitional matrix can be computed as:  isFilled(x)
TRANSITION_DENSITY = x∈C
#C
where C is the set of cells in the transitional matrix and isFilled : C → {0, 1} returns 1 if the specified cell is filled and 0 otherwise. Two visual paths can have the same convex hull and the same spatial density but different transitional densities.
Fig. 7 shows an example of a visual path on the display grid and table in the figure represents its transition matrix. A cell at position (x, y) that contains the value 1 means that a transition is from the cell x to the cell y.
4.3.3. From previous experiments
Taupe can compute several statistics about the time spent on each question for each group of subjects. These metrics do not come from the field of eye-tracking but are useful to compare the subjects’ performance. Thus, it provides a command to obtain statistics about the fixations’ duration, the subjects’ time spent in a specified area of interest, and the transitional matrix. The whole set of metrics and algorithms implemented and available currently in Taupe are explained in its users’ guide (see footnote 8).
As illustrated in Fig. 1, two subjects can have two different visual paths on the same diagram while answering the same questions. These two visual paths show that the two subjects understood the diagram differently. The difference between two visual paths is computed in Taupe using an edit distance algorithm, the Levenshtein algorithm [22], which compares two strings. Using Taupe, the user can also choose a specified percentage of kept fixations to generate these visual paths. A merged fixation is all the consecutive fixations that are in an area of interest without leaving this area (the duration of this resulting fixation is the sum of all fixations’ durations). The kept percentage of fixations is related to the longer merged fixations.
Jeanmart [28] suggested the metric Normalized Fixations per Area of Interest that is the ratio between the normalized number of fixations in an area of relevant interest and the normalized number of fixations in an area of irrelevant interest. The NORM_RATEi metric can be used to assess a subject’s effort:
#FAORIi NORM_RATEi = #AORIj
where Aj is the set of answers related to the question j, FAORIi is the set of fixations contained in an area of relevant interest for the subject’s answer i, AORIj is the set of areas of relevant interest in the question j, FAOIIi is the set of fixations contained in an area of irrelevant interest for the subject’s answer i, AOIIj is the set of areas of irrelevant interest in the question j, and # returns the cardinality of a set.
4.4. Taupe use
Fig. 8 shows the main user interface of Taupe. The AOIMaker (Area of Interest Maker) command allows users to create a set of areas of interest for a specific image. The Results command executes a selected algorithm on eye-trackers’ data while the visualization Tool command displays fixations, saccades, areas of interest, and visual paths.
4.4.1. Areas of interest
As mentioned in Section 4.2, each question is related to a file that contains its set of areas of interest. Although a user can create such a file using Taupe’s AOIMaker command, shown in Fig. 9; they can also write such a file manually by specifying a list of coordinates for each AIO. They must respect the EBNF grammar [25] in Fig. 10.
   #FAOIIi #AOIIj

B. De Smet et al. / Science of Computer Programming 79 (2014) 260–278 269
 Fig. 8. The Taupe main interface.
 Fig. 9. Areas of interest maker.
        1 [<id> <type> <name> <coordinate> <coordinate> <coordinate>+ <EOL>]*
2 <id> ::= integer
3 <type> ::= NULL | AOI | AORI
4 <coordinate > ::= "("integer "," integer")"
5 <EOL > ::= EndOfLine
                Fig. 10. The EBNF grammar of the question file.
270 B. De Smet et al. / Science of Computer Programming 79 (2014) 260–278
 4.4.2. Visualization
Fig. 11. The visualization tool.
Although the other analysis software systems in the field of eye-tracking allow the users to visualize a set of data recorded by these devices, Taupe brings extensibility in the visualization of the data and of the analyses performed on the data, for example, the visualization interface can display, in addition to fixations and saccades, the convex hull of a chosen percentage of fixations.
The convex hull of a set of fixations is the smallest polygon that includes all fixations. Taupe generates convex hulls for a series of percentages on the range of [5% . . . 100%]. The percentage corresponds to the percentage of fixations that are considered by Taupe per AOI. For example, 50% means that only half of the fixations (the longest ones) are considered per AOI. As shown in Fig. 11, a convex hull is drawn using orange lines that encompass the whole fixations that are available for this images.
4.4.3. Input files
An input file to Taupe is simply a set of fixations and saccades listed in a text file. Different eye-tracking devices provide different input formats but with essentially the same content: a time (sometimes a timestamp in milliseconds), a type (fixation or saccade), a set of coordinates (x and y), and a duration. Other pieces of data, such as the subjects’ pupil sizes or the saccades’ peak velocity, can also appear in an input file.
Eye-link II: The SR-Research’s software system generates .edf files that Taupe cannot read directly. A tool named edf2xml (provided by the same company) can be used to generate .xml files from the .edf files. Then, these .xml files can be loaded in Taupe, which include the saccades’ peak velocity and amplitude.
GazeTracker: The GazeTracker software system can generate .out files that can be loaded in Taupe using the appropriate parser. A screencast on how to export data from GazeTracker is available on-line (see footnote 6).
4.5. Taupe development
The main objectives of the Taupe software system are compatibility and extensibility. During the development of its
version 2.0, decisions were taken to satisfy these two objectives.
4.5.1. Compatibility
The compatibility of the Taupe system with respect to the eye-tracking devices is achieved using an open architecture in which each core component of the system, including parsers, experiments, commands, and printers, as shown in Fig. 4,
B. De Smet et al. / Science of Computer Programming 79 (2014) 260–278 271
can be specialized or new, such component can be added to the system. We achieve such an open architecture using several design patterns and Java reflection mechanism.
First, Taupe implements the architectural pattern Model View Controller (MVC) in its implementation for its user interface, as described by Gamma et al. [14]. The MVC uses the Observer design pattern to facilitate the communication between the model and its views.
The Composite and Visitor design patterns are implemented to represent and visit the data in Taupe: fixations, saccades, questions, and so on. Results from commands also implement the Composite and Visitor design patterns to allow hierarchical output and the combination of the outputs of several commands. The printers, which generate output files that contain the results, are a set of visitors, which define how to print each result (using some visit methods) while the results describe how to navigate their hierarchy (using some accept methods).
The Taupe system handles one experiment at a time and, therefore, the Experiment class implements the Singleton design pattern. It also only returns Iterator s on lists and sets as a defensive measure against user code modifying unwillingly or maliciously the content of the experiment or of the results.
4.5.2. Extensibility
Extensibility is one of the two main objectives of the Taupe system. It pertains to maintainability [58]: ‘‘the ease with which a software system or component can be modified to correct faults, improve performance or other attributes, or adapt to a changed environment’’. Kienle et al. [32] also mentioned extensibility as on of the important requirements that a tool builder must consider. We achieve extensibility in Taupe by following two complementary directions: one related to its implementation and the use of the Java reflection mechanism, another related to its documentation.
First, Taupe makes extensive use of the Java reflection mechanism to increase its extensibility by allowing other developers to easily add their own parsers, commands, and printers through the implementation of the appropriate interfaces and the integration of their implementation in specific folders. New commands (such as new algorithms, new parsers ...) can be added to Taupe simply by extending the correct interface (or the correct abstract class) with a new class by putting it in the right package. Taupe will dynamically load these classes without the need for further modifications in Taupe’s code. For example, to create a new group of subjects in the system, a software engineer only must create a new class that extends the abstract class Group and add this new class in the package laigle.taupe.viewer.utils.group.data. There are five abstract methods to be implemented to make the group effective:
[getName() : String] This method returns the name of the group according to its members’ characteristics. For example, ‘‘Gender’’ would be the name for a group whose members are organized according to their gender.
[getPrefixName() : String] This methods returns the short name of the group and is used as an alternative when, for example, outputting groups in a CSV file.
[getAvailableValues() : Iterator<Object>] This method returns the possible values of the characteristics of the group members. Typically, the GenderGroup would have three available values: female, male, and unknown.
[newInstance(Object o) : Group] This method plays the role of constructor with an argument that represents the common characteristic’s value of the group members.
[isEligible(s : SubjectData) : Boolean] This method checks whether or not a subject s can be added to the group according to its characteristics.
Second, the extensibility of Taupe also depends on the ease of extending it by developers. Complete documentation is available online (see footnote 8): both a user’s guide and a developer’s guide are available. The developer’s guide includes explanations to extend eight different kinds of core components of Taupe. It was written with the objective to ensure that all future developers of Taupe clearly understand how Taupe works and to describe the means to add new and–or modify existing components. The system source code is extensively documented using the Javadoc mechanism, with more than 820 methods of the system described. Consequently, new components can be easily added to Taupe by other researchers.
4.5.3. Validation and verification
The development of the Taupe system has included explicit validation and verification phases. As the Taupe system is used in scientific research, it was crucial that all of its results are correct. The validation process [58] for Taupe led to the development of several test cases using the JUnit framework.10 The package laigle.taupe.tests contains all of the current 50 test cases. These test cases use as oracles values computed by hand for the different implemented algorithms. For example, they include test cases for the computation of the convex hull of a set of fixations or the computation of the edit distance between two areas of interest.
The verification process [58] of Taupe also led to the development of test cases that target the inner working of the system. In particular, several test cases target the core components of the system and the implementations of the various design patterns.
 10 http://www.junit.org/.
272
B. De Smet et al. / Science of Computer Programming 79 (2014) 260–278
Table 1
Metric values computed on Taupe source code.
v0.1 v1.0 v2.0
  Total number of lines
Total lines of code
Number of packages Number of classes
Number of interfaces Number of methods Number of attributes
Lack of cohesion of methods
3193 8036 55,698 812 4912 12,488 3 11 25 10 71 150 0 10 7 40 490 820 30 248 338 0.329 0.370 0.224
 Finally, to give an overall idea of the quality of the current version, Table 1 reports several metrics computed using Eclipse’s Metrics plug-in11 on the three available versions of the system.
4.6. Tool builders’ issues
While building Taupe, we encountered several issues and learnt many lessons for future development. First, understanding the domain is of utmost importance, in particular when dealing with highly-specific domains, such as eye- tracking. Indeed, in the design and implementation of the first versions of Taupe, we overlooked certain concepts that we had to integrate into the latest version. Such concepts include that of ignorable areas of interest. We learnt the lesson that a thorough and systematic domain analysis is always necessary, even if experts are available, to formally describe all the concepts. We thus recommend developers to produce an exhaustive list of concepts and their relations before any implementation.
Second, understanding the expected use of the tools is necessary, again in particular when dealing with highly-specific domains. Indeed, as researchers used Taupe and came up with novel experiments and analyses to perform, we realized that the first versions of Taupe were missing certain concepts and features to help researchers during their experiments. Such features includes the ability to add new exporters. We learnt the lesson that, in research prototypes such as Taupe, it is important to provide access points to all the features of the tool so that researchers can customize the tools to their particular needs. We thus recommend developers to carefully design their tools with extensibility in mind.
Third, making explicit the process of installation, use, and extension of the tools greatly increase its ease of use by researchers. Indeed, it is important to ensure as much as possible that there is one and only one means to extend the tool to avoid confusing researchers when they want to implement a new analysis. We learnt the lesson that extensibility is a double-edge sword and that having one and a single access point only for each concept manipulated in the tool to ease the implementation of new analyses and to ensure that different researchers do not use different paths to access the same concepts, thus possibly leading to conflicts or problem of consistency.
5. Controlled experiments
We now present three case studies that show the use and the relevance of Taupe to edit, visualize, and analyze eye- tracking data to further our understanding of program comprehension. The first case study was conducted by Jeanmart et al. [30] on the impact of the Visitor design pattern on comprehension and modification tasks; the second study was performed by van den Plas et al. [60] on the impact of the Composite and Observer design patterns on comprehension and modification tasks; and the third study was realized by De Smet, Lempereur et al. on the impact of the MVC architectural style on comprehension and modification tasks. For each case study, we succinctly recall its goal, null hypothesis, design, and results; then we describe the use of Taupe in the study. We summarize the three controlled experiments in Table 2.
5.1. Impact of the visitor design pattern
Goal. The goal of this study is to analyze the impact of the Visitor design pattern [14] on comprehension and modification tasks in the context of the maintenance of programs. In comprehension tasks, subjects were asked about different functionalities provided by the program. In modification tasks, subjects were asked to specify classes, methods, or attributes that should be added or modified to add new features to the program or to modify an existing feature.
Hypotheses. The study hypotheses were:
• HC01 : A class diagram with the Visitor does not reduce the subjects’ efforts during program comprehension when
compared to a class diagram without it. 11 http://metrics.sourceforge.net/.

Table 2
The summary of the controlled experiments.
B. De Smet et al. / Science of Computer Programming 79 (2014) 260–278
273
 Goal
5.1 Impact of visitor design pat- tern on comprehension and modification
5.2 Impact of Composite and Observer design pattern comprehension and modifi- cation
5.3 Impact of MVC architecture style on maintenance tasks.
Eye-tracker Eye-link II
Eye-link II
FaceLAB
Independent variable Visitor design pattern
Composite pattern Observer pattern
Different variants of MVC design pattern
Dependent variable
Average number of fixation Average duration of fixation effort
Spatial density Transitional matrix Average fixation’s duration ON target / All target
Subject’s speed Subject’s accuracy
Measures
Norm-Rate ADRF NRRF
SD TM ADRF IN _Alli
Average time Correctness
    • HC02 : A class diagram using the canonical representation of the Visitor does not reduce the subjects’ efforts during program comprehension when compared to a class diagram using the canonical representation of Visitor with another layout.
• HM01 : A class diagram with the Visitor does not reduce the subjects’ efforts during program modification when compared to a class diagram without.
• HM02 : A class diagram using the canonical representation of the Visitor does not reduce the subjects’ efforts during program modification when compared to a class diagram using the canonical representation of Visitor with another layout.
Design. Three open-source projects were used in this experiment: JHotDraw,12 JRefactory,13 and PADL.14 JHotDraw is a frame- work to implement technical and structured drawings, it provides support for the creation of geometric and user-defined shapes. JRefactory is a code refactoring tool for Java programs. PADL is a meta-model for describing object-oriented programs, it is similar to the UML meta-model.
The dependent variables chosen in the experiment were the Average Number of Relevant Fixations and the Average Duration of Relevant Fixations.
The Average Number of Relevant Fixations is used in the NORM _RATEi measure as described in Section 4.3.3. In addition, the
Average Duration of Relevant Fixations is denoted by the variable ADRF where d(c) is a function that gives the total duration
of the fixations made to a class c.

d(c ) ADRF = #{Rel.Classes}
The authors also defined a variable called NRRF to calculate the amount of a subjects’ effort while performing a task on a diagram.
c ε {Rel.Classes} f (c) #{Rel.Classes}
NRRF = c ε #{Rel.Classes}∪{NonRel.Classes} f (c) (1) #{Rel.Classes}+{NonRel.Classes}
The eye-tracking device used in this study was the Eye-link II; 24 subjects participated in the study.
Results. The results of this study were that the presence of the Visitor design pattern as well as its layout do not have a significant impact on the comprehension of UML class diagrams when the subjects must perform comprehension tasks but it has a statistically significant impact on modifications tasks [29].
Relevance of Taupe. Before collecting the eye-tracking data, they decided to use the NORM_RATE measure to analyze the data and statistically test the various null hypotheses. Jeanmart first implemented the formula in an Excel sheet, which he planned to use by copying/pasting the eye-tracking data into the sheet. It become quickly painfully apparent that copying/pasting thousands of pieces of data collected for each subject was a daunting and error-prone task. Then, Jeanmart implemented the formula as a command in the Taupe system. After less than a week of implementation and validation and verification, they were able to analyze all the collected data and statistically test their null hypotheses using Taupe user interface. The measure and statistical analyses are now available in Taupe for future studies.
12 http://www.jhotdraw.org.
13 http://jrefactory.sourceforge.net/.
14 http://wiki.ptidej.net/doku.php?id=padl.
c ε {Rel.Classes}

274 B. De Smet et al. / Science of Computer Programming 79 (2014) 260–278
Table 3
Average edit distance between group of subjects per program and ratios.
 Beginners
Experts Experts/Beginners (%)
75.3 84.5 95.5 106 151.3 88.9 140.8 179.0 107.4
ArgoUML JUnit
QuickUML
  5.2. Impact of the composite and observer design patterns
Goal. The goal of this experiment was to analyze the impact of the Composite and Observer design patterns [14] during
comprehension and modification tasks.
Hypotheses. The null hypotheses of the study were:
• H01 : The impact of the Composite design pattern on the average effort of subjects to perform comprehension and modification tasks is the same for beginners and for experts.
• H02 : The impact of the Observer design pattern on the average effort of subjects to perform comprehension and modification tasks is the same for beginners and for experts.
Design. Three open-source programs were selected to compose the questions of the study: JUnit, QuickUML,15 and ArgoUML.16 JUnit is a unit test framework for the Java programs. QuickUML is a diagramming program to draw UML class diagrams. ArgoUML is also a diagramming program that supports all UML diagrams and provides reverse-engineering facilities as well as exporting in various format.
Four metrics were selected as independent variables in this study to assess the subjects’ effort: the spatial density, the transitional matrix, the average fixation’s duration, and the ON-target/ ALL-target measure. The ON-target/ ALL-target measure is defined using the number of fixation within the specific area of interest, divided by the number of all fixations. Complete descriptions of the implementations of these in Taupe are available in Taupe user’s guide (see footnote 8).
#fix(i, j)
IN_ALLi =
#Fj
The level of expertise of the subjects was assessed using their employment. A subset of the subjects were experts from the Pyxis software company17 while other subjects were students in the Ptidej Team (see footnote 7) and the Soccer Laboratory.18
The eye-tracker used to conduct this study was the Eye-link II; 24 subjects took part in the study.
Results. It was not possible to reject the null hypotheses using the ON-target/ ALL-target measure. However, the analysis of the subjects visual paths showed significant commonalities among experts on the one hand and beginners on the other and significant differences between the experts’ visual paths and the beginners’. Table 3 reports that the average edit distance between the beginners’ visual paths is always lower than that of experts’ visual paths, independently of the considered program. This observation shows that beginners systematically browse a diagram while experts use their expertise to quickly gather the important information from the diagram.
Relevance of Taupe. To the best of our knowledge, the Taupe system is the only such system providing analyses of the visual path and able to compute the edit distance among a set of visual paths. Thus, it was instrumental in showing the commonalities among beginners and experts and the differences between beginners and experts.
5.3. Impact of different forms of the MVC design pattern
Goal. The goal of this experiment was to analyze the impact of different variants of the Model View Controller (MVC) [14]
architectural style during maintenance tasks. Hypotheses. The null hypothesis was:
• H01 : The different variants of the MVC architectural style are all equivalent during the maintenance of a program. The time and the visual path needed to complete the maintenance tasks on UML diagrams are the same, no matter the variant of the MVC.
15 http://sourceforge.net/projects/quj/.
16 http://argouml.tigris.org/.
17 http://www.ptidej.net/research/taupe/downloads/.
18 http://web.soccerlab.polymtl.ca/.

B. De Smet et al. / Science of Computer Programming 79 (2014) 260–278 275
 Fig. 12. Superposition of all the studied forms of the MVC style. Table 4
Results collected for the different forms of the MVC architectural style. Comprehension Modification
   MD MVC MVP
Average time (s) 66 87 47 Correctness (%) 54 77 60
MD MVC MVP
40 64 41 54 77 60
  Design. The class diagrams of two different programs were obtained by reverse-engineering and studied: the diagram of the JTable from Java’s Swing GUI widget toolkit19 and that of JFreeChart,20 an open-source framework for Java programming language, which allows the creation of complex charts.
Different variations of the MVC were then implemented from these two diagrams and used in the study: the canonical form of the MVC style [14], the Model-Delegate style [57] (also called UI-Model or Document-View, MD), and the Model View Presenter style [41] (MVP). These variations are shown in Fig. 12.
The eye-tracking system used in this study was FaceLAB as described in 2.3.2. A total of 23 subjects participated to the study. However, only 18 subjects out of the 23 were valid. The ‘‘mortality’’ was due to the instability of GazeTracker, which sometimes fails to save the data on the hard drive and also to the poor quality of the recorded data at first.
Results. Table 4 reports the main data collected during this study. The analysis of this data showed that subjects spent less time on the MVP variant and that their answer were less correct with the MD style. Average times showed that the MVP variant is usually easier to understand than the two other variants. The correctness of the subjects’ answers showed that the MVC variant is easier to understand than the others variants.
Relevance of Taupe. Again, Taupe eased the process of analyzing the recorded eye-tracking data by allowing us to integrate the algorithms necessary to compute the various measures as commands of the system. Moreover, we also used its visual path algorithm to further analyze the collected data, which results we plan to release in the near future.
6. Summary
We now summarize our contributions and future improvements.
19 http://java.sun.com/products/jfc/tsc/articles/architecture/. 20 http://www.jfree.org/jfreechart/.

276 B. De Smet et al. / Science of Computer Programming 79 (2014) 260–278 6.1. Contributions
To the best of our knowledge, Taupe is the first open-source analysis system for eye-tracking data built with compatibility and extensibility as main objectives. Compatibility is realized through the implementations of parsers for the data collected by two eye-tracking devices and the ability to add new such parsers. Extensibility is achieved by providing clear sets of application programming interfaces (APIs) to the various core components of the system and by using reflection to load automatically new implementations. Taupe is released under the GPL license.
Taupe is designed to accept data from multiple eye-tracking systems as input because eye-tracking techniques evolve every day and some new devices appear on the market. The fact that Taupe is easy to evolve can contribute to the development of the field of eye-tracking.
Taupe offers a set of algorithms of analyses in the field of eye-tracking. Although some of these algorithms were well- known and widely used in the field, others are offered for the first time in such a system, for example the analyses of the visual paths and their edit distances.
6.2. Improvements
In the next versions of Taupe, we plan to add a feature to manually correct fixations with some constraints based on their relative distances. These constraints could be used when there is no static offset but an offset that is different in different areas of interest.
We also plan to extend the AOIMaker so that it can be used to manipulate fixations and saccades recorded by an eye- tracking device, for example to offset the data through drag and drop of the mouse to ease the users’ analysis tasks.
We will also improve the performances of the Taupe system in general and of its parsers in particular, which can be time consuming when thousands of fixations and saccades have been recorded, typically during long experimental sessions.
The usability of the Taupe system could be improved by the introduction of graphical user interfaces allowing the users to interact directly with all of Taupe input files, for example to input some information about the subjects instead of editing manually the .subject files.
The Taupe system’s feedback to the users could also be improved by showing more information though the progress bar and the list in the main window. A more thorough usage of the progress bar would improve the user interface. Data and results could also be displayed using charts to improve their visual analyses. Merging the fixations of a subject and coloring them according to their durations would allow Taupe to generate heatmaps [62], which could further help researchers in identifying relevant area of interests.
Obviously, users can use the files generated with Taupe in some external data-mining software systems, such as R.21 Therefore, we also will create a specific printer for such systems. We will also study the feasibility of generating directly .xls or .ods files.
The current version of Taupe only uses fixations and saccades collected by the eye-tracker devices but new devices provide new kind of data, such as pupil size, head position, blinking rate, and so on. Future versions should use this data to allow the development of even more sophisticated measures and analyses.
7. Conclusion
The activity of program comprehension has been studied by many researchers but only recently visual data have been gathered using eye-tracking devices to further improve our understanding of program comprehension processes. An eye- tracker records the coordinates of a subject’s gaze when looking at a computer screen. It provides a new perspective on a subject’s comprehension processes because it shows the areas attracting the subject’s attention as well as the visual path of her gaze on the screen [11]. A subject’s attention and visual path together form a window on her cognitive processes [43]. Thus, analyzing the data recorded using an eye-tracking device allows understanding in detail a subject’s process of acquiring data, for example during program comprehension.
However, there were still some major obstacles to the widespread adoption of eye-tracking devices. Besides the costs of such devices, the provided analysis software systems were not open-source and often not extensible, preventing the development and seamless integration of new sophisticated analyses [27].
Consequently, we undertook the development of the Taupe system to visualize, analyze and edit eye-tracking data with the main objectives of compatibility and extensibility. We presented the Taupe system: its context, implementation (based on good practices and a thorough documentation, validation, and verification process), and three case studies using Taupe.
Taupe is being developed by the Ptitdej Team and released under the GPL and its development will continue in the future to improve its architecture, design, user interface, and to provide more sophisticated measures and analyses. We encourage researchers and practitioners to download its source code (see footnote 8) and to contribute with measures and analyses of their own.
 21 http://www.r-project.org/.
Appendix. Supplementary data
B. De Smet et al. / Science of Computer Programming 79 (2014) 260–278 277
Supplementary data to this article can be found online at http://dx.doi.org/10.1016/j.scico.2012.01.004. References
[1] A.V. Aho, J.E. Hopcroft, J.D. Ullman, The Design and Analysis of Computer Algorithms, Addison-Wesley, 1974.
[2] R.Bednarik,M.Tukiainen,Aneye-trackingmethodologyforcharacterizingprogramcomprehensionprocesses,in:Proceedingsof5thsymposiumon
Eye Tracking Research & Applications, ACM Press, 2006, pp. 125–132.
[3] B.Bellay,H.Gall,Acomparisonoffourreverseengineeringtools,in:I.Baxter,A.Quilici(Eds.),Proceedingsofthe4thWorkingConferenceonReverse
Engineering, IEEE Computer Society Press, 1997, pp. 2–11.
[4] D. Binkley, M. Davis, D. Lawrie, C. Morrell, To camelcase or under_score, in: Proceedings of the 17th International Conference on Program
Comprehension, 2009, pp. 158–167.
[5] B.Boehm,H.D.Rombach,M.V.Zelkowitz,FoundationsofEmpiricalSoftwareEngineering:TheLegacyofVictorR.Basili,1sted.,Springer-Verlag,2005.
[6] B.W. Boehm, Software Engineering Economics, Springer-Verlag New York, Inc., New York, USA, 2002, pp. 641–686.
[7] R. Brooks, Using a Behavioral Theory of Program Comprehension in Software Engineering, IEEE Press, Piscataway, NJ, USA, 1978.
[8] R. Brooks, Using a behavioral theory of program comprehension in software engineering, in: M.V. Wilkes, L. Belady, Y.H. Su, H. Hayman, P. Enslow
(Eds.), Proceedings of the 3rd International Conference on Software Engineering, IEEE Computer Society Press, 1978, pp. 196–201.
[9] C.F. Chabris, S.M. Kosslyn, Representational correspondence as a basic principle of diagram design, in: Knowledge and Information Visualization,
Springer-Verlag, 2005, pp. 36–57.
[10] H.C. Purchase, J.A. Allder, D. Carrington, Graph layout aesthetics in UML diagrams: user preferences, Journal of Graph Algorithms and Applications 6
(2002) 255–279.
[11] A.T. Duchowski, Eye tracking methodology, Theory and Practice 328 (2007).
[12] H.Eichelberger,Niceclassdiagramsadmitgooddesign?in:J.T.Stasko(Ed.),Proceedingsofthe1stSymposiumonSoftwareVisualization,ACMPress,
2003, pp. 159–168.
[13] A. Endres, D. Rombach, A Handbook of Software and Systems Engineering, 1st ed., Addison-Wesley, 2003.
[14] Gamma Erich, R.J. Richard Helm, J. Vlissides, Design Patterns Elements of Reusable Object-Oriented Software, Addison-Wesley Pub. Co., 1995.
[15] Eye Response Technologies, 2009. GazeTracker Reference Manual. Eye Response Technologies Inc.
[16] P.M. Fitts, R.E. Jones, J.L. Milton, Eye movements of aircraft pilots during instrument-landing approaches, Aeronautical Engineering Review 9 (1950)
24–29.
[17] E. Gamma, K. Beck, Test infected: programmers love writing tests, Java Report 3 (1998) 37–50.
[18] J.H. Goldberg, X.P. Kotval, Computer interface evaluation using eye movements: methods and constructs, International Journal of Industrial
Ergonomics 24 (1999) 631–645.
[19] Y.G.Guéhéneuc,Areverseengineeringtoolforpreciseclassdiagrams,in:J.Singer,H.Lutfiyya(Eds.),Proceedingsofthe14thIBMCentersforAdvanced
Studies Conference, CASCON, ACM Press, 2004, pp. 28–41. 14 pages.
[20] Y.G. Guéhéneuc, Taupe: towards understanding program comprehension, in: H. Erdogmus, E. Stroulia (Eds.), Proceedings of the 16th IBM Centers for
Advanced Studies Conference, CASCON, ACM Press, 2006, pp. 1–13. 13 pages.
[21] Y.G. Guéhéneuc, A theory of program comprehension—joining vision science and program comprehension, International Journal of Software Science
and Computational Intelligence 1 (2009) 47 pages.
[22] D. Gusfield, Algorithms on Strings, Trees, and Sequences: Computer Science and Computational Biology, Cambridge University Press, New York, NY,
USA, 1997.
[23] I. Hadar, O. Hazzan, On the contribution of UML diagrams to software system comprehension, Journal of Object Technology 3 (2004) 143–156.
[24] D. Hamlet, Foundations of Empirical Software Engineering, vol. 19, Springer, Berlin Heidelberg, 2005.
[25] ISO/IEC, 1996. EBNF Grammar Specification. Technical Report ISO/IEC 14977.
[26] D. Jackson, A. Waingold, Lightweight extraction of object models from bytecode, in: D. Garlan, J. Kramer (Eds.), Proceedings of the 21st International
Conference on Software Engineering, ACM Press, 1999, pp. 194–202.
[27] R.J.K. Jacob, K.S. Karn, Commentary on section 4: eye tracking in human–computer interaction and usability research: ready to deliver the promises,
2002.
[28] S. Jeanmart, Evaluation de l’impact d’un patron de conception sur la comprehension et la maintenance de programmes — une experimentation par
un systeme d’eye-tracking, Master’s Thesis. Facultes Universitaires Notre-Dame de la Paix. Namur, Belgium, 2008.
[29] S. Jeanmart, A study of the impact of design patterns on program comprehension and maintenance activities, ACM SIGSOFT 2008/FSE 16, 2008.
[30] S. Jeanmart, Y.G. Guéhéneuc, H. Sahraoui, N. Habra, Impact of the visitor pattern on program comprehension and maintenance, in: J. Miller, R. Selby
(Eds.), Proceedings of the 3rd International Symposium on Empirical Software Engineering and Measurement, ESEM, IEEE Computer Society Press,
2009, 10 pages.
[31] M. Just, P.A. Carpenter, Eye fixations and cognitive processes, Cognitive Psychology 8 (1976) 441–480.
[32] H.M. Kienle, H.A. Muller, The tools perspective on software reverse engineering: requirements, construction, and evaluation, Advances in Computers
(2010) 189–290.
[33] M.M. Lehman, Programs life cycles and laws of software evolution, Proceedings of the IEEE 68 (1980) 1060–1076.
[34] A. von Mayrhauser, Program comprehension during software maintenance and evolution, IEEE Computer 28 (1995) 44–55.
[35] G.C. Murphy, M. Kersten, M.P. Robillard, D. Čubraniś, The emergent structure of development tasks, in: A.P. Black (Ed.), Proceedings of the 19th
European Conference on Object-Oriented Programming, Springer-Verlag, 2005, pp. 33–48.
[36] A. Newell, You can’t play 20 questions with nature and win, in: W. Chase (Ed.), Visual Information Processing, Academic Press, 1973.
[37] S.E. Palmer, Vision Science: Photons to Phenomenology, 1st ed., The MIT Press, 1999.
[38] D.E. Perry, A.L. Wolf, Foundations for the study of software architecture, SIGSOFT Software Engineering Notes 17 (1992) 40–52.
[39] A. Poole, L.J. Ball, In search of salience: a response time and eye movement analysis of bookmark recognition, People and Computers XVIII-Design for
Life: Proceedings of HCI 2004, 2004.
[40] A. Poole, L.J. Ball, Eye tracking in human–computer interaction and usability research: current status and future prospects, in: Claude Ghaoui (Ed.),
Encyclopedia of Human Computer Interaction, 2006.
[41] M. Potel, MVP: Model-view-presenter — the taligent programming model for C++ and java. Taligent, Inc., 1996.
[42] V.Rajlich,Programcomprehensionasalearningprocess,in:Y.Wang(Ed.),Proceedingsofthe1stInternationalConferenceonCognitiveInformatics,
IEEE Computer Society Press, 2002, pp. 343–347.
[43] K. Rayner, Eye movements in reading and information processing: 20 years of research, Psychological Bulletin 124 (1998) 372–422.
[44] J. San Agustin, H. Skovsgaard, E. Mollenbach, M. Barret, M. Tall, D.W. Hansen, J.P. Hansen, Evaluation of a low-cost open-source gaze tracker,
in: Proceedings of the 2010 Symposium on Eye-Tracking Research #38; Applications, ACM, New York, NY, USA, 2010, pp. 77–80.
[45] Seeing Machine, 2010. Seeing Machine’s website — FaceLAB. http://www.seeingmachines.com/product/facelab/ (accessed 23.12.2010).
[46] B. Sharif, J. Maletic, The effect of layout on the comprehension of uml class diagrams: a controlled experiment, in: 5th IEEE International Workshop
on Visualizing Software for Understanding and Analysis, VISSOFT 2009, IEEE, 2009, pp. 11–18.
278 B. De Smet et al. / Science of Computer Programming 79 (2014) 260–278
[47] B. Sharif, J.I. Maletic, An eye tracking study on camelcase and under_score identifier styles, in: Proceedings of the 18th International Conference on Program Comprehension, 2010, pp. 196–205.
[48] B. Sharif, J.I. Maletic, An eye tracking study on the effects of layout in understanding the role of design patterns, in: Proceedings of the 2010 IEEE International Conference on Software Maintenance, IEEE Computer Society, Washington, DC, USA, 2010, pp. 1–10.
[49] F. Simon, F. Steinbrückner, C. Lewerentz, Metrics based refactoring, in: P. Sousa, J. Ebert (Eds.), Proceedings of the 5th Conference on Software Maintenance and Reengineering, IEEE Computer Society Press, 2001, pp. 30–38.
[50] E. Soloway, Learning to program = learning to construct mechanisms and explanations, Communications of ACM 29 (1986) 850–858.
[51] I. Sommerville, Software Engineering — Fifth Edition, Addison-Wesley Publishing Company, Reading, MA, 1996.
[52] D. Spinellis, Code Reading: The Open Source Perspective, 1st ed., Addison Wesley, 2003.
[53] SR Research Ltd., 2006. EyeLink II User Manual version (07/02/2006). SR Research Ltd.
[54] M.A.D.Storey,F.D.Fracchia,H.A.Müller,Cognitivedesignelementstosupporttheconstructionofamentalmodelduringsoftwareexploration,Journal of Systems and Software 44 (1999) 171–185.
[55] Cepeda Porras Gerardo, Y.G. Guéhéneuc, An empirical study on the efficiency of different design pattern representations in UML class diagrams, Empirical Software Engineering 15 (2010) 27 pages.
[56] D. Sun, K. Wong, On evaluating the layout of UML class diagrams for program comprehension, in: J.R. Cordy, H. Gall (Eds.), Proceedings of the 13th International Workshop on Program Comprehension, IEEE Computer Society Press, 2005, pp. 317–326.
[57] F. Swartz, Ui-model structure, 2004. http://www.leepoint.net/notes-java/GUI/structure/30presentation-model.html (accessed 18.10.2010).
[58] The Institute of Electrical and Electronics Engineers, 1990. IEEE standard glossary of software engineering terminology. IEEE Standard.
[59] H.Uwano,M.Nakamura,A.Monden,K.ichiMatsumoto,Analyzingindividualperformanceofsourcecodereviewusingreviewers’eyemovement,in:
Proceedings of the 2006 symposium on Eye Tracking Research & Applications, 2006, pp. 133–140.
[60] B. Van Den Plas, La theorie ‘‘Vision-Comprehension’’ appliquee aux patrons de conception, Master’s Thesis, Facultes Universitaires Notre-Dame de la
Paix, Namur, Belgium, 2009.
[61] A.VonMayrhauser,A.M.Vans,ProgramComprehensionDuringSoftwareMaintenanceandEvolution,IEEEComputerSocietyPress,LosAlamitos,CA,
USA, 1995, volume 28, pp. 44–55.
[62] L. Wilkinson, The history of the cluster heat map, American Statistician 63 (2009) 179–184.
[63] S.Yusuf,H.Kagdi,J.I.Maletic,AssessingthecomprehensionofUMLclassdiagramsviaeyetracking,in:E.Stroulia,P.Tonella(Eds.),Proceedingsofthe
The Reading Assistant: Eye Gaze Triggered Auditory Prompting for Reading Remediation
John L. Sibert and Mehmet Gokturk
Department of Computer Science The George Washington University Washington D.C. 20052, USA Tel: 1-202-994-4953
E-mail: sibert/gokturk@seas.gwu.edu
ABSTRACT
We have developed a system for remedial reading instruction that uses visually controlled auditory prompting to help the user with recognition and pronunciation of words. Our underlying hypothesis is that the relatively unobtrusive assistance rendered by such a system will be more effective than previous computer aided approaches. We present a description of the design and implementation of our system and discuss a controlled study that we undertook to evaluate the usability of the Reading Assistant.
KEYWORDS: eye tracking, eye gaze, reading disability, interaction techniques
INTRODUCTION
In recent years the computer has become a key classroom tool for the remediation of reading disabilities. To assist the reader, multimedia educational software is available that will allow text, displayed on a computer screen, to be sequentially highlighted and spoken by the computer. Existing software either “reads aloud” with the student reading along, or requires the student to request assistance;
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
UIST ’00. San Diego, CA USA
 2000 ACM 1-58113-212-3/00/11... $5.00
Robert A. Lavine
Department of Physiology and Experimental Medicine
The George Washington University Washington D.C. 20052, USA Tel: 1-202-994-3550 E-mail: phyral@gwumc.edu
usually by mouse selection. We feel that an ideal computer-based remediation tool would allow the student to concentrate on the reading task assisted by automated computer response as necessary. This is borne out by anecdotal evidence. Remedial reading teachers have indicated to us that an effective technique for helping children with reading problems is for the teacher to read in a very low voice, or whisper, with a slight delay as the student reads aloud.
In this paper, we present the Reading Assistant; a tool to evaluate the effectiveness of visually activated prompting in the context of a remedial reading program. Potential beneficiaries include the estimated 10 million children with dyslexia in the 50,000 school districts in the U.S., as well as large numbers of adults with learning disabilities.
BACKGROUND
Research using eye movement tracking technologies can be characterized as interaction technique development [1,3,14] or observational research (e.g. in physiology or psychology) [2,8,10,12,14]. In our research we are doing both. We developed the GWGazer system as a test bed for experiments relating eye movements during display scanning to predicting user alertness and attentiveness [4,5]. The Reading Assistant is an extension of GWGazer that tracks the reader’s eye movements and, using principles derived from reading research, aids the reader by pronouncing words that appear to be hard to recognize.
 CHI Letters vol 2, 2
101
Eye movements in reading research
Eye movement during reading has been investigated for many years, and ranges of values for various eye movement measures have been established for normal readers [12]. Increased text difficulty imposes a greater burden on cognitive processing capacity that is reflected in increased length of fixation and other features of eye movement. During adult reading, eyes move in a sequence of pauses or fixations separated by rapid, intermittent movements or saccades (see Figure 1). The saccades are generally rightward or progressive, but sometimes may be leftward or regressive. Variables that have been measured include number of fixations, duration of fixations, location of fixations in the line of text, number of progressive saccades, number of regressive saccades, and size of saccades [10,11,12,15].
The average progressive saccade encompasses about 7-9 character spaces, equivalent to about 2 degrees of visual angle [10,12], which overlaps the estimated extent of the foveal region of the retina. The typical regressive saccade size is smaller, about 3-4 letters. The number of regressive saccades is about 10-20% of all saccades in skilled readers. These regressions are believed to take place when there is difficulty understanding the text, misinterpretation of the text, or overshooting of the target area. The average fixation duration for skilled readers is reported to be about 250 msec [10,12], but there are great individual differences. For a specific reader, fixation duration may range from 100 to over 500 msec.
Figure 1: Typical reading pattern. The gray circles represent fixations while the lines represent saccades.
Experiments have explored the effect of fixation position within a word on fixation duration (before the following forward saccade) [10]. The duration of a fixation is correlated with reading difficulty, so that words that are more difficult typically require longer fixations for their identifications. The length of saccades may be influenced
by "crude visual clues" [10]. For example, the saccades entering or leaving longer words tend to be longer.
Marked developmental trends in eye movement during reading by children also have been described [12,15]. With increasing grade level in elementary school, there is a decrease in mean fixation duration and an increase in mean saccade length. According to McConkie and Zola [8] "...research involving eye movement monitoring can help in understanding the nature of the mental processes involved in reading, how these develop as one learns to read, and what processing strategies or characteristics are more common in those children who fail to show normal progress in learning to read. Eye movement data are useful in analyzing simultaneously collected data, such as ...oral reading protocols... (and) ...can be used for controlling experimental manipulations during ongoing reading."
Remediation with mouse-activated prompting
Previous research [6,9] has shown substantial improvement in word-identification skills in children with reading disabilities using mouse-activated prompting. Reading from desktop computers with synthetic speech prompting of difficult words has improved timed word recognition in an experimental reading remediation program [18]. Subjects were children in grades 3 to 6, reading at or below the 10th percentile. During daily 25 minute periods for a total of about 10 hours, each student read from the monitor and used the mouse to highlight and then pronounce difficult words or sub word components. Results showed a significant improvement by the experimental group for timed word and non-word recognition. This remedial program is an example of "Perhaps the most promising of all computer-assisted aids for reading acquisition..." [16].
In a more recent publication [19], the basic deficit in dyslexia, the most common reading disorder, is described as word identification associated with impairment in phoneme awareness (segmenting and manipulating sounds within a syllable) and phonological decoding (translating print to sound). After children highlighted a word with the mouse, the word was pronounced as a whole, in syllables, or in segments within each syllable. Improvements were similar "whether the computer speech assistance for 'targeted' words was in whole or segmented form".
Although such mouse-activated acoustic prompting appears to be effective, the need to indicate difficult words by clicking on them with a mouse requires precise eye-hand
   CHI Letters vol 2, 2
102
                     EYEGAZE ENGINE
s
hared system data
WHOLETEXT OBJECT
WORD OBJECT
     DRAW
                 CORE MANAGER
HIGHLIGHT
    R E C T A N GL E
               window manager
PRONOUNCE
          eyegaze log record
                   ANALYSIS
ASS
IST ENGINE
SOUND DICTIONARY
    ENGINE
             coordination and adds a significant delay. Furthermore, the user may not always be aware immediately of when he or she is having trouble identifying a word. In addition, the use of the mouse requires the user to interrupt the cognitive process of oral reading and understanding by interjecting a manual task. If this interruption were not necessary, the natural learning process could proceed more naturally and may be enhanced. A technique in which prompting is continuously adapted to the reader's eye movement, eliminates these interrupting factors allowing the natural learning process to proceed more smoothly
THE GWGazer READING ASSISTANT
The Reading Assistant is a visually activated, interactive reading program for use with multimedia PC systems. The system uses eye tracking to trigger synthetic speech feedback as children read text from the monitor. The system takes advantage of (1) the ability of unobtrusive eye tracking systems to follow eye motion and (2) the ability of text-to-speech software to help children learn to read. As students read text displayed on a computer screen, a video camera, mounted below the screen, monitors the students' eye motions. The eye tracking system analyzes the infrared video image of the eye and computes the coordinates of the gaze-point (the point at which the eye is looking) on the screen and sends them to the GWGazer application that we have developed for our research. This application keeps track of the user’s scan of the displayed text in real time. Visual and auditory cues are produced that depend on where the student is looking and whether changes in scan pattern indicate difficulties in identifying a word.
The Reading Assistant is a straightforward extension of the GWGazer system (Figure 2) which we have been developing as part of our eye tracking research program[4,5,13]. GWGazer includes an EYEGAZE ENGINE which communicates with the eye tracker (in this case an Eyegaze Development System from LC Technologies that is running on a second PC) and provides most of the functionality for gaze point tracking and data capture and analysis.
Figure 2: GWGazer Reading Assistant. The dashed line incorporates GWGazer.
The Reading Assistant extension includes an ASSIST ENGINE that determines when to highlight and pronounce a word using the algorithm described below in the section on fixations and dwell time. Words are represented as word objects with methods for drawing, highlighting, indicating their rectangular extent, and pronouncing themselves. A text object is an array of lines where each line is an array of words.
A preprocessing step allows any standard text file to be displayed on the screen in font sizes ranging from 12 pt to 96 pt. The text is displayed in dark blue on a pure white background. The researcher can interactively change the color of the text as well as line and word separation.
Digitized speech corresponding to each word in the text sample is stored in the form of sound files in a sound dictionary. The dictionary currently consists of a directory containing sound files in wave format (*.wav), sampled at 4khz and 8 bit resolution. For example, the word “hello” has a corresponding “hello.wav” file. Each word object contains a reference to its associated sound file. We chose this approach over voice synthesis because of its simplicity and ability to provide tight control over each word, and to provide more natural speech. In English, multiple meanings and pronunciations can be associated with the same spelling of a word. For example, "It is not appropriate to appropriate someone else's paper." Our approach stores two different word objects with the same name but different pronunciations in the array representing this sentence.
Gaze point data is used by the "ASSIST ENGINE" to trigger highlighting and pronunciation of words. When the
               CHI Letters vol 2, 2
103
 reader gazes at a particular word longer than a predetermined duration, the word is first highlighted and then spoken by the system.
The system collects eye gaze data during reading at 30Hz or 60Hz sampling rates, depending on which version of the Eyegaze system we use. The data is saved in a special format for post processing and analysis.
In our preliminary studies, all words had the same duration threshold. However, since threshold levels are embedded within the word object, each word is able to have a different threshold, related to the difficulty and frequency of the word in the language.
WORD SELECTION ALGORITHM
Determining gaze duration on a single word presents some difficulties. We can only approximate the exact gaze-point. The accuracy of the system is about 0.25 inch on the screen when viewed from a normal reading distance (20 inches). If subjects move around, the gaze-point can drift. Also there is a difference between the physiological gaze-point and the location of letters that are perceived and processed cognitively. For example, a “perceptual span” of approximately 6-7 letters has been found between the gaze- point and the location of cognitive interest [10]. This tendency for readers to “gaze ahead” suggests the need for rapid selection of the correct word so that it can be pronounced before the reader becomes aware of the time lag.
Because of these difficulties, the system must use additional information in order to determine the exact word that the reader is focusing on at any given moment. We try to predict the word of interest by taking advantage of the sequential nature of the reading task.
Previous research has shown that a reader tends to fixate on a word or group of words in sequence while reading a line of text. These visual fixations are separated by small horizontal saccadic eye movements. This pattern is illustrated in figure 3. At the end of each line of text, the reader’s gaze “flies back” in a horizontal saccade to the beginning of the next line.
Word sequence determines the next word to be highlighted and pronounced. Words within a line of text are only highlighted and pronounced in left to right order. The current line of text is determined by differentiating between
lines using horizontal fly backs that are observable after reading one complete line. The system looks for horizontal fly backs before attempting to change the current line due to vertical variations in eye gaze. If the reader glances briefly away from the text we are less likely to incorrectly deduce a line change.
CHI Letters vol 2, 2
104
800 700 600 500 400 300 200 100
0 78000
1000
900
800 700
600 500 400 300 200 100
0 78000
83000
88000
93000
98000
103000
83000
88000 93000
98000
103000
Vertical
Horizontal
Figure 3: Typical eye gaze movement during reading a text consisting of eight lines.
We have also observed a tendency for users to skip the correct line and briefly fixate on a lower line immediately following a fly back to the left margin of the text. The following word fixations usually return to the correct line. Because the reading task requires sequential reading of the text, we constrain our gaze point track such that the next line is selected even when the Y position drifts down to a lower line. In practice this has turned out to work quite well.
  X i
Xw
rang
Yi
Dwell Treshold2: A higher threshold suggesting that the subject is having difficulty recognizing the word. If Dwell Treshold2 is exceeded the word is pronounced by the system.
READING AND EYE MOVEMENT OBSERVATIONS
In our initial testing we felt that the system was promising although it would sometimes miss words or even pronounce the wrong word. We were interested to see how well it would perform under more rigorous testing so we designed a pilot study to help us determine the usability of the system. A detailed description of the study and its results may be found in [13].
Subjects were 8 children, aged 10-14, whose parents provided informed consent. Four of them were fifth grade students with a history of reading problems and receiving Learning Disabilities (LD) services. Reading passages used were obtained from the Gray Oral Reading Test, Third Edition [17], each was presented twice in immediate succession to observe effects of familiarization and practice. Trials were video and audio taped to help us localize reading errors.
The study was run using a PC system with MS-Windows 95 and 16” monitor (resolution 1024 X 768). Viewing distance was approximately 20 inches. After repeated trials in order to optimize system performance, parameters were set as follows:
Font Type: Times New Roman Font Size: 42 point
Line Separation: 70 pixels Word Distance: 14 pixels Horizontal Margin: 30 pixels Vertical Margin: 60 pixels Dwell Treshold1: 240 msec Dwell threshold2: 360 msec
Results included recorded samples of simultaneous visual scan and oral reading of text passages that can be replayed and analyzed. Data was obtained on reading speed and accuracy, fixations per second and per word, fixation duration, reading errors, and acoustic prompts provided by the system. Some examples of the data follow.
Table 1 compares performance between the first and second trials on both tasks for subjects 1-4 who were reading at or
         The
 bell
      Yw
Line 1 Y axis
Line 2 Y axis
yesterday
    Figure 4: Descriptive parameters for text
Various size and spacing parameters are embedded in word objects and distances can be set interactively by the experimenter (Figure 4):
Xi : Distance between two word objects. This distance is equal for all words and modifiable through the options dialog. The researcher can set the desired word distance according to the experiment.
Yi : Distance between lines of text. This distance is also modifiable. However, it should be greater than the maximum word height so that lines do not overlap.
Xw: The width of a word object. This value is embedded within each word object.
Yw: The height of the word object. Similar to the width value, this value is also contained within the word object.
(Xr,Yr) : The coordinate of the top left corner of the word object’s bounding box in screen space. This value is contained in the word object as well. Therefore each word knows where to draw itself on the screen.
Fixations and Dwell
We define a fixation as a sequence of gaze-points (samples) located within a given distance of each other over at least 100 milliseconds duration. Because fixation duration is frequently insufficient for the perceptual and cognitive processing necessary for word recognition, more than one fixation may occur while reading a word. Therefore, we define “Dwell” as a series of one or more fixations separated by short saccadic motions. Now we can define thresholds of Dwell within a given word’s bounding rectangle:
Dwell Treshold1: A lower threshold signifying that a particular word is the focus of attention. We use Dwell Treshold1 as a trigger to visually highlight the word.
CHI Letters vol 2, 2
105
above grade level.
feasible. Although there were some glitches, we were able to run eight young subjects successfully.
We plan a series of parametric studies to investigate the effects of our settable parameters on system usability. For example, in our pilot study, we used the same dwell time threshold for all of our subjects. However we know that the optimum threshold varies with individuals, as well as with the level of difficulty of the reading material. We hope to develop a method for establishing these parameters for individuals at differing levels of difficulty
Of course our pilot study does not establish the effectiveness of the system. In order to do that, we need to carry out a long-term controlled trial with students using the system regularly in their classroom. We have in fact proposed such a study and hope to be able to carry it out within the next few years.
ACKNOWLEDGEMENTS
We gratefully acknowledge the financial support of the Jet Propulsion Laboratory, the technical support of LC Technologies, and the all around support, suggestions, and comments of the HCI research group at GWU.
REFERENCES
1. Bolt, R. A. (1982). "Eyes at the Interface," Proc. ACM Human Factors in Computer Systems Conference, pp. 360-362.
2. Chapman, J.E., Lavine, R.A., Cleveland, D., Joubert, M., and Fontana, J. (1997). Effects of non-optical color absorptive lenses on eye movement patterns during reading and on low contrast vision. 4th World Congress on Dyslexia, Halkidiki, Greece, September 23-26.
3. Jacob, R. J. K. (1993). "Eye-Movement Based Human-Computer Interaction Techniques: Toward Non-Command Interfaces," in H. R. Hartson and D. Hix (eds) Advances in Human-Computer Interaction, Vol 4. Ablex Publishing Co., Norwood, NJ.
4. Lavine, R.A., Dickens, B., McNamara, J., Sibert, J., Gokturk, M., and Heckman, K.(1999). Eye-tracking in a visual vigilance task. In Jensen, R.S., Callister, J.D., Cox, B., and Lavis, R. (eds.), Proceedings of the Tenth International Symposium on Aviation Psychology. Columbus, Ohio, Ohio State University, v.2., 903-909.
   5th grade text
  10th grade text
  I
 II
  I
 II
 time(s)
 37.0
 30.0
  84.8
 68.6
 words/s
 2.9
 3.6
  1.3
 2.2
 errors
 0.4
 0.0
  4.8
 3.0
 fixations
 122.6
 112.4
  254.8
 240.4
 fixns/sec
 3.3
 3.7
  3.0
 3.5
 fixns/word
 1.2
 1.1
  1.7
 1.6
 fix dur (ms)
 231.4
 228.4
  232.0
 241.0
 prompts
  1.2
  0.4
   9.0
  3.6
          Table 1: Trials I and II for subjects 1-4.
As we expected, there was a learning effect. From trial 1 to trial 2, all subjects showed increased reading speed, more fixations/second, and shorter fixation duration. When we compare the results of the first and second presentation of the reading task, practice on this system was associated with improved reading performance. Subjects increased their reading speed on the fifth-grade paragraph from 2.91 to 3.60 words/sec, and from 3.31 to 3.75 fixations/sec. They increased on the tenth-grade paragraph from 1.28 to 2.24 words/sec, and from 3.00 to 3.50 fixations/sec. Most children read the fifth-grade text without errors even the first time. On the tenth-grade text their errors decreased from 5 to 3 on the second presentation.
We can also compare the number of oral reading errors made with the number of prompts provided by the system. Notice that, for the 10th grade text, the ratio of prompts to reading errors was about two to one for the first trial but dropped almost to one to one for the second. This may indicate that, as students become more acquainted with either the system or the text or both, the prompting becomes more efficient.
We used a questionnaire to determine the subjective reactions of the students. In general, they liked the system and found it easy to use and unobtrusive. Interestingly, the most obtrusive part of the system was the video camera.
CONCLUSIONS AND FUTURE DIRECTIONS
Motivated by a desire to improve reading instruction for children, and adults, with reading disability we have implemented a system which uses a reader’s visual scanning pattern of the text to identify, and pronounce, words that the reader is having difficulty recognizing. Our pilot study encourages us to believe that our approach is
  CHI Letters vol 2, 2
106
5. Lavine, R.A., Sibert, J., and Gokturk, M. (1999). Visual scanning accuracy in a vigilance task: Application to driver alertness monitoring. In Carroll, R., ed., Ocular Measures of Driver Alertness: Technical Conference Proceedings. Herndon, Virginia: Federal Highway Administration and National Highway Traffic Safety Administration, US DOT, 108-117.
6. Lundberg, I. (1995). The computer as a tool of remediation in the education of students with reading disabilities--a theory-based approach. Learning Disability Quarterly 18, 89-100.
7. L yon, G.R. (1995). T oward a definition of dyslexia. Annals of Dyslexia, 45: 3-27.
8. McConkie, G. W. and Zola, D. (1986). Eye Movement Techniques in Studying Differences Among Developing Readers. Technical Report No. 377. Bolt, Beranek and Newman, Inc. Cambridge, Mass; Illinois University Center for the Study of Reading.
9. Olson, R., Forsberg H., Wise, B., and Rack, J. (1994). Measurement of word recognition, orthographic, and phonological skills. Lyon, G.R., ed., op.cit., 243-268.
10. O'Regan, J.K. (1990). Eye movements and reading. In Kowler, E., ed., Eye movements and their role in visual and cognitive processes. Elsevier.
11. Patchberg, J. P., and Yomas, A. (1978). The Effects of the Reader's Skill and the Difficulty of the Text on the Perceptual Span in Reading. Journal of Experimental Psychology: Human Perception and Performance 4:
545-552.
12. Rayner, K. (1978). Eye movements in reading and information processing. Psychological Bulletin 85: 618-660.
13. Sibert, J., Gokturk, M. and Lavine, R Reading Remediation with Eye Gaze Initiated Auditory Feedback Submitted to ACM CHI2001.
14. Sibert, L. and Jacob, R.J.K.(2000) Evaluation of eye gaze interaction, ACM Press, Proceedings of CHI 2000, 281-88.
15. Taylor, S. E. (1965). Eye Movements in Reading: Facts and Fallacies. American Educational Research Journal 2: 187-202.
16. Torgeson, J.K. and Barker, T.A. (1995). Computers as aids in the prevention and remediation of reading disabilities. Learning Disability Quarterly 18, 76-88.
17. Wiederholt, J.L. and Bryant, B.R. (1992). Gray Oral Reading Tests: Third Edition. Austin, Texas: Pro-Ed.
18. Wise, B., Olson, R., Anstett, M., Andrews, L., Terjak, M., Schneider, V., Kostuch, J., and Kriho,L. (1989). Implementing a long-term computerized remedial reading program with synthetic speech feedback: Hardware, software, and real-world issues. Behavior Research Methods, Instruments, & Computers 21: 173- 180.
19. Wise, B.W. and Olson, R.K. (in press), Computer- based phonological awareness and reading instruction. Annals of Dyslexia, 50, p. 95.
  CHI Letters vol 2, 2
     See discussions, stats, and author profiles for this publication at:
https://www.researchgate.net/publication/228616958
The 35th Sir Frederick Bartlett lecture: Eye movements and attention in reading, scene perception, and visual...
Article
CITATIONS READS
168 738
10 authors, including: Denis Drieghe
University of Southampton
84 PUBLICATIONS 1,523 CITATIONS SEE PROFILE
Simon Liversedge
University of Southampton
205 PUBLICATIONS 4,072 CITATIONS SEE PROFILE
      Some of the authors of this publication are also working on these related projects:
Modes of Address in Pictorial Art View project
Eye Movements during Complex Information Processing in Autism Spectrum Disorder
View project
   All content following this page was uploaded by Simon Liversedge on 23 June 2016. The user has requested enhancement of the downloaded file.
THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY 2009, 62 (8), 1457–1506
The 35th Sir Frederick Bartlett Lecture
Eye movements and attention in reading, scene perception, and visual search
    Keith Rayner
University of California, San Diego, CA, USA
Eye movements are now widely used to investigate cognitive processes during reading, scene percep- tion, and visual search. In this article, research on the following topics is reviewed with respect to reading: (a) the perceptual span (or span of effective vision), (b) preview benefit, (c) eye movement control, and (d) models of eye movements. Related issues with respect to eye movements during scene perception and visual search are also reviewed. It is argued that research on eye movements during reading has been somewhat advanced over research on eye movements in scene perception and visual search and that some of the paradigms developed to study reading should be more widely adopted in the study of scene perception and visual search. Research dealing with “real- world” tasks and research utilizing the visual-world paradigm are also briefly discussed.
Keywords: Eye movements; Attention; Reading; Scene perception; Visual search.
Correspondence should be addressed to Keith Rayner, Department of Psychology, University of California, San Diego 92093, CA, USA. E-mail: krayner@ucsd.edu
Preparation of this article was supported by Grant HD26765 from the National Institute of Health. Thanks to Jane Ashby, Chuck Clifton, Denis Drieghe, Harold Greene, John Henderson, Barbara Juhasz, Simon Liversedge, Alexander Pollatsek, and Adrian Staub for helpful comments on a prior draft.
  http://www.psypress.com/qjep
# 2009 The Experimental Psychology Society 1457 DOI:10.1080/17470210902816461
RAYNER
It is well known that viewers can allocate attention independent of eye position in simple tasks in which they must hold their eyes on a fixation target while other stimuli are presented elsewhere in the visual field (Posner, 1980). But, how easy is it to have attention and eye position in different locations in complex tasks like reading, scene per- ception, and visual search? It is my contention that most of the time in such tasks, either (a) eye location (overt attention) and covert attention are overlap- ping and at the same location or (b) attention disen- gagement is a product of a saccade programme (wherein attention precedes the eyes to the next saccade target). Thus, while attention and eye pos- ition can be dissociated in these tasks, any such dissociation is generally a property of the processing system rather than some type of strategy used by readers or viewers in these tasks. Specifically, atten- tion precedes a saccade to a given saccade target location (Deubel & Schneider, 1996; Henderson, 1993; Hoffman & Subramaniam, 1995; Irwin & Gordon, 1998; Irwin & Zelinsky, 2002; Kowler, Anderson, Dosher, & Blaser, 1995; Rayner, McConkie, & Ehrlich, 1978; Shepherd, Findlay, & Hockey, 1986), and this is a property of the pro- cessing system. There are many excellent studies of attention that do not involve eye movements, and they have been very informative with respect to understanding covert attention. However, that type of research is not discussed here. Rather, my goal is to review research on eye movements, as measures of overt attention, during complex cogni- tive processing tasks.
It is quite apparent that research utilizing eye movements to examine cognitive processing tasks is burgeoning as more and more researchers have started to use eye tracking techniques in the last few years. In large part, this is because systems to track the eyes have become more readily available and more user friendly. However, we have also entered an era of eye movement research, particu- larly in the domain of reading, in which a great deal of research is being driven by predictions that emerge from computational models of eye movement control. In this article, research on the following topics are reviewed with respect to reading: (a) the perceptual span (or span of
effective vision), (b) preview benefit, (c) eye move- ment control, and (d) models of eye movements. I also discuss some related issues with respect to eye movements during scene perception and visual search. The review is somewhat selective in that I focus largely on research from my laboratory and related work. Furthermore, the review is not as systematic with respect to scenes and search as it is with reading; some issues that I find interest- ing with respect to scenes and search are the focus of this part of the review. In large part, it is argued that research on eye movements during reading has been somewhat advanced over research on eye movements in scene perception and visual search and that some of the paradigms developed to study reading should be more widely adopted in the study of scene perception and visual search. Research dealing with “real-world” tasks and research utilizing the visual-world paradigm are also briefly discussed.
Background information on eye movements
The two basic components of eye movements in the various tasks under consideration here are the movements themselves (called saccades) and the fixations (the period of time when the eyes remain fairly still and new information is acquired from the visual array). Since vision is suppressed during a saccade (Matin, 1974), new information is only acquired during fixations. There are special situations wherein information can be acquired during a saccade (see Campbell & Wurtz, 1979; Uttal & Smith, 1968), but under most normal cir- cumstances we do not obtain new information during a saccade because the eyes are moving so quickly across the stable visual stimulus that only a blur would be perceived. Furthermore, masking, caused by the information available before and after the saccade, eliminates any perception of blurring. While new information is not encoded during saccades, cognitive processing does continue in most situations during the saccade (Irwin, 1998; Irwin & Carlson-Radvansky, 1996).
Eye movements are motor responses that take time to plan and execute. Saccade latency, the time needed to encode the location of a target in
1458 THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8)
the visual field and initiate an eye movement, is of the order of 175 – 200 ms (Becker & Ju ̈ rgens, 1979; Rayner, Slowiaczek, Clifton, & Bertera, 1983). However, it varies quite a bit as a function of the exact nature of the situation. Saccade duration, the amount of time that is takes to actually move the eyes, is a function of the distance moved. A 2-deg saccade, typical of reading, takes about 30 ms, while a 5-deg saccade, typical of scene perception, takes around 40 to 50 ms (Abrams, Meyer, & Kornblum, 1989; Rayner, 1978a).
Eye movements are necessary because of the anatomy of the retina and limitations due to acuity outside of the fovea. In reading, for example, the line of text that the reader is looking at can be divided into three regions: the foveal region (2 degrees in the centre of vision), the parafoveal region (extending from the foveal region to about 5 degrees on either side of fixation), and the peripheral region (everything beyond the parafoveal region). Although acuity is very good in the fovea, it is not nearly so good in the parafovea, and it is even poorer in the periphery. Hence, viewers move their eyes so as to place the fovea on that part of the stimulus they want to see clearly. While the same kind of constraints hold for scene perception and visual search, unless the array is particularly dense, viewers can typically process more information around their fixation point (i.e., further into eccentric vision) in these tasks than in reading.
It is tempting to think that eye movements in each of these tasks would be controlled by the same mechanisms, and that the same principles, with respect to eye movements, should hold across the three tasks. After all, the neural circuitry for controlling eye movements is the same across the tasks. However, it is actually somewhat hazardous to generalize across these tasks in
terms of eye movement behaviour. For example, Rayner, Li, Williams, Cave, and Well (2007c) demonstrated that fixation durations and saccade lengths in reading do not correlate with those measures in scene perception and search (see also T. J. Andrews & Coppola, 1999). Interestingly, fixation duration and saccade length did not corre- late significantly with each other1 within any of these tasks (see also Castelhano & Henderson, 2008a; Rayner & McConkie, 1976). Why do eye movement measures in reading not correlate well with the same measures in scene perception and visual search despite having the same underlying neural circuitry? Presumably, the cognitive mech- anisms involved in the different tasks, and how the cognitive system interacts with the oculomotor system, differ as a function of the task.
In addition to the fact that there are not particularly high correlations in eye movement measures across tasks,2 it is also the case that the basic descriptive characteristics of eye movements differ as well. Table 1 shows the range of average fixation durations typically associated with silent reading, oral reading, scene perception, and visual search. From Table 1, it is obvious that fix- ations tend to be longer (a) in oral reading than in silent reading and (b) in scene perception than in reading, and that (c) the range of fixation dur- ations is greater in visual search than in the other tasks. Fixations are longer in oral reading than silent reading because (a) the reader has to produce each word as it is read, and (b) the eyes (which move faster than the reader can produce words) often stay in place longer so that they do not get too far ahead of the voice. Fixations in scene perception tend to be longer than those in reading because information is taken in from a wider area in scenes than in reading. And, the large range associated with fixation durations in
EYE MOVEMENTS AND ATTENTION
 1 While across a large passage of text, the correlation between fixation duration and saccade length tends to be close to zero (Rayner & McConkie, 1976), it is the case that for certain segments of text one can find reasonable-sized correlations. Thus, when the text is difficult, readers make long fixations and short saccades, leading to a significant correlation.
2 Within task, there are very high correlations for eye movement characteristics. Thus, Castelhano and Henderson (2008a) found that fixation durations for a group of viewers tended to correlate highly independently of whether a photo or line drawing was used as the stimulus. Likewise, saccade length was highly correlated. But, again, there was little correlation between fixation duration and saccade length per se.
THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8) 1459
RAYNER
Table 1. The range of mean fixation durations and the mean saccade length in silent reading, oral reading, scene perception, and visual search
long as 500 – 600 ms (or more), and saccade length can be as short as 1 letter space and as long as 15 – 20 letter spaces (or more). Regressions (saccades that move backwards in the text) are the third important component of eye movements in reading and occur about 10–15% of the time in skilled readers. The long saccades just mentioned tend to follow a regression since readers typically move forward in the text past the point from which they originally launched the regression. Most regressions are to the immediately preceding word, though when comprehension is not going well or the text is particularly difficult, more long-range regressions occur to earlier words in the text. Regressions are not particularly well understood because it is difficult to control them experimentally (though see Inhoff & Weger, 2005; Murray & Kennedy, 1988; Rayner, Juhasz, Ashby, & Clifton, 2003a; Weger & Inhoff, 2006, 2007; for an interesting discussion of regressions due to sentence parsing difficulties, see Mitchell, Shen, Green, & Hodgson, 2008). Finally, regressions need to be distinguished from return sweeps, which are right-to-left saccades from the end of one line to the beginning of the next. It is also instructive to note that the first and last fixations on a line are typically 5–7 letter spaces from the end of the line. Thus, about 80% of the text typically falls between the extreme fixations.
The values shown in Table 1 are for skilled readers of English. However, these values can be very much influenced by text difficulty, reading skill, and characteristics of the writing system. Thus, as text gets more difficult, fixations get longer, saccades get shorter, and more regressions are made (Rayner, 1998). Also, typographical variables like font difficulty can influence eye movements; more difficult to encode fonts yield longer fixations, shorter saccades, and more regressions (Rayner, Reichle, Stroud, Williams, & Pollatsek, 2006d; Slattery & Rayner, 2009). Beginning and dyslexic readers have longer fixations, shorter saccades, and more regressions than skilled readers (Rayner, 1998), as do less skilled readers (Ashby, Rayner, & Clifton, 2005). As far as writing system is concerned, the one that is most different from English is Chinese.
 SL
  Silent reading Oral reading Scene perception Visual search
FD (ms) Deg
225 – 250 2 275 – 325 1.5 260 – 330 4–5 180 – 275 3
Letters
7–9 6–7
 Note: FD 1⁄4 fixation duration; SL saccade length. The average fixation duration in scene perception and visual search can very much be influenced by the exact nature of the task that participants are given.
visual search is presumably related to how simple or complex the search array is.
With respect to saccade lengths, for reading the appropriate metric is letters rather than visual angle since the distance the eyes traverse from one saccade to the next is determined by letters rather than visual angle as long as the text is of normal size (Morrison & Rayner, 1981; see also McDonald, 2006a). However, for comparability to the other tasks, saccade size during reading is also given in degrees of visual angle in Table 1. It is also obvious that the distance the eyes move in scene perception and visual search is typically larger than that in reading (again, presumably because more information is being taken in on each fixation), but saccade size in visual search can be highly variable depending on the complex- ity of the array; when the array is complex and crowded, saccades are shorter (the same would hold for a highly complex scene).
Eye movements in reading
As indicated in Table 1, the average fixation dur- ation in reading is on the order of 225 – 250 ms, and the average saccade length is 7 – 9 letter spaces for readers of English and other alphabetic writing systems. However, it is important to keep in mind that these values are averages, and there is considerable variability in both. Thus, fixation durations can be as short as 50 – 75 ms and as
1460 THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8)
Chinese readers tend to have average fixations durations that are quite similar to readers of English, and their regression rate does not differ dramatically. Where they do differ is that their average saccade length is much shorter than that of readers of English as they typically move their eyes only 2 – 3 characters (which makes sense given that linguistic information in Chinese is more densely packed than in English). Likewise, readers of Hebrew (a language that is also more densely packed than English largely because vowels are deleted in the printed orthography) tend to yield shorter saccades (about 5.5 letter spaces) than readers of English (Pollatsek, Bolozky, Well, & Rayner, 1981), though their fixation durations are comparable.
One great virtue of eye movement data is that they give a good moment-to-moment indication of cognitive processes during reading. Thus, vari- ables such as word frequency and predictability have strong influences on fixation times on a word (Rayner, 1998). However, the average fixation duration measure is not a particularly informative measure for inferring moment-to- moment processing; it is a valuable global measure, but there are also a number of local measures that provide more informative estimates of moment-to-moment processing time.
The problem with average fixation duration is related to two components of reading. First, readers skip words during reading;3 content words are fixated about 85% of the time, while function words are fixated about 35% of the time. Function words are skipped more because they tend to be short, and there is a clear relation- ship between the probability of fixating a word and its length. As word length increases, the prob- ability of fixating a word increases (Rayner & McConkie, 1976; Rayner, Sereno, & Raney, 1996) or conversely as the length of the word decreases, the probability of fixating it decreases, and the probability of skipping it increases (Brysbaert, Drieghe, & Vitu, 2005; Rayner,
1998). Words that are 2 – 3 letters long are only fixated around 25% of the time, whereas words that are 8 letters or more are almost always fixated. Second, longer words are often fixated more than once before leaving the word; that is, they are refixated (see McConkie, Kerr, Reddix, Zola, & Jacobs, 1989; McDonald & Shillcock, 2004; Vergilino & Beauvillain, 2000, 2001; Vergilino-Perez, Collins, & Dore-Mazars, 2004). The joint problem of skipping and refixations has led to the development of alternative measures of fixation time. These measures are first-fixation duration (the duration of the first fixation on a word), single-fixation duration (those cases where only a single fixation is made on a word), and gaze duration (the sum of all fixations on a word prior to moving to another word). All of the measures are contingent on the word being fixated on a first-pass forward fixation.
If it were the case that readers fixated each word and only once on each word, then average fixation duration on a word would be a useful measure. But, as noted above, the reality is that many words are skipped, and some words are refixated. There is good reason (see below for discussion) to believe that the words that are skipped are pro- cessed on the fixation prior to the skip (and after the skip to some extent), and likewise there is good reason to think that words are refixated in order to fully process their meaning. The solution to this possible conundrum is to utilize all three measures just described, which provide a reason- able, though not perfect, estimate of how long it takes to process each word (Rayner, 1998). The reasons these measures are not a perfect estimate are, as is discussed below, that (a) preview infor- mation is obtained from a word prior to fixating it, and (b) there are spillover effects, wherein the processing of a given word spills over to the next fixation (Rayner & Duffy, 1986). When regions of interest are larger than a single word, a number of other measures like first-pass reading time, second-pass reading time, go-past time
EYE MOVEMENTS AND ATTENTION
 3 The fact that words are skipped obviously means that readers do not invariably move forward in the text fixating on each successive word in its canonical order. However, some type of inner speech code presumably aids the reader to maintain the correct word order.
THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8) 1461
RAYNER
(the elapsed time from when a reader first enters a region until they move past it forward in the text, sometimes also referred to as regression path duration), and total reading time are generally computed.
When readers reread passages of text, the fixation pattern tends to remain fairly similar across readings (Hyo ̈na ̈ & Niemi, 1990; Raney & Rayner, 1995; Schnitzer & Kowler, 2006). Fixations become a bit shorter, and saccades become a bit larger; the main difference is that fewer fixations are made, and the frequency of regressions decreases. While average fixation dur- ations and saccade sizes are only minimally affected when passages are reread, when fixation times associated with specific target words are examined via local measures (first fixation and gaze duration), there is a larger decrease in fixation time. This further demonstrates why average fix- ation time is not a particularly good measure of moment-to-moment processing.
Do the two eyes move in synchrony during reading (and other tasks)? It has generally been assumed that there is near-perfect binocular coordination during reading and that the two eyes start moving at the same time and land on the same letter. However, recent research (see Kirkby, Webster, Blythe, & Liversedge, 2008, for a comprehensive review) has demonstrated that up to 40–50% of the time the eyes are on different letters, and sometimes the two eyes are crossed (Heller & Radach, 1999; Liversedge, Rayner, White, Findlay, & McSorley, 2006a; Liversedge, White, Findlay, & Rayner, 2006b). Interestingly, the amount of disparity tends to be greater in beginning readers than in skilled readers (Blythe et al., 2006). More importantly, perhaps, word frequency and case alternation affect fixation duration in reading, but not fixation disparity (Juhasz, Liversedge, White, & Rayner,
2006). Thus, while researchers may need to worry about those rare situations in which the eyes would be on different words, it is clear that when both eyes are fixated within the same word robust effects like the frequency effect still emerge.
Four central issues with respect to eye move- ments during reading are now discussed. They are: (a) the perceptual span, (b) preview benefit, (c) the control of eye movements, and (d) models of eye movements.
The perceptual span in reading
Each time the eyes pause for roughly 225 – 250 ms, how much information is the reader able to process and use during that fixation? Readers often have the impression that they can clearly see the entire line of text, even the entire page of text. But, this is an illusion as experiments utilizing a gaze-con- tingent moving-window paradigm (McConkie & Rayner, 1975; Rayner & Bertera, 1979) have clearly demonstrated.
In experiments using the moving-window paradigm, the rationale is to vary how much infor- mation is available to a reader and then determine how large the window of normal text has to be before readers read normally (see Figure 1). Conversely, how small can the window be before there is disruption to reading? Thus, in the experi- ments, within the window area text is normally displayed, but outside of the window the letters are replaced (with other letters or with Xs or a homogenous masking pattern).4 Research using this paradigm has demonstrated that skilled readers of English and other alphabetic writing systems obtain useful information from an asymmetric region extending roughly 3 – 4 character spaces to the left of fixation (McConkie & Rayner, 1976a; Rayner, Well, & Pollatsek, 1980b; N. R. Underwood & McConkie, 1985)5 to about 14 – 15 character spaces to the right of fixation
 4 In the most extreme situation, the window contains only the fixated letter, thereby creating a situation in which the reader is literally forced to read letter by letter. In this situation, normal readers’ eye movement data are very much like the eye movement data of brain-damaged pure alexic or letter-by-letter readers (Johnson & Rayner, 2007; Rayner & Johnson, 2005).
5 In some situations, readers can obtain information further to the left of fixation (Binder, Pollatsek, & Rayner, 1999). For example, when a word is skipped, attention may often be directed to the left of fixation following the skip.
1462 THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8)
Figure 1. Examples of the moving-window paradigm. The top line shows a normal line of text. The moving windows consist of the following examples (with two fixations, marked by the þ , shown): a 15-character moving window (7 characters to the left and to the right of fixation), a 29-character window (14 characters to the left and right of fixation), an asymmetric window extending 3 characters to the left of fixation and 7 to the right (with the spaces between words filled in outside of the window), a one-word window, a three-word window, and a two- word window with the letters outside the window replaced with visually similar letters (first line) or random letters (second line). The data generally show that reading is easier when the spaces are not filled in. Also, when the window is small, reading performance is generally better when xs are outside the window than when letters are.
(Continued)
THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8) 1463
EYE MOVEMENTS AND ATTENTION

RAYNER
 Figure 1. Continued.
(DenBuurman, Boersma, & Gerrissen, 1981; McConkie & Rayner, 1975; Rayner & Bertera, 1979; Rayner, Well, Pollatsek, & Bertera, 1982; N. R. Underwood & McConkie, 1985; N. R. Underwood & Zola, 1986). Indeed, if readers have the fixated word and the word to the right of fixation available on a fixation (and all other letters are replaced with visually similar letters), they are not aware that the words outside of the window are not normal, and their reading speed only decreases by about 10%. If two words to the right of fixation are available within the window, there is very little slowdown in reading (Rayner et al., 1982).
In some recent clever experiments, Miellet, O’Donnell, and Sereno (in press) introduced a variation of the moving-window paradigm, which they termed parafoveal magnification, in which letters around the fixation were normal sized, but letters increased in size in eccentric vision so as to compensate for the loss of acuity associated with parafoveal vision. They demon- strated that the size of the perceptual span to the right of fixation remained about 14 letter spaces (thus replicating prior research by McConkie & Rayner, 1975; Rayner & Bertera, 1979) with parafoveal magnification, and they also replicated
1464 THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8)
Figure 2. Examples of the moving-mask paradigm. Three consecutive fixations with a 7-character mask.
the finding (Morrison & Rayner, 1981) that the distance the eyes move is driven by letters. These findings are quite consistent with the view that attention and ongoing processing constraints, and not visual acuity, determine how much infor- mation can be obtained on each eye fixation in reading.
Readers do not utilize information from lines below the currently fixated line (Inhoff & Briihl, 1991; Inhoff & Topolski, 1992; Pollatsek, Raney, LaGasse, & Rayner, 1993). However, if the task is visual search rather than reading, then information can be obtained below the currently fixated line (Pollatsek et al., 1993). Finally, in moving-mask experiments (Fine & Rubin, 1999a, 1999b, 1999c; Rayner & Bertera, 1979; Rayner, Inhoff, Morrison, Slowiaczek, & Bertera, 1981) in which a visual mask moves in synchrony with the eyes on each fixation, thus covering the letters in the centre of vision (see Figure 2), reading is very difficult, if not imposs- ible, as central foveal vision is masked (and only letters in parafoveal vision are available for reading). In essence, the moving-mask paradigm creates an artificial foveal scotoma mimicking patients with brain damage that effectively elimin- ates their use of foveal vision.
Just as the characteristics of the writing system influence saccade size in reading, they also influ- ence the size of the perceptual span. Thus, for readers of Chinese (which is now typically read from left to right in mainland China), the
perceptual span extends 1 character to the left of fixation to 2 – 3 characters to the right (H. Chen & Tang, 1998; Inhoff & Liu, 1998). And the per- ceptual span for Hebrew readers is asymmetric and larger to the left of fixation (Pollatsek et al., 1981) since they read from right to left.
Reading skill also influences the size of the perceptual span since beginning readers (Ha ̈ikio ̈, Bertram, Hyo ̈ na ̈ , & Niemi, 2009; Rayner, 1986) and dyslexic readers (Rayner, Murphy, Henderson, & Pollatsek, 1989) have smaller spans than more skilled readers. Presumably, difficulty encoding the fixated word leads to smaller spans for both beginning and dyslexic readers. Older readers read more slowly than younger college age readers (Laubrock, Kliegl, & Engbert, 2006; Rayner et al., 2006d) and their perceptual span seems to be slightly smaller and less asymmetric than that of younger readers (Rayner, Castelhano, & Yang, in press).
Preview benefit in reading
Research using another type of gaze-contingent display change paradigm, the boundary paradigm (Rayner, 1975), has revealed important information about what kind of information is integrated across saccades. In the boundary paradigm, an invisible boundary is just to the left of a target word (see Figure 3), and before the reader crosses the bound- ary, there is typically a preview different from the target word. When the eyes cross the boundary, the preview is replaced by the target word. The
THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8) 1465
EYE MOVEMENTS AND ATTENTION

RAYNER
 Figure 3. Examples of the boundary paradigm. In all of the examples, the boundary is located after the last letter in the word that; when the reader’s eyes cross the boundary the preview word (shown in the first line of each example) changes to the target word (viewers, as in the second line of each example). In the first example, the preview is another word (readers); in the second example, the preview is a nonword (vievcnr) that is orthographically similar and shares the initial letters with the target word; and in the third example the preview is a random string of letters (hbgkrsk).
sentence always makes sense with the target word, and readers are unaware of the identity of the preview and of the display change.6 Research using this paradigm has revealed that when readers have a valid preview of the word to the right of fixation, they spend less time fixating that word (following a saccade to it) than when they do not have a valid preview (i.e., another word or
nonword or random string of letters initially occu- pied the target location). The size of this preview benefit is typically of the order of 30 – 50 ms. Research using this technique has revealed that readers do not combine a literal representation of the visual information across saccades (McConkie & Zola, 1979; Rayner, McConkie, & Zola, 1980a). Rather, information about the beginning
 6 In the boundary paradigm, the display change from the preview to the target word occurs during a saccade when vision is sup- pressed. Typically, readers are not aware of the change. But, can the results of boundary experiments be attributed to artefacts associ- ated with the change? Inhoff, Starr, Liu, and Wang (1998) directly tested this by varying the speed of the display change and the refresh rate of the display monitor. They found no evidence to suggest that results of gaze-contingent change experiments were arte- facts of the paradigm. Thus, when the timing of the display change is such that the change occurs during the saccade, researchers can be confident that the only thing that influences the data is the experimental manipulation. However, when the timing is such that the display change is too slow, a different pattern of data will appear. Thus, readers who are aware of the display change show a different pattern of data than do readers who are not aware (White et al., 2005a).
1466 THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8)
and ending letters of words (Briihl & Inhoff, 1995; Inhoff, 1989b; Johnson, Perea, & Rayner, 2007; Lima & Inhoff, 1985; Rayner et al., 1982) and ortho- graphic codes (Balota, Pollatsek, & Rayner, 1985; Drieghe, Rayner, & Pollatsek, 2005b; Rayner, 1975; White, Rayner, & Liversedge, 2005b; Williams, Perea, Pollatsek, & Rayner, 2006) is inte- grated across saccades, as are abstract letter codes and phonological information (Ashby & Rayner, 2004; Ashby, Treiman, Kessler, & Rayner, 2006; Chace, Rayner, & Well, 2005; Miellet & Sparrow, 2004; Pollatsek, Lesch, Morris, & Rayner, 1992; Rayner et al., 1980a; Sparrow & Miellet, 2002).
While information about letter position, abstract letter codes, orthographic codes, and pho- nological codes is integrated across saccades (and serves as the basis of the preview benefit effect), it is quite interesting that semantic information is not (Altarriba, Kambe, Pollatsek, & Rayner, 2001; Hyo ̈na ̈ & Ha ̈ikio ̈, 2005; Rayner, Balota, & Pollatsek, 1986; Rayner et al., 1980a). That is, words that typically produce priming in a standard naming or lexical decision task (e.g., the prime word tune primes the target word song) do not yield priming when the prime word is in parafoveal vision (with the target word presented as soon as the reader crosses the invisible boundary location). Although this result is sometimes considered puzzling, it is probably due to the fact that words in parafoveal vision are degraded sufficiently that readers cannot typically process their meaning. This is not to say that words in parafoveal vision cannot be identified, because they can. When words are quite short or sufficiently constrained by context, as discussed below, readers skip over them, and it is generally agreed that these words are identified on the fixation prior to the skip.
Just as there is no strong evidence that semantic information is integrated across saccades, there is also no evidence that morphological information is integrated across saccades when reading English (Inhoff, 1989a; Juhasz, White, Liversedge, & Rayner, 2008; Kambe, 2004; Lima, 1987). On
the other hand, readers of Hebrew do integrate morphological information across saccades (Deutsch, Frost, Peleg, Pollatsek, & Rayner, 2003; Deutsch, Frost, Pollatsek, & Rayner, 2000, 2005). Morphological information is more central to processing Hebrew than English, and the difference in findings presumably reflects this fact.
The amount of preview benefit readers obtain varies as a function of the difficulty of the fixated word. If the fixated word is difficult to process, readers get little or no preview benefit from the word to the right of fixation (Henderson & Ferreira, 1990; Kennison & Clifton, 1995; White, Rayner, & Liversedge, 2005a); if the fixated word is easy to process, readers get better preview benefit from the word to the right (Balota et al., 1985; Drieghe et al., 2005b). Interestingly, it has also been found that preview benefit is larger within words than across words (Hyo ̈na ̈, Bertram, & Pollatsek, 2004; Juhasz, Pollatsek, Hyo ̈ na ̈ , Drieghe, & Rayner, 2009; Pollatsek & Hyo ̈na ̈, 2005). This finding is discussed further below. Finally, while eye fixations land nearer to the beginning of misspelled words, the effect holds whether the previous word is easy or difficult to process (White & Liversedge, 2006b).
Another interesting issue concerns the spatial extent of preview benefit. Specifically, do readers obtain preview benefit from word n þ 2 (the word two to the right of the currently fixated word)? While it is clear that readers generally obtain preview benefit from word n þ 1, readers typically do not get preview benefit from word n þ 2 (Angele, Slattery, Yang, Kliegl, & Rayner, 2008; Kliegl, Risse, & Laubrock, 2007; McDonald, 2005, 2006b; Rayner, Juhasz, & Brown, 2007a). It may be that when word n þ 1 is a very short high-frequency word (2 – 3 letters) preview benefit is obtained from word n þ 2 and that when readers target their next saccade to word n þ 2 preview benefit is obtained.7 However, when readers fixate word n þ 1 and
EYE MOVEMENTS AND ATTENTION
 7 Chinese readers obtain preview benefit from characters and words to the right of their fixation (Inhoff & Liu, 1998; Liu, Inhoff, Ye, & Wu, 2002; Pollatsek, Tan, & Rayner, 2000b; Tsai, Lee, Tzeng, Hung, & Yen, 2004; J. Yang, Wang, Xu, & Rayner, in press; Yen, Tsai, Tzeng, & Hung, 2008), and they can sometimes obtain preview benefit from word n þ 2.
THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8) 1467
RAYNER
word n þ 2 in sequence, they obtain preview benefitfromwordn þ 1butnotwordn þ 2.
Parafoveal-on-foveal effects. A topic that has been highly controversial recently is the extent to which the characteristics of the word to the right of fixation can influence the duration of the fixation on the currently fixated word. Such effects are referred to as parafoveal-on-foveal effects. Some studies have found that orthographic properties of the word to the right of fixation can influence the duration of the current fixation (Drieghe, Brysbaert, & Desmet, 2005a; Inhoff, Starr, & Shindler, 2000b; Pynte, Kennedy, & Ducrot, 2004; Rayner, 1975; Starr & Inhoff, 2004), while other studies have found no such effects (Rayner et al., 2007a; White & Liversedge, 2004, 2006a). Furthermore, some recent studies have suggested that the meaning of the word to the right of fixation can produce parafoveal-on-foveal effects (Kennedy, Murray, & Boissiere, 2004; Kennedy & Pynte, 2005; Kennedy, Pynte, & Ducrot, 2002; Kliegl, 2007; Kliegl, Nuthmann, & Engbert, 2006; Pynte & Kennedy, 2006).8 Yet, other studies have shown inconsistent (Hyo ̈ na ̈ & Bertram, 2004) or no parafoveal-on-foveal effects due to word frequency (Calvo & Meseguer, 2002; Henderson & Ferreira, 1993; Rayner, Fischer, & Pollatsek, 1998a; Schroyens, Vitu, Brysbaert, & d’Ydewalle, 1999; White, 2008). Other studies have likewise shown no evidence of lexical parafo- veal-on-foveal effects (Altarriba et al., 2001; Hyo ̈ na ̈ & Ha ̈ikio ̈, 2005; Inhoff, Starr, & Shindler, 2000b; Rayner, 1975; Rayner et al., 2007a).
Are parafoveal-on-foveal effects real or are there other reasons why such effects sometimes appear in the eye movement record? There are two possible (and reasonable) explanations for parafoveal-on-foveal effects that do not assume that such effects are real. First, some fixations in reading are mislocated because saccades are not perfectly accurate and do not land on the intended target (McConkie, Kerr, Reddix, & Zola, 1988;
Nuthmann, Engbert, & Kliegl, 2005); thus, parafoveal-on-foveal effects may arise due to inaccurately targeted saccades (Drieghe, Rayner, & Pollatsek, 2008b; Rayner, Warren, Juhasz, & Liversedge, 2004b). That is, some saccades that are meant to land on a given target word fall short of the target and land on the end of the pre- vious word. However, in this scenario, attention is still allocated to the originally intended saccade target word such that processing of the target word influences the fixation on the previous word. Second, the studies that have typically reported evidence of parafoveal-on-foveal effects have largely been based on analyses of large data sets across a corpus of data (Kennedy & Pynte, 2005; Kliegl, 2007; Kliegl et al., 2006; Pynte & Kennedy, 2006), while those that have found no evidence for lexical parafoveal-on-foveal effects are based on experimental studies that provide greater control over other variables (see Rayner, Pollatsek, Drieghe, Slattery, & Reichle, 2007d).
At this point, there seems to be some conver- ging agreement concerning the validity of ortho- graphic parafoveal-on-foveal effects. However, there is controversy concerning the validity of lexical parafoveal-on-foveal effects (Rayner & Juhasz, 2004; Rayner, White, Kambe, Miller, & Liversedge, 2003d; Starr & Rayner, 2001; White, 2008). Given the possibility of misloca- lized fixations and the fact that most of the positive evidence for these effects is based on corpus-based analyses, it seems reasonable to view such effects with caution (Rayner et al., 2007d; White, 2008).
The control of eye movements in reading
There are two components to the issue of eye movement control: What determines when to move the eyes, and what determines where to move? As noted above, across large segments of text, there is typically no correlation between how long the eyes remain fixated and how far they move (Rayner & McConkie, 1976). This has generally been taken to suggest that these
 8 A number of studies using either multiple isolated word-processing tasks, in which participants must look at, for example, three words in succession or tasks that mimic reading, have also reported evidence for parafoveal-on-foveal effects (Kennedy, 2000; Kennedy et al., 2004; Kennedy et al., 2002).
1468 THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8)
two decisions are made somewhat independently. Furthermore, there is compelling evidence for this view. Specifically, Rayner and Pollatsek (1981) varied the physical aspects of the text and found that properties of an eye movement mirrored aspects of the available display. Fist, they used the moving-window paradigm and varied the size of the window from fixation to fixation. They found that saccade length varied as a function of the immediately preceding window size. Thus, when the window was small, saccade size was shorter than when the window was large. Second, the text was delayed at the onset of a fixation by a mask (with the time of the delay varying randomly from fixation to fixation). Rayner and Pollatsek found that a large percentage of fixation durations varied according to the delay. From this, they argued that most fixations in reading are under direct cognitive control, though there was also a subset of fixations that appeared to be prepro- grammed (see also Morrison, 1984). Importantly, the manipulations affected saccade length and fixation duration independently, reinforcing the view that the decisions about where and when to move are made somewhat independently.
In general, it is clear that the decision of where to move next is largely driven by low-level proper- ties of the text while the decision of when to move the eyes is largely driven by lexical properties of the fixated word (Rayner, 1998). However, as discussed below, lexical effects (specifically related to word predictability and word frequency) have some influence on where the eyes move (and considerable influence on how long they remain fixated).
Where to move the eyes. For English and other alphabetic languages, where to move the eyes next is strongly influenced by low-level cues pro- vided by word length and space information. Thus, saccade length is influenced by the length
of the fixated word and the word to the right of fixation (Inhoff, Radach, Eiter, & Juhasz, 2003; Juhasz et al., 2008; O’Regan, 1979, 1980; Rayner, 1979; White et al., 2005b). If the word to the right of fixation is either very long or very short, the next saccade will be longer than when a medium-size word is to the right of fixation. For example, if the 11 character spaces to the right of the fixated word consisted of two 5- letter words (with a space between) or a single 11-letter word, the saccade will be longer in the latter case (Juhasz et al., 2008; Rayner, 1979; White et al., 2005b). If there was a short word (2 to 4 letters) to the right of fixation, the next saccade would tend to be longer than when the next word is 5 to 7 letters, largely because the short word would be skipped (Juhasz et al., 2008).
It is also clear that the spaces between words (which demarcate how long words are) are used in targeting where the next saccade will land. When spaces are removed, reading slows down by as much as 30 – 50% (Morris, Rayner, & Pollatsek, 1990; Pollatsek & Rayner, 1982; Rayner et al., 1998a; Rayner & Pollatsek, 1996; Spragins, Lefton, & Fisher, 1976).9 Of course, spaces between words are not present in all writing systems. Interestingly, Kohsom and Gobet (1997) demonstrated that when space information was provided for readers of Thai (who are not used to reading with spaces between words), they read more effectively than normal. Also, work with three-lexeme compound words in German has shown that inserting spaces between the lexemes actually reduces overall reading time on the com- pounds (Inhoff, Radach, & Heller, 2000a). In the experiment by Inhoff et al., the interword spaces were more beneficial than other manipulations that were used to mark lexeme boundaries (e.g., capitalizing the first letter of each lexeme). More recently, Bai, Yan, Liversedge, Zang, and Rayner (2008) inserted spaces between Chinese words or
EYE MOVEMENTS AND ATTENTION
 9 Although most research has indicated that word length is an important cue in deciding where to look next, Epelboim, Booth, and Steinman (Epelboim, Booth, Askkenazy, Taleghani, & Steinman, 1997; Epelboim, Booth, & Steinman, 1994, 1996) argued that word length per se is not a critical cue for eye guidance. Indeed, they claimed that reading unspaced text is “relatively easy”. However, analyses by Rayner and Pollatsek (1996) demonstrated that even in their experiments, most readers slowed down when reading unspaced text.
THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8) 1469
RAYNER
between Chinese characters (for a similar study with Japanese, see Sainio, Hyo ̈na ̈, Bingushi, & Bertram, 2007). Whereas inserting spaces between characters interfered with reading, inserting spaces between words did not. Actually, it is quite surprising that the insertion of spaces between words was not inter- fering given that the Chinese readers have a lifetime of experience reading without spaces.10 All of these pieces of evidence suggest that even when interword spaces are orthographically illegal, they are beneficial to reading.
Although low-level visual information influences where readers move their eyes, semantic information does not. While word predictability and word frequency have influences (discussed below), seman- tic preprocessing of words does not influence where readers move their eyes. It was reported (Underwood, Bloomfield, & Clews, 1988; Underwood, Hyo ̈na ̈, & Niemi, 1987) that readers move their eyes further into a word when the infor- mative information is at the end of the word than when it is at the beginning and that semantic pre- processing was responsible for the effect. However, neither Rayner and Morris (1992) nor Hyo ̈na ̈ (1995) replicated the effect. If readers processed the meaning of the end of an upcoming word it would involve rather complex processing. Thus, the failure to find effects is not surprising.
Landing position effects. The spaces between words provide information about an upcoming word’s length in parafoveal vision leading to systematic tendencies with respect to where the eyes typically land. Rayner (1979) demonstrated that readers’ eyes tend to land halfway between the middle of a word and the beginning of that word, the preferred viewing location (PVL). It is generally argued that readers attempt to target the centre of words, but their saccades tend to fall short (McConkie et al., 1988; Rayner, 1979). When readers’ eyes land at a nonoptimal position in a word, they are more likely to refixate that word (O’Regan, 1990; Rayner et al., 1996). Experiments using the
boundary change paradigm, which provided readers with an incorrect length preview of an upcoming word in the parafovea, have demon- strated that when readers send their eyes to what will turn out to be a nonoptimal position in the par- afoveal word there will be an increase in reading time on the word once fixated (Inhoff et al., 2003; Juhasz et al., 2008; White et al., 2005b).
Where readers fixate in a word can be viewed not only as a landing site for that word but also as the launch site for the next saccade. Although the PVL in a word lies between the beginning and the middle of a word, this position varies as a function of the prior launch site. Thus, if the launch site for a saccade landing on a target word is far from that word (say 8 to 10 letter spaces), the landing position will be shifted to the left. Likewise, if the distance is small (2 – 3 letter spaces), the landing position is shifted to the right. Thus, the landing site distribution on a word depends on its launch site (McConkie et al., 1988; Rayner et al., 1996).
The inverted optimal viewing position effect. In con- trast to the PVL, which represents where readers fixate in words, the optimal viewing position (OVP) represents the location in a word at which recognition time is minimized; the best place to be in a word to recognize it most quickly is at the centre of the word. The OVP effect has been exam- ined in the context of isolated word recognition studies, in which eye movements were monitored (O’Regan & Jacobs, 1992; O’Regan, Le ́vy- Schoen, Pynte, & Brugaille`re, 1984), and two general effects have been reported. First, there is a refixation effect such that the further the eyes are from the optimal viewing position the more likely it is that a refixation will be made on the word. Second, there is a processing cost effect such that for every letter that the eyes deviate from the optimal viewing position, there is a cost of roughly 20 ms (O’Regan et al., 1984). Interestingly, however, although the refixation effect remains in
 10 The interesting question about Chinese readers is how they segment words given the lack of space information (Inhoff & Wu, 2005; Li, Rayner, & Cave, in press; Wu, Slattery, Pollatsek, & Rayner, 2008).
1470 THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8)
reading (as opposed to isolated word recognition), the processing cost is either greatly attenuated or absent (Rayner et al., 1996; Vitu, O’Regan, & Mittau, 1990). This finding is consistent with the view that either contextual information in reading overrides low-level visual processing or the infor- mation acquired about a word before it is directly fixated affects its later processing.
With respect to the OVP effect, another inter- esting finding has recently been documented. Specifically, and somewhat counterintuitively, when readers make only one fixation on a word, if that fixation is at the centre of the word (which is the optimal viewing position), then the fixation is longer than when a single fixation is at the end of the word. This effect, the inverted optimal viewing position (IOVP) effect, was first documented by Vitu, McConkie, Kerr, and O’Regan (2001), and possible reasons for the effect have been examined by Vitu, Lancelin, and Marrier d’Unienville (2007). Perhaps the best explanation for the effect has been put forward by Nuthmann, Engbert, and Kliegl (2005, 2007) who suggested that mislocated fixations are the primary source of the effect (see also Pollatsek et al., 2006d). The basic nature of this argument is that many single fixations falling on the begin- nings or ends of words are not on the targeted word (though the targeted word is being pro- cessed), but are due to overshoots (for fixations falling on the beginning of words) and under- shoots (for fixations falling at the end of a word) of the oculomotor system. Via clever modelling techniques, Nuthmann et al. showed how misloca- lized fixations could account for this effect. Perhaps more interestingly, however, it is the case that there are also word frequency effects independent of where the eyes land in a word when single fixations are made (Rayner et al., 1996; Vitu et al., 2001). Thus, although single fixations are longer when they fall in the middle of a word than at the ends, in both cases low- frequency words receive longer fixations than high-frequency words.
Skipping effects. As noted earlier, words are some- times skipped during reading. Obviously, skipped
words must be processed in parafoveal vision (where stimuli are degraded by acuity limitations), which also reduces the speed of processing of these words (Rayner & Morrison, 1981). Two factors have a big impact on skipping: word length and contextual constraint. First, the most important variable in skipping is word length (Brysbaert et al., 2005; Drieghe, Brysbaert, Desmet, & De Baecke, 2004; Drieghe, Desmet, & Brysbaert, 2007; Rayner, 1998): Short words are much more likely to be skipped than long words. When two or three short words occur in succes- sion, there is a good chance that two of them will be skipped. And short words (like the) preced- ing a content word are often skipped (Drieghe, Pollatsek, Staub, & Rayner, 2008a; Gautier, O’Regan, & LaGargasson, 2000; Radach, 1996). Second, words that are highly constrained by the prior context are much more likely to be skipped than those that are not predictable (Balota et al., 1985; Binder et al., 1999; S. F. Ehrlich & Rayner, 1981; Rayner & Well, 1996; Schustack, Ehrlich, & Rayner, 1987; Vitu, 1991). Chinese words that are predictable from prior context are also skipped more than those that are not (Rayner, Li, Juhasz, & Yan, 2005). Word fre- quency also has an effect on word skipping, but the effect is smaller than that of predictability (Rayner et al., 1996). While predictability influ- ences whether or not a word is skipped, it does not influence where in the word the fixation lands (Rayner, Binder, Ashby, & Pollatsek, 2001a; Vainio, Hyo ̈ na ̈ , & Pajunen, 2009), though (see below) predictability does influence how long readers look at a word.
It is a mistake to think that if a word is skipped it is not processed. Fisher and Shebilske (1985) demonstrated this by examining the eye movements of readers on a passage of text. They then deleted all words from the passage that these readers had skipped and asked a second group of readers to read the passage. This second group of readers had a difficult time understanding the text. So, skipped words do get processed. But, when are they processed? There is evidence to suggest that when a word is skipped, it is processed on the fixation prior to or after the skip. Thus, the
THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8) 1471
EYE MOVEMENTS AND ATTENTION
RAYNER
fixation prior to skipping has been shown to be inflated in comparison to when a given target word is not skipped (Kliegl & Engbert, 2005; Pollatsek, Rayner, & Balota, 1986; Rayner et al., 2003a), as has the fixation after the skip (Reichle, Rayner, & Pollatsek, 2003).
When to move the eyes. Over the past few years, it has become very clear that the ease or difficulty associated with processing the fixated word strongly influences when the eyes move (Liversedge & Findlay, 2000; Rayner, 1998; Starr & Rayner, 2001). Thus, fixation time on a word is influenced by a host of lexical and linguis- tic variables11 such as word frequency (Inhoff & Rayner, 1986; Kliegl, Grabner, Rolfs, & Engbert, 2004; Rayner & Duffy, 1986; Rayner et al., 1996; Schilling, Rayner, & Chumbley, 1998; Slattery, Pollatsek, & Rayner, 2007; Vitu, 1991; White, 2008), word predictability (Balota et al., 1985; Drieghe et al., 2005a; S. F. Ehrlich & Rayner, 1981; Kliegl et al., 2004; Rayner & Well, 1996; Zola, 1984), number of meanings (Binder & Morris, 1995; Binder & Rayner, 1998; Dopkins, Morris, & Rayner, 1992; Duffy, Morris, & Rayner, 1988; Folk & Morris, 2003; Kambe, Rayner, & Duffy, 2001; Rayner, Cook, Juhasz, & Frazier, 2006b; Rayner & Frazier, 1989; Sereno, O’Donnell, & Rayner, 2006), age of acquisition (Juhasz & Rayner, 2003, 2006), phonological properties of words (Ashby, 2006; Ashby & Clifton, 2005; Folk, 1999; Jared, Levy, & Rayner, 1999; Rayner, Pollatsek, & Binder, 1998b; Sereno & Rayner, 2000; Slattery, Pollatsek, & Rayner, 2006), semantic relations between the fixated word and prior words (Carroll & Slowiaczek, 1986; Morris, 1994), and word familiarity (Chaffin, Morris, & Seely, 2001; R. S. Williams & Morris, 2004). The effect of word frequency (Yan, Tian, Bai, & Rayner, 2006) and word predictability (Rayner et al., 2005) on fixation times on words also holds for skilled readers of Chinese. It is also interesting
that when viewers are presented passages of text and are asked to search for a target word in the text, the frequency effect disappears (Rayner & Fischer, 1996; Rayner & Raney, 1996). This is consistent with the view that what influences when to move the eyes during reading is different from visual search.
A number of other variables have also been examined with respect to processing time on words. McDonald and Shillcock (2003a, 2003b) reported that the transitional probability between two words (e.g., defeat following accept has a high transitional probability whereas losses following accept has a low transitional probability) influences how long readers look at a word. However, Frisson, Rayner, and Pickering (2005) sub- sequently found that the difference was due to the predictability of the target word. When pre- dictability was controlled, there was no difference between words with high and low transitional probabilities. Miller, Juhasz, and Rayner (2006) examined the extent to which the location of the uniqueness point in a word (the point, moving from left to right, at which a given target word could only be that word) influenced fixation times and found no differences between early and late uniqueness points in terms of how long readers look at words. Thus, these two extralexical variables (transitional probability and uniqueness point) do not seem to influence the amount of time readers look at words. On the other hand, morphological properties of words do influ- ence fixation times on target words. Thus, the frequency of the different lexemes in longer com- pound words (which overall tend to be low-fre- quency words) influences fixation time on each lexeme. Specifically, a high-frequency beginning or ending lexeme in a compound word is fixated for a shorter duration than are low-frequency lexemes (S. Andrews, Miller, & Rayner, 2004; Bertram & Hyo ̈na ̈, 2003; Bertram, Pollatsek, & Hyo ̈na ̈, 2004; Hyo ̈na ̈ et al., 2004; Hyo ̈na ̈ & Pollatsek, 1998; Juhasz, 2007, 2008; Juhasz,
 11 It is not possible to cite all of the many studies demonstrating the effects that are described here. Thus, in general the articles originally documenting the demonstrations and some recent demonstrations of the effects are listed (with an emphasis on studies from my lab).
1472 THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8)
Inhoff, & Rayner, 2005; Juhasz, Starr, Inhoff, & Placke, 2003; Pollatsek & Hyo ̈ na ̈ , 2005; Pollatsek,Hyo ̈na ̈,&Bertram,2000a).And,as noted earlier, the amount of preview benefit for lexemes comprising the second half of a compound word is larger, around 100 ms, than the preview benefit obtained across words (Hyo ̈na ̈ et al., 2004; Juhasz et al., 2009). Thus, Juhasz et al. found that there was more preview benefit when a correct preview (ball) was provided (in the boundary paradigm) for the lexeme at the end of a compound word (basketball) than when a partial preview (badk) was provided. More criti- cally, there was more preview benefit for the lexeme ball in basketball than in tennis ball.
Experiments examining the influence of neigh- bourhood frequency on eye movements (Perea & Pollatsek, 1998; Pollatsek, Perea, & Binder, 1999) have generally found that the effects typically do not occur in first-pass reading-time measures but in later measures (like regressions and spillover times). Rayner, White, Johnson, and Liversedge (2006e) and White, Johnson, Liversedge, and Rayner (2008) examined eye movements when raeding wrods with jubmled lettres and found that while it was fairly easy to read such text, there was always a cost associated with transposing the letters.
In short, it is the case that some variables have strong influences immediately when a word is fixated (such as frequency, age of acquisition, and predictability), while other variables seem to yield later occurring effects. However, there is no doubt that cognitive processing activities have a strong influence on when the eyes move. Perhaps the most compelling evidence that cognitive processing of the fixated word is driving the eyes through the text comes from experiments in which the fixated word either disappears or is masked after 50 – 60 ms (Ishida & Ikeda, 1989; Liversedge et al., 2004; Rayner et al., 1981; Rayner, Liversedge, & White, 2006c; Rayner, Liversedge, White, & Vergilino-Perez, 2003b). Basically, these studies show that if readers are allowed to see the fixated word for 50 – 60 ms before it disappears, they read quite normally. This does not mean that words are completely
processed in 50 – 60 ms, but rather that this amount of time is sufficient for the processing system to encode the word. Interestingly, if the word to the right of fixation also disappears or is masked, then reading is disrupted (Rayner et al., 2006c); this quite strongly demonstrates that the word to the right of fixation is very important in reading. More critically, when the fixated word disappears after 50 – 60 ms, how long the eyes remain in place is determined by the frequency of the word that disappeared: If it is a low-fre- quency word, the eyes remain in place longer (Rayner et al., 2003b, 2006c). Thus, even though the word is no longer there, how long the eyes remain in place is determined by that word’s fre- quency. This is very compelling evidence that the cognitive processing associated with a fixated word is the engine driving the eyes through the text.
It is thus quite clear that lexical variables have strong and immediate effects on how long readers look at a word. While other linguistic vari- ables can have an influence on how soon readers move on in the text, it is generally the case that higher level linguistic variables have somewhat later effects, unless the variable more or less “smacks you in the eye”. So, for example, when readers fixate on the disambiguating word in a syntactic garden path sentence there is increased fixation time on the word (Frazier & Rayner, 1982; Rayner, Carlson, & Frazier, 1983; Rayner & Frazier, 1987) and/or a regression from the disambiguating word back to earlier parts of the sentence (Frazier & Rayner, 1982; Meseguer, Carreiras, & Clifton, 2002; Mitchell et al., 2008). Readers also have longer fixations at the end of clauses and sentences (Hirotani, Frazier, & Rayner, 2006; Just & Carpenter, 1980; Rayner, Kambe, & Duffy, 2000; Rayner, Sereno, Morris, Schmauder, & Clifton, 1989). And, when readers encounter an anomalous word, they fixate on it longer, and the effect is quite immediate (Rayner et al., 2004b; Staub, Rayner, Pollatsek, Hyo ̈na ̈, & Majewski, 2007; Warren & McConnell, 2007); when a word indicates an implausible, but not truly anomalous, event, there will be an effect registered in the eye
THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8) 1473
EYE MOVEMENTS AND ATTENTION
RAYNER
movement record, but it is typically delayed a bit, showing up in later processing measures (Joseph et al., 2008; Rayner et al., 2004b).
Interestingly, when sentences with an anomalous word (such as carrot in “Jane used a pump to inflate the large carrot”) are embedded in cartoon or fantasy-like contexts and are compared with real- world contexts where inflating a carrot with a pump is not anomalous (Warren, McConnell, & Rayner, 2008), the earliest measures of fixation time (first- fixation duration and gaze duration) still yielded longer fixations on the anomalous word than the control word (carrot in a sentence like “Jane used a knife to chop the large carrot”). However, the go- past measure revealed disruption only in the real- world context. These results suggest that contextual information did not eliminate the initial disruption, but moderated it quickly thereafter.12
Using eye movements to study sentence and discourse processing. In much of the foregoing discussion, the premise has largely been that lexical processing is the engine driving the eyes through the text. However, there is good reason to believe that higher order comprehension processes influence eye movements primarily when something does not compute (Clifton, Staub, & Rayner, 2007; Staub & Rayner, 2007). When readers are garden- pathed by syntactic ambiguity (Binder, Duffy, & Rayner, 2001; Boland & Blodgett, 2001; Clifton et al., 2003; Ferreira & Clifton, 1986; Frazier & Rayner, 1982; Rayner et al., 1983; Rayner & Frazier, 1987; Rayner, Garrod, & Perfetti, 1992) their fixations get longer, and they often make shorter saccades and more regressions (Altmann, Garnham, & Dennis, 1992; Rayner & Sereno, 1994). In cases such as this, higher order comprehen- sion processes can override the normal default situation in which lexical processing is driving the eyes and result in longer fixations or regressions back to earlier parts of the text.
It is interesting that eye movement data have more or less become the gold standard in exper- iments, which are too numerous to list here, dealing with sentence processing and syntactic ambiguity resolution. Because of its precise temporal properties, eye tracking is generally deemed to be the preferred way to study online sentence processing. In contrast, it is striking that there have not been nearly as many studies utilizing eye movement data to examine online comprehension and dis- course-processing effects. While there are a few studies (see, for example, Birch & Rayner, 1997; Cook & Myers, 2004; Duffy & Keir, 2004; Duffy & Rayner, 1990; K. Ehrlich & Rayner, 1983; Garrod, O’Brien, Morris, & Rayner, 1990; Garrod & Terras, 2000; O’Brien, Raney, Albrecht, & Rayner, 1997; O’Brien, Shank, Myers, & Rayner, 1988; Rayner, Chace, Slattery, & Ashby, 2006a; Sturt, 2003) in which eye movements were moni- tored to assess immediate comprehension in dis- course processing, the number of such studies pales in comparison to those that used more gross reading-time measures. The time may be ripe for more studies to use eye movement data to under- stand moment-to moment discourse processing.
Models of eye movement control in reading
Given the vast amount of information about eye movements during reading that has accumulated in the past 25 – 30 years, it is not surprising that a number of models of eye movements in reading have recently appeared. The E-Z Reader model (Pollatsek, Reichle, & Rayner, 2006d; Rayner, Ashby, Pollatsek, & Reichle, 2004a; Rayner, Pollatsek, & Reichle, 2003c; Rayner, Reichle, & Pollatsek, 1998c; Reichle, Pollatsek, Fisher, & Rayner, 1998; Reichle, Pollatsek, & Rayner, 2006; Reichle, Rayner, & Pollatsek, 1999; Reichle et al., 2003) is typically regarded as the most influential of these models. Due to space limitations, other models are not discussed in length here.13 Other
 12 See also Filik (2008) for slightly different data on this issue, and Ferguson and Sanford (2008) for data consistent with Warren et al. (2008).
13 For a comprehensive overview of these models, see the special issue of Cognitive Systems Research (Reichle, 2006), and see Reichle et al. (2003) for a comparison of the models. See also Reichle and Laurent (2006) for a model describing how learning influ- ences eye movements.
1474 THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8)
models include SWIFT (Engbert, Nuthmann, Richter, & Kliegl, 2005), SHARE (Feng, 2006), Glenmore (Reilly & Radach, 2006), EMMA (Salvucci, 2001), SERIF (McDonald, Carpenter, & Shillcock, 2005), Mr. Chips (Legge, Hooven, Klitz, Mansfield, & Tjan, 2002; Legge, Klitz, & Tjan, 1997), and the competition/activation model (S. Yang, 2006; Yang & McConkie, 2001). These models are all fully implemented, but they differ on a number of dimensions. Mr. Chips is an ideal observer type of model, whereas all of the others strive to account for the actual performance of readers. In some of the models the eyes are driven by lexical processing whereas in others eye movements are largely viewed as being primarily influenced by oculomotor constraints. Some models allow for (lexical) parallel processing of words, whereas in others lexical processing is serial so that the meaning of word n þ 1 is not accessed until the lexical processing is complete (or nearly complete) for word n.
These models share many similarities, but they differ on some precise processing details and on how certain effects are explained. E-Z Reader has the most lexical involvement, and the compe- tition/activation model has the least amount. E-Z Reader involves serial lexical processing of words, whereas SWIFT and Glenmore allow for parallel lexical processing of words. It is generally agreed that lexical processing has to have a strong influ- ence on the decision of when to move the eyes in order to account for much of the data described above.
E-Z Reader does a good job of predicting how long readers look at words, which words they skip, and which words will most likely be refixated. Importantly, it can account for global aspects of eye movements in reading, as well as more local processing characteristics (competitor models also account for much of the data). As a computational model, E-Z Reader has the virtue of being highly transparent, so it makes very clear predictions, and when it cannot
account for certain data it is very clear why it cannot (thus enabling one to modify the model). The model has also generated interesting research questions (Drieghe, 2008; Inhoff, Eiter, & Radach, 2005; Inhoff, Radach, & Eiter, 2006; Miellet, Sparrow, & Sereno, 2007; Pollatsek, Reichle, & Rayner, 2006b, 2006c; Rayner et al., 2004a; Reingold & Rayner, 2006) because it makes clear predictions. Also, changes have been made to the model when resulting data have necessitated it (Rayner et al., 2004a). Furthermore, the model has also made it possible to account for data patterns that in the past may have been difficult to explain (see Pollatsek, Juhasz, Reichle, Machacek, & Rayner, 2008; Rayner et al., 2004a, Rayner et al., 2006d, for examples). However, the model is not perfect, and it has some limitations. For example, in the model as initially developed, higher order processes due to sentence parsing and discourse variables had no influence, and, as noted above, higher order processes can at times influence when the eyes move and how long they remain in place. As a first step in accounting for such nonlexical variables, a new version of the model (E-Z Reader 10) strives to account in part for these effects (Reichle, Warren, & McConnell, 2009).14 E-Z Reader has also been extended to account for eye movement data of elderly readers (Rayner et al., 2006d), as has SWIFT (Laubrock et al., 2006), and for eye movement data of Chinese readers (Rayner, Li, & Pollatsek, 2007b).
With careful experimentation and with the implementation of computational models that simulate eye movements during reading, great advances have been made in understanding eye movements in reading (and inferring the mental pro- cesses associated with reading). In the next two sec- tions, eye movements during scene perception and visual search are discussed. Although there has not been as much eye movement research on these areas as on reading, it is still the case that some
EYE MOVEMENTS AND ATTENTION
 14 A recent neurophysiologically inspired model (Heinzle, Martin, & Hepp, 2009) has many of the same properties as those inherent in E-Z Reader, including serial lexical processing.
THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8) 1475
RAYNER
clear conclusions emerge from the work that has been done.
Eye movements in scene perception
Fixation durations during scene perception tend to be longer than those in reading, and saccade size tends to be larger. The average fixation duration tends to be closer to 300 ms (but varies as a function of the task and the characteristics of the scene). Average saccade size tends to be 4–5 deg (though it too can vary as a function of task and the exact nature of the scene15). Whereas there is a well-defined task for readers, exactly what partici- pants should do in a scene perception task is more variable. Sometimes participants are asked to look at the scene in anticipation of a memory test while other times they may be asked to indicate if a certain object is present in the scene. Under the latter instructions, scene perception becomes very much a visual search task.
Examination of the eye movement pattern (or the scan path) of a viewer on a scene demonstrates that viewers do not fixate every part of the scene. Most fixations tend to fall on the informative parts of the scene. Thus, viewers tend to not fixate on the sky or the road in front of a house. Furthermore, viewers are able to obtain the gist of the scene in a single glance. That is, the gist is understood so quickly that it is obtained even before the eyes begin to move (De Graef, 2005). Castelhano and Henderson (2008b) found that when viewers are shown a scene for as little as 40 ms, they are able to extract enough information to understand the scene gist. Thus, the gist is thought to be acquired during the first fixation in order to orient subsequent fixations to appropriate/interesting regions in the scene. Actually, it is not well understood how the gist is acquired so rapidly, and this issue requires further research.
The perceptual span in scene perception
While the gist of the scene can pretty much be obtained on the first fixation, how much infor- mation do viewers obtain on each fixation as they look around a scene? It is clear that information is acquired over a wider range of the visual field in scene perception than is the case for reading. As with reading, the best way to address this issue is via gaze-contingent paradigms. Yet surprisingly few such studies have been reported. Henderson, McClure, Pierce, and Schrock (1997) used a moving-mask procedure (to cover the part of the scene around the fixation point) and found that although the presence of a foveal mask influenced looking time, it did not have nearly as serious effects for object identification as a foveal mask has for reading (Rayner & Bertera, 1979). Saida and Ikeda (1979) used a moving-window paradigm and found that the functional field of view is quite large and can consist of about half of the total scene regardless of the absolute size of the scene (at least for scenes that were up to 14.4 by 18.8 degrees). They found a serious deterioration in recognition of a scene when the window was limited to a small area (about 3.3   3.3 degrees) on each fix- ation. Performance gradually improved as the window size became larger up to about 50% of the entire scene. Saida and Ikeda noted that there was considerable overlap of information across fixations. In this study and other studies that have used the moving-window paradigm (Castelhano & Henderson, 2007; van Diepen & d’Ydewalle, 2003; van Diepen, Ruelens, & d’Ydewalle, 1999; van Diepen & Wampers, 1998) scene information was presented normally within the window area around a fixation point, but the information outside of the window was degraded in a systematic way.16
Other studies have attempted to address the question via other techniques. For example,
 15 More densely packed scenes lead to longer fixations and shorter saccades.
16 A very promising variation on gaze-contingent moving-windows and moving-masks paradigms, discussed in the context of visual search, that has not yet been fully exploited is to use multiresolution displays (Loschky & McConkie, 2002; Reingold & Loschky, 2002; Reingold, Loschky, McConkie, & Stampe, 2003). With these types of display, a clear view of the scene can be pro- vided around the fixation point with increasing degradation of the scene outside of the window.
1476 THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8)
Nelson and Loftus (1980) examined how close to fixation an object had to be for it to be recog- nized as having been in the scene. They found that objects located within about 2.6 degrees from fixation were generally recognized, but recognition depended to some extent on the characteristics of the object. They also suggested that qualitatively different information is acquired from the region within 1.5 degrees around fixation than from regions further away (see also Nodine, Carmody, & Herman, 1979). While a study by Parker (1978) suggested that the functional field of view for specific objects in a scene is quite large (with a radius of at least 10 degrees around fixation resulting in a perceptual span of up to 20 degrees), other more recent studies using better controlled stimuli and more natural images (Henderson & Hollingworth, 1999; Henderson, Williams, Castelhano, & Falk, 2003) suggest that the func- tional field of view only extends about 4 degrees away from fixation.
It appears that the answer to the question of how large the perceptual span in scene perception is has not been answered as conclusively as it has in reading. Viewers typically gain useful information from a fairly wide region of the scene, which also probably varies as a function of the scene and the task. For instance, the ease with which an object is identified has been linked to its orientation (De Graef, 2005), frequency within a scene context (Hollingworth & Henderson, 1998), and how well camouflaged it is (De Graef, Christiaens, & d’Ydewalle, 1990). As has been shown in reading (Henderson & Ferreira, 1990), it is likely that the ease of identifying a fixated object has an effect on the extent of processing in eccentric vision.
Preview benefit in scene perception
Just as in reading, viewers obtain preview benefit from objects that they have not yet fixated (Henderson, 1992; Henderson, Pollatsek, & Rayner, 1987, 1989; Henderson & Siefert, 1999, 2001; Pollatsek, Rayner, & Collins, 1984; Pollatsek, Rayner, & Henderson, 1990) with the amount of the preview benefit of the order of
100 ms; thus, preview benefit is larger in scene per- ception than in reading. Interestingly, as is now rather well known, viewers are rather insensitive to changes in scenes. Grimes and McConkie (1995) and McConkie and Currie (1996) asked observers to view scenes with the task of memorizing what they saw. They were also told that changes could be made to the image while they were examining it, and they were instructed to press a button if they detected the changes. While observers viewed the scenes, changes were made during a saccade (when vision is suppressed). Remarkably, observers were unaware of most changes, which included the appearance and disappearance of large objects and the changing of colours. Henderson and Hollingworth (2003) likewise demonstrated that low-level sensory information is not preserved from one fixation to the next (see also Henderson, Brockmole, & Gajewski, 2008). Other studies found that viewers were unable to detect changes when there was a corresponding dis- ruption to processing, such as with the simultaneous onset of patches covering portions of the scene (O’Regan, Rensink, & Clark, 1999). However, this lack of awareness of changes does not mean that there is no recollection of any visual details, but rather that the likelihood of remembering visual information is highly dependent on the processing of that information (Henderson & Hollingworth, 2003; Hollingworth & Henderson, 2002; Hollingworth, Williams, & Henderson, 2001). Thus, knowing about the processes that go on during a fixation is very important if one wants to predict how well information from a scene is stored.
Early theories of transsaccadic memory (Jonides, Irwin, & Yantis, 1982; McConkie & Rayner, 1976b) proposed that information is integrated across saccades in an integrative visual buffer (with properties like iconic memory). However, experiments described above in the context of reading (McConkie & Zola, 1979; Rayner et al., 1980a) as well as nonreading experiments using relatively simply arrays (Irwin, Yantis, & Jonides, 1983; O’Regan & Le ́vy- Schoen, 1983; Rayner & Pollatsek, 1983) demon- strated that this view is incorrect and that viewers do not integrate sensory information presented on
THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8) 1477
EYE MOVEMENTS AND ATTENTION
RAYNER
separate fixations in a visual buffer. More recent work with more naturalistic scenes has arrived at the same conclusion, and there is evidence that visual short-term memory (VSTM), which is thought to be at a higher level than a visual buffer, serves a primary role in integrating infor- mation across saccades (Hollingworth, Richard, & Luck, 2008). Thus, memory across saccades during scene perception appears to be due to higher level visual codes (Carlson-Radvansky, 1999; Carlson-Radvansky & Irwin, 1995; Hollingworth et al., 2008; Irwin, 1991, 1992; Irwin & Gordon, 1998), which are abstracted away from precise sensory representations, with VSTM as the basis for integration.
Eye movement control in scene perception
Where do viewers look in scenes? Since the pioneer- ing work of Buswell (1935) and Yarbus (1967), it has been widely recognized that viewers’ eyes are drawn to important aspects of the visual scene and that their goals in looking at the scene very much influence their eye movements. Quite a bit of early research demonstrated that the eyes are quickly drawn to informative areas in a scene (Antes, 1974; Mackworth & Morandi, 1967). It is also clear that the saliency of different parts of the scene influence what part of the scene is fixated (Mannan, Ruddock, & Wooding, 1995, 1996; Parkhurst & Niebur, 2003). A large amount of empirical and computational research has recently been devoted to understanding the factors that govern fixation position in scenes (Foulsham, Kingstone, & Underwood, 2008; Henderson, 2003; Itti & Koch, 2000; Melcher & Kowler, 2001; Parkhurst, Law, & Niebur, 2002; Rutishauser & Koch, 2007; Tatler, Baddeley, & Vincent, 2006; Underwood, Foulsham, van Loon, Humphreys, & Bloyce, 2006), and much of this work revolves around how saliency (which is typically defined in terms of low-level components of the scene, such as contrast, colour, intensity, brightness, spatial frequency, etc.) influences where viewers look.
Saliency is not the only factor involved in deter- mining where to look, and there are questions about how important it is (Foulsham &
Underwood, 2008; Henderson, Brockmole, Castelhano, & Mack, 2007). It has also become increasingly clear that there are strong cognitive influences on where viewers look as well (see Henderson, 2007; Torralba, Oliva, Castelhano, & Henderson, 2006). Neider and Zelinksy (2006a) examined the influence that context has on the placement of eye movements in search of certain objects within pseudo-realistic scenes. Viewers were asked to look for target objects that are typically constrained to certain parts of the scene (i.e., jeep on the ground, hot air balloon in the sky). When a target was present, fixations were largely limited to the area where one would expect to find the target object (i.e., ground or sky), while, when the target was absent, search was less restricted to these areas. They also found that when the target was in the expected area, search times were on average 19% faster. From these results, it seems that not only do viewers focus their fixations in areas of a scene that most likely contain the target to improve search, but also they are flexible in the application of these restrictions, and they very quickly adopt a “look everywhere” strategy when necessary. Thus, it seems that search strategies in scenes are guided by the scene context, but not with strict adherence.
Henderson (1993) found that viewers tended to fixate near the centre of an object and that there was a greater tendency to undershoot the centre than to overshoot. Furthermore, landing position influenced fixation time as the duration of the first fixation on an object decreased, and the probability of refixating the object increased as the deviation of the initial landing position from the centre of the object increased. Also, Mannan et al. (1995, 1996) found that viewers tend to look in pretty much the same locations across three viewings of a scene even though the scenes had been either high-pass or low-pass filtered. Thus, low-level visual information must be critical in deciding where to look next.
What role does memory play in directing fix- ations in scenes? Many of the models using saliency as the primary driving force of eye movements do
1478 THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8)
not consider how information gathered initially may influence the placing of subsequent fixations. In a recent study, Castelhano and Henderson (2007) investigated whether this initial represen- tation of a scene can be used to guide subsequent eye movements on a real-world scene. Viewers were shown a very short preview of the search scene and then were asked to find the target object. The moving-window technique was used in this phase of the study, thus eliminating any immediately available visual information. A preview of the search scene itself elicited the most efficient searches when compared to a meaningless control (the preview yielded fewer fixations and the shortest scan path to the target). When a preview of another scene within the same semantic category was shown (thereby providing general semantic information without the same visual details) there was no improvement in search. These results suggest that the initial representation used to improve search efficiency was not based on general semantic properties, but on something more specific. When a reduced scale of the search scene was shown as the preview, search efficiency measures were as high as when the full-scale preview was shown. Taken together, these results suggest that the initial scene representation is based on abstract, visual information that is useful across changes in spatial scales. Thus, the infor- mation used to guide eye movements has two sources: the saliency of the scene and the infor- mation in memory about that scene and scene type.
Are the eyes drawn to unusual parts of a scene? A somewhat contentious issue concerns the extent to which the eyes are drawn to highly informative, unusual, or emotional aspects of a scene; the evi- dence is somewhat uneven as some experiments indicate that they are, while others suggest they are not. Early experiments found that the eyes move quickly to an object that is out of place in a scene (Friedman, 1979; Loftus & Mackworth, 1978). Unfortunately, these studies did not control physical distinctiveness very well (Rayner & Pollatsek, 1992), and, when it was controlled, studies (Brockmole & Henderson, 2008; De Graef, 1998; De Graef et al., 1990; Henderson,
Weeks, & Hollingworth, 1999) failed to replicate the finding that semantically inconsistent objects were fixated earlier than consistent objects (but see Underwood & Foulsham, 2006; Underwood, Humphreys, & Cross, 2007).
More recently, Becker, Pashler, and Lubin (2007) and Harris, Kaplan, and Pashler (2008) renewed interest in the extent to which semanti- cally incongruent objects in scenes attract the eyes. Unlike many earlier experiments that used line-drawings of scenes, Becker et al. used colour photographs, which viewers examined for 8 seconds. Some aspect of the scene was anomalous; for example, a stop sign was green instead of red. Unlike the earlier experiments, a large number of viewers looked at a rather limited number of scenes. Becker et al. found that viewers fixated anomalous items (the green stop sign) earlier (both in time and in order of eye fixations) than the control objects (the red stop sign) and argued that the results indicate that violations of canonical form can be detected from extrafoveal vision and can affect the likelihood of fixating them.
Harris et al. (2008) introduced emotional, yet somewhat unusual, aspects into scenes; for example, in a control scene people are tossing a beach ball at a beach, while in the emotional scene the beach ball is replaced by a baby. They found that viewers looked earlier at the emotional aspect of the scene. A number of other recent studies (Calvo & Lang, 2005; Calvo & Nummenmaa, 2007; Kirchner & Thorpe, 2006; Nummenmaa, Hyo ̈na ̈, & Calvo, 2006) have likewise reported that the eyes quickly move to emotional objects or scenes (though in these studies, the object/scene is usually presented in parafoveal vision, and the latency of a saccade from a central fixation point is measured for an emotional scene/object versus a neutral scene/object).
Rayner, Castelhano, and Yang (2009a) used the scenes from Harris et al. (2008) as well as a large number of other scenes with weird or unusual aspects. Like Becker et al. (2007) and Harris et al., they found that the eyes were drawn to the weird parts of the scene earlier than when the weird aspect was missing. Yet, the eyes being drawn to the weird part of the scene was not instantaneous;
THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8) 1479
EYE MOVEMENTS AND ATTENTION
RAYNER
it was fixated within 3 fixations (in comparison to the same part of the scene being fixated within 3.5 fixations in the control condition). Underwood, Templeman, Lamming, and Foulsham (2008) like- wise reported that the incongruous objects attract attention earlier than congruous objects, but the effect was not apparent until the picture has been displayed for several seconds.
Are there cultural influences on where to look in scenes? It has recently been suggested that Asian (Chinese) participants look at scenes differently than English-speaking participants. Specifically, Chua, Boland, and Nisbett (2005) reported that Chinese viewers spent less time looking at the focal objects in a scene and more time looking at the scene background than their English-speaking counterparts. These results were discussed in the wider context of a general theory of cultural differ- ences in cognition (Nisbett, 2003) whereby Asian cultures lead people to not place as much value on the individual as on the group, while the American culture places more emphasis on the individual. According to Chua et al., this under- lying cultural difference in thinking led the Chinese viewers to look more at the background and spend relatively less time (in comparison to the Americans) looking at the focal objects.
However, three recent reports have raised some questions about the validity of Chua et al.’s (2005) findings. Rayner, Li et al. (2007b) reported no differences in the looking patterns of Chinese and American participants, with both groups looking more at focal objects than the background information. Boland, Chua, and Nisbett (2008) noted that the study was not a direct replication of Chua et al. given that the same materials were not used (and the focal objects were more apparent in their study than in the Rayner et al. study), and the task varied between the two studies (expec- tation of a memory test in Rayner et al. and a rating task for scene likeability in Chua et al.). However, Evans, Rotello, Li, and Rayner (2009) used the original scenes from Chua et al. (as well as additional scenes for increased power) and Chua et al.’s task, and also found no differences between the two groups (either with the entire
set of stimuli or with the subset that had been pre- viously used by Chua et al.). Furthermore, Rayner et al. (2009a) argued that if Chinese viewers paid more attention to the background information in a scene it might take them longer to notice the weird object in the scene. But, there was no differ- ence between the Chinese and American viewers in their study.
When do viewers move their eyes when looking at scenes? Given that attention precedes an eye move- ment to a new location within a scene (Henderson, 1992; van Diepen & d’Ydewalle, 2003), it follows that the eyes will move once information at the centre of vision has been processed, and a new fix- ation location has been chosen. When this shift in attention (from the centre of fixation to the per- iphery) takes place in the course of a fixation was investigated by van Diepen and d’Ydewalle (2003). They had observers view scenes whose information at the centre of fixation was masked during the initial part of fixations (with the mask coming on 20 – 90 ms from fixation onset). In another case, the periphery was masked at the beginning of each fixation (for 10 – 85 ms). They found that when the centre of fixation was masked initially, fixation durations increased with longer mask durations. When the periphery was masked, they found a slight increase in fixation dur- ations, but not as much as with a central mask. Interestingly, they found that the average saccades size decreased, and the number of fixations increased, with longer mask durations in the per- iphery. They argued that with the longer peripheral masking durations the visual system does not wait for the unmasking of peripheral information, but instead chooses information that is immediately available. These results suggest that the extraction of information at the fovea occurs fairly rapidly, and attention is then directed to the periphery to choose a viable saccade target almost immediately following the extraction of foveal information. The general timing of the switch between central and peripheral information processing needs further investigation, but it is likely that the varia- bility of information across scenes will make it
1480 THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8)
more difficult to delineate a specific time frame as has been done in reading.
Recent studies by Henderson and Pierce (2008) and Rayner, Smith, Malcolm, and Henderson (2009b) are quite informative with respect to issues related to the timing of eye movements in scene perception. Henderson and Pierce (2008) adapted the paradigm used by Rayner and Pollatsek (1981; Morrison, 1984) to create a scene onset delay paradigm. In these experiments, a visual mask was presented at the beginning of eye fixations as viewers examined a scene. The duration of the mask was varied (with scene onset delays as short as 40 ms and as long as 1200 ms), and the scene did not appear until the designated mask dur- ation was exceeded. Then the scene appeared and remained visible until the viewer made a saccade. Scene onset delays took place on every tenth fixation. As in reading, Henderson and Pierce found that one population of fixations was under direct control of the current scene, increasing in duration as the delay increased. However, a second population of fixations was relatively con- stant across delay. Also like reading, the data pattern did not change whether the scene delay dur- ation was random or blocked, suggesting that the effects are not under the strategic control of viewers. The results support a mixed model of eye movement control and indicate that the durations of some fixations are determined regardless of scene presence while others are under direct moment-to-moment control of scene analysis. Interestingly, the percentage of fixations under direct control was much greater in reading than in scene perception.
Rayner et al. (2009b) adapted the disappearing text/masked text paradigm (Rayner et al., 1981, 2003b, 2006c) discussed above to create a situation in which a scene was masked at certain points after the onset of each new fixation. Interestingly, they found that viewers needed 150 ms to view the scene before the mask was not disruptive. Obviously, this is much longer than the 50– 60 ms that was needed in reading for the mask to not cause disruption and also longer than one might predict given that viewers can obtain the gist of a scene on the first fixation.
Models of eye movement control in scene perception. A number of models of eye movement control in scene perception have recently appeared. For the most part, these models focus on where to move the eyes next, and little effort has been made to specify when the eyes move (or what influences the decision to move the eyes). A fair number of computational models (Baddeley & Tatler, 2006; Itti & Koch, 2000, 2001; Parkhurst et al., 2002) use the concept of a saliency map (following from Findlay & Walker, 1999) to model eye fixation locations in scenes. In this approach, bottom-up properties in a scene make explicit the locations of the most visually prominent regions of the scene. The models are basically used to derive predictions about the distribution of fixations on a given scene.
While these models can account for some of the variability in where viewers fixate in a scene, they are limited in that the assumption is that fixation locations are driven primarily by bottom-up factors, and it is clear that higher level factors also come into play in determining where to look next in a scene (Castelhano & Henderson, 2007). A model that includes more in the way of top-down and cognitive strategies was recently presented by Torralba et al. (2006). Indeed, as noted previously, while there has been a lot of research to localize where viewers move their eyes while looking at scenes, there has been precious little in the way of research attempting to deter- mine what controls when the eyes move. This is in contrast with reading where the issues of where to move the eyes and when to move the eyes have both received considerable attention. Models of eye movement control in scene percep- tion need to better explain the factors influencing when to move the eyes.
Eye movements and visual search
Visual search has received considerable attention over the past 40 years. However, the majority of this research has been done without measuring eye movements (Findlay & Gilchrist, 1998), and it has often been assumed that they are not particu- larly important in understanding search. However,
THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8) 1481
EYE MOVEMENTS AND ATTENTION
RAYNER
this attitude seems to be largely changing as many recent experiments have utilized eye movements to understand the process. Many of these studies deal with very low-level aspects of search and often focus on using the search task to uncover properties of the saccadic eye movement system (see Findlay & Gilchrist, 2003). But, it is becoming very clear that eye movement studies of visual search, like reading and scene perception, can provide important information on moment-to- moment processing in search (Trukenbrod & Engbert, 2007; Williams & Pollatsek, 2007; Zelinsky, Rao, Hayhoe, & Ballard, 1997).
Here the focus is primarily on research using eye movements to examine how viewers search through arrays to find specific targets. As noted at the outset, fixation durations in search tend to be highly vari- able. Some studies report average fixation times as short as 180 ms while others report averages on the order of 275ms. This wide variability is undoubtedly due to the fact that how difficult the search array is (or how dense or cluttered it is) and the exact nature of the search task strongly influence how long viewers pause on average. Typically, saccade size is a bit larger than that in reading (though saccades can be quite short with dense arrays) and a bit shorter than that in scene perception. Two important points regarding eye movements during search are first discussed. Then a brief review of (a) the perceptual span, (b) preview benefit, (c) eye movement control, and (d) models of eye movements is presented.
The search array matters. Perhaps the most obvious thing about visual search is that the search array makes a big difference in how easy it is to find a target. When the array is cluttered and/or dense (with many objects and/or distractors) search is more costly than when the array is simple (or less dense), and eye movements typically reflect this fact (Bertera & Rayner, 2000; Greene & Rayner, 2001a, 2001b). The number of fixations and fixation duration both increase as the array becomes more complicated, and the average saccade size decreases (Vlaskamp & Hooge, 2006). Additionally, the configuration of the search array has an effect on the pattern of eye
movements. In an array of objects arranged in an arc, fixations tend to fall in between objects, pro- gressively getting closer to the area where viewers think the target is located (Zelinsky & Loschky, 2005; Zelinsky et al., 1997). On the other hand, in randomly arranged arrays, other factors such as colour of the items and shape similarity to the target object influence the placement of fixations (Williams, Henderson, & Zacks, 2005).
Does visual search have a memory? Horowitz and Wolfe (1998) initially proposed that during visual search viewers do not have good memory and that the same item will be resampled during the search process. This question has provoked a considerable amount of research. However, Horowitz and Wolfe made this assertion based on reaction time functions, and eye movement data are ideal for addressing the issue (since one can examine how frequently the eyes return to a previously sampled part of the array). Indeed, a number of eye movement experiments (Beck, Peterson, Boot, Vomela, & Kramer, 2006a; Beck, Peterson, & Vomela, 2006b; Geyer, von Mu ̈hlenen, & Mu ̈ller, 2007; Peterson, Kramer, Wang, Irwin, & McCarley, 2001) make it quite clear that viewers generally do not return to previously searched items.
The perceptual span in visual search
Rayner and Fisher (1987a, 1987b) used the moving-window paradigm as viewers searched through horizontally arranged letter strings for a specified target letter. They found that the size of the perceptual span varied as a function of the difficulty of the distractor letters; when the distrac- tor letters were visually similar to the target letter, the size of the perceptual span was smaller than when they were distinctly different from the target letter. They suggested that there were two qualitatively different regions within the span: a decision region (where information about the pre- sence or absence of a target is available), and a preview region where some letter information is available but where information on the absence of a target is not available.
1482 THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8)
Bertera and Rayner (2000) had viewers search though a randomly arranged array of letters and digits for the presence of a target letter. They used both the moving-window and moving-mask techniques. They varied the size of the array (so thatitwas13by10deg,6by6deg,or5by3.5 deg), but the number of items was held constant (so in the smaller arrays, the information was more densely packed). Mask size was 0.3, 1, 1.7, 2.3, or 3 deg; window size was 1, 2.3, 3.7, 5, and 5.7 deg. There were also control conditions in which no mask or window was present. Not sur- prisingly, the moving mask had a deleterious effect on search time and accuracy, and the larger the mask, the longer the search time, the more fixations were made, and the longer were the fixations; saccade size was affected by array size, but mask size had little effect. In the moving- window condition, search performance reached asymptote when the window was 5 deg (all letters/digits falling within 2.5 deg from the fixation point were visible with such a window size while all other letters were masked).
In a study that is conceptually very similar to that of Bertera and Rayner (2000), Cornelissen, Bruin, and Kooijman (2005) asked viewers to search for the letter O among distractors (Cs, with the orientation of the opening randomly varied). The search arrays consisted of a 7   5 hexagonal matrix (three rows of seven and two rows of six objects) containing 32 Cs (distractors) and a single O (the target). The overall array size was 38   28 deg, and the objects were 5 deg apart. Mask size and window size was 5, 10, or 15 deg. Like Bertera and Rayner and other studies that have used gaze-contingent displays (see also Greene, 2006; Greene & Rayner, 2001b; Pomplun, Reingold, & Shen, 2001), Cornelissen et al. found that search times, fixation durations, and number of fixations all became larger as mask size increased and as window size became smaller. However, saccade length appeared to be more strongly affected by window and mask size manip- ulations in the Cornelissen et al. study than in the Bertera and Rayner study. Given that the arrays differed between Bertera and Rayner’s study and Cornelissen et al. (they were more structured and
larger in the latter study), it is difficult to make gen- eralizations regarding any differences in results. What is certain from the two studies is that, as noted earlier, array size matters in search, and foveal masks (which mimic central scotomas) provide greater disruption to search than moving windows (which create tunnel vision).
Finally, other recent studies have investigated the perceptual span via gaze-contingent multi- resolution moving windows (Geisler, Perry, & Najemnik, 2006; Loschky, McConkie, Yang, & Miller,2005). Within this paradigm, information outside the window is degraded in a manner that simulates resolution degradation (i.e., blurring) at various eccentricities from an observer’s area of fix- ation. The eccentricity in degrees at which display resolution drops to one half of its value at the fix- ation point is termed 12. In a multiresolution display, the value of 12 controls the extent of blur- ring into the parafovea, such that the smaller the value of 12, the steeper the drop-off in resolution. Findings from these studies suggest that during any single fixation, when 12 is about 6 deg, the par- afoveal blur imposed on a scene is not detectable. Thus, viewers do not notice that the scene has been artificially blurred. Consistent with other studies discussed here dealing with the use of information beyond the point of fixation, even when artificial blurring went undetected in eccentric vision, search performance was affected.
Preview benefit
It is undoubtedly the case that viewers obtain preview benefit during search (and the issue is probably related to whether or not there is memory for items in search discussed earlier). Typically, studies of preview benefit in visual search provide a viewer with a preview of the search array (or part of the array) for a set period of time (such as 500 ms), or no preview in a control condition. Then the array is presented in its entirety. Generally, it is found (Watson & Inglis, 2007) that there are fewer fixations on pre- viewed stimuli (and if they are fixated, for shorter durations) in the preview condition than in the control condition in which no preview of the array is provided. In a variation of the preview
THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8) 1483
EYE MOVEMENTS AND ATTENTION
RAYNER
presentation paradigm, van Zoest, Lleras, Kingstone, and Enns (2007) showed that when a search display was blanked for 900 ms and was re-presented during search, viewers were quick to respond to targets that were near the point of fix- ation before the interruption. In contrast, under similar search conditions when a gaze-contingent paradigm was used to present the target at the current point of fixation after the blank interval, viewers were not quick to respond. In effect, viewers were better at responding to the target if they autonomously found it, presumably with some preview from the previous fixation.
While all of the studies reviewed here are inter- esting, and suggestive of preview benefit, it is strik- ing that there is no research directly using the types of gaze-contingent boundary paradigms that have been used in reading to study preview benefit in visual search. An approximation towards this is found in a study by Pomplun et al. (2001). They showed that in visual search with gaze-contingent windows, when particular features are visible outside the window, saccades are selectively made towards these particular features within the window. Perhaps the time is ripe to develop bound- ary paradigms (as used in reading research) to study preview benefit in visual search.
Eye movement control in visual search
Where and when to move the eyes. While there have been considerable efforts undertaken to determine the factors involved in deciding where and when to move the eyes in visual search (Greene, 2006; Greene & Rayner, 2001a, 2001b; Hooge & Erkelens, 1996, 1998; Hooge, Vlaskamp, & Over, 2007; Jacobs, 1986; Pomplun, Reingold, & Shen, 2003; Vaughan, 1982), a clear answer to the issue has not emerged. Some have concluded that fixation durations in search are the result of a combination of preprogrammed saccades and fix- ations that are influenced by the fixated infor- mation (Hooge et al., 2007; Vaughan, 1982). Others have suggested that the completion of foveal analysis is not necessarily the trigger for an eye movement (Hooge & Erkelens, 1996, 1998) while others have suggested that it is (Greene & Rayner, 2001b). Still others (Trukenbrod &
Engbert, 2007) have demonstrated that fixation position is an important predictor of the next saccade and influences both the fixation duration and selection of the next saccade target. Rayner (1995) suggested that the trigger to move the eyes in a search task is something like: Is the target present in the decision area of the perceptual span? If it is not, a new saccade is programmed to move the eyes to a location that has not been examined (see also Motter & Belky, 1998a, 1998b; Najemnik & Geisler, 2005, for similar arguments). As with reading and scene perception, attention would move to the region targeted for the next saccade.
The decisions about where and when to move the eyes is undoubtedly strongly influenced by the characteristics of the specific search task and the density of the visual array, as well as viewer strategies (van Zoest, Donk, & Theeuwes, 2004). It seems that parallels between visual search and scene perception are greater than with reading, in that visual saliency plays a greater role in directing fixations. Also, search for targets in visual search displays and scenes have different dimensions that are more variable than reading. For instance, with respect to search tasks, there are many different types of targets that people may be asked to search for. Searching for a certain product in a grocery store shelf or searching for a particular person in a large group picture or for a word in a dictionary may well yield very different strategies than skimming text for a word (and hence influence eye movements in different ways). Although the task is generally much better defined in visual search than in scene perception, it typically is not as well specified as in reading.
Models of eye movement control in visual search
Perhaps the most well-known model of eye movement control is that of Findlay and Walker (1999). This model focuses on saccade generation based on parallel processing and competitive inhibition and, like many of the models of scene perception, relies heavily on the notion of a saliency map. While the model is unques- tionably very interesting and very much tied to
1484 THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8)
neurophysiological properties of the oculomotor system, it is not a fully implemented model.
One fully implemented model is the target acquisition model (TAM) of Zelinsky (2008). TAM accounts for eye movements in search con- texts ranging from fully realistic scenes to objects arranged in circular arrays to search for Os embedded in Qs. It also accounts for manipulations such as set size, target eccentricity, and target – dis- tractor similarity, and it handles a number of important findings on eye movements and visual search (X. Chen & Zelinsky, 2006; Dickinson & Zelinsky, 2005, 2007; Neider & Zelinsky, 2006a, 2006b; Zelinsky, 1996, 2001; Zelinsky & Loschky, 2005; Zelinsky & Sheinberg, 1997). Comparisons of scan paths of the model to human viewers reveal that the model nicely mimics viewers’ behaviour, and it is difficult when presented with scan paths of the model and viewers to determine which is which. As impressive as TAM is with respect to simulating eye move- ments and scan paths during search, it does not provide an account of the determinants of when to move the eyes, and, hence, it does not predict fix- ation durations in search. Hopefully, future instan- tiations of TAM will lead to a better understanding of the mechanisms involved in accounting for how long the eyes pause in search.
Eye movements and visual cognition
Although there are obviously many differences between reading, scene perception, and visual search, there are some important generalizations that can be made. First, how much information is processed on any fixation (the perceptual span or functional field of view) varies as a function of the task. The perceptual span is obviously smaller in reading than in scene perception and visual search. Thus, for example, fixations in scene perception tend to be longer, and saccades are longer because more information is being
processed on a fixation. Second, the difficulty of the stimulus influences eye movements: In reading, when the text becomes more difficult, eye fixations get longer, and saccades get shorter; likewise in scene perception and visual search, when the array is more difficult (crowded, clut- tered, dense), fixations get longer, and saccades get shorter. Third, the difficulty of the task (reading for comprehension vs. reading for gist, searching for a person in a scene vs. looking at the scene for a memory test, and so on) clearly influences eye movements. Finally, in all three tasks it seems that viewers integrate visual infor- mation somewhat poorly across saccades (Najemnik & Geisler, 2005; Rayner, 1998), and that what is most critical is that there is efficient processing of information on each fixation.
To this point, research on reading, scene perception, and visual search has been the entire focus. In the next section, two lines of recent research utilizing eye movements are discussed: “real-world” tasks and the visual-world paradigm. Space limitations preclude discussion of eye movements during music perception (Gilman & Underwood, 2003; Land & Furneaux, 1997; Rayner & Pollatsek, 1997; Truitt, Clifton, Pollatsek, & Rayner, 1997), face perception (Henderson, Williams, & Falk, 2005; Williams & Henderson, 2007), typing (Inhoff, 1991; Inhoff & Gordon, 1997), driving (Chapman & Underwood, 1998; Land & Tatler, 2001; Pollatsek, Narayanaan, Pradhan, & Fisher, 2006a; Recarte & Nunes, 2000, 2003), problem solving and concept learning (Knoblich, Ohlsson, & Raney, 2001; Rehder & Hoffman, 2005), sports (Huber & Krist, 2004; Land & McLeod, 2001), mental rotation (Nakatani & Pollatsek, 2004), chess (Reingold, Charness, Pomplun, & Stampe, 2001), and advertising (Pieters, Wedel, & Liechty, 2008; Rayner, Miller, & Rotello, 2008; Rayner, Rotello, Stewart, Keir, & Duffy, 2001b).17 These, and other areas not mentioned,
EYE MOVEMENTS AND ATTENTION
 17 The research on advertisements is quite interesting in the context of examining how viewers alternate their attention between pictorial and written information. The research indicates that the strategy of the viewer and their goal very much influence where they look.
THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8) 1485
RAYNER
are now active areas of research that utilize eye movement data to elucidate underlying cognitive processes in the various tasks.
Eye movements in “real-world” tasks and the visual-world paradigm
Recently, there has been considerable interest in using eye movements in tasks that presumably share many components with scene perception and visual search. Specifically, there has been con- siderable interest in “real-world” or “natural” tasks (Hayhoe & Ballard, 2005; Hayhoe, Shrivastava, Mruczek, & Pelz, 2007; Land & Hayhoe, 2001) and in a paradigm called the visual-world paradigm (Altmann, 2004; Altmann & Kamide, 1999; Spivey, Tanenhaus, Eberhard, & Sedivy, 2002; Tanenhaus & Spivey-Knowlton, 1996; Tanenhaus, Spivey-Knowlton, Eberhard, & Sedivy, 1995). In the so-called real-world or natural tasks, people’s eye movements are moni- tored as they engage in tasks such as making a sand- wich or a cup of tea. While these studies are very interesting and while they demonstrate the much greater flexibility in recording eye movements that the new generations of eye tracking devices afford, there is something puzzling about the term. Are reading, scene perception, and visual search not real-world or natural tasks? Perhaps they are called real-world or natural tasks simply for the lack of a better descriptor. Certainly, there are many experiments on eye movements and driving (which is obviously also a real-world task), but these studies are typically not designated as real- world or natural tasks. A more appropriate label for these “real-world” studies would probably be something like eye movements and “action tasks” where some overt action is required on the part of the participant. Nonetheless, these types of studies do reveal important information about the
relationship between eye movements and action plans (and the execution of such plans).
In the visual-world paradigm,18 participants hear some type of auditory input with a visual array in front of them. Eye movements show a systematic relationship between what is being listened to and where the eyes tend to go (and how quickly they go) in the visual array. The visual-world paradigm as such combines aspects of scene perception, visual search, and language processing. While the work is quite interesting, it is somewhat curious that there have been no real critics of the paradigm to date. For example, what do participants in the visual-world paradigm think they are supposed to do? What else are they supposed to do besides look at the visual array? There are also issues related to the nature of data analysis in the paradigm (see Altmann & Kamide, 2004, for a good discussion). Virtually all researchers who have adopted the paradigm have been advocates of its virtues (and there are obviously many). However, it is generally the case that paradigms that prove to be the most useful over the long run are also those where the basic assumptions underlying the paradigm are challenged, and empirical evidence is brought to bear on the issues at hand. This has certainly been the case over the past 40 years with respect to eye movements during reading.
CONCLUDING COMMENTS
Hopefully it is apparent that a great deal of knowledge has been gleaned from studies using eye movements to examine reading, language pro- cessing, scene perception, visual search, and other cognitive-processing tasks. As noted earlier, the present review is not intended as a comprehensive or exhaustive review as were earlier articles
 18 The task originated with Cooper (1974) but has been effectively utilized by Tanenhaus, Altmann, and others to study a number of topics ranging from auditory word recognition to syntactic parsing. Some recent work by Altmann (2004), Richardson and Spivey (2000), and Ferreira, Apel, and Henderson (2008) is very interesting in that they show, perhaps surprisingly, that listeners fixate on now-empty regions that had previously been occupied by relevant objects. Ferreira et al. suggested that the “looking at nothing” finding perhaps provides some clues about how the visual system creates and stores internal memory representations and that looking at nothing aids retrieval of these representations.
1486 THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8)
(Rayner, 1978b, 1998). Rather, I have largely reviewed work with some ties to my laboratory or work that I otherwise find interesting and appealing.
My contention is that research on eye move- ments during reading has advanced more rapidly and systematically than research on scene percep- tion and visual search. This is probably due to the fact that stimulus characteristics (in reading, there is a limited set of letters that make up words, whereas in scene perception the scene is not as constrained by stimulus properties) and the task (the task in reading and visual search is quite straightforward, but exactly what viewers do in scene perception is not as obvious) are more amenable to experimental manipulations in reading than in scenes (especially) and search. Research on reading has significantly benefited from the use of the gaze-contingent paradigm, and while researchers in the domains of scene perception and visual search have been utilizing such paradigms recently, it is the case that there are many issues in both domains where the para- digms could be effectively used. It would also be appropriate for research of the latter type to acknowledge the prior work done on reading (which, unfortunately, does not always happen).
Another area where research on reading has been advanced over that on scene perception and visual search is with respect to the development of computational models to account for eye move- ment data. Models of eye movement control in reading tend to do a good job of accounting both for where readers look and how long they look at words. Models of eye movement control in scene perception and visual search have largely focused on where viewers look to the exclusion of when they move their eyes. Hopefully, this will be reme- died in the next few years.
All in all, it is clear that major advances have been made with respect to understanding eye movements in reading, scene perception, and visual search. Although it has become increasingly clear that eye movements provide a very good (and precise) index of mental processing in various tasks, it is the case that eye movement research perhaps does not have quite the status many concede to various brain imaging techniques
(even though eye movement data typically have better temporal resolution). On the other hand, it is certainly the case that more and more researchers are turning to eye movement recording and data as a means to examine important issues about how the brain/mind handles information in various tasks. Many brain imaging techniques now enable researchers to also record eye move- ments (though rather crudely), and attempts to simultaneously record eye movements and event- related potentials in reading and other tasks look very promising (Baccino & Manunta, 2005; Dambacher & Kliegl, 2007; Sereno & Rayner, 2003). Thus, the future looks very bright with respect to the possibility of learning more about cognitive processing and how information is pro- cessed in the tasks described above via the use of eye movements.
Original manuscript received 7 May 2008 Accepted revision received 4 September 2008 First published online 14 May 2009
REFERENCES
Abrams, R. A., Meyer, D. E., & Kornblum, S. (1989). Speed and accuracy of saccadic eye movements: Characteristics of impulse variability in the oculomo- tor system. Journal of Experimental Psychology: Human Perception and Performance, 15, 529–543.
Altarriba, J., Kambe, G., Pollatsek, A., & Rayner, K. (2001). Semantic codes are not used in integrating information across eye fixations in reading: Evidence from fluent Spanish–English bilinguals. Perception & Psychophysics, 63, 875–890.
Altmann, G. T. M. (2004). Language-mediated eye movements in the absence of a visual world: The “blank screen paradigm”. Cognition, 93, B79–B87.
Altmann, G. T. M., Garnham, A., & Dennis, Y. (1992). Avoiding the garden path: Eye movements in context. Journal of Memory and Language, 31, 685 – 712.
Altmann, G. T. M., & Kamide, Y. (1999). Incremental interpretation at verbs: Restricting the domain of sentence reference. Cognition, 73, 247–264.
Altmann, G. T. M., & Kamide, Y. (2004). Now you see it, now you don’t: Mediating the mapping between language and the visual world. In J. M. Henderson
THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8) 1487
EYE MOVEMENTS AND ATTENTION
RAYNER
& F. Ferreira (Eds.), The interface of language, vision, and action: Eye movements and the visual world (pp. 347–386). New York: Psychology Press.
Andrews, S., Miller, B., & Rayner, K. (2004). Eye movements and morphological segmentation of compound words: There is a mouse in mousetrap. European Journal of Cognitive Psychology, 16, 285 – 311.
Andrews, T. J., & Coppola, D. M. (1999). Idiosyncratic characteristics of saccadic eye movements when viewing different visual environments. Vision Research, 39, 2947–2953.
Angele, B., Slattery, T. J., Yang, J., Kliegl, R., & Rayner, K. (2008). Parafoveal processing in reading: Manipulating n þ 1 and n þ 2 previews simul- taneously. Visual Cognition, 16, 697–707.
Antes, J. R. (1974). The time course of picture viewing. Journal of Experimental Psychology, 103, 62–70.
Ashby, J. (2006). Prosody in skilled silent reading: Evidence from eye movements. Journal of Research in Reading, 29, 318–333.
Ashby, J., & Clifton, C. (2005). The prosodic property of lexical stress affects eye movements during silent reading. Cognition, 96, B89–B100.
Ashby, J., & Rayner, K. (2004). Representing syllable information during silent reading: Evidence from eye movements. Language and Cognitive Processes, 19, 391–426.
Ashby, J., Rayner, K., & Clifton, C. (2005). Eye movements of highly skilled and average readers: Differential effects of frequency and predictability. Quarterly Journal of Experimental Psychology, 58A, 1065 – 1086.
Ashby, J., Treiman, R., Kessler, B., & Rayner, K. (2006). Vowel processing in silent reading: Evidence from eye movements. Journal of Experimental Psychology: Learning, Memory, and Cognition, 32, 416–424.
Baccino, T., & Manunta,Y. (2005). Eye-fixation-related potentials: Insight into parafoveal processing. Journal of Psychophysiology, 19, 204–215.
Baddeley, R. J., & Tatler, B. W. (2006). High frequency edges (but not contrast) predict where we fixate: A Bayesian system identification analysis. Vision Research, 46, 2824–2833.
Bai, X, Yan, G., Liversedge, S. P., Zang, X., & Rayner, K. (2008). Reading spaced and unspaced Chinese text: Evidence from eye movements. Journal of Experimental Psychology: Human Perception and Performance, 34, 1277–1287.
Balota, D. A., Pollatsek, A., & Rayner, K. (1985). The interaction of contextual constraints and parafoveal
visual information in reading. Cognitive Psychology,
17, 364–390.
Beck, M. R., Peterson, M. S., Boot, W. R., Vomela, M.,
& Kramer, A. F. (2006a). Explicit memory for rejected distractors during visual search. Visual Cognition, 14, 150–174.
Beck, M. R., Peterson, M. S., & Vomela, M. (2006b). Memory for where, but not what, is used during visual search. Journal of Experimental Psychology: Human Perception and Performance, 32, 235–250.
Becker, W., & Ju ̈rgens, R. (1979). An analysis of the saccadic system by means of double step stimuli. Vision Research, 199, 1967–1983.
Becker, M. W., Pashler, H., & Lubin, J. (2007). Object- intrinsic oddities draw early saccades. Journal of Experimental Psychology: Human Perception and Performance, 33, 20–30.
Bertera, J. H., & Rayner, K. (2000). Eye movements and the span of effective vision in visual search. Perception & Psychophysics, 62, 576–585.
Bertram, R., & Hyo ̈na ̈, J. (2003). The length of a complex word modifies the role of morphological structure: Evidence from eye movements when reading short and long Finnish compounds. Journal of Memory and Language, 48, 615–634.
Bertram, R., Pollatsek, A., & Hyo ̈ na ̈ , J. (2004). Morphological parsing and the use of segmentation cues in reading Finnish compounds. Journal of Memory and Language, 51, 325–345.
Binder, K. S., Duffy, S. A., & Rayner, K. (2001). The effects of thematic fit and discourse context on syn- tactic ambiguity resolution. Journal of Memory and Language, 44, 297–324.
Binder, K. S., & Morris, R. K. (1995). Eye movements and lexical ambiguity resolution: Effects of prior encounter and discourse topic. Journal of Experimental Psychology: Learning, Memory, and Cognition, 21, 1186–1196.
Binder, K. S., Pollatsek, A., & Rayner, K. (1999). Extraction of information to the left of the fixated word in reading. Journal of Experimental Psychology: Human Perception and Performance, 25, 1162–1172.
Binder, K. S., & Rayner, K. (1998). Contextual strength does not modulate the subordinate bias effect: Evidence from eye fixations and self-paced reading. Psychonomic Bulletin & Review, 5, 271–276.
Birch, S., & Rayner, K. (1997). Linguistic focus affects eye movements during reading. Memory & Cognition, 25, 653–660.
Blythe, H. I., Liversedge, S. P., Joseph, H. S. S. L., White, S. J., Findlay, J. M., & Rayner, K. (2006).
1488 THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8)
The binocular coordination of eye movements during reading in children and adults. Vision Research, 46, 3898–3908.
Boland, J. E., & Blodgett, A. (2001). Understanding constraints on syntactic generation: Lexical bias and discourse congruency effects on eye movements. Journal of Memory and Language, 45, 391–411.
Boland, J. E., Chua, H. F., & Nisbett, R. E. (2008). How we see it: Culturally different eye movement patterns over visual scenes. In K. Rayner, D. Shen, X. Bai, & G. Yan (Eds.), Cognitive and cultural influences on eye movements (pp. 363 – 378). Tianjin, China: Tianjin People’s Publishing House/ Psychology Press.
Briihl, D., & Inhoff, A. W. (1995). Integrating information across fixations during reading: The use of orthographic bodies and of exterior letters. Journal of Experimental Psychology: Learning, Memory, and Cognition, 21, 55–67.
Brockmole, J. R., & Henderson, J. M. (2008). Prioritizing new objects for eye fixation in real- world scenes: Effects of object-scene consistency. Visual Cognition, 16, 375–390.
Brysbaert, M., Drieghe, D., & Vitu, F. (2005). Word skipping: Implications for theories of eye movement control in reading. In G. Underwood (Ed.), Cognitive processes in eye guidance (pp. 53 – 78). Oxford, UK: Oxford University Press.
Buswell, G. T. (1935). How people look at pictures: A study of the psychology of perception in art. Chicago: University of Chicago Press.
Calvo, M. G., & Lang, P. J. (2005). Parafoveal semantic processing of emotional visual scenes. Journal of Experimental Psychology: Human Perception and Performance, 31, 502–519.
Calvo, M. G., & Meseguer, E. (2002). Eye movements and processing stages in reading: Relative contri- bution of visual, lexical, and contextual factors. Spanish Journal of Psychology, 5, 66–77.
Calvo, M. G., & Nummenmaa, L. (2007). Processing of unattended emotional visual scenes. Journal of Experimental Psychology: General, 136, 347–369.
Campbell, F. W., & Wurtz, R. H. (1979). Saccadic omission: Why we do not see a gray-out during a saccadic eye movement. Vision Research, 18, 1297 – 1301.
Carlson-Radvansky, L. A. (1999). Memory for rela- tional information across eye movements. Perception & Psychophysics, 61, 919–934.
Carlson-Radvansky, L. A., & Irwin, D. E. (1995). Memory for structural information across eye
movements. Journal of Experimental Psychology:
Learning, Memory, and Cognition, 21, 1441–1458. Carroll, P. J., & Slowiaczek, M. L. (1986). Constraints on semantic priming in reading: A fixation time
analysis. Memory & Cognition, 14, 509–522. Castelhano, M. S., & Henderson, J. M. (2007). Initial scene representations facilitate eye movement guidance in visual search. Journal of Experimental Psychology: Human Perception and Performance, 33,
753 – 763.
Castelhano, M. S., & Henderson, J. M. (2008a). Stable
individual differences across images in human sacca- dic eye movements. Canadian Journal of Experimental Psychology, 62, 1–14.
Castelhano, M. S., & Henderson, J. M. (2008b). The influence of color on perception of scene gist. Journal of Experimental Psychology: Human Perception and Performance, 34, 660–675.
Chace, K. H., Rayner, K., & Well, A. D. (2005). Eye movements and phonological parafoveal preview benefit: Effects of reading skill. Canadian Journal of Experimental Psychology, 59, 209–217.
Chaffin, R., Morris, R. K., & Seely, R. E. (2001). Learning new word meanings from context: A study of eye movements. Journal of Experimental Psychology: Learning, Memory, and Cognition, 27, 225–235.
Chapman, P. R., & Underwood, G. (1998). Visual search of driving situations: Danger and experience. Perception, 27, 951–964.
Chen, H., & Tang, C. (1998). The effective visual field in reading Chinese. Reading and Writing, 10, 245– 254.
Chen, X., & Zelinsky, G. J. (2006). Real-world visual search is dominated by top-down guidance. Vision Research, 46, 4118–4133.
Chua, H. F., Boland, J. E., & Nisbett, R. E. (2005). Cultural variation in eye movements during scene perception. Proceedings of the National Academy of Sciences, 102, 12629–12633.
Clifton, C., Staub, A., & Rayner, K. (2007). Eye move- ments in reading words and sentences. In R. van Gompel, M. H. Fischer, W. S. Murray, & R. L. Hill (Eds.), Eye movements: A window on mind and brain (pp. 341–372). Oxford, UK: Elsevier.
Clifton, C., Traxler, M. J., Mohamed, M. T., Williams, R. S., Morris, R. K., & Rayner, K. (2003). The use of thematic role information in parsing: Syntactic processing autonomy revisited. Journal of Memory and Language, 49, 317–334.
Cook, A. E., & Myers, J. L. (2004). Processing discourse roles in scripted narratives: The influences
THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8) 1489
EYE MOVEMENTS AND ATTENTION
RAYNER
of context and world knowledge. Journal of Memory
and Language, 50, 268–288.
Cooper, R. M. (1974). The control of eye fixation by the
meaning of spoken language: A new methodology for the real-time investigation of speech perception, memory, and language processing. Cognitive Psychology, 6, 84–107.
Cornelissen, F. W., Bruin, L. J., & Kooijman, A. C. (2005). The influence of artificial scotomas on eye movements during visual search. Optometry and Vision Science, 82, 27–35.
Dambacher, M., & Kliegl, R. (2007). Synchronizing timelines: Relations between fixation durations and N400 amplitudes during sentence reading. Brain Research, 1155, 147–162.
De Graef, P. (1998). Prefixational object perception in scenes: Objects popping out of schemas. In G. Underwood (Ed.), Eye guidance in reading and scene perception (pp. 315–338). Oxford, UK: Elsevier.
De Graef, P. (2005). Semantic effects on object selection in real-world scene perception. In G. Underwood (Ed.), Cognitive processes in eye guidance (pp. 189– 211). Oxford, UK: Oxford University Press.
De Graef, P., Christiaens, D., & d’Ydewalle, G. (1990). Perceptual effects of scene context on object identification. Psychological Research, 52, 317–329.
DenBuurman, R., Boersma, T., & Gerrissen, J. F. (1981). Eye movements and the perceptual span in reading. Reading Research Quarterly, 16, 227–235.
Deubel, H., & Schneider, W. X. (1996). Saccade target selection and object recognition: Evidence for a common attentional mechanism. Vision Research, 36, 1827–1837.
Deutsch, A., Frost, R., Peleg, S., Pollatsek, A., & Rayner, K. (2003). Early morphological effects in reading: Evidence from parafoveal preview benefit in Hebrew. Psychonomic Bulletin & Review, 10, 415 – 422.
Deutsch, A., Frost, R., Pollatsek, A., & Rayner, K. (2000). Early morphological effects in word recog- nition: Evidence from parafoveal preview benefit. Language and Cognitive Processes, 15, 487–506.
Deutsch, A., Frost, R., Pollatsek, A., & Rayner, K. (2005). Morphological parafoveal preview benefit effects in reading: Evidence from Hebrew. Language and Cognitive Processes, 20, 341–371.
Deutsch, A., & Rayner, K. (1999). Initial fixation location effects in reading Hebrew words. Language and Cognitive Processes, 14, 393–421.
Dickinson, C. A., & Zelinsky, G. J. (2005). Marking rejected distractors: A gaze-contingent technique
for measuring memory during search. Psychonomic
Bulletin & Review, 12, 1120–1126.
Dickinson, C. A., & Zelinsky, G. J. (2007). Memory for
the search path: Evidence for a high-capacity rep- resentation of search history. Vision Research, 47, 1745 – 1755.
Dopkins, S., Morris, R. K., & Rayner, K. (1992). Lexical ambiguity and eye movements in reading: A test of competing models of lexical ambiguity resolution. Journal of Memory and Language, 31, 461 – 477.
Drieghe, D. (2008). Foveal processing and word skip- ping during reading. Psychonomic Bulletin & Review, 15, 856–860.
Drieghe, D., Brysbaert, M., & Desmet, T. (2005a). Parafoveal-on-foveal effects on eye movements in reading: Does an extra space make a difference? Vision Research, 45, 1693–1706.
Drieghe, D., Brysbaert, M., Desmet, T., & De Baecke, G. (2004). Word skipping in reading: On the interplay of linguistic and visual factors. European Journal of Cognitive Psychology, 16, 79–103.
Drieghe, D., Desmet, T., & Brysbaert, M. (2007). How important are linguistic factors in word skipping during reading? British Journal of Psychology, 98, 157 – 171.
Drieghe, D., Pollatsek, A., Staub, A., & Rayner, K. (2008a). The word grouping hypothesis and eye move- ments in reading. Journal of Experimental Psychology: Learning, Memory, and Cognition, 34, 1552–1560.
Drieghe, D., Rayner, K., & Pollatsek, A. (2005b). Eye movements and word skipping during reading revisited. Journal of Experimental Psychology: Human Perception and Performance, 31, 954–969.
Drieghe, D., Rayner, K., & Pollatsek, A. (2008b). Mislocated fixations can account for parafoveal-on- foveal effects in eye movements during reading. Quarterly Journal of Experimental Psychology, 61, 1239 – 1249.
Duffy, S. A., & Keir, J. A. (2004). Violating stereotypes: Eye movements and comprehension processes when text conflicts with world knowledge. Memory & Cognition, 32, 551–559.
Duffy, S. A., Morris, R. K., & Rayner, K. (1988). Lexical ambiguity and fixation times in reading. Journal of Memory and Language, 27, 429–446.
Duffy, S. A., & Rayner, K. (1990). Eye movements and anaphor resolution: Effects of antecedent typicality and distance. Language and Speech, 33, 103–119.
Ehrlich, K., & Rayner, K. (1983). Pronoun assignment and semantic integration during reading: Eye
1490 THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8)
movements and immediacy of processing. Journal of
Verbal Learning and Verbal Behavior, 22, 75–87. Ehrlich, S. F., & Rayner, K. (1981). Contextual effects on word recognition and eye movements during reading. Journal of Verbal Learning and Verbal
Behavior, 20, 641–655.
Engbert, R., Nuthmann, A., Richter, E., & Kliegl, R.
(2005). SWIFT: A dynamical model of saccade generation during reading. Psychological Review, 112, 777–813.
Epelboim, J., Booth, J. R., Ashkenazy, R., Taleghani, A., & Steinman, R. M. (1997). Fillers and spaces in text: The importance of word recognition in reading. Vision Research, 37, 2899–2914.
Epelboim, J., Booth, J. R., & Steinman, R. M. (1994). Reading unspaced text: Implications for theories of reading eye movements. Vision Research, 34, 1735 – 1766.
Epelboim, J., Booth, J. R., & Steinman, R. M. (1996). Much ado about nothing: The place of space in text. Vision Research, 36, 465–470.
Evans, K., Rotello, C. M., Li, X., & Rayner, K. (2009). Scene perception and memory revealed by eye move- ments and ROC analyses: Does a cultural difference truly exist? Quarterly Journal of Experimental Psychology, 62, 276–285.
Feng, G. (2006). Eye movements as time-series random variables: A stochastic model of eye movement control in reading. Cognitive Systems Research, 7, 70–95.
Ferguson, H. J., & Sanford, A. J. (2008). Anomalies in real and counterfactual worlds: An eye-movement investigation. Journal of Memory and Language, 58, 609 – 626.
Ferreira, F., Apel, J., & Henderson, J. M. (2008). Taking a new look at looking at nothing. Trends in Cognitive Sciences, 12, 405–410.
Ferreira, F., & Clifton, C. (1986). The independence of syntactic processing. Journal of Memory and Language, 25, 348–368.
Filik, R. (2008). Contextual override of pragmatic anomalies: Evidence from eye movements. Cognition, 106, 1038–1046.
Findlay, J. M., & Gilchrist, I. D. (1998). Eye guidance and visual search. In G. Underwood (Ed.), Eye guidance in reading and scene perception (pp. 295– 312). Oxford, UK: Elsevier.
Findlay, J. M., & Gilchrist, I. D. (2003). Active vision: The psychology of looking and seeing. Oxford, UK: Oxford University Press.
Findlay, J. M., & Walker, R. (1999). A model of saccade generation based on parallel processing
and competitive inhibition. Behavioral and Brain
Sciences, 22, 661–674.
Fine, E. M., & Rubin, G. S. (1999a). Reading with a
central field loss: Number of letters masked is more important than the size of the mask in degrees. Vision Research, 39, 747–756.
Fine, E. M., & Rubin, G. S. (1999b). Reading with simulated scotomas: Attending to the right is better than attending to the left. Vision Research, 39, 1039–1048.
Fine, E. M., & Rubin, G. S. (1999c). The effects of simulated cataract on reading with normal vision and simulated central scotoma. Vision Research, 39, 4274 – 4285.
Fisher, D. F., & Shebilske, W. L. (1985). There is more that meets the eye than the eyemind assumption. In R. Groner, G. W. McConkie, & C. Menz (Eds.), Eye movements and human information processing (pp. 149–158). Amsterdam: North Holland
Folk, J. R. (1999). Phonological codes are used to access the lexicon during silent reading. Journal of Experimental Psychology: Learning, Memory, and Cognition, 25, 892–906.
Folk, J. R., & Morris, R. K. (2003). Effects of syntactic category assignment on lexical ambiguity resolution in reading: An eye movement analysis. Memory & Cognition, 31, 87–99.
Foulsham, T., Kingstone, A., & Underwood, G. (2008). Turning the world around: Patterns in saccade direc- tion vary with picture orientation. Vision Research, 48, 1777–1790.
Foulsham, T., & Underwood, G. (2008). What can saliency models predict about eye movements? Spatial and sequential aspects of fixations during encoding and recognition. Journal of Vision, 8(2):6, 1–17, doi:10.1167/8.2.6.
Frazier, L., & Rayner, K. (1982). Making and correcting errors during sentence comprehension: Eye move- ments in the analysis of structurally ambiguous sentences. Cognitive Psychology, 14, 178–210.
Friedman, A. (1979). Framing pictures: The role of knowledge in automatized encoding and memory for gist. Journal of Experimental Psychology: General, 108, 316–355.
Frisson, S., Rayner, K., & Pickering, M. J. (2005). Effects of contextual predictability and transitional probability on eye movements during reading. Journal of Experimental Psychology: Learning, Memory, and Cognition, 31, 862–877.
Garrod, S., O’Brien, E. J., Morris, R. K., & Rayner, K. (1990). Elaborative inferencing as an active or
THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8) 1491
EYE MOVEMENTS AND ATTENTION
RAYNER
passive process. Journal of Experimental Psychology:
Learning, Memory, and Cognition, 16, 250–257. Garrod, S., & Terras, M. (2000). The contribution of lexical and situational knowledge to resolving dis- course roles: Bonding and resolution. Journal of
Memory and Language, 42, 526–544.
Gautier, V., O’Regan, J. K., & LaGargasson, J. F.
(2000). “The skipping” revisited in French: program- ming saccades to skip the article “les”. Vision Research, 40, 2517–2531.
Geisler, W. S., Perry, J. S., & Najemnik, J. (2006). Visual search: The role of peripheral information measured using gaze-contingent displays. Journal of Vision, 6, 858–873.
Geyer, T., von Mu ̈ hlenen, A., & Mu ̈ ller, H. J. (2007). What do eye movements reveal about the role of memory in visual search. Quarterly Journal of Experimental Psychology, 60, 924–935.
Gilman, E., & Underwood, G. (2003). Restricting the field of view to investigate the perceptual spans of pianists. Visual Cognition, 10, 201–232.
Greene, H. H. (2006). The control of fixation duration in visual search. Perception, 35, 303–315.
Greene, H. H., & Rayner, K. (2001a). Eye-movement control in direction-coded visual search. Perception, 30, 147–157.
Greene, H. H., & Rayner, K. (2001b). Eye movements and familiarity effects in visual search. Vision Research, 41, 3763–3773.
Grimes, J., & McConkie, G. W. (1995). On the insensitivity of the human visual system to image changes made during saccades. In K. Atkins (Ed.), Problems in perception. Oxford, UK: Oxford University Press.
Ha ̈ikio ̈, T., Bertram, R., Hyo ̈na ̈, J., & Niemi, P. (2009). Development of the letter identity span in reading: Evidence from the eye movement moving window paradigm. Journal of Experimental Child Psychology, 102, 167–181.
Harris, C. R., Kaplan, R. L., & Pashler, H. (2008).
Alarming events in the corner of your eye: Do they trigger
early saccades? Manuscript submitted for publication. Hayhoe, M., & Ballard, D. (2005). Eye movements in natural behavior. Trends in Cognitive Sciences, 9,
188 – 194.
Hayhoe, M. M., Shrivastava, A., Mruczek, R., & Pelz,
J. B. (2007). Visual memory and motor planning in
a natural task. Journal of Vision, 3, 49–63.
Heinzle, J., Martin, K., & Hepp, K. (2009). A biologi- cally realistic cortical model of eye movement control in
Heller, D., & Radach, R. (1999). Eye movements in reading: Are two eyes better than one? In W. Becker, H. Deubel, & T. Mergner (Eds.), Current oculomotor research: Physiological and psychological aspects (pp. 341–348). New York: Plenum Press.
Henderson, J. M. (1992). Identifying objects across saccades: Effects of extrafoveal preview and flanker object context. Journal of Experimental Psychology: Learning, Memory, and Cognition, 18, 521 – 530.
Henderson, J. M. (1993). Eye movement control during visual object processing: Effects of initial fixation position and semantic constraint. Canadian Journal of Experimental Psychology, 47, 79–98.
Henderson, J. M. (2003). Human gaze control during real-world scene perception. Trends in Cognitive Sciences, 7, 498–504.
Henderson, J. M. (2007). Regarding scenes. Current Directions in Psychological Science, 16, 219–222.
Henderson, J. M., Brockmole, J. R., Castelhano, M. S., & Mack, M. (2007). Visual saliency does not account for eye movements during visual search in real-world scenes. In R. P. G. van Gompel, M. H. Fischer, W. S. Murray, & R. L. Hill (Eds.), Eye movements: A window on mind and brain (pp. 539 – 562). Oxford, UK: Elsevier.
Henderson, J. M., Brockmole, J. R., & Gajewski, D. A. (2008). Differential detection of global luminance and contrast changes across saccades and flickers during active scene perception. Vision Research, 48, 16–29.
Henderson, J. M., & Ferreira, F. (1990). Effects of foveal processing difficulty on the perceptual span in reading: Implications for attention and eye movement control. Journal of Experimental Psychology: Learning, Memory, and Cognition, 16, 417–429.
Henderson, J. M., & Ferreira, F. (1993). Eye movement control during reading: Fixation measures reflect foveal, but not parafoveal processing difficulty. Canadian Journal of Experimental Psychology, 47, 201 – 221.
Henderson, J. M., & Hollingworth, A. (1999). The role of fixation position in detecting scene changes across saccades. Psychological Science, 10, 438–443.
Henderson, J. M., & Hollingworth, A. (2003). Global transsaccadic change blindness during scene perception. Psychological Science, 14, 493–497.
Henderson, J. M., McClure, K. K., Pierce, S., & Schrock, G. (1997). Object identification without foveal vision: Evidence from an artificial scotoma paradigm. Perception & Psychophysics, 59, 323–346.
reading. Manuscript submitted for publication.
1492 THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8)
Henderson, J. M., & Pierce, G. (2008). Eye movements during scene viewing: for mixed control of fixation durations. Psychonomic Bulletin & Review, 15, 566 – 573.
Henderson, J. M., Pollatsek, A., & Rayner, K. (1987). The effects of foveal priming and extrafoveal preview on object identification. Journal of Experimental Psychology: Human Perception and Performance, 13, 449–463.
Henderson, J. M., Pollatsek, A., & Rayner, K. (1989). Covert visual attention and extrafoveal information use during object identification. Perception & Psychophysics, 45, 196–208.
Henderson, J. M., & Seifert, A. B. C. (1999). The influence of enantiomorphic transformation on transsaccadic object integration. Journal of Experimental Psychology: Human Perception and Performance, 25, 243–255.
Henderson, J. M., & Seifert, A. B. C. (2001). Types and tokens in transsaccadic object identification: Effects of spatial position and left-right orientation. Psychonomic Bulletin & Review, 8, 753–760.
Henderson, J. M., Weeks, P. A., Jr., & Hollingworth, A. (1999). Effects of semantic consistency on eye movements during scene viewing. Journal of Experimental Psychology: Human Perception and Performance, 25, 210–228.
Henderson, J. M., Williams, C. C., Castelhano, M. S., & Falk, R. J. (2003). Eye movements and picture processing during recognition. Perception & Psychophysics, 65, 725–734.
Henderson, J. M., Williams, C. C., & Falk, R. J. (2005). Eye movements are functional during face learning. Memory & Cognition, 33, 98–106.
Hirotani, M., Frazier, L., & Rayner, K. (2006). Punctuation and intonation effects on clause and sentence wrap-up: Evidence from eye movements. Journal of Memory and Language, 54, 425–443.
Hoffman, J. E., & Subramaniam, B. (1995). The role of visual attention in saccadic eye movements. Perception & Psychophysics, 57, 787–795.
Hollingworth, A., & Henderson, J. M. (1998). Does consistent scene context facilitate object perception? Journal of Experimental Psychology: General, 127, 398 – 415.
Hollingworth, A., & Henderson, J. M. (2002). Accurate visual memory for previously attended objects in natural scenes. Journal of Experimental Psychology: Human Perception and Performance, 28, 113–136.
Hollingworth, A., Richard, A. M., & Luck, S. J. (2008). Understanding the function of visual short-term
memory: Transsaccadic memory, object correspon- dence, and gaze correction. Journal of Experimental Psychology: General, 137, 163–181.
Hollingworth, A., Williams, C. C., & Henderson, J. M. (2001). To see and remember: Visually specific infor- mation is retained in memory from previously attended objects in natural scenes. Psychonomic Bulletin & Review, 8, 761–768.
Hooge, I. T. C., & Erkelens, C. J. (1996). Control of fixation duration during a simple search task. Perception & Psychophysics, 58, 969–976.
Hooge, I. T. C., & Erkelens, C. J. (1998). Adjustment of fixation duration during visual search. Vision Research, 38, 1295–1302.
Hooge, I. T. C., Vlaskamp, B. N. S., & Over, E. A. B. (2007). Saccadic search: On the duration of a fix- ation. In R. van Gompel, M. Fischer, W. Murray, & R. Hill (Eds.), Eye movement research: Insights into mind and brain. Oxford, UK: Elsevier.
Horowitz, T. S., & Wolfe, J. M. (1998). Visual search has no memory. Nature, 394, 575–577.
Huber, S., & Krist, H. (2004). When is the ball going to hit the ground? Duration estimates, eye movements, and mental imagery of object motion. Journal of Experimental Psychology: Human Perception and Performance, 30, 431–444.
Hyo ̈na ̈, J. (1995). Do irregular letter combinations attract readers’ attention? Evidence from fixation locations in words. Journal of Experimental Psychology: Human Perception and Performance, 2, 68–81.
Hyo ̈na ̈, J., & Bertram, R. (2004). Do frequency charac- teristics of nonfixated words influence the processing of fixated words during reading? European Journal of Cognitive Psychology, 16, 104–127.
Hyo ̈na ̈, J., Bertram, R., & Pollatsek, A. (2004). Are long compound words identified serially via their constituents? Evidence from an eye-movement- contingent display change study. Memory & Cognition, 32, 523–532.
Hyo ̈na ̈, J., & Ha ̈ikio ̈, T. (2005). Is emotional content obtained from parafoveal words during reading? An eye movement analysis. Scandinavian Journal of Psychology, 46, 475–483.
Hyo ̈na ̈, J., & Niemi, P. (1990). Eye movements during repeated reading of a text. Acta Psychologica, 73, 259 – 280.
Hyo ̈na ̈,J.,&Pollatsek,A.(1998).ReadingFinnish compound words: Eye fixations are affected by component morphemes. Journal of Experimental Psychology: Human Perception and Performance, 24, 1612 – 1627.
THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8) 1493
EYE MOVEMENTS AND ATTENTION
RAYNER
Inhoff, A. W. (1989a). Lexical access during eye fixations in reading: Are word access codes used to integrate lexical information across interword fixations? Journal of Memory and Language, 28, 444 – 461.
Inhoff, A. W. (1989b). Parafoveal processing of words and saccade computation during eye fixations in reading. Journal of Experimental Psychology: Human Perception and Performance, 15, 544–555.
Inhoff, A. W. (1991). Effects of word frequency during copytyping. Journal of Experimental Psychology: Human Perception and Performance, 17, 478–488.
Inhoff, A. W., & Briihl, D. (1991). Semantic processing of unattended text during selective reading: How the eyes see it. Perception & Psychophysics, 49, 289 – 294.
Inhoff, A. W., Eiter, B. M., & Radach, R. (2005). Time course of linguistic information extraction from con- secutive words during eye fixations in reading. Journal of Experimental Psychology: Human Perception and Performance, 31, 979–995.
Inhoff, A. W., & Gordon, A. M. (1997). Eye move- ments and eye – hand coordination during typing. Current Directions in Psychological Science, 6, 153 – 157.
Inhoff, A. W., & Liu, W. (1998). The perceptual span and oculomotor activity during the reading of Chinese sentences. Journal of Experimental Psychology: Human Perception and Performance, 24, 20–34.
Inhoff, A.W., Radach, R., & Eiter, B. (2006). Temporal overlap in the linguistic processing of successive words in reading: Reply to Pollatsek, Reichle, and Rayner (2006a). Journal of Experimental Psychology: Human Perception and Performance, 32, 1490 – 1495.
Inhoff, A. W., Radach, R., Eiter, B. M., & Juhasz, B. (2003). Distinct subsystems for the parafoveal processing of spatial and linguistic information during eye fixations in reading. Quarterly Journal of Experimental Psychology, 56A, 803–828.
Inhoff, A. W., Radach, R., & Heller, D. (2000a). Complex compounds in German: Interword spaces facilitate segmentation but hinder assignment of meaning. Journal of Memory and Language, 42, 23 – 50.
Inhoff, A. W., & Rayner, K. (1986). Parafoveal word processing during fixations in reading: Effects of word frequency. Perception & Psychophysics, 40, 431 – 439.
Inhoff, A. W., Starr, M., Liu, W., & Wang, J. (1998). Eye-movement-contingent display changes are not compromised by flicker and phosphor persistence. Psychonomic Bulletin & Review, 5, 101–106.
Inhoff, A. W., Starr, M., & Shindler, K. L. (2000b). Is the processing of words during eyefixations in reading strictly serial? Perception & Psychophysics, 62, 1474–1484.
Inhoff, A. W., & Topolski, R. (1992). Lack of semantic activation from unattended text during passage reading. Bulletin of the Psychonomic Society, 30, 365 – 366.
Inhoff, A. W., & Weger, U. W. (2005). Memory for word location during reading: Eye movements to previously read words are spatially selective but not precise. Memory & Cognition, 33, 447–461.
Inhoff, A. W., & Wu, C. (2005). Eye movements and the identification of spatially ambiguous words during Chinese sentence reading. Memory & Cognition, 33, 1345–1356.
Irwin, D. E. (1991). Information integration across saccadic eye-movements. Cognitive Psychology, 23, 420 – 445.
Irwin, D. E. (1992). Visual memory within and across fixation. In K. Rayner (Ed.), Eye movements and visual cognition: Scene perception and reading (pp. 146–165). New York: Springer-Verlag.
Irwin, D. E. (1998). Lexical processing during saccadic eye movements. Cognitive Psychology, 36, 1–27.
Irwin, D. E., & Carlson-Radvansky, L. A. (1996). Cognitive suppression during saccadic eye move- ments. Psychological Science, 7, 83–88.
Irwin, D. E., & Gordon, R. D. (1998). Eye movements, attention, and transsaccadic memory. Visual Cognition, 5, 127–156.
Irwin, D. E., Yantis, S., & Jonides, J. (1983). Evidence against visual integration across saccadic eye move- ments. Perception & Psychophysics, 34, 49–57.
Irwin, D. E., & Zelinsky, G. J. (2002). Eye movements and scene perception: Memory for things observed. Perception & Psychophysics, 64, 882–985.
Ishida, T., & Ikeda, M. (1989). Temporal properties of information extraction in reading studied by a text-mask replacement technique. Journal of the Optical Society A: Optics and Image Science, 6, 1624 – 1632.
Itti, L., & Koch, C. (2000). A saliency-based search mechanism for overt and covert shifts of visual attention. Vision Research, 40, 1489–1506.
Itti, L., & Koch, C. (2001). Computational modeling of visual attention. Nature Reviews Neuroscience, 2, 194 – 203.
Jacobs, A. M. (1986). Eye movement control in visual search: How direct is visual span control? Perception & Psychophysics, 39, 47–58.
1494 THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8)
Jared, D., Levy, B. A., & Rayner, K. (1999). The role of phonology in the activation of word meanings during reading: Evidence from proofreading and eye movements. Journal of Experimental Psychology: General, 128, 219–264.
Johnson, R. L., Perea, M., & Rayner, K. (2007). Transposed-letter effects in reading: Evidence from eye movements and parafoveal preview. Journal of Experimental Psychology: Human Perception and Performance, 33, 209–229.
Johnson, R. L., & Rayner, K. (2007). Top-down and bottom-up effects in pure alexia: Evidence from eye movements. Neuropsychologia, 45, 2246–2257.
Jonides, J., Irwin, D. E., & Yantis, S. (1982). Integrating information from successive fixations. Science, 215, 192 – 194.
Joseph, H. S. S. L., Liversedge, S. P., Blythe, H. I., White, S. J., Gathercole, S. E., & Rayner, K. (2008). Children’s and adults’ processing of implau- sibility during reading. Quarterly Journal of Experimental Psychology, 61, 708–723.
Juhasz, B. J. (2007). The influence of semantic transpar- ency on eye movements during English compound word recognition. In R. van Gompel, W. Murray, & M. Fischer (Eds.), Eye movements: A window on mind and brain. Oxford, UK: Elsevier.
Juhasz, B. J. (2008). The processing of compound words in English: Effects of word length on eye movements during reading. Language and Cognitive Processes, 23, 1057 – 1088.
Juhasz, B. J., Inhoff, A. W., & Rayner, K. (2005). The role of interword spaces in the processing of English compound words. Language and Cognitive Processes, 20, 291–316.
Juhasz, B. J., Liversedge, S. P., White, S. J., & Rayner, K. (2006). Binocular coordination of the eyes during reading: Word frequency and case alternation affect fixation duration but not fixation disparity. Quarterly Journal of Experimental Psychology, 59, 1614 – 1625.
Juhasz, B. J., Pollatsek, A., Hyo ̈na ̈, J., Drieghe, D., & Rayner, K. (2009). Parafoveal processing within and between words. Quarterly Journal of Experimental Psychology, 62, 1356–1376.
Juhasz, B. J., & Rayner, K. (2003). Investigating the effects of a set of intercorrelated variables on eye fixation durations in reading. Journal of Experimental Psychology: Learning, Memory, and Cognition, 29, 1312 – 1318.
Juhasz, B. J., & Rayner, K. (2006). The role of age of acquisition and word frequency in reading: Evidence
from eye fixation durations. Visual Cognition, 13,
846 – 863.
Juhasz, B. J., Starr, M. S., Inhoff, A. W., & Placke, L.
(2003). The effects of morphology on the processing of compound words: Evidence from naming, lexical decisions and eye fixations. British Journal of Psychology, 94, 223–244.
Juhasz, B. J., White, S. J., Liversedge, S. P., & Rayner, K. (2008). Eye movements and the use of parafoveal word length information in reading. Journal of Experimental Psychology: Human Perception and Performance, 34, 1560–1579.
Just, M. A., & Carpenter, P. A. (1980). A theory of reading: From eye fixations to comprehension. Psychological Review, 87, 329–354.
Kambe, G. (2004). Parafoveal processing of prefixed words during eye fixations in reading: Evidence against morphological influences on parafoveal preprocessing. Perception & Psychophysics, 66, 279 – 292.
Kambe, G., Rayner, K., & Duffy, S. A. (2001). Global context effects on processing lexically ambiguous words: Evidence from eye fixations. Memory & Cognition, 29, 363–372.
Kennedy, A. (2000). Parafoveal processing in word recognition. Quarterly Journal of Experimental Psychology, 53A, 429–456.
Kennedy, A., Murray, W. S., & Boissiere, C. (2004). Parafoveal pragmatics revisited. European Journal of Cognitive Psychology, 16, 128–153.
Kennedy A., & Pynte, J. (2005). Parafoveal-on-foveal effects in normal reading. Vision Research, 45, 153 – 68.
Kennedy, A., Pynte, J., & Ducrot, S. (2002). Parafoveal-on-foveal interactions in word recog- nition. Quarterly Journal of Experimental Psychology, 55A, 1307–1338.
Kennison, S. M., & Clifton, C. (1995). Determinants of parafoveal preview benefit in high and low working memory capacity readers: Implications for eye movement control. Journal of Experimental Psychology: Learning, Memory, and Cognition, 21, 68–81.
Kirchner, H., & Thorpe, S. J. (2006). Ultra-rapid object detection with saccadic eye movements: Visual processing speed revisited. Vision Research, 46, 1762 – 1776.
Kirkby, J., A., Webster, L. A., Blythe, H. I., & Liversedge, S. P. (2008). Binocular coordination during reading and non-reading tasks. Psychological Bulletin, 134, 742–763.
THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8) 1495
EYE MOVEMENTS AND ATTENTION
RAYNER
Kliegl, R. (2007). Towards a perceptual-span theory of distributed processing in reading: A reply to Rayner, Pollatsek, Drieghe, Slattery, and Reichle (2007). Journal of Experimental Psychology: General, 136, 530–537.
Kliegl, R., & Engbert, R. (2005). Fixation durations before word skipping in reading. Psychonomic Bulletin & Review, 12, 132–138.
Kliegl, R., Grabner, E., Rolfs, M., & Engbert, R. (2004). Length, frequency, and predictability effects of words on eye movements in reading. European Journal of Cognitive Psychology, 16, 262–284.
Kliegl, R., Nuthmann, A., & Engbert, R. (2006). Tracking the mind during reading: The influence of past, present, and future words on fixation durations. Journal of Experimental Psychology: General, 135, 12–35.
Kliegl, R., Risse, S., & Laubrock, J. (2007). Preview benefit and parafoveal-on-foveal effects from word n þ 2. Journal of Experimental Psychology: Human Perception and Performance, 33, 1250–1255.
Knoblich, G., Ohlsson, S., & Raney, G. E. (2001). An eye movement study of insight problem solving. Memory & Cognition, 29, 1000–1009.
Kohsom, C., & Gobet, F. (1997). Adding spaces to Thai and English: Effects on reading. Proceedings of the Cognitive Science Society, 19, 388–393.
Kowler, E., Anderson, E., Dosher, B., & Blaser, E. (1995). The role of attention in the programming of saccades. Vision Research, 35, 1897–1916.
Land, M. F., & Furneaux, S. (1997). The knowledge base of the oculomotor system. Philosophical Transactions of the Royal Society of London B, 352, 1231 – 1239.
Land, M. F., & Hayhoe, M. (2001). In what ways do eye movements contribute to everyday activities? Vision Research, 41, 3559–3565.
Land, M. F., & McLeod, P. (2001). From eye movements to action: How batsmen hit the ball. Nature Neuroscience, 3, 1340–1345.
Land, M. F., & Tatler, B. W. (2001). Steering with the head: The visual strategy of a racing driver. Current Biology, 11, 1215–1220.
Laubrock, J., Kliegl, R., & Engbert, R. (2006). SWIFT explorations of age differences in eye movements during reading. Neuroscience and Biobehavioral Reviews, 30, 872–884.
Legge, G. E., Hooven, T. A., Klitz, T. S., Mansfield, J. S., & Tjan, B. S. (2002). Mr. Chips 2002: New insights from an ideal-observer model of reading. Vision Research, 42, 2219–2234.
Legge, G. E., Klitz, T. S., & Tjan, B. S. (1997). Mr. Chips: An ideal-observer model of reading. Psychological Review, 104, 524–553.
Li, X., Rayner, K., & Cave, R. K. (in press). On the segmentation of Chinese words during reading. Cognitive Psychology.
Lima, S. D. (1987). Morphological analysis in sentence reading. Journal of Memory and Language, 26, 84–99. Lima, S. D., & Inhoff, A. W. (1985). Lexical access during eye fixations in reading: Effects of word-initial letter sequences. Journal of Experimental Psychology: Human
Perception and Performance, 11, 272–285.
Liu, W., Inhoff, A. W., Ye, Y., & Wu, C. (2002). Use of parafoveally visible characters during the reading of Chinese sentences. Journal of Experimental Psychology:
Human Perception and Performance, 28, 1213–1227. Liversedge, S. P., & Findlay, J. M. (2000). Saccadic eye movements and cognition. Trends in Cognitive
Sciences, 4, 6–14.
Liversedge, S. P., Rayner, K., White, S. J., Findlay,
J. M., & McSorley, E. (2006a). Binocular coordi- nation of the eyes during reading. Current Biology, 16, 1726–1729.
Liversedge, S. P., Rayner, K., White, S. J., Vergilino- Perez, D., Findlay, J. M., & Kentridge, R. W. (2004). Eye movements while reading disappearing text: Is there a gap effect in reading? Vision Research, 44, 1013–1024.
Liversedge, S. P., White, S. J., Findlay, J. M., & Rayner, K. (2006b). Binocular coordination of eye movements during reading. Vision Research, 46, 2363–2374.
Loftus, G. R., & Mackworth, N. H. (1978). Cognitive determinants of fixation location during picture viewing. Journal of Experimental Psychology: Human Perception and Performance, 4, 565–572.
Loschky, L. C., & McConkie, G. W. (2002). Investigating spatial vision and dynamic attentional selection using a gaze-contingent multiresolutional display. Journal of Experimental Psychology: Applied, 8, 99–117.
Loschky, L. C., McConkie, G. W., Yang, J., & Miller, M. E. (2005). The limits of visual resolution in natural scene viewing. Visual Cognition, 12, 1057 – 1092.
Mackworth, N. H., & Morandi, A. J. (1967). The gaze selects informative details within pictures. Perception & Psychophysics, 2, 547–552.
Mannan, S. K., Ruddock, K. H., & Wooding, D. S. (1995). Automatic control of saccadic eye move- ments made in visual inspection of briefly presented 2-D images. Spatial Vision, 9, 363–386.
1496 THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8)
Mannan, S. K., Ruddock, K. H., & Wooding, D. S. (1996). The relationship between the locations of spatial features and those of fixation made during the visual examination of briefly presented images. Spatial Vision, 10, 165–188.
Matin, E. (1974). Saccadic suppression: A review and an analysis. Psychological Bulletin, 81, 899–917.
McConkie, G. W., & Currie, C. B. (1996). Visual stability across saccades while viewing complex pictures. Journal of Experimental Psychology: Human Perception and Performance, 22, 563–581.
McConkie, G. W., Kerr, P. W., Reddix, M. D., & Zola, D. (1988). Eye movement control during reading: I. The location of initial fixations in words. Vision Research, 28, 1107–1118.
McConkie, G. W., Kerr, P. W., Reddix, M. D., Zola, D., & Jacobs, A. M. (1989). Eye movement control in reading: II. Frequency of refixating a word. Perception & Psychophysics, 46, 245–253.
McConkie, G. W., & Rayner, K. (1975). The span of the effective stimulus during a fixation in reading. Perception & Psychophysics, 17, 578–586.
McConkie, G. W., & Rayner, K. (1976a). Asymmetry of the perceptual span in reading. Bulletin of the Psychonomic Society, 8, 365–368.
McConkie, G. W., & Rayner, K. (1976b). Identifying the effective stimulus during a fixation in reading: Literature review and theories of reading. In H. Singer & R. N. Ruddell (Eds.), Theoretical models and processes of reading (2nd ed.). Newark, DE: International Reading Association.
McConkie, G. W., & Zola, D. (1979). Is visual information integrated across successive fix- ations in reading? Perception & Psychophysics, 25, 221 – 224.
McDonald, S. A. (2005). Parafoveal preview benefit in reading is not cumulative across multiple saccades. Vision Research, 45, 1829–1834.
McDonald, S. A. (2006a). Effects of number-of-letters on eye movements during reading are independent from effects of spatial word length. Visual Cognition, 13, 89–98.
McDonald, S. A. (2006b). Parafoveal preview benefit in reading is only obtained from the saccade goal. Vision Research, 46, 4416–4424.
McDonald, S. A., Carpenter, R. H. S., & Shillcock, R. C. (2005). An anatomically-constrained, stochastic model of eye movement control in reading. Psychological Review, 112, 814–840.
McDonald, S. A., & Shillcock, R. C. (2003a). Eye movements reveal the on-line computation of
lexical probabilities during reading. Psychological
Science, 14, 648–652.
McDonald, S. A., & Shillcock, R. C. (2003b). Low-
level predictive inference in reading: The influence of transitional probabilities on eye movements. Vision Research, 43, 1735–1751.
McDonald, S. A., & Shillcock, R. C. (2004). The potential contribution of preplanned refixations to the preferred viewing location. Perception & Psychophysics, 66, 1033–1045.
Melcher, D., & Kowler, E. (2001). Visual scene memory and the guidance of saccadic eye movements. Vision Research, 41, 3597–3611.
Meseguer, E., Carreiras, M., & Clifton, C. (2002). Overt reanalysis strategies and eye movements during the reading of garden path sentences. Memory & Cognition, 30, 551–561.
Miellet, S., O’Donnell, P. J., & Sereno, S. C. (in press). Parafoveal magnification: Visual acuity does not modulate the perceptual span in reading. Psychological Science.
Miellet, S., & Sparrow, L. (2004). Phonological codes are assembled before word fixation: Evidence from boundary paradigm in sentence reading. Brain and Language, 90, 299–310.
Miellet, S., Sparrow, L., & Sereno, S. C. (2007). Word frequency and predictability effects in reading French: An evaluation of the E-Z Reader model. Psychonomic Bulletin & Review, 14, 762–769.
Miller, B., Juhasz, B. J., & Rayner, K. (2006). The ortho- graphic uniqueness point and eye movements in reading. British Journal of Psychology, 97, 191–216.
Mitchell, D. C., Shen, X., Green, M. J., & Hodgson, T. L. (2008). Accounting for regressive eye- movements in models of sentence processing: A reappraisal of the selective reanalysis hypothesis. Journal of Memory and Language, 59, 266–293.
Morris, R. K. (1994). Lexical and message-level sentence context effects on fixation times in reading. Journal of Experimental Psychology: Learning, Memory, and Cognition, 20, 92–103.
Morris, R. K., Rayner, K., & Pollatsek, A. (1990). Eye movement guidance in reading: The role of parafoveal letter and space information. Journal of Experimental Psychology: Human Perception and Performance, 16, 268–281.
Morrison, R. E. (1984). Manipulation of stimulus onset delay in reading: Evidence for parallel pro- gramming of saccades. Journal of Experimental Psychology: Human Perception and Performance, 10, 667 – 682.
THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8) 1497
EYE MOVEMENTS AND ATTENTION
RAYNER
Morrison, R. E., & Rayner, K. (1981). Saccade size in reading depends upon character spaces and not visual angle. Perception & Psychophysics, 30, 395 – 396.
Motter, B. C., & Belky, E. J. (1998a). The guidance of eye movements during active visual search. Vision Research, 38, 1805–1815.
Motter, B. C., & Belky, E. J. (1998b). The zone of focal attention during active visual search. Vision Research, 38, 1007–1022.
Murray, W. S., & Kennedy, A. (1988). Spatial coding in the processing of anaphor by good and poor readers. Quarterly Journal of Experimental Psychology, 40A, 693 – 718.
Najemnik, J., & Geisler, W. S. (2005). Optimal eye movement strategies in visual search. Nature, 434, 387 – 391.
Nakatani, C., & Pollatsek, A. (2004). An eye movement analysis of “mental rotation” of simple scenes. Perception & Psychophysics, 66, 1227–1245.
O’Brien, E. J., Shank, D. M., Myers, J. L., & Rayner, K. (1988). Elaborative inferences during reading: Do they occur on-line? Journal of Experimental Psychology: Learning, Memory, and Cognition, 14, 410–420.
O’Regan, J. K. (1979). Eye guidance in reading: Evidence for linguistic control hypothesis. Perception and Psychophysics, 25, 501–509.
O’Regan, J. K. (1980). The control of saccade size and fixation duration in reading: The limits of linguistic control. Perception & Psychophysics, 28, 112 – 117.
O’Regan, J. K. (1990). Eye movements and reading. In E. Kowler (Ed.), Eye movements and their role in visual and cognitive processes (pp. 395 – 453). Amsterdam: Elsevier.
O’Regan, J. K., & Jacobs, A. M. (1992). The optimal viewing position effect in word recognition: A challenge to current theory. Journal of Experimental Psychology: Human Perception and Performance, 18, 185 – 197.
O’Regan, J. K., & Le ́vy-Schoen, A. (1983). Integrating visual information from successive fixations: Does trans-saccadic fusion exist? Vision Research, 23, 765 – 768.
O’Regan, J. K., Le ́vy-Schoen, A., Pynte, J., & Brugaille`re, B. (1984). Convenient fixation location within isolated words of different length and structure. Journal of Experimental Psychology: Human Perception and Performance, 10, 250–257.
O’Regan, J. K., Rensink, R. A., & Clark, J. J. (1999). Change blindness as a result of “mudsplashes”. Nature, 398, 34.
Parker, R. E. (1978). Picture processing during recog- nition. Journal of Experimental Psychology: Human Perception and Performance, 4, 284–293.
Parkhurst, D., Law, K., & Niebur, E. (2002). Modeling the role of salience in the allocation of overt visual attention. Vision Research, 42, 107–123.
Parkhurst, D., & Niebur, E. (2003). Scene context selected by active vision. Spatial Vision, 16, 125 – 154. Perea, M., & Pollatsek, A. (1998). The effects of neighborhood frequency in reading and lexical decision. Journal of Experimental Psychology: Human
Perception and Performance, 24, 767–779.
Peterson, M. S., Kramer, A. F., Wang, R. F., Irwin, D. E., & McCarley, J. S. (2001). Visual search has
memory. Psychological Science, 12, 287–292.
Pieters, R., Wedel, M., & Liechty, J. (2008). The influence of goals on the time course of eye move- ments across advertisements. Journal of Experimental
Neider, M. B., & Zelinsky, G. J. guides eye movements during 46, 614–621.
Neider, M. B., & Zelinsky, G. camouflaged targets: Effects similarity on visual search. 2217 – 2235.
Nelson, W. W., & Loftus, G. R.
visual field during picture
Experimental Psychology: Human Learning and Memory, 6, 391–399.
Nisbett, R. E. (2003). The geography of thought: How Asians and Westerners think differently. . .and why. New York: The Free Press.
Nodine, C. E., Carmody, D. P., & Herman, E. (1979). Eye movements during visual search for artistically embedded targets. Bulletin of the Psychonomic Society, 13, 371–374.
Nummenmaa, L., Hyo ̈na ̈, J., & Calvo, M. G. (2006). Eye movement assessment of selective attentional capture by emotional pictures. Emotion, 6, 257 – 268.
Nuthmann, A., Engbert, R., & Kliegl, R. (2005). Mislocated fixations during reading and the inverted optimal viewing position effect. Vision Research, 45, 2201 – 2217.
Nuthmann, A., Engbert, R., & Kliegl, R. (2007). The IOVP effect in mindless reading: Experiment and modeling. Vision Research, 47, 990–1002.
O’Brien, E., Raney, G. E., Albrecht, J. E., & Rayner, K. (1997). Processes involved in the resolution of explicit anaphors. Discourse Processes, 23, 1–24.
(2006a). Scene context search. Vision Research,
J. (2006b). Search for of target-background Vision Research, 46,
(1980). The functional viewing. Journal of
Psychology: Applied, 14, 129–138. 1498 THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8)
Pollatsek, A., Bolozky, S., Well, A. D., & Rayner, K. (1981). Asymmetries in the perceptual span for Israeli readers. Brain and Language, 14, 174–180.
Pollatsek, A., & Hyo ̈na ̈, J. (2005). The role of semantic transparency in the processing of Finnish compound words. Language and Cognitive Processes, 20, 261 – 290.
Pollatsek, A., Hyo ̈na ̈, J., & Bertram, R. (2000a). The role of morphological constituents in reading Finnish compound words. Journal of Experimental Psychology: Human Perception and Performance, 26, 820 – 833.
Pollatsek, A., Juhasz, B. J., Reichle, E. D., Machacek, D., & Rayner, K. (2008). Immediate and delayed effects of word frequency and word length on eye movements in reading: A reversed delayed effect of word length. Journal of Experimental Psychology: Human Perception and Performance, 34, 726–750.
Pollatsek, A., Lesch, M., Morris, R. K., & Rayner, K. (1992). Phonological codes are used in integrating information across saccades in word identification and reading. Journal of Experimental Psychology: Human Perception and Performance, 18, 148–162.
Pollatsek, A., Narayanaan, V., Pradhan, A., & Fisher, D. L. (2006a). The use of eye movements to evaluate the effect of PC-based risk awareness training on an advanced driving simulator. Human Factors, 48, 447 – 464.
Pollatsek, A., Perea, M., & Binder, K. S. (1999). The effects of “neighborhood size” in reading and lexical decision. Journal of Experimental Psychology: Human Perception and Performance, 25, 1142–1158.
Pollatsek, A., Raney, G. E., LaGasse, L., & Rayner, K. (1993). The use of information below fixation in reading and in visual search. Canadian Journal of Experimental Psychology, 47, 179–200.
Pollatsek, A., & Rayner, K. (1982). Eye movement control during reading: The role of word boundaries. Journal of Experimental Psychology: Human Perception and Performance, 8, 817–833.
Pollatsek, A., Rayner, K., & Balota, D. A. (1986). Inferences about eye movement control from the perceptual span in reading. Perception & Psychophysics, 40, 123–130.
Pollatsek, A., Rayner, K., & Collins, W. E. (1984). Integrating pictorial information across eye move- ments. Journal of Experimental Psychology: General, 113, 426–442.
Pollatsek, A., Rayner, K., & Henderson, J. H. (1990). Role of spatial location in integration of pictorial information across saccades. Journal of Experimental
Psychology: Human Perception and Performance, 16,
199 – 210.
Pollatsek, A., Reichle, E. D., & Rayner, K. (2006b).
Attention to one word at a time is still a viable hypothesis: Rejoinder to Inhoff, Radach, and Eiter. Journal of Experimental Psychology: Human Perception and Performance, 32, 1496–1500.
Pollatsek, A., Reichle, E. D., & Rayner, K. (2006c). Serial processing is consistent with the time course of linguistic information extraction from consecutive words during eye fixations in reading: A response to Inhoff, Eiter, and Radach (2005). Journal of Experimental Psychology: Human Perception and Performance, 32, 1485–1489.
Pollatsek, A., Reichle, E. D., & Rayner, K. (2006d). Tests of the E-Z Reader model: Exploring the interface between cognition and eye-movement control. Cognitive Psychology, 52, 1–56.
Pollatsek, A., Tan, L. H., & Rayner, K. (2000b). The role of phonological codes in integrating infor- mation across saccadic eye movements in Chinese character identification. Journal of Experimental Psychology: Human Perception and Performance, 26, 607 – 633.
Pomplun, M., Reingold, E. M., & Shen, J. (2001). Investigating the visual span in comparative search: The effects of task difficulty and divided attention. Cognition, 81, B57–B67.
Pomplun, M., Reingold, E. M., & Shen, J. (2003). Area activation: A computational model of saccade selectivity in visual search. Cognitive Science, 27, 299 – 312.
Posner, M. I. (1980). Orienting of attention. Quarterly Journal of Experimental Psychology, 32, 3–25.
Pynte, J., & Kennedy, A. (2006). An influence over eye movements in reading exerted from beyond the level of the word: Evidence from English and French. Vision Research, 46, 3786–3801.
Pynte, J., Kennedy, A., & Ducrot, S. (2004). The influ- ence of parafoveal typographical errors on eye move- ments in reading. European Journal of Cognitive Psychology, 16, 178–202.
Radach, R. (1996). Blickbewegungen beim Lesen: Psychologische Aspekte der Determination von Fixationspositionen [Eye movements in reading: Psychological factors that determine fixation locations]. Mu ̈nser, Germany: Waxman.
Raney, G. E., & Rayner, K. (1995). Word frequency effects and eye movements during two readings of a text. Canadian Journal of Experimental Psychology, 49, 151–172.
THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8) 1499
EYE MOVEMENTS AND ATTENTION
RAYNER
Rayner, K. (1975). The perceptual span and peripheral cues during reading. Cognitive Psychology, 7, 65 – 81. Rayner, K. (1978a). Eye movement latencies for parafo- veally presented words. Bulletin of the Psychonomic
Society, 11, 13–16.
Rayner, K. (1978b). Eye movements in reading and
information processing. Psychological Bulletin, 85,
618 – 660.
Rayner, K. (1979). Eye guidance in reading: Fixation
locations in words. Perception, 8, 21–30.
Rayner, K. (1986). Eye movements and the perceptual span in beginning and skilled readers. Journal of
Experimental Child Psychology, 41, 211–236. Rayner, K. (1995). Eye movements and cognitive pro- cesses in reading, visual search, and scene perception. In J. M. Findlay, R. Walker, & R. W. Kentridge (Eds.), Eye movement research: Mechanisms, processes and applications (pp. 3 – 22). Amsterdam: North
Holland.
Rayner, K. (1998). Eye movements in reading and
information processing: 20 years of research.
Psychological Bulletin, 124, 372–422.
Rayner, K., Ashby, J., Pollatsek, A., & Reichle, E. D.
(2004a). The effects of frequency and predictabil- ity on eye fixations in reading: Implications for the E-Z Reader model. Journal of Experimental Psychology: Human Perception and Performance, 30, 720 – 732.
Rayner, K., Balota, D. A., & Pollatsek, A. (1986). Against parafoveal semantic preprocessing during eye fixations in reading. Canadian Journal of Psychology, 40, 473–483.
Rayner, K., & Bertera, J. H. (1979). Reading without a fovea. Science, 206, 468–469.
Rayner, K., Binder, K. S., Ashby, J., & Pollatsek, A. (2001a). Eye movement control in reading: Word predictability has little influence on initial landing positions in words. Vision Research, 41, 943–954.
Rayner, K., Carlson, M., & Frazier, L. (1983). The interaction of syntax and semantics during sentence processing: Eye movements in the analysis of seman- tically biased sentences. Journal of Verbal Learning and Verbal Behavior, 22, 358–374.
Rayner, K., Castelhano, M. S., & Yang, J. (2009a). Eye movements when looking at unusual/weird scenes. Are there cultural differences? Journal of Experimental Psychology: Learning, Memory, and Cognition, 35, 254–259.
Rayner, K., Castelhano, M. S., & Yang, J. (in press). Eye movements and the perceptual span in older and younger readers. Psychology and Aging.
Rayner, K., Chace, K. H., Slattery, T. J., & Ashby, J. (2006a). Eye movements as reflections of compre- hension processes in reading. Scientific Studies of Reading, 10, 241–255.
Rayner, K., Cook, A. E., Juhasz, B. J., & Frazier, L. (2006b). Immediate disambiguation of lexically ambiguous words during reading: Evidence from eye movements. British Journal of Psychology, 97, 467 – 482.
Rayner, K., & Duffy, S. A. (1986). Lexical complexity and fixation times in reading: Effects of word frequency, verb complexity, and lexical ambiguity. Memory & Cognition, 14, 191–201.
Rayner, K., & Fischer, M. H. (1996). Mindless reading revisited: Eye movements during reading and scanning are different. Perception & Psychophysics, 58, 734–747.
Rayner, K., Fischer, M. H., & Pollatsek, A. (1998a). Unspaced text interferes with both word identifi- cation and eye movement control. Vision Research, 38, 1129–1144.
Rayner, K., & Fisher, D. L. (1987a). Eye movements and the perceptual span during visual search. In J. K. O’Regan & A. Levy-Schoen (Eds.), Eye movements: From physiology to cognition. New York: Elsevier Science Publishers.
Rayner, K., & Fisher, D. L. (1987b). Letter processing during eye fixations in visual search. Perception & Psychophysics, 42, 87–100.
Rayner, K., & Frazier, L. (1987). Parsing temporarily ambiguous complements. Quarterly Journal of Experimental Psychology, 39A, 657–678.
Rayner, K., & Frazier, L. (1989). Selection mechanisms in reading lexically ambiguous words. Journal of Experimental Psychology: Learning, Memory, and Cognition, 15, 779–790.
Rayner, K., Garrod, S., & Perfetti, C. A. (1992). Discourse influences during parsing are delayed. Cognition, 45, 109–139.
Rayner, K., Inhoff, A. W., Morrison, R. E., Slowiaczek, M. L., & Bertera, J. H. (1981). Masking of foveal and parafoveal vision during eye fixations in reading. Journal of Experimental Psychology: Human Perception and Performance, 7, 167–179.
Rayner, K., & Johnson, R. L. (2005). Letter-by-letter acquired dyslexia is due to the serial encoding of letters. Psychological Science, 16, 530–534.
Rayner, K., & Juhasz, B. J. (2004). Eye movements in reading: Old questions and new directions. European Journal of Cognitive Psychology, 16, 340–352.
1500 THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8)
Rayner, K., Juhasz, B. J., Ashby, J., & Clifton, C. (2003a). Inhibition of saccade return in reading. Vision Research, 43, 1027–1034.
Rayner, K., Juhasz, B. J., & Brown, S. J. (2007a). Do readers obtain preview benefit from word n þ 2? A test of serial attention shift versus distributed lexical processing models of eye movement control in reading. Journal of Experimental Psychology: Human Perception and Performance, 33, 230–245.
Rayner, K., Kambe, G., & Duffy, S. A. (2000). Clause wrap-up effects on eye movements during reading. Quarterly Journal of Experimental Psychology, 53, 1061 – 1080.
Rayner, K., Li, X., Juhasz, B. J., & Yan, G. (2005). The effect of word predictability on the eye movements of Chinese readers. Psychonomic Bulletin & Review, 12, 1089 – 1093.
Rayner, K., Li, X., & Pollatsek, A. (2007b). Extending the E-Z Reader model of eye movement control to Chinese readers. Cognitive Science, 31, 1021 – 1034.
Rayner, K., Li, X., Williams, C. C., Cave, K. R., & Well, A. D. (2007c). Eye movements during infor- mation processing tasks: Individual differences and cultural effects. Vision Research, 47, 2714–2726.
Rayner, K., Liversedge, S. P., & White, S. J. (2006c). Eye movements when reading disappearing text: The importance of the word to the right of fixation. Vision Research, 46, 310–323.
Rayner, K., Liversedge, S. P., White, S. J., & Vergilino- Perez, D. (2003b). Reading disappearing text: Cognitive control of eye movements. Psychological Science, 14, 385–389.
Rayner, K., & McConkie, G. W. (1976). What guides a reader’s eye movements? Vision Research, 16, 829 – 837.
Rayner, K., McConkie, G. W., & Ehrlich, S. E. (1978). Eye movements and integrating information across fixations. Journal of Experimental Psychology: Human Perception and Performance, 4, 529–544.
Rayner, K., McConkie, G. W., & Zola, D. (1980a). Integrating information across eye movements. Cognitive Psychology, 12, 206–226.
Rayner, K., Miller, B., & Rotello, C. M. (2008). Eye movements when looking at print advertisements: The goal of the viewer matters. Applied Cognitive Psychology, 22, 697–707.
Rayner, K., & Morris, R. K. (1992). Eye movement control in reading: Evidence against semantic preprocessing. Journal of Experimental Psychology: Human Perception and Performance, 18, 163–172.
Rayner, K., & Morrison, R. E. (1981). Eye movements and identifying words in parafoveal vision. Bulletin of the Psychonomic Society, 17, 135–138.
Rayner, K., Murphy, L., Henderson, J. M., & Pollatsek, A. (1989). Selective attentional dyslexia. Cognitive Neuropsychology, 6, 357–378.
Rayner, K., & Pollatsek, A. (1981). Eye movement control during reading: Evidence for direct control. Quarterly Journal of Experimental Psychology, 33A, 351 – 373.
Rayner, K., & Pollatsek, A. (1983). Is visual informa- tion integrated across saccades? Perception & Psychophysics, 34, 39–48.
Rayner, K., & Pollatsek, A. (1992). Eye movements and scene perception. Canadian Journal of Psychology, 46, 342 – 376.
Rayner, K., & Pollatsek, A. (1996). Reading unspaced text is not easy: Comments on the implications of Epelboim et al.’s study for models of eye movement control in reading. Vision Research, 36, 461 – 465.
Rayner, K., & Pollatsek, A. (1997). Eye movements, the eye-hand span, and the perceptual span during sightreading music. Current Directions in Psychological Science, 6, 49–53.
Rayner, K., Pollatsek, A., & Binder, K. S. (1998b). Phonological codes and eye movements in reading. Journal of Experimental Psychology: Learning, Memory, and Cognition, 24, 476–497.
Rayner, K., Pollatsek, A., Drieghe, D., Slattery, T. J., & Reichle, E. D. (2007d). Tracking the mind during reading via eye movements: Comments on Kliegl, Nuthmann, and Engbert (2006). Journal of Experimental Psychology: General, 136, 520–529.
Rayner, K., Pollatsek, A., & Reichle, E. D. (2003c). Eye movements in reading: Models and data. Behavioral and Brain Sciences, 26, 507–526.
Rayner, K., & Raney, G. E. (1996). Eye movement control in reading and visual search: Effects of word frequency. Psychonomic Bulletin & Review, 3, 245 – 248.
Rayner, K., Reichle, E. D., & Pollatsek, A. (1998c). Eye movement control in reading: An overview and a model. In G. Underwood (Ed), Eye guidance in reading and scene perception (pp. 243–268). Oxford, UK: Elsevier.
Rayner, K., Reichle, E. D., Stroud, M. J., Williams, C. C., & Pollatsek, A. (2006d). The effect of word frequency, word predictability, and font difficulty on the eye movements of young and older readers. Psychology and Aging, 21, 448–465.
THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8) 1501
EYE MOVEMENTS AND ATTENTION
RAYNER
Rayner, K., Rotello, C. M., Stewart, A., Keir, J., & Duffy, S. A. (2001b). Integrating text and pictorial information: Eye movements when looking at print advertisements. Journal of Experimental Psychology: Applied, 7, 219–226.
Rayner, K., & Sereno, S. C. (1994). Regressive eye movements and sentence parsing: On the use of regression contingent analyses. Memory & Cognition, 22, 281–285.
Rayner, K., Sereno, S. C., Morris, R. K., Schmauder, A. R., & Clifton, C. (1989). Eye movements and on-line comprehension processes [Special Issue]. Language and Cognitive Processes, 4, 21–49.
Rayner, K., Sereno, S. C., & Raney, G. E. (1996). Eye movement control in reading: comparison of two types of models. Journal of Experimental Psychology: Human Perception and Performance, 22, 1188 – 1200.
Rayner, K., Slowiaczek, M. L., Clifton, C., & Bertera, J. H. (1983). Latency of sequential eye movements: Implications for reading. Journal of Experimental Psychology: Human Perception and Performance, 9, 912 – 922.
Rayner, K., Smith, T. J., Malcolm, G., & Henderson, J. M. (2009b). Eye movements and encoding during scene perception. Psychological Science, 20, 6–10.
Rayner, K., Warren, T., Juhasz, B. J., & Liversedge, S. P. (2004b). The effect of plausibility on eye movements in reading. Journal of Experimental Psychology: Learning, Memory, and Cognition, 30, 1290 – 1301.
Rayner, K., & Well, A. D. (1996). Effects of contextual constraint on eye movements in reading: A further examination. Psychonomic Bulletin & Review, 3, 504 – 509.
Rayner, K., Well, A. D., & Pollatsek, A. (1980b). Asymmetry of the effective visual field in reading. Perception & Psychophysics, 27, 537–544.
Rayner, K., Well, A. D., Pollatsek, A., & Bertera, J. H. (1982). The availability of useful information to the right of fixation in reading. Perception & Psychophysics, 31, 537–550.
Rayner, K., White, S. J., Johnson, R. L., & Liversedge, S. P. (2006e). Raeding wrods with jubmled lettres: There is a cost. Psychological Science, 17, 192–193.
Rayner, K., White, S. J., Kambe, G., Miller, B., & Liversedge, S. P. (2003d). On the processing of meaning from parafoveal vision during eye fixation in reading. In J. Hyo ̈na ̈, R. Radach, & H. Deubel (Eds.), The mind’s eye: Cognitive and applied aspects
of eye movements (pp. 213 – 234). Amsterdam:
Elsevier Science.
Recarte, M. A., & Nunes, L. M. (2000). Effects of
verbal and spatial-imagery tasks on eye fixations while driving. Journal of Experimental Psychology: Applied, 6, 31–43.
Recarte, M. A., & Nunes, L. M. (2003). Mental work- load while driving: Effects on visual search, discrimi- nation, and decision making. Journal of Experimental Psychology: Applied, 9, 119–137.
Rehder, B., & Hoffman, A. B. (2005). Eyetracking and selective attention in category learning. Cognitive Psychology, 51, 1–41.
Reichle, E. D. (Ed.). (2006). Models of eye-movement control in reading [special issue]. Cognitive Systems Research, 7, 1–96.
Reichle, E. D., & Laurent, P. A. (2006). Using reinforcement learning to understand the emergence of “intelligent” eye-movement behavior during reading. Psychological Review, 113, 390–408.
Reichle, E. D., Pollatsek, A., Fisher, D. L., & Rayner, K. (1998). Toward a model of eye movement control in reading. Psychological Review, 105, 125 – 157.
Reichle, E. D., Pollatsek, A., & Rayner, K. (2006). E-Z Reader: A cognitive-control, serial-attention model of eye-movement behavior during reading. Cognitive Systems Research, 7, 4–22.
Reichle, E. D., Rayner, K., & Pollatsek, A. (1999). Eye movement control in reading: Accounting for initial fixation locations and refixations within the E-Z Reader model. Vision Research, 39, 4403 – 4411.
Reichle, E. D., Rayner, K., & Pollatsek, A. (2003). The E-Z Reader model of eye-movement control in reading: Comparisons to other models. Behavioral and Brain Sciences, 26, 445–476.
Reichle, E. D., Warren, T., & McConnell, K. (2009). Using E-Z Reader to model the effects of higher- level language processing on eye movements during reading. Psychonomic Bulletin & Review, 16, 1–21.
Reilly, R., & Radach, R. (2006). Some empirical tests of an interactive activation model of eye movement control in reading. Cognitive Systems Research, 7, 34–55.
Reingold, E. M., Charness, N., Pomplun, M., & Stampe, D. M. (2001). Visual span in expert chess players: Evidence from eye movements. Psychological Science, 12, 48–55.
Reingold, E. M., & Loschky, L. C. (2002). Saliency of peripheral targets in gaze-contingent
1502 THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8)
multi-resolutional displays. Behavior Research
Methods, Instrumentation & Computers, 34, 491 – 499. Reingold, E. M., Loschky, L. C., McConkie, G. W., & Stampe, D. M. (2003). Gaze-contingent multi- resolutional displays: An integrative review. Human
Factors, 45, 307–328.
Reingold, E. M., & Rayner, K. (2006). Examining the
word identification stages hypothesized by the E-Z
Reader model. Psychological Science, 17, 742–746. Richardson, D. C., & Spivey, M. J. (2000). Representation, space and Hollywood squares: Looking at things that aren’t there anymore.
Cognition, 76, 269–295.
Rutishauser, U., & Koch, C. (2007). Probabilistic mod-
eling of eye movement data during conjunction search via feature-based attention. Journal of Vision, 7, 1–20.
Saida, S., & Ikeda, M. (1979). Useful field size for pattern perception. Perception & Psychophysics, 25, 119–125.
Sainio,M.,Hyo ̈na ̈,J.,Bingushi,K.,&Bertram,R. (2007). The role of interword spacing in reading Japanese: An eye movement study. Vision Research, 47, 2575–2584.
Salvucci, D. D. (2001). An integrated model of eye movements and visual encoding. Cognitive Systems Research, 1, 201–220.
Schilling, H. E. H., Rayner, K., & Chumbley, J. I. (1998). Comparing naming, lexical decision, and eye fixation times: Word frequency effects and individual differences. Memory & Cognition, 26, 1270 – 1281.
Schnitzer, B., & Kowler, E. (2006). Eye movements during multiple readings of the same text. Vision Research, 46, 1611–1636.
Schroyens, W., Vitu, F., Brysbaert, M., & d’Ydewalle, G. (1999). Eye movement control during reading: Foveal load and parafoveal processing. Quarterly Journal of Experimental Psychology, 52A, 1021 – 1046.
Schustack, M. W., Ehrlich, S. F., & Rayner, K. (1987). Local and global sources of contextual facilitation in reading. Journal of Memory and Language, 26, 322 – 340.
Sereno, S. C., O’Donnell, P. J., & Rayner, K. (2006). Eye movements and lexical ambiguity resolution: Investigating the subordinate-bias effect. Journal of Experimental Psychology: Human Perception and Performance, 32, 335–350.
Sereno, S. C., & Rayner, K. (2000). Spelling – sound regularity effects on eye fixations in reading. Perception & Psychophysics, 62, 402–409.
Sereno, S. C., & Rayner, K. (2003). Measuring word recognition in reading: Eye movements and event- related potentials. Trends in Cognitive Sciences, 7, 489 – 493.
Shepherd, M., Findlay, J. M., & Hockey, R. J. (1986). The relationship between eye movements and spatial attention. Quarterly Journal of Experimental Psychology, 38A, 475–491.
Slattery, T. J., Pollatsek, A., & Rayner, K. (2006). The time course of phonological and orthographic processing of acronyms in reading: Evidence from eye movements. Psychonomic Bulletin & Review, 13, 412 – 417.
Slattery, T. J., Pollatsek, A., & Rayner, K. (2007). The effect of the frequencies of three consecutive content words on eye movements during reading. Memory & Cognition, 35, 1283–1292.
Slattery, T. J., & Rayner, K. (2009). The influence of text legibility on eye movements during reading. Manuscript submitted for publication.
Sparrow, L., & Miellet, S. (2002). Activation of phono- logical codes during reading: Evidence from errors detection and eye movements. Brain and Language, 81, 509–516.
Spivey, M. J., Tanenhaus, M. K., Eberhard, K. M., & Sedivy, J. C. (2002). Eye movements and spoken language comprehension: Effects of visual context on syntactic ambiguity resolution. Cognitive Psychology, 45, 447–481.
Spragins, A. B., Lefton, L. A., & Fisher, D. F. (1976). Eye movements while reading spatially transformed text: A developmental study. Memory & Cognition, 4, 36–42.
Starr, M. S., & Inhoff, A. W. (2004). Attention allocation to the right and left of a fixated word: Use of orthographic information from multiple words during reading. European Journal of Cognitive Psychology, 16, 203–225.
Starr, M. S., & Rayner, K. (2001). Eye movements during reading: Some current controversies. Trends in Cognitive Sciences, 5, 156–163.
Staub, A., & Rayner, K. (2007). Eye movements and on-line comprehension processes. In G. Gaskell (Ed.), The Oxford handbook of psycholinguistics (pp. 327–342). Oxford, UK: Oxford University Press.
Staub, A., Rayner, K., Pollatsek, A., Hyo ̈na ̈, J., & Majewski, H. (2007). The time course of plausibility effects on eye movements during reading: Evidence from noun – noun compounds. Journal of Experimental Psychology: Learning, Memory, and Cognition, 33, 1162–1169.
THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8) 1503
EYE MOVEMENTS AND ATTENTION
RAYNER
Sturt, P. (2003). The time-course of the application of binding constraints in reference resolution. Journal of Memory and Language, 48, 542 – 562.
Tanenhaus, M. K., & Spivey-Knowlton, M. J. (1996). Eye-tracking. Language and Cognitive Processes, 11, 583 – 588.
Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard, K. M., & Sedivy, J. C. (1995). Integration of visual and linguistic information during spoken language comprehension. Science, 268, 1632–1634.
Tatler, B. W., Baddeley, R. J., & Vincent, B. T. (2006). The long and the short of it: spatial statistics at fixation vary with saccade amplitude and task. Vision Research, 46, 1857–1862
Torralba, A., Oliva, A., Castelhano, M. S., & Henderson, J. M. (2006). Contextual guidance of eye movements and attention in real-world scenes: The role of global features in object search. Psychological Review, 113, 766–786.
Truitt, F. E., Clifton, C., Pollatsek, A., & Rayner, K. (1997). The perceptual span and the eye-hand span in sight reading music. Visual Cognition, 4, 143–161.
Trukenbrod, H. A., & Engbert, R. (2007). Oculomotor control in a sequential search task. Vision Research, 47, 2426–2443.
Tsai, J., Lee, C., Tzeng, O. J. L., Hung, D. L., & Yen, N. (2004). Use of phonological codes for Chinese characters: Evidence from processing of parafoveal preview when reading sentences. Brain and Language, 91, 235–244.
Underwood, G., Bloomfield, R., & Clews, S. (1988). Information influences the pattern of eye fixations during sentence comprehension. Perception, 17, 267 – 278.
Underwood, G., & Foulsham, T. (2006). Visual saliency and semantic incongruency influence eye movements when inspecting pictures. Quarterly Journal of Experimental Psychology, 59, 1931–1949.
Underwood, G., Foulsham, T., van Loon, E., Humphreys, L., & Bloyce, J. (2006). Eye movements during scene inspection: A test of the saliency map hypothesis. European Journal of Cognitive Psychology, 18, 321–342.
Underwood, G., Humphreys, L., & Cross, E. (2007). Congruency, saliency and gist in the inspection of objects in natural scenes. In R. P. G. van Gompel, M. H. Fischer, W. S. Murray, & R. L. Hill (Eds.), Eye movements: A window on mind and brain (pp. 563–580). Oxford, UK: Elsevier.
Underwood, G., Hyo ̈na ̈, J., & Niemi, P. (1987). Scanning patterns on individual words during the comprehension of sentences. In J. K. O’Regan & A. Levy-Schoen (Eds.), Eye movements: From physiology to cognition (pp. 467 – 477). Amsterdam: North-Holland.
Underwood, G., Templeman, E., Lamming, L., & Foulsham, T. (2008). Is attention necessary for object identification? Evidence from eye movements during the inspection of real-world scenes. Consciousness and Cognition, 17, 159–170.
Underwood, N. R., & McConkie, G. W. (1985). Perceptual span for letter distinctions during reading. Reading Research Quarterly, 20, 153–162.
Underwood, N. R., & Zola, D. (1986). The span of letter recognition of good and poor readers. Reading Research Quarterly, 21, 6–19.
Uttal, W. R., & Smith, P. (1968). Recognition of alpha- betic characters during voluntary eye movements. Perception & Psychophysics, 3, 257–264.
Vainio, S., Hyo ̈na ̈, J., & Pajunen, A. (2009). Lexical predictability exerts robust effects on fixation dur- ation, but not on initial landing position during reading. Experimental Psychology, 56, 66–74.
van Diepen, P. M. J., & d’Ydewalle, G. (2003). Early peripheral and foveal processing in fixations during scene perception. Visual Cognition, 10, 79 – 100.
van Diepen, P. M. J., Ruelens, L., & d’Ydewalle, G. (1999). Brief foveal masking during scene percep- tion. Acta Psychologica, 101, 91–103.
van Diepen, P. M. J., & Wampers, M. (1998). Scene exploration with Fourier-filtered peripheral infor- mation. Perception, 27, 1141–1151.
van Zoest, W., Donk, M., & Theeuwes, J. (2004). The role of bottom-up control in saccadic eye move- ments. Journal of Experimental Psychology: Human Perception and Performance, 30, 746–759.
van Zoest, W., Lleras, A., Kingstone, A., & Enns, J. T. (2007). In sight, out of mind: The role of eye move- ments in the rapid resumption of visual search. Perception & Psychophysics, 69, 1204–1217.
Vaughan, J. (1982). Control of fixation duration in visual search and memory search: Another look. Journal of Experimental Psychology: Human Perception and Performance, 8, 709–723.
Vergilino, D., & Beauvillain, C. (2000). The planning of refixation saccades in reading. Vision Research, 40, 3527–3538.
Vergilino, D., & Beauvillain, C. (2001). Reference frames in reading: Evidence from visually guided
1504 THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8)
and memory-guided saccades. Vision Research, 41,
3547 – 3557.
Vergilino-Perez, D., Collins, T., & Dore-Mazars, K.
(2004). Decision and metrics of refixations in reading
isolated words. Vision Research, 44, 2009–2017.
Vitu, F. (1991). The influence of parafoveal proces- sing and linguistic context on the optimal landing position effect. Perception & Psychophysics,
50, 58–75.
Vitu, F., Lancelin, D., & Marrier d’Unienville, V.
(2007). A perceptual-economy account for the inverted-optimal viewing position effect. Journal of Experimental Psychology: Human Perception and Performance, 33, 1220–1249.
Vitu, F., McConkie, G. W., Kerr, P., & O’Regan, J. K. (2001). Fixation location effects on fixation durations during reading: An inverted optimal viewing pos- ition effect. Vision Research, 41, 3513–3533.
Vitu, F., O’Regan, J. F., & Mittau, M. (1990). Optimal landing position in reading isolated words and continuous text. Perception & Psychophysics, 47, 583 – 600.
Vlaskamp, B. N. S., & Hooge, I. T. C. (2006). Crowding degrades visual search. Vision Research, 46, 417–425.
Warren, T., & McConnell, K. (2007). Investigating effects of selectional restriction violations and plausibility violation severity on eye-movements in reading. Psychonomic Bulletin & Review, 14, 770 – 775.
Warren, T., McConnell, K., & Rayner, K. (2008). Effects of context on eye movements when reading about possible and impossible events. Journal of Experimental Psychology: Learning, Memory, and Cognition, 34, 1001–1010.
Watson, D. G., & Inglis, M. (2007). Eye movements and time-based selection: Where do the eyes go in preview search? Psychonomic Bulletin & Review, 14, 852 – 857.
Weger, U. W., & Inhoff, A. W. (2006). Attention and eye movements in reading: Inhibition of return predicts the size of regressive saccades. Psychological Science, 17, 187–191.
Weger, U. W., & Inhoff, A. W. (2007). Long-range regressions to previously read words are guided by spatial and verbal memory. Memory & Cognition, 35, 1293–1306.
White, S. J. (2008). Eye movement control during reading: Effects of word frequency and orthographic familiarity. Journal of Experimental Psychology: Human Perception and Performance, 34, 205–223.
White, S. J., Johnson, R. L., Liversedge, S. P., & Rayner, K. (2008). Eye movements when reading transposed text: The importance of word beginning letters. Journal of Experimental Psychology: Human Perception and Performance, 34, 1261–1276.
White, S. J., & Liversedge, S. P. (2004). Orthographic familiarity influences initial eye positions in reading. European Journal of Cognitive Psychology, 16, 52–78.
White, S. J., & Liversedge, S. P. (2006a). Foveal processing difficulty does not modulate non-foveal orthographic influences on fixation positions. Vision Research, 46, 426–437.
White, S. J., & Liversedge, S. P. (2006b). Linguistic and nonlinguistic influences on the eyes’ landing positions during reading. Quarterly Journal of Experimental Psychology, 59, 760–782.
White, S. J., Rayner, K., & Liversedge, S. P. (2005a). Eye movements and the modulation of parafoveal processing by foveal processing difficulty: A reexa- mination. Psychonomic Bulletin & Review, 12, 891 – 896.
White, S. J., Rayner, K., & Liversedge, S. P. (2005b). The influence of parafoveal word length and contex- tual constraint on fixation durations and word skipping in reading. Psychonomic Bulletin & Review, 12, 466–471.
Williams, C. C., & Henderson, J. M. (2007). The face inversion effect is not a consequence of aberrant eye movements. Memory & Cognition, 35, 1977–1985.
Williams, C. C., Henderson, J. M., & Zacks, R. T. (2005). Incidental visual memory for targets and dis- tractors in visual search. Perception & Psychophysics, 67, 816–827.
Williams, C. C., Perea, M., Pollatsek, A., & Rayner, K. (2006). Previewing the neighborhood: The role of orthographic neighbors as parafoveal previews in reading. Journal of Experimental Psychology: Human Perception and Performance, 32, 1072–1082.
Williams, C. C., & Pollatsek, A. (2007). Searching for an O in an array of Cs: Eye movements track moment-to-moment processing in visual search. Perception & Psychophysics, 69, 372–381.
Williams, R. S., & Morris, R. K. (2004). Eye move- ments, word familiarity, and vocabulary acquisition. European Journal of Cognitive Psychology, 16, 312 – 339.
Wu, J., Slattery, T. J., Pollatsek, A., & Rayner, K. (2008). Word segmentation in Chinese reading. In K. Rayner, D. Shen, X. Bai, & G. Yan (Eds.), Cognitive and cultural influences on eye movements
THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8) 1505
EYE MOVEMENTS AND ATTENTION
RAYNER
(pp. 303 – 314). Tianjin, China: Tianjin People’s
Publishing House/Psychology Press.
Yan, G., Tian, H., Bai, X., & Rayner, K. (2006). The
effect of word and character frequency on the eye movements of Chinese readers. British Journal of Psychology, 97, 259–268.
Yang, J., Wang, S., Xu, Y., & Rayner, K. (in press). Do Chinese readers obtain preview benefit from word n þ 2? Evidence from eye movements. Journal of Experimental Psychology: Human Perception and Performance.
Yang, S. (2006). An oculomotor-based model of eye movements in reading: The competition/activation model. Cognitive Systems Research, 7, 56–69.
Yang, S., & McConkie, G. W. (2001) Eye movements during reading: A theory of saccade initiation times. Vision Research, 41, 3567–3585.
Yarbus, A. (1967). Eye movements and vision. New York: Plenum Press.
Yen, M.-H., Tsai, J.-L., Tzeng, O. J. I., & Hung, D. L. (2008). Eye movements and parafoveal word proces- sing in reading Chinese. Memory & Cognition, 36, 1033–1045.
Zelinsky, G. J. (1996). Using eye saccades to assess the selectivity of search movements. Vision Research, 36, 2177 – 2187.
Zelinsky, G. J. (2001). Eye movements during change detection: Implications for search constraints, memory locations, and scanning strategies. Perception & Psychophysics, 63, 209–225.
Zelinsky, G. J. (2008). A theory of eye movements during target acquisition. Psychological Review, 115, 787 – 835.
Zelinsky, G. J., & Loschky, L. C. (2005). Eye move- ments serialize memory for objects in scenes. Perception & Psychophysics, 67, 676–690.
Zelinsky, G., Rao, R. P. N., Hayhoe, M. M., & Ballard, D. H. (1997). Eye movements reveal the spatiotem- poral dynamics of visual search. Psychological Science, 8, 448–453.
Zelinsky, G. J., & Sheinberg, D. L. (1997). Eye move- ments during parallel-serial visual search. Journal of Experimental Psychology: Human Perception and Performance, 23, 244–262.
Zola, D. (1984). Redundancy and word perception during reading. Perception & Psychophysics, 36, 277–284.
1506 THE QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY, 2009, 62 (8)
 View publication stats
 User-Oriented Document Summarization through Vision-Based Eye-Tracking
Songhua Xu♮,♯,‡∗
♮: College of Computer
Science and Technology, Zhejiang University, Hangzhou, Zhejiang, 310027, P.R. China
ABSTRACT
We propose a new document summarization algorithm which is personalized. The key idea is to rely on the attention (read- ing) time of individual users spent on single words in a doc- ument as the essential clue. The prediction of user atten- tion over every word in a document is based on the user’s attention during his previous reads, which is acquired via a vision-based commodity eye-tracking mechanism. Once the user’s attentions over a small collection of words are known, our algorithm can predict the user’s attention over every word in the document through word semantics analy- sis. Our algorithm then summarizes the document according to user attention on every individual word in the document. With our algorithm, we have developed a document summa- rization prototype system. Experiment results produced by our algorithm are compared with the ones manually sum- marized by users as well as by commercial summarization software, which clearly demonstrates the advantages of our new algorithm for user-oriented document summarization.
General Terms
Algorithms, Design, Experimentation, Human Factors, Lan- guages, Measurement, Performance
Author Keywords
User-oriented document summarization, personalized discourse abstract, user attention, implicit user feedback, commodity eye-tracking
ACM Classification Keywords
H.1.2 User/Machine Systems: human factors, human infor- mation processing; H.3.1 Content Analysis and Indexing:
∗Contact him at A DOT B AT C DOT com in which A = “songhua”, B = “xu”, and C = “gmail”.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
IUI’09, February 8 - 11, 2009, Sanibel Island, Florida, USA. Copyright 2009 ACM 978-1-60558-331-0/09/02...$5.00.
Hao Jiang‡
♯: Department of Computer
Science,
Yale University, New Haven, Connecticut, 06520-8285, USA
Francis C.M. Lau‡
‡: Department of Computer
Science,
The University of Hong Kong, Pokfulam Road, Hong Kong, P.R. China
abstracting methods; H.3.3 Information Search and Retrieval: information filtering, relevance feedback; H.5.2 User Inter- faces: input devices and strategies, interaction styles; I.2.7 Natural Language Processing: discourse, text analysis; I.7.5 Document Capture: document analysis
INTRODUCTION
Document summarization has continuously been an active area of research in artificial intelligence, information retrieval and natural language processing for more than two decades [1]. Summarizations can help users to efficiently locate the desired information or quickly convey the key messages em- bedded in an article or a text corpus of any size. To the best of our knowledge, despite the importance of and the attention given to this area, studies on document summa- rization so far have rarely taken human factors into consid- eration. In this paper, we propose a personalized document summarization method which carefully considers the user’s interests. Based on the method, we have implemented a per- sonalized document summarization system. Our algorithm tracks a user’s attention times over individual words using a vision-based commodity eye-tracking mechanism. Then user attention time over any arbitrary word is predicted by a data mining process according to pairwise word semantic similarity.
As an independent thread of research, eye-tracking has at- tracted researchers in the fields of human-computer interac- tion, user modeling, computer graphics and interactive tech- niques. The main advantage of eye-tracking is that it is uninstrusive when acquiring user feedbacks. User feedbacks are needed for determining users’ preferences in order to build adaptive systems. To date, however, very few research projects in such fields as information retrieval and natural language processing have taken advantage of modern eye- tracking technologies. In this paper, we present how we use commodity eye-tracking to develop a personalized doc- ument summarization system. With eye-tracking, we can ac- quire user attention data over individual words from online materials that the user has seen or read. These data can then be used to predict the user’s attention on the contents of a document to be summarized. Based on this prediction at the sentence level, our algorithm produces a personalized sum-
 7
marization of the document which is optimized with respect to the user’s reading interests.
MAIN IDEA
For every sentence Si in a document, we denote the user Uj ’s attention on it as AT (Si , Uj ), which is equal to the time the user spends on reading the sentence. For two sentences Si1 and Si2 in the document, after they have both been read by the user, if we have AT(Si1,Uj) > AT(Si2,Uj), then it is reasonable to infer that the user is more interested in Si1 than Si2 .
Based on the above, to produce an optimal summarization for a document for a particular user, our algorithm would predict the attention of the user on individual sentences in the document. Given the prediction, we can then return an ordered list of sentences in descending order in terms of the predicted user attention. With a user tunable cutoff thresh- old, we can then obtain a personalized document summariza- tion result. Overall, this is very similar to the rating problem in e-commerce—i.e., given a user’s ratings on a number of items (sentences), how to predict his ratings on new items (sentences) which he has not rated.
Although similar to the classical rating scenario, our method
is more comprehensive: traditional rating of an item is “atomic”, meaning that the rating is applied to the item as a whole, but not its subcomponents or its different features separately;
in our scenario, by contrast, user attention is compositive— e.g., for a sentence, the user’s attention is the accumulated attention of reading the words within the sentence, and for
a paragraph, its attention is the sum of the attention on its every sentence. In fact, in reality, when human beings form their preferences, it is often a decision based on smaller deci- sions. For a typical rating system under today’s e-commerce settings, it is uncommon to ask users to specify a rating over many components or facets of a product. The users would find it to be too bothersome. In fact, getting users to feed- back on an entire document is already difficult; getting them
to feedback on subcomponents might simply be impossible. Given the uninstrusive nature of eye-tracking, we can over- come this barrier and obtain user evaluation on the subcom- ponents of the target document. In our algorithm design, we carefully make use of the compositive structure of user attention for inferring user preference. The convenience of vision-based commodity eye-tracking makes our approach easy-to-adopt on any scale.
The remainder of the paper is organized as follows. We first survey the most related work. We then explain how to ac- quire user attention time on a document. After that, we discuss how to infer user attention via content-based data mining. Given the user attention estimation, we then in- troduce our algorithm for personalized document summa- rization based on the predicted user attention. We present experiment results to demonstrate the effectiveness of our method. Finally we conclude the paper and point out some future work directions.
RELATED WORK
Document Content Summarization
Much research has been dedicated to automatic summariza- tion for both generic and domain-specific documents. The online article [1] at http://www.summarization.com/ gives a comprehensive list of research papers published on doc- ument summarization studies till October 2008, containing 758 papers.
Jones [23] suggests that a document summarization process usually consists of two steps: (1) source representation which is built from the source text, and (2) summary generation, which synthesizes a summarization based on the source rep- resentation. Alternatively, summary can be also obtained through analyzing the semantics of the source text. Gong and Liu [14] presented a generic document summarization method that employs latent semantic analysis to identify se- mantically important sentences to create document summaries. Others have applied machine learning algorithms. Lin [28] used a selection function for key information extraction from documents, and a machine-learning process to automatically learn an optimized function which combines several heuris- tics for document summarization. Yeh et al. [46] proposed
a trainable document summarizer which takes into account such features as text position, positive keywords, negative keywords, centrality, and resemblance of text to paper title. The weights to attach to these different features are obtained from a scoring function trained by genetic algorithms.
There also exist many public domain document summariza- tion tools. Two of the most widely used ones are the MEAD system [35] and the LexRank system [10]. The MEAD sys- tem is an open source multilingual summarization package which has been successfully used in a variety of summariza- tion applications, including summarization on mobile de- vices and web page summarization for use in search engines. The LexRank system is a stochastic graph-based document summarization system which computes relative importance of textual units via natural language processing. For ordi- nary users using the Microsoft Windows operation system, the “AutoSummarize” functionality provided in MS Word [33] can usually provide a satisfying solution.
Implicit User Feedback
Query history
Query history probably is the most widely used implicit user feedback at present. Google’s personalized search service (http://www.google.com/psearch) allows users to store their search history in their Google account which will be ana- lyzed for personalizing their future search. In general, there exist two classes of methods for providing personalized search based on query history: those based on the whole query his- tory of a user and those based on query history in a par- ticular search session. For the methods that use the whole query history, usually a user profile is generated to describe his search preference. For example, Liu et al. [30] con- structed user profiles using the whole search history through an adaptive Rocchio algorithm [20]. Speretta and Gauch [40] demonstrated that using user profiles can significantly improve search engine performance. The query history in
8
a query session is also often called query chain [36]. Query chain is used to automatically suggest or to complete a query question for a particular user based on the query history in the same search session [19]. It is also used to expand the current query based on the query chain history.
Click data
Click data is another type of implicit user feedback, which has been intensively utilized, e.g., [9, 21]. The basic idea is that when a user clicks on a document, the document is con- sidered to be of more interest to the user than other unclicked ones. There are many ways to infer user preference from click behaviors. For example, a simple approach would be when a user clicks on the i-th link in a ranked list of web- pages before clicking on any of the first i − 1 links, we can safely infer that the first i − 1 documents are no more im- portant than the i-th document. Among the sophisticated approaches, people have applied ranking SVM algorithm to find the best webpage rank according to a user click dataset [22].
Attention time
Attention time, also often referred to as display time or read- ing time, is a newly recognized type of implicit user feed- backs that is receiving increasing popularity even though the reliability of which for predicting user interest has yet to be confirmed. One side of the opinion is represented by ar- guments made by Kelly and Belkin [25, 24], claiming that there is no reliable relationship between the interestingness of a document and its display time; in their study the display time is measured as the average reading time by a group of users on articles of different topics in the Web. The other side of the opinion is like what is pointed out by Halabi et al. [18], which is that for a fixed user in a certain query session, attention time gives a strong indication of the user interest—the more time a user spends on reading a docu- ment, the more important the document is to him. We think these different conclusions are not contradicting as display time is calculated differently by the two groups.
In our prior work, we proposed using attention time as an effective clue for producing personalized webpage ranking [45] as well as personalized recommendations for documents, images and videos [44]. We obtained positive results in both studies. In this paper, we introduce a new user-oriented document summarization algorithm based on user attention time.
Other types of implicit user feedbacks
Other types of implicit user feedbacks include display time, scrolling, annotation, bookmarking and printing behaviors. Some researchers have started recently to combine multiple types of implicit feedbacks in order to obtain better inference of user interest [31]. Fox et al. [12] have made a comprehen- sive study and proposed a decision tree based method aug- mented by Bayesian modeling to infer user preference from a set of mixed types of implicit user feedbacks.
Eye-Tracking Strategy
An eye tracker accumulates a series of points at which the eyes are looking. These points then define the part of con- tents which the user is reading. In [37], the authors presented a software system that automates eye tracking data analysis for Web usability studies. Similar to this work is a recent eye-tracking study for highlighted text by Chi et al. [5]. They have shown evidences derived from the eye-tracking data that user foraging behaviors are influenced by different highlighting strategies, including no highlights for any parts of a document, only highlighting keywords, and other more sophisticated key sentence highlighting strategies. Their re- port suggests a compelling need for new interface design such that people can easily digest information in a document collection, especially those on the Web. Bulling et al. [3] analyzed the eye movements of people in transit in an ev- eryday environment using a wearable electro-oculographic (EOG) system; they achieved a decent rate for recognizing reading activity in daily-life scenarios through analyzing the captured eye-tracking data. The purpose of their research is to study whether different reading behaviors and levels of user attention on written texts can be automatically de- tected with the aid of eye-tracking in an unobtrusive man- ner. In another paper, Bulling et al. [4] described the poten- tial of using wearable EOG goggles for context-awareness and mobile HCI applications. Despite these fruitful research on eye-tracking, however, as of today, eye-tracking tech- niques have been rarely employed as a user feedback ap- proach for intelligently tuning adaptive systems in a user friendly way. For online reading and Web browsing appli- cations, such an approach is especially desired but has been largely overlooked by knowledge management and informa- tion retrieval researchers so far. The work introduced in this paper presents our preliminary attempt along this desired di- rection.
User-Oriented Search Engines
User-oriented search engines—one of the most popular types of personalized online services—as a relatively new track of research is drawing more and more attention these days, e.g., [34, 8]. All the user-oriented search engines so far rely on user feedbacks of various kinds, which can be broadly classi- fied into two categories—explicit and implicit; both of them aim at inferring user intentions or preferences for customiz- ing some search engine [39, 42, 43]. Because users gen- erally would be least interested in providing explicit feed- backs, the recent trend is to derive search preferences from implicit feedbacks [16, 17, 13]. The most popular implicit user feedbacks currently utilized in commercial search sys- tems are query history and click data which we have already discussed.
ACQUIRING USER ATTENTION VIA COMMODITY EYE- TRACKING
Obtaining Gaze Samples through Vision-Based Commod-
ity Eye Tracking
Eye tracking is the technology to measure either the gaze, i.e., the spot a user is looking at, or the motion of the human eyes (http://en.wikipedia.org/wiki/Eye tracking). In our work,
 9
we use eye-tracking to measure the attention time of a user over a document appearing on the screen through identify- ing the part of the screen area the user is looking at and for how long. However, commercial eye-tracking devices are very expensive. Some researchers therefore turned to ordi- nary web cameras as eye-tracking devices [26, 7, 38, 29, 41, 15]. We followed suit and have assembled an eye-tracking setup using a simple web camera (Logitech Quickcam Note- book Pro) and an existent eye-tracking algorithm borrowed from the Opengazer project [47]. Together with some vision techniques we created our custom eye-tracking component. This design of an eye tracking component is cost-effective and can be widely adopted on personal computers as many PCs these days are equipped with web cameras. The error of the detected gaze location on the screen by our commodity eye-tracking component is between 1–2 cm, depending on which area of the screen the user is looking at: the center of of the screen has tracking error of around 1 cm whereas the boundary of the screen (a 19” screen monitor) has tracking error of around 2 cm.
Anchoring Gaze Samples onto Individual Words
Through our commodity eye-tracking component, we obtain a number of fixation points on the screen, which indicate the detected gaze area of the user. For our summarization algorithm to work, we need to anchor these gaze samples onto individual words. Apparently, the more gaze samples a word receives, the more interesting the word is to the user. We now look at how to anchor gaze samples onto individual words.
We first introduce the term “snapshot of the document” to refer to the part of the document that is displayed on the screen at a certain moment. When the user changes the part of the document being displayed, we say a new snapshot is formed. This happens for example when the user resizes the displaying window or scrolls to a different part of the document. For each snapshot of the document, we assign the gaze samples onto the corresponding words in the docu- ment in a fractional manner. To carry this out, we introduce a Gaussian kernel in the assignment process. Assuming at a certain moment, the detected gaze central point is positioned at (x, y) on the screen space. For each word wi that is dis- played in the current document snapshot, we first compute the central displaying point of the word as the center of the bounding box of the word’s displaying region, which is de- noted as (xi,yi). Then the fraction of the gaze sample the word wi receives is:
word occurs multiple times in the document, we accumulate all the gaze samples assigned to these occurrences. Finally, the overall attention of a user over a word is the sum of the word’s attention across all the documents the user has read previously. During processing, we remove the stop words (http://en.wikipedia.org/wiki/Stop words) since they are not providing much meaning and thus should not really have at- tracted the user attention. Notice that for words in the doc- uments that are not displayed, their attention is unspecified rather than being assigned zero.
PREDICTION OF USER ATTENTION OVER A SENTENCE
Predicting User Attention for Words
Our attention time prediction for a word is based on the se- mantic similarity of two words. We assume if the seman- tics of two words are sufficiently similar semantically, then a user shall have more or less the same amount of interest to read either of them. We use Sim(wi,wj) to denote the semantic similarity between word wi and word wj, where Sim(wi,wj) ∈ [0,1]. Correct estimation on Sim(wi,wj) plays a critical role in our attention time prediction algo- rithm. Fortunately, there exists a large collection of algo- rithms on word semantic distance estimation. In our current system implementation, we calculate S im(wi , wj ) using the semantic similarity measuring algorithm proposed in [27].
We denote the user Uj ’s attention samples on the words w1 , ··· ,wn asAT(w1,Uj),···,AT(wn,Uj)respectively,which are acquired through our vision-based eye-tracker. For an ar- bitrary word w which is not among {w1, · · · , wn}, we cal- culate the similarity between w and every wi (i = 1, · · · , n). We then select k words which share the highest semantic similarity with w. In our current experiment, k is set as min(10, n), where n is the number of attention samples ac- quired via eye-tracking. Without loss of generality and for ease of notation, we assume they are the words wi (i = 1, · · · , k). Then we use the following equation to predict the attention time for w:
 (xi − x)2 AT(wi) = exp(− 2σx2
(yi − y)2
− 2σy2 ). (1)
ρ(wi, w) =
1 If Sim(wi, w) > 0.1; (3) 0 Otherwise.
AT(w,U )= j
i=1      ki=1 Sim(wi,w)ρ(wi,w) +ε
, (2)
 k    AT(wi,Uj)Sim(wi,w)ρ(wi,w)
 where ε is a small positive number to avoid the divide-by- zero error. The function of ρ(, ) filters out the effects of those documents whose similarity is below a certain threshold and is defined as:

  The free parameters σx and σy specify how “diffusively” a reader scans words when reading documents. In our current implementation, we initialize σx and σy to be the average width and height of a word’s displaying bounding box in the document. For each gaze detected by our eye-tracking mod- ule, we assign the gaze samples to the words in the docu- ment in this manner. The overall attention that a word in the document receives is the sum of all the fractional gaze sam- ples it is assigned in the above process. Notice that when a
Predicting User Attention for Sentences
We estimate the total attention of a certain user on a sen- tence as the sum of the user’s attention over all the words in the sentence. Mathematically, assuming a sentence s in a target document to be summarized consists of n distinct words w1, · · · , wn. We can then predict user Uj ’s attention over the sentence s, denoted as AT (s, Uj ), as follows:
10
AT(s,Uj) =

wi ∈s
AT(wi,Uj)δ(wi,Uj). (4)
 Figure 1. A snapshot of our custom document browser which works together with our vision-based eye tracking component to acquire user attention time samples. Each red circle represents a fixation point of the user. Only points inside the client area of the browser window are recorded. Except during debugging, these red circles will not be displayed. They are shown here for the purpose of illustration.
In the above, recall AT (wi , Uj ) is user Uj ’s attention over the word wi, which is either sampled from the user’s previ- ous reading activities via (1) or predicted via (2) if our algo- rithm has never sampled this word during this user’s previ- ous reading process. The term δ(wi, Uj ) in the above equa- tion gives different weights to the per word user attention term AT (wi , Uj ). Currently, we configure the weighting function δ(wi, Uj ) in the following way: δ(wi, Uj ) = 0 if the word wi is a stop word; δ(wi,Uj) = 0.6 if there is no attention sample for the user Uj over the word wi known to our algorithm and thus has to be estimated via (2); otherwise, δ(wi, Uj ) = 1, which means the user Uj ’s attention over the word wi is acquired from sampling over Uj ’s previous read- ing activities via (1).
PERSONALIZED DOCUMENT SUMMARIZATION
Now we can construct a personalized document summariza- tion algorithm based on the acquired and predicted user at- tention data for individual users. To experiment with our algorithm, we developed a prototype program, which con- sists of a custom document browser interface for acquiring the gaze samples of individual users on individual words in a document, and a summarization module for producing a per- sonalized document summarization based on the prediction of user attention on the target document to be automatically summarized.
Custom Document Browser Interface
For the custom document browser interface, which looks very much like a document browser, the acquisition method introduced earlier is implemented. It operates with the vision- based eye-tracking device we assembled using a simple web camera (Logitech Quickcam Notebook Pro) and an existent eye-tracking algorithm coming from the Opengazer project [47]. The captured user gaze samples are provided to the summarization module. Any fixation points outside the browser window will be ignored during the user attention sampling process. A snapshot of our custom document browser inter- face is shown in Figure 1.
Summarization Module
After the user attentions on all the individual words in a tar- get document are known, which are either sampled from or predicted based on the user’s previous reading processes, the whole document can be represented as a collection of sen- tences or key phrases, each of which has a known attention value. The functionality of our summarization module is to select several key sentences or phrases from the whole docu- ment which can best satisfy the user’s reading interests. Our summarization algorithm can work at either the sentence level or the key word or phrases level, and in principle can be extended to sentence groups or paragraphs without having to change the algorithm in any way. The selected top sentences
11
or keywords that the user is most likely interested in form the personalized summarization of the document. Similarly, if we select the few top words from the whole document as the output, we have an algorithm for personalized keyword extraction.
In a typical summarization software package, end users can tune a free parameter called “compression rate” to specify the ratio between the length of the summarized texts and the length of the whole document. In some advanced summa- rization packages, more control parameters are allowed to specify the expectation over the summarization algorithm’s behavior. For simplicity, we are only concerned with the compression rate. More concretely, inspired by the summa- rization software provided in Microsoft Office Professional Edition 2003, called “Microsoft Word AutoSummarize”, we introduce a percentage parameter c into our algorithm, which restricts our algorithm to only output the top c% sentences (with highest user attention times) as the summarization re- sult for the target document.
A Hybrid Summarization Approach
In our early experiments, we noticed that the performance of our user-oriented document summarization algorithm heav- ily depends on the amount of available user attention time samples. As with many machine learning based algorithms, our algorithm also has the cold start problem, i.e., if there are not enough samples to learn from, the machine summarized result appears to be inferior. To address the issue, we inte- grate our method with a conventional automatic document summarization algorithm, resulting in a hybrid approach for document summarization. The MEAD summarizer (or sim- ply MEAD) [35], which is a portable multi-document sum- marization system, is adopted to perform the conventional summarization function in our hybrid document summariza- tion approach. The reason for choosing this system is that it is in the public domain and yet its performance is compara- ble to other state-of-the-art systems. When the summariza- tion module receives a summarization request submitted by a certain user, the application will first forward the request to MEAD to obtain the document’s summarized text. Then our user attention time prediction module will estimate the attention time of the user over each sentence in the docu- ment. After that, we use the following equation to compute an attention time offset for the user, denoted as Uj, on an arbitrary document sentence si:
n  AToffset(si,Uj)   (1 − κ)max{AT(sk,Uj)}δ(si,Uj),
k=1 (5)
where δ (si , Uj ) = 1 if sentence si is selected by MEAD
in its document summarization result and δ (s , U ) = 0 ij
otherwise. With such a value assignment for the function
δ (·), document sentences appearing in the summarization result produced by MEAD receive higher attention time off- set values than those not included in the summary gener- ated by MEAD. The free parameter κ used in (5) is user tunable, which balances the document summarization result produced by MEAD and the result generated by our user- oriented summarization method. When κ = 1, our hybrid
Table 1. Statistics of the two sets of articles used in our experiment. “Set I” corresponds to the set of articles on science topics and “Set II” is the set of articles on topics about entertainment and leisure. “Man- ual compression rate” is the average ratio between the word length of the user manual summary and the word length of the corresponding original article.
document summarization approach operates without employ- ing the MEAD procedure; when κ = 0, our hybrid approach degenerates into a summarization procedure performed solely by MEAD. In our experiment, κ is set as 0.5 by default.
Once the user Uj’s attention time AT(si,Uj) and attention offset time AToffset(si,Uj) are both known for a sentence si, we can derive a calibrated attention time for the sentence as:
ATcal (si , Uj )   AT (si , Uj ) + ATof f set (si , Uj ). (6)
After we derive the calibrated user attention times for all the sentences in the target document, our targetted document summarization can be produced by selecting the top c% sen- tences in the document that achieve the largest overall user attention, i.e., the sum of these sentences’ calibrated atten- tion times will be maximized.
EXPERIMENT RESULTS
Two general classes of methods have been proposed for eval- uating the performance of text summarization: extrinsic eval- uations and intrinsic evaluations [32]. Extrinsic evaluations verify the quality of summarization texts according to the number of citations on the summarized texts by third parties; intrinsic evaluations determine the quality of summarization texts based on the coverage comparison with a ground truth summarization result. In this paper, we evaluate the qual- ity of our document summarization result using the intrinsic evaluation approach because of its ease of execution. More concretely, we evaluate the performance of our algorithm in terms of summarization quality by comparing the document summarization results produced by our algorithm with those generated by two popular text summarization algorithms. To evaluate the summarization quality of any of the three al- gorithmic approaches, we compare the machine generated summary with the corresponding human summary result us- ing the intrinsic evaluation method.
In our experiments, we use two sets of articles. Articles in the first set are all about science and articles in the second set are all about entertainment and leisure. We choose these two sets of articles because they cover two distinctive groups of readers and hence two types of reading behaviors—one tends to be more serious than the other one. Sixty scien- tific articles are randomly selected from the website of the
Article set
 I
  II
 I + II
 Articles in the set
 60
  60
 120
 Words per article
 979.0
  942.3
 960.7
 Sentences per article
 37.6
  53.2
 45.4
 Paragraphs per article
 9.1
  11.3
 10.2
 Sentences per manual summary
 12.4
  14.7
 13.6
 Manual compression rate
 33.0%
  27.6%
 29.8%
         12
Table 2. Statistics for comparing the performance of three automatic document summarization algorithms: the “Microsoft Word AutoSumma- rize” toolkit, the MEAD summarizer system and our algorithm. We provide performance evaluation statistics for each algorithm for three typical compression rates, 10%, 20%, 30%. In all these experiments, our algorithm consistently outperforms the other two summarization algorithms.
(a) Algorithm performance statistics for article set I
(b) Algorithm performance statistics for article set II
(c) Algorithm performance statistics for both article sets
   Summarization Algorithm
  Compression Rate
  10%
    20%
30%
   Recall
   Precision
F-rate
   Recall
 Precision
F-rate
   Recall
 Precision
F-rate
  MS Word AutoSummarize
 0.13
   0.25
0.17
   0.20
 0.27
0.23
   0.23
 0.27
0.25
  MEAD
 0.18
   0.47
0.26
   0.25
 0.44
0.32
   0.30
 0.42
0.35
  Our Algorithm
 0.28
   0.64
0.39
   0.42
 0.60
0.49
   0.53
 0.58
0.55
            Summarization Algorithm
  Compression Rate
  10%
    20%
30%
   Recall
   Precision
F-rate
   Recall
 Precision
F-rate
   Recall
 Precision
F-rate
  MS Word AutoSummarize
 0.16
   0.23
0.19
   0.21
 0.28
0.24
   0.23
 0.30
0.26
  MEAD
 0.18
   0.36
0.24
   0.26
 0.45
0.33
   0.29
 0.60
0.39
  Our Algorithm
 0.25
   0.70
0.37
   0.44
 0.64
0.52
   0.56
 0.61
0.58
            Summarization Algorithm
  Compression Rate
  10%
    20%
30%
   Recall
   Precision
F-rate
   Recall
 Precision
F-rate
   Recall
 Precision
F-rate
  MS Word AutoSummarize
 0.15
   0.24
0.18
   0.20
 0.27
0.23
   0.23
 0.29
0.26
  MEAD
 0.18
   0.41
0.25
   0.25
 0.44
0.32
   0.30
 0.51
0.37
  Our Algorithm
 0.27
   0.67
0.38
   0.43
 0.62
0.50
   0.54
 0.59
0.57
         “Science” magazine [11] to form the first article set, and an- other sixty articles are randomly selected from the travel and sports section on “New York Times” [6] to form the sec- ond article set. We also invited twelve people with differ- ent knowledge backgrounds to read some selected articles from the two article sets using our custom document browser equipped with our eye-tracking device. After reading each article, they are asked to provide a summary for the article they just read. In this way, we develop a manual summary set for the two sets of articles. Key statistics on the two sets of articles are reported in Table 1.
Inspired by the typical way to measure the performance of an information retrieval system [2], three measurements— Recall (R), Precision (P ) and F-rate (F )—are introduced to evaluate the machine summarization quality against the human summary result. Let SUe be the human summary result for an article, and SU be the summary automatically generated by an algorithm. We then calculate the above three measurements as follows:
P   Number of common sentences in SUe and SU , (7) Number of sentences in SU
R   Number of common sentences in SUe and SU , (8) Number of sentences in SUe
F   2PR . (9) P+R
As mentioned earlier, we asked the twelve people to help us develop human summaries for our article sets. In our ex- periment, for each participant, the person was asked to read some selected articles from the two article sets using our cus-
tom document browser with our eye-tracking device turned on. After reading an article, the person was further asked to provide a summary for the article he just read. And then our algorithm used the person’s user attention time samples over the article to generate a machine summary of the article for the user. Given the machine generated summary and the user provided manual summary, we can measure the quality of the article summarization produced by our algorithm using (7)–(9). Finally, the overall performance of our algorithm is measured as the average performance of our algorithm for all the articles read and summarized by these twelve partici- pants.
We compare the performance of our algorithm with two pop- ular document summarization software packages—“Microsoft Word AutoSummarize” as provided in Microsoft Office Pro- fessional Edition 2003 and the MEAD summarizer system. Concrete statistics for comparing the performance of the three algorithms in summarizing documents are provided in Ta- ble 2. As revealed by these experiment results, our algorithm consistently outperforms the other two algorithms with sig- nificant margins.
As discussed earlier, our hybrid document summarization approach relies on a modulating parameter κ to balance its behavior between the traditional discourse analysis algorithm and our personalized document summarization algorithm. We also conduct an experiment to evaluate the performance of our hybrid approach under different settings for the pa- rameter κ. Results of the experiment are shown in Table 3. In this experiment, the document compression rate is always set as 30%. From these experiment results, we can see the hybrid approach can effectively help us to overcome the cold
   13
Table 3. Performance measurement statistics of our summarization al- gorithm under different settings for the free parameter κ which bal- ances our hybrid document summarization approach between the pure discourse analysis based approach and the machine learning based ap- proach. (a)–(c) report performance measurement statistics for article sets I and II, and for both sets. The experiment repeats with different values for κ (0.00, 0.25, 0.50, 0.75, 1.00). In all these experiments, the compression rate is always set to 30%.
(a) Performance measurement statistics for article set I
(b) Performance measurement statistics for article set II
(c) Performance measurement statistics for both article sets
start problem when there are not enough learning samples for our user-oriented document summarization algorithm to perform. Also, from this experiment we can see for sum- marizing articles on entertainment and leisure topics where personal reading interests vary significantly, using our hy- brid document summarization approach is especially advan- tageous.
In conclusion, by the results of the above experiments, we confirm that our user-oriented document summarization al- gorithm can indeed produce personalized document sum- maries that are more reflective of a user’s reading interest and summary preference than other approaches.
CONCLUSION AND FUTURE WORK
In this paper, we propose a new user-oriented document sum- marization algorithm based on individual users’ attention time on document words and sentences. In our experiments, we compare the performance of our algorithm with both the human summarization result and two popular existing text summarization algorithms implemented in the Microsoft Word AutoSummarize toolkit and the MEAD summarizer system respectively. The experiment results clearly show that our novel algorithm can satisfactorily produce user-oriented doc- ument summarization in better agreement with the user’s ex- pectation and preference.
Having validated the domain specific prototype document summarization system we have developed here via exper- imentation, we hope to demonstrate more the potential of employingcommodityeye-trackingtechniquesforacquiring pervasive and unintrusive user feedbacks in building various
types of user-oriented information retrieval and extraction systems in the future.
Achieving an optimal balance between document summa- rization following the traditional discourse analysis approach and our learning based approach is an open problem. In our current implementation, such a decision is left to the end users via a user tunable free parameter. In the future we plan to explore adaptive methods which can adjust this parameter following a reinforcement learning based method via utiliz- ing some user feedbacks. In that way, we expect to develop an intelligent algorithm for assisting users to find the op- timal balance between traditional discourse analysis based summarization and our machine learning based personalized summarization in order to generate better user-oriented doc- ument summarizations that match more closely with individ- ual users’ preferences.
Right now, when producing user-oriented document sum- marization for a user, our algorithm relies on the use of the user’s attention time samples for the current document. This restricts our current algorithm to be applicable only in sce- narios where a person has read an article and wants to make an automatic summary for the article for future reference by himself or for later readers. We plan to explore the possibil- ity to remove this constraint so that even for an article which the user has not read, it is still possible to generate automatic user-oriented summary of the article for the user.
In our algorithm, estimating semantic similarity between two words or sentences is crucial for producing a quality user- oriented document summarization. Hence in the future, we intend to improve the text content similarity metrics used in our system by incorporating some human intelligence through learning the human feedbacks. This can be realized as an on-line learning algorithm. We also intend to strengthen the data mining capabilities of our algorithm, especially to op- timize the performance of our algorithm in predicting user attention over words and sentences. Finally, setting up a ro- bust user-oriented document summarization system based on our algorithm for large-scale industrial strength deployment would be very meaningful and commercially attractive.
ACKNOWLEDGEMENTS
The first author would like to thank David Gelernter for many inspiring discussions on document summarization research, from which the idea of this paper stemmed. This work has a patent pending.
REFERENCES
1. Bibliography summarization papers. http://www.summarization.com/summ.pdf, last updated on October 20, 2008. last visited on December 11, 2008.
2. R. A. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 1999.
3.A.Bulling,J.A.Ward,H.Gellersen,andG.Tro ̈ster. Robust recognition of reading activity in transit using
   Measurement
 κ
 0.00
  0.25
 0.50
  0.75
 1.00
 Recall
 0.33
  0.41
 0.53
  0.58
 0.61
 Precision
 0.35
  0.44
 0.58
  0.65
 0.70
 F-rate
 0.34
  0.42
 0.55
  0.61
 0.63
          Measurement
 κ
 0.00
  0.25
 0.50
  0.75
 1.00
 Recall
 0.39
  0.44
 0.56
  0.56
 0.46
 Precision
 0.42
  0.48
 0.61
  0.60
 0.50
 F-rate
 0.40
  0.45
 0.58
  0.58
 0.47
          Measurement
 κ
 0.00
  0.25
 0.50
  0.75
 1.00
 Recall
 0.36
  0.42
 0.54
  0.57
 0.54
 Precision
 0.38
  0.46
 0.59
  0.62
 0.60
 F-rate
 0.37
  0.44
 0.57
  0.60
 0.55
       14
wearable electrooculography. In Pervasive ’08: Proceedings of the 6th International Conference on Pervasive Computing, pages 19–37, 2008.
4. A. Bulling, D. Roggen, and G. Tro ̈ster. It’s in your eyes: towards context-awareness and mobile HCI using wearable EOG Goggles. In UbiComp ’08: Proceedings of the 10th International Conference on Ubiquitous Computing, pages 84–93, New York, NY, USA, 2008. ACM.
5. E. H. Chi, M. Gumbrecht, and L. Hong. Visual foraging of highlighted text: An eye-tracking study. In HCII ’07: Proceedings of HCI International Conference, pages 589–598, 2007.
6. The New York Times Company. The New York Times, http://www.nytimes.com/, last visited on December 11, 2008.
7. T. Darrell, N. Checka, A. Oh, and L. Morency. Exploring vision-based interfaces: How to use your head in dual pointing tasks. MIT AI Memo 2002-001, 2002.
8. Z. Dou, R. Song, and J.-R. Wen. A large-scale evaluation and analysis of personalized search strategies. In WWW ’07: Proceedings of International Conference on World Wide Web, pages 581–590, New York, NY, USA, 2007. ACM.
9. G. Dupret, V. Murdock, and B. Piwowarski. Web search engine evaluation using clickthrough data and a user model. In WWW ’07: Proceedings of International Conference on World Wide Web, Banff, Canada, 2007.
10. G. Erkan and D. Radev. Lexrank: Graph-based lexical centrality as salience in text summarization. Journal of Artificial Intelligence Research (JAIR), 22:457–479, 2004.
11. American Association for the Advancement of Science. Science magazine, http://www.sciencemag.com/, last visited on December 11, 2008.
12. S. Fox, K. Karnawat, M. Mydland, S. Dumais, and
T. White. Evaluating implicit measures to improve web search. ACM Transactions on Information Systems, 23(2):147–168, 2005.
13. X. Fu. Evaluating sources of implicit feedback in web searches. In RecSys ’07: Proceedings of the 1st ACM International Conference on Recommender Systems, pages 191–194, New York, NY, USA, 2007. ACM.
14. Y. Gong and X. Liu. Generic text summarization using relevance measure and latent semantic analysis. In SIGIR ’01: Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 19–25, New York, NY, USA, 2001. ACM.
15. D. Gorodnichy. Perceptual cursor–a solution to the broken loop problem in vision-based hands-free computer control devices. National Research Council Canada Publication, NRC-48472:1–23, 2006.
16. L. A. Granka, T. Joachims, and G. Gay. Eye-tracking analysis of user behavior in www search. In SIGIR ’04: Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 478–479, New York, NY, USA, 2004. ACM.
17. Z. Guan and E. Cutrell. An eye tracking study of the effect of target rank on web search. In CHI ’07: Proceedings of SIGCHI Conference on Human Factors in Computing Systems, pages 417–420, New York, NY, USA, 2007. ACM.
18. W. S. A. Halabi, M. Kubat, and M. Tapia. Time spent on a web page is sufficient to infer a user’s interest. In IMSA ’07: Proceedings of IASTED European Conference, pages 41–46, Anaheim, CA, USA, 2007. ACTA Press.
19. C.-K. Huang, Y.-J. Oyang, and L.-F. Chien. A contextual term suggestion mechanism for interactive web search. In WI ’01: Proceedings of Asia-Pacific Conference on Web Intelligence: Research and Development, pages 272–281, London, UK, 2001. Springer-Verlag.
20. T. Joachims. A probabilistic analysis of the rocchio algorithm with tfidf for text categorization. In ICML ’97: Proceedings of International Conference on Machine Learning, pages 143–151, San Francisco, CA, USA, 1997. Morgan Kaufmann Publishers Inc.
21. T. Joachims. Optimizing search engines using clickthrough data. In KDD ’02: Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 133–142, New York, NY, USA, 2002. ACM.
22. T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately interpreting clickthrough data as implicit feedback. In SIGIR ’05: Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 154–161, New York, NY, USA, 2005. ACM.
23. K. S. Jones. What might be in a summary. In Information Retrieval 93: Von der Modellierung zur Anwendung, pages 9–26, 1993.
24. D. Kelly and N. J. Belkin. Reading time, scrolling and interaction: exploring implicit sources of user preferences for relevance feedback. In SIGIR ’01: Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 408–409, New York, NY, USA, 2001. ACM.
25. D. Kelly and N. J. Belkin. Display time as implicit feedback: understanding task effects. In SIGIR ’04: Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 377–384, New York, NY, USA, 2004. ACM.
15
26. K.-N. Kim and R. Ramakrishna. Vision-based eye-gaze tracking for human computer interface. SMC ’99: Proceedings of IEEE International Conference on Systems, Man, and Cybernetics, 2:324–329, 1999.
27. Y. Li, Z. A. Bandar, and D. Mclean. An approach for measuring semantic similarity between words using multiple information sources. IEEE Transactions on Knowledge and Data Engineering, 15(4):871–882, 2003.
28. C.-Y. Lin. Training a selection function for extraction. In CIKM ’99: Proceedings of the 8th ACM International Conference on Information and Knowledge Management, pages 55–62, New York, NY, USA, 1999. ACM.
29. Y.-P. Lin, Y.-P. Chao, C.-C. Lin, and J.-H. Chen. Webcam mouse using face and eye tracking in various illumination environments. EMBS ’05: Proceedings of 27th IEEE Annual International Conference of Engineering in Medicine and Biology Society, pages 3738–3741, 2005.
30. F. Liu, C. Yu, and W. Meng. Personalized web search by mapping user queries to categories. In CIKM ’02: Proceedings of the 11th ACM International Conference on Information and Knowledge Management, pages 558–565, New York, NY, USA, 2002. ACM.
31. Y. Lv, L. Sun, J. Zhang, J.-Y. Nie, W. Chen, and
W. Zhang. An iterative implicit feedback approach to personalized search. In ACL ’06: Proceedings of International Conference on Computational Linguistics, pages 585–592, Morristown, NJ, USA, 2006. Association for Computational Linguistics.
32. I. Mani. Advances in Automatic Text Summarization. MIT Press, Cambridge, MA, USA, 1999.
33. Microsoft. Word (software), http://office.microsoft.com/word/, Microsoft Corporation, last visited on December 11, 2008.
34. J. Pitkow, H. Schu ̈tze, T. Cass, R. Cooley, D. Turnbull, A. Edmonds, E. Adar, and T. Breuel. Personalized search. Communications of the ACM, 45(9):50–55, 2002.
35. D. Radev, T. Allison, S. Blair-Goldensohn, J. Blitzer,
A. C ̧ elebi, S. Dimitrov, E. Drabek, A. Hakim, W. Lam, D. Liu, J. Otterbacher, H. Qi, H. Saggion, S. Teufel,
M. Topper, A. Winkel, and Z. Zhang. MEAD–A platform for multidocument multilingual text summarization. In LREC ’04: The 2nd International Conference on Language Resources and Evaluation, Lisbon, Portugal, 2004.
36. F. Radlinski and T. Joachims. Query chains: learning to rank from implicit feedback. In KDD ’05: Proceedings of ACM SIGKDD International Conference on Knowledge Discovery in Data Mining, pages 239–248, New York, NY, USA, 2005. ACM.
37. R. W. Reeder, P. Pirolli, and S. K. Card. Webeyemapper and weblogger: tools for analyzing eye tracking data collected in web-use studies. In CHI ’01: CHI ’01 Extended Abstracts on Human Factors in Computing Systems, pages 19–20, New York, NY, USA, 2001. ACM.
38. R. Ruddarraju, A. Haro, K. Nagel, Q. T. Tran, I. A. Essa, G. Abowd, and E. D. Mynatt. Perceptual user interfaces using vision-based eye tracking. In ICMI ’03: Proceedings of 5th International Conference on Multimodal Interfaces, pages 227–233, New York, NY, USA, 2003. ACM.
39. G. Salton and C. Buckley. Improving retrieval performance by relevance feedback. Journal of the American Society for Information Science, 41(4):288–297, 1990.
40. M. Speretta and S. Gauch. Personalized search based on user search histories. In WI ’05: Proceedings of IEEE/WIC/ACM International Conference on Web Intelligence, pages 622–628, Washington, DC, USA, 2005. IEEE Computer Society.
41. M.-C. Su, S.-Y. Su, and G.-D. Chen. A low-cost vision-based human-computer interface for people with severe disabilities. Biomedical Engineering Applications, Basis, and Communications, 17:284–292, 2005.
42. R. White, J. M. Jose, and I. Ruthven. Comparing explicit and implicit feedback techniques for web retrieval: Trec-10 interactive track report. In TREC, 2001.
43. R. White, I. Ruthven, and J. M. Jose. The use of implicit evidence for relevance feedback in web retrieval. In Proceedings of BCS-IRSG European Colloquium on IR Research, pages 93–109, London, UK, 2002. Springer-Verlag.
44. S. Xu, H. Jiang, and F. C. Lau. Personalized online document, image and video recommendation via commodity eye-tracking. In RecSys ’08: Proceedings of the 2nd ACM International Conference on Recommender Systems, pages 83–90, New York, NY, USA, 2008. ACM.
45. S. Xu, Y. Zhu, H. Jiang, and F. C. M. Lau. A user-oriented webpage ranking algorithm based on user attention time. In AAAI ’08: Proceedings of the 23rd AAAI Conference on Artificial Intelligence, pages 1255–1260, Chicago, Illinois, USA, 2008. AAAI.
46. J.-Y. Yeh, H.-R. Ke, W.-P. Yang, and I.-H. Meng. Text summarization using a trainable summarizer and latent semantic analysis. Information Processing and Management, 41(1):75–95, 2004.
47. P. Zielinski. Opengazer: open-source gaze tracker for ordinary webcams (software), Samsung and The Gatsby Charitable Foundation. http://www.inference.phy.cam.ac.uk/opengazer/, last visited on December 11 2008.
{HAUPTBEITRAG / THE EYEBOOK The eyeBook
Using Eye Tracking to Enhance the Reading Experience
  Introduction
A rapid development of eye tracking technology has been observed in recent years. Today’s eye trackers can determine the current focus point of the eye precisely while being relatively unobtrusive in their application.
Also, a variety of research and commercial groups has been working on this technology, and there is a growing interest for such devices on the market. Eye tracking has great potential and it can be assumed that it will advance further and might become a widespread technology used at a large number of personal or office computer workplaces. Approaches using simple webcams for eye track- ing already exist, for example webcams integrated into laptop computers by default. Thus, they al- low for new kinds of applications using eye gaze data.
However, not only eye tracking technology is advancing rapidly to an easily usable state. Addition- ally, during the past 100 years researchers gathered a considerable amount of knowledge on eye move- ments, why and how they occur, and what they might mean.
So, today, we have the technology and know- ledge for tracking and analyzing eye movements, making an excellent starting point for sophisticated interactive gaze-based applications.
Naive approaches where gaze data is directly em- ployed for interacting with the system, e. g., pressing buttons on the screen with the “blink of an eye” gen- erally have serious problems. Because the eyes are organs used for perceiving the world and not for ma- nipulating the world, it is hard and against human nature to control eye movements deliberately.
However, a highly promising approach is just to observe eye movements of the user during his or her daily work in front of the computer, to infer user intentions based on eye movement behavior, and to provide assistance where helpful. Gaze can be seen as a proxy for the user’s attention, and eye movements are known to be usually tightly coupled with cognitive processes in the brain, so that a great deal about those processes can be observed by eye tracking. For example, by interpreting eye move- ments, reading behavior of the user can be detected, which most likely entails cognitive processes of understanding with regard to the currently read text.
In this paper we are focusing particularly on reading behavior since reading is probably the most common activity of knowledge workers sit- ting in front of a computer screen. We present
an algorithm for online reading detection based on eye tracking data and introduce an applica- tion for assisted and augmented reading called the eyeBook.
The idea behind the eyeBook is to create an interactive and entertaining reading experience. The system observes, which text parts are currently being read by the user on the screen and generates ap- propriate effects such as playing sounds, presenting
DOI 10.1007/s00287-009-0381-2 © Springer-Verlag 2009
Ralf Biedert · Georg Buscher · Andreas Dengel
Knowledge Management Department, DFKI,
Trippstadter Str. 122, 67663 Kaiserslautern
E-Mail: {ralf.biedert, georg.buscher, andreas.dengel}@dfki.de
Ralf Biedert · Georg Buscher · Andreas Dengel University of Kaiserslautern, Gottlieb-Daimler-Strasse, 67663 Kaiserslautern
Ralf Biedert · Georg Buscher Andreas Dengel
  272 Informatik_Spektrum_33_3_2010

 Fig. 1 User interface of the eyeBook. If the reader passes the marked position, a sound can be heard and a new image is shown
pictures, or changing the color scheme. An example of the eyeBook interface is presented in Fig. 1. It is an entertaining application introducing a new kind of story telling experience, and the eyeBook tech- nology has the potential to be used for information assistance purposes.
Eye Tracking Technology
For the past hundred years some interesting ad- vances have been observed concerning the analysis and study of eye movements both with respect to eye tracking techniques and their applications. Here, we give a brief overview on how modern eye trackers work and what kinds of applications eye trackers are mostly used for today.
Starting from the manual observation of the eye for the purpose of understanding eye movements during reading to interactive systems reacting on just the flick of an eye, a diversity of measurement systems, scenarios and programs have been de- veloped addressing one main question: What can be read from eyes?
Modern Remote Eye Tracking
While even today many different classes of eye track- ing devices exist, the trend in end user devices, e. g., used by disabled users, research, and usability labs, goes to remote eye tracking. This technique usually works by illuminating the eye with one or more in- frared light sources, and the reflection on the eye is in turn recorded by one or more cameras. One of the advantages of infrared light is its invisibility to the user, and therefore it does not distract people while working with the eye tracker.
The exact mechanisms used by proprietary devices (like the eye tracker shown in Fig. 2) are disclosed, but published techniques often work by computing the center of the pupil and the center of the corneal reflection, called glint, and determining their relative position to each other. Yet, some eye trackers also consider further reflective zones of the eye to gain a higher tracking precision (compare Fig. 3). These relative positions can then be used in conjunction with reference points to compute the on-screen gaze position for a fixed head orienta-
Informatik_Spektrum_33_3_2010 273

 {THE EYEBOOK
274 Informatik_Spektrum_33_3_2010
Fig. 2 A modern desk-mounted eye tracker (Tobii T60) with an integrated camera below the screen
So, given that the computer knows where peo- ple look on the screen, how can this information be used?
Present Applications of Gaze
The application of gaze data can be distinguished as being either diagnostic or interactive [7]. Because computer equipment was not available a century ago, diagnostic usage was historically the first to emerge and it has kept its importance to the present day. Interactive applications on the other hand address new ways of human–computer interaction using gaze input.
Diagnostic Applications. Diagnostic applications cover a wide range of domains. One of the first us- ages of eye tracking was the examination of reading and information processing, a topic that is still of interest today, as the rest of this article shows.
As we are confronted with a constantly changing environment, it is reasonable to ask how we visu- ally perceive our surroundings – a field generally called scene perception. One of the main applica- tions in this area is the analysis of advertisements. Companies investing several thousand Euros into the distribution of a billboard have a keen interest to objectively verify that customers actually notice what it was designed for, e. g., to notice the brand or company name.
A different diagnostic domain is the study of Human-Machine-Interface-related tasks. Experts solve problems differently to novices. Their ac- tions, but also their focus of attention, vary from those of untrained operators. A comparative gaze analysis can uncover the potential for optimiza- tion or refinement of training, for example in aviation, driving or the interpretation of medical images.
Furthermore, within the automobile industry for example, there is ongoing research regarding the integration of eye tracking systems for driver assis- tance. Drowsiness is a significant cause of accidents and a real-time diagnosis alerting the driver of his or her potentially hazardous state will help to avoid that danger.
Interactive Applications. In contrast to diagnostic purposes, interactive applications facilitate gaze as an input to alter the runtime behavior of the system. Most notably this category comprises se-
Fig. 3 Schema of an eye. Different locations of light reflections used for eye tracking are marked with asterisks
tion. The reference points are obtained in a process called calibration where the user usually has to fo- cus on a small number of different locations on the screen for a short time (e. g., the 9 different locations depicted in Fig. 2).
Based on these computations the device usu- ally delivers raw gaze data to the application. This data may include gaze positions of the individual eyes in screen coordinates and optionally additional information such as the head position in x-, y- and z-coordinates and the observed pupil size. The raw gaze values, however, are rarely used directly and are usually further filtered and post-processed since they are subject to random noise, such as measurement errors, systematic errors and drift from calibration.
lection applications, where the user actively uses his or her gaze to control or perform a certain action.
As previously stated, the use of gaze in order
to manipulate user interfaces creates new prob- lems (especially when considering that eyes mainly evolved as organs to perceive information), for example those of accidental manipulation. One note- worthy problem in this regard is called the Midas Touch, referring to the mythical king who turned everything he touched into gold. In the domain
of eye tracking, this term denotes the difficulty to infer an intent of manipulation from the purely two dimensional position of the user’s gaze. One workaround concerning a selection process is dwell time: a button or control element is charged by gaze time until a certain threshold is reached. Looking away from the element before activation will dis- charge it. However, while a sufficiently long dwell time could effectively eliminate the Midas Touch, the systems would then be rendered unusable be- cause it would just take too long to interact with them.
Another category of interactive gaze-based ap- plications not only serves the purpose of aiding handicapped users. Starker and Bolt [18] for ex- ample created an interactive narrator which provides additional information about items gazed upon
in a three-dimensional world by speech synthesis. Yet even more directly, eye-tracking control of 3D- applications such as ego-shooters, World of Warcraft or Second Life was implemented recently with vary- ing success. But also 2D applications can benefit from gaze data: Learning environments [12], for example, present related information, dictionaries show unknown vocabulary [8] or text windows scroll automatically [10].
Eye Movements and Perception
As we have seen, gaze data can easily be recorded nowadays. Yet, what do typical eye movements look like and what do they tell us?
Typically, while awake, the eyes move rapidly with a frequency of around four times a second in order to focus on different parts of the cur- rent visual scene. Eye movements are most often coupled with cognitive processes in the brain,
so that it is possible to derive information about those processes by observing and interpreting eye movements.
Here, we give a short overview of general phys- iological characteristics of the eyes and how their movement is controlled especially during reading behavior.
Characteristics of the Human Eye
Human eyes can only perceive a limited fraction of the visual world at one point in time. During a fixa- tion (i. e., when the eyes are steadily looking at one spot in the visual scene), both eyes together pro- vide a roughly elliptical view of the world which is approximately 200◦ of visual angle wide and 130◦ high [9]. However, not all parts of this view are per- ceived with equal acuity because the retina of the eye has a varying structure and composition. Generally, it is composed of two types of visual receptors, i. e., rods and cones, that are unevenly distributed.
As depicted in Fig. 4, the foveal area is a small central region of the retina of around 2◦ to 3◦ of visual angle where visual acuity is maximal [9, 15]. It lies in the center of the retina and covers the area around the fixation point of the eye. It is almost com- pletely composed of cones which are specialized in processing detail, for acuity, and which also serve to distinguish between colors.
With increasing distance from the foveal area, the composition of the receptors on the retina changes: the density of cones decreases while the density of rods increases. Rods are specialized in discriminating different levels of brightness and also in detecting movement. In accordance with the change in composition, visual acuity decreases by 50% at a distance of 5◦ from the fovea and by 90% at 40◦ [9]. The area around the foveal area up to 10◦ of visual angle is called parafoveal area. The remainder around the parafoveal area is called peripheral area.
In order to achieve the most accurate visual im- pression of a visual scene, the eyes rapidly move in mostly ballistic jumps (i. e., saccades) from one spot to another, fixating each spot only for a short while of around 250 ms.
Among those rather large saccadic eye move- ments that an attentive person can easily observe from his or her own experience there are three different, much smaller types of eye movements,
i. e., fixational eye movements like tremor, drift, and microsaccades [11]. Their function is to avoid saturation effects of the visual receptors on the retina which would lead to fading perception. How- ever, people are unaware of those tiny movements
Informatik_Spektrum_33_3_2010 275

 {THE EYEBOOK
276 Informatik_Spektrum_33_3_2010
Fig. 4 Foveal, parafoveal, and peripheral regions on the retina
and they can hardly be detected by state-of-the-art unobtrusive eye trackers.
Reading Psychology
During the last one hundred years a lot of research has been conducted concerning eye movements while reading. When silently reading an English text, as summed up by Rayner [14], the eye shows
a very characteristic behavior composed of fixations of about 200–250 ms and saccades in specific direc- tions. The mean left-to-right saccade on a line of text has a distance of 7–9 letter spaces. Approximately 10–15% of the eye movements during reading are regressions, which are saccades to the left along the current line or to a previously read line. Short regres- sions mostly occur due to oculomotor errors or due to problems of the reader with the previously fixated (e. g., unknown) word. There is evidence that longer regressions (more than 10 letter spaces to the left of the currently focused line or to a previous line) occur because the reader did not understand the content of the text.
Words can be identified only during fixations (not during saccades), namely up to ca. 7–8 letters spaces to the right of the fixation point (i. e., foveal vision). However, the total perceptual span, where at least some useful information about the text can be extracted, extends about 14–15 letter spaces to the right of the fixation point.
The factors influencing where, when, how and why eye movements are performed during reading are accounted for in the E–Z reader model of eye movement control in reading [16] in great detail.
Note that there is a high variability of the above-mentioned average values, both concerning individual differences between readers and also con- cerning document-induced differences for the same reader. For example, the fixation durations for the same reader can vary between ca. 100–500 ms, while the saccade sizes can range between 1 and 15 char- acters. Amongst others, this variability is influenced by features of the text such as difficulty and word predictability [6], as well as by characteristics of the reader, like background knowledge and read- ing strategy. For example, there is evidence that if
a text becomes more difficult then fixation duration increases, saccade length decreases, and regression frequency increases [15].
Reading Detection
Having this knowledge on the physiological char- acteristics of the human eye and how and why we control eye movements in mind, it is possible to detect reading behavior based on the very specific sequence of saccades involved. Knowing when the user is reading and which text parts he or she is reading is highly valuable information that can be used as implicit feedback for the computer system. Since reading behavior most often entails cogni- tive processes of understanding in the user’s mind, a computer system might use this information for estimating the user’s current topical interests and for providing assistance during reading. Thus, using gaze data for rather indirect reading detec- tion and not for direct interaction with a computer (creating problems like Midas Touch) seems to be a very promising field of application. Since eye trackers of today have sufficient tracking accuracy and are relatively unobtrusive in their applica- tion, it seems possible in the future to acquire information about what the user has read on the screen.
What is generally meant by “reading” can be quite different. When people look at a map, study an overview of the latest soccer results in a newspaper, scan through programming code to find an error, all of these could be called “reading” in general. Yet, according to Rayner and Pollatsek [15], we narrow down the meaning of reading by focusing on skilled reading which occurs when reading a text book (or this article) in order to understand its content. An example of fixation placement during such reading behavior is given in Fig. 6.
Based on the knowledge about eye movement behavior during reading, we designed an algorithm that can detect such behavior. Since the two con- cepts “reading” and “skimming” exist in common language usage, we also introduced a functionality that roughly differentiates between them.
Reading Detection Algorithm
The general idea of the algorithm is related to an algorithm by Campbell and Maglio [5] and ideas by Beymer and Russell [1] and works as first described in [2]:
First, fixations are detected. A new fixation is detected if successive nearby gaze locations within a time interval of 100 ms can be observed from the eye tracker. 100 ms is the minimum fixation duration according to the literature. Gaze points are consid- ered nearby when they fit together in a circle with
a diameter of 30 pixels (compare Figs. 5 and 6: clas- sification of raw gaze locations produced by the eye tracker into fixations. The diameter of the circles in Fig. 6 corresponds to the fixation durations).
Second, each transition from one fixation to the next (i. e., each saccade) is classified according to its length and direction. This results in features describ- ing saccades that occur more or less often during reading or skimming, e. g., saccades to the right or left with different distances. A list of all possible features is given in Table 1.
Third, scores associated with the features are ac- cumulated. In order to differentiate between reading and skimming behavior, we apply two different sets of scores as shown in Table 1, one used by a reading detector, and one used by a skimming detector.
Finally, it is determined whether thresholds for “reading” and “skimming” behavior are exceeded.
Fig. 5 Raw gaze data during reading as produced by the eye tracker
Fig. 6 Fixations and saccades during reading behavior
   Fig. 7 Reading detection on a saccade sequence
Informatik_Spektrum_33_3_2010 277

 {THE EYEBOOK
Saccade classification and detector
scores
From Reading to Sound
The eyeBook framework, like any other HTML viewer, transforms strings from its book files into rendered text on the screen. The available screen size is evaluated, styles are applied and eventually text is put into layout and rendering, hence associating string positions with areas on the screen.
Calculating a text position for a screen coordi- nate enables us to perform the next step, integrating the aforementioned algorithms into the eyeBook:
If we detect reading we associate the momentar- ily viewed position with it, thus we are able to flag elements of our book file as read and evaluate cor- responding annotations (as depicted in Fig. 8). If there are annotations for reading events for the text section currently being read, then those events are executed.
These annotations can principally be divided into two types: singular markers and block markers. Singular markers span a certain short amount of text and are only executed once during normal operation. Depending on their position in the text, for example at the beginning of a line, singular markers may be expanded to cover more than their initial area, in order to increase the likelihood of an activation by eye gaze. Block markers in contrast usually annotate several lines of text, if not whole sections. Events associated will be executed randomly while the user reads in the area they cover.
While it is imaginable that plenty of effects can be produced upon reading, we focus on a basic set for multimedia: The ability to play sound, music, display images and change the color theme. Usu- ally there is one audio track acting as background music throughout the reading session. It is faded
in smoothly and kept running until explicitly ter- minated. Starting a new music track will cross-fade from the old one. Sound effects in contrast can be played simultaneously and, depending on the book, there are up to three sound layers in parallel: ambient effects like wind or water, foreground sounds most frequently directly connected to the actual text cur- rently focused on by the user (like a howling wolf, as can be seen in Fig. 9) and interface feedback sounds like a short click sound on scrolling.
The remaining two effect classes are images and themes. Images are faded into a separate picture area on the right side of the screen and remain there until a new image has to be displayed. Themes are basically color sets for the background and the fore-
Table 1
278 Informatik_Spektrum_33_3_2010
If this is the case then the respective most plausible behavior is detected as shown by an example in Fig. 7.
The eyeBook
The eyeBook is an intelligent book that facilitates gaze to offer a new, multimedia experience of read- ing. It won first prize in the COGAIN Competition: Creative Gaze1.
Basic Concepts
This section outlines two parts: Our application framework, called eyeBook framework, which is
a system that reads book-bundles, loads additional multimedia content, renders the result on the screen and interprets gaze, compare Fig. 1. We also present the individual books created for this reading engine and their format. They are called eyeBooks as well. These books form a bundle of raw text, annotations (also called markers) and multimedia files such as images or sounds along with meta information.
Book-bundles are basically similar to zip- archives with a specific internal structure. A main configuration lists which files actually compose the text of the given book and which page should be opened upon the first reading session. The pages are described by a markup-language derived from HTML. The language contains additional elem- ents like event handlers dealing with specific eye movement patterns of the user and action elem- ents for modifying the user interface or playing sounds. Thus, most2 valid HTML files are, in fact, valid eyeBook files.
1 http://www.cogain.org/competition/creative-gaze.html, last access 3 September 2009
2 With some exceptions of the usage of JavaScript and CSS that are not fully supported by the rendering engine.
 Fig. 8 If the user reads over an annotation its corresponding event will be executed. In this case the image is changed
                                                                                                                                                                     Fig. 9 Example of an eyeBook markup. The element will react if its surrounding area has been read
ground of the text which are intended to intensify the mood of the currently read scene.
Augmented Books
At the moment, there is a set of three eyeBook in- stances: annotated versions of Dracula, The Little Prince as well as a Making Of.
The first six chapters of The Little Prince were augmented with multimedia effects for reading. The book starts with the author making acquaintance with the prince. The music is rather melancholic and audio effects happen less frequently, also at- tributed to the book’s stream of consciousness. Images displayed were taken from the ones drawn by Saint-Exupéry himself and appear exactly at the location they are referenced. For example in the text
“And after some work with a colored pencil I suc- ceeded in making my first drawing. My Drawing Number One. It looked something like this.”
the end of the first sentence was annotated with
a sound of a pencil scribbling on paper, while the middle of the third sentence was linked to an image of the Drawing Number One (i. e., the hat).
For Dracula we decided to use some of the first days of Jonathan Harker’s diary, a scene outside in
which he takes a ride in a caleche at night through a mountain area, surrounded by wolves and even- tually arrives at the Count’s residence. The book
is kept very dark, the background color is almost set to black, the music was taken from the movie soundtrack, and numerous ambient sounds found on freesounds.org were integrated. Wolves can be heard howling or the caleche’s driver is whispering when the user is reading over the appropriate parts of the text. At the same time in the background the wind roars constantly. Images taken from the movie were artistically converted into stills and shown at appropriate locations.
Explicit Interaction
During normal operation, interaction with the appli- cation is purely gaze-controlled. After startup, a user identification takes place and personal settings are loaded including the previous reading progress at word level. After selecting a book to read, the cho- sen title is opened with a small visual cue at the last read position (if any) and the background music, the color theme, and the sidebar images are restored.
While gaze operation is the default setting, mouse emulation is also possible for demonstration and debugging purposes. With its help the reader may use the mouse to follow his or her gaze in order to trigger reading annotations or activate controls.
All buttons inside are dwell-time-based, but exhibit learning behavior. This pays tribute to the balancing problem of the Midas Touch: Buttons which are gazed upon the first time get activated slowly, but decrease their required time with ensu-
Informatik_Spektrum_33_3_2010 279

{THE EYEBOOK
 280 Informatik_Spektrum_33_3_2010
ing operations. This time is also dependent on the impact of the targeted control. Scrolling text, for example, is performed with the help of two areas around the upper and lower border of the screen that have a very low initial activation time: In case of an accidental activation the reading flow is in- terrupted only minimally and the inverse operation (e. g., scroll up, when the previous command was scroll down) is instantly accessible. The quit-control on the other hand has a high initial activation time, as an accidental execution stops the application and manual interaction is required to start it up again.
Outlook
We will perform more research on augmented text and augmented reading. With augmented text we denote text which reacts to reading, and con- sider the eyeBook engine as one of its instances. Augmented reading, on the other hand, is the pro- cess of enhancing reading by the gaze input of eye tracking devices. Both domains offer plenty of ideas worth investigating, not only bound to entertainment.
Reading annotations, for example, can be used for recontextualization purposes [3]: It often hap- pens that one re-opens a document after some time in order to find exactly the same information as dur- ing the last time in order to resume a task where
it was suspended before. In that case information about which parts have been read before can be used as a search filter applied to documents to find relevant passages quickly and accurately.
This is closely related to the field of person- alization in information retrieval: If two different users pose the same query to the search engine, then often the same result list is returned to them. How- ever, users have different individual background knowledge, interests, and information needs so that they expect to find different pieces of information depending on their situation. In this respect, eye tracking and the analysis of reading behavior is
a very promising feedback technique. For example, it can then be used to disambiguate the user’s query by adding terms that describe the background and current topical context of the user [4].
It is also possible to analyze how exactly the user is reading a text with respect to eye movement measures like fixation duration, saccade length, etc., in order to determine whether the contents of the
text seems relevant or interesting to the user or not. This line of research has also produced some very promising results so far (compare [2, 13, 17]).
Other researchers showed that difficulties of un- derstanding when reading text in foreign languages can be recognized and addressed [8] by providing translation on the fly. In a more generalized ap- proach, additional information may be provided down to word level, read or skimmed text may be analyzed, stored or shared, yet collaborative ap- proaches within workgroups – collective reading
– are conceivable. Education and edutainment are other promising fields, particularly with respect to learning languages.
In the long term, the prevalence of these tech- niques will depend on how common eye tracking devices become. We think that if these devices be- come as miniaturized, reliable, and mainstream
as webcams are today, then the emergence of aug- mented reading will be likely because it provides several advantages. However, it might also be imag- inable that those devices, which would commonly serve as eBook readers of the future, will contain embedded eye trackers. We anticipate that in this case it is less a question of if augmented text will serve as an artistic instrument, but rather to what extent?
Conclusion
Overall, we have shown some great future potentials of eye tracking. First, the necessary hardware ad- vanced to an easily usable state and can provide data about the focus point of the eyes with sufficiently high precision for many applications. Given the cur- rent commercial interest and competition and the rising interest on the market, it can well be assumed that such devices become more widespread in the future.
Second, today, we can look at a vast amount of research results from over 100 years in the area of cognitive science, particularly from the fields of eye movement and reading research. This knowledge enables us today to build applications making use of gaze data in a reasonable way. One application of that knowledge is the detection of reading behavior for which we presented a robust, online detection algorithm.
Third, the eyeBook application demonstrates the great future potentials for an augmented multimedia reading experience of a new kind. Furthermore, the

technologies used are well-suited for additional ap- plications in personal or office computer workplaces, such as applications for information assistance.
In general, there are a great many possibilities open to eye tracking and their applications which might have a considerable impact on the day-to-day work in front of computer screens.
References
1. Beymer D, Russell DM (2005) Webgazeanalyzer: a system for capturing and ana- lyzing web reading behavior using eye gaze. In: CHI ’05 extended abstracts on Human factors in computing systems, pp 1913–1916
2. Buscher G, Dengel A, van Elst L (2008) Eye movements as implicit relevance feed- back. In: CHI ’08 extended abstracts on Human factors in computing systems,
pp 2991–2996
3. Buscher G, Dengel A, van Elst L, Mittag F (2008) Generating and using gaze-based document annotations. In: CHI ’08 extended abstracts on Human factors in com- puting systems, pp 3045–3050
4. Buscher G, Dengel A, van Elst L (2008) Query expansion using gaze-based feed- back on the subdocument level. In: SIGIR ’08: Proceedings of the 31st annual in- ternational ACM SIGIR conference on Research and development in information retrieval, January 2008
5. Campbell CS, Maglio PP (2001) A robust algorithm for reading detection. In: Pro- ceedings of the 2001 workshop on Perceptive user interfaces, pp 1–7
6. Clifton C, Staub A, Rayner K (2007) Eye movements in reading words and senten- ces. In: van Gompel R (ed) Eye movements: A window on mind and brain, 1st edition. Elsevier Science, Oxford Amsterdam, pp 341–371
7. DuchowskiAT(2002)Abreadth-firstsurveyofeyetrackingapplications.Behav Res Meth Instrum Comput 34(4):455–470
8. HyrskykariA(2006)Eyesinattentiveinterfaces:Experiencesfromcreatingidict, a gaze-aware reading aid. http://acta.uta.fi, last access 3 September 2009
9. IrwinD(2004)Fixationlocationandfixationdurationasindicesofcognitivepro- cessing. In: Henderson JM, Ferreira F (eds) The interface of language, vision and action: Eye movements and the visual world, 1st edition. Psychology Press, New York, pp 105–134
10. KumarM,WinogradT,PaepckeA(2007)Gaze-enhancedscrollingtechniques.In: Proceedings of the 20th annual ACM symposium on User interface software and technology, pp 213–216
11. Martinez-CondeS,MacknikS,HubelD(2004)Theroleoffixationaleyemovements in visual perception. Nature Rev Neurosci 5:229–240
12. Mödritscher F, Garc ́ıa-Barrios V, Gütl C, Helic D (2006) The first adele prototype at a glance. In: Proceedings of the World Conference on Educational Multimedia, Hypermedia and Telecommunications, Jan 2006
13. Moe KK, Jensen JM, Larsen B (2007) A qualitative look at eye-tracking for implicit relevance feedback. In: Proceedings of the Workshop on Context-Based Informa- tion Retrieval, pp 36–47, Aug 2007
14. Rayner K (1998) Eye movements in reading and information processing: 20 years of research. Psych Bull 124(3):372–422
15. Rayner K, Pollatsek A (1989) The psychology of reading. Lawrence Erlbaum Asso- ciates, Hillsdale, NJ
16. Reichle ED, Rayner K, Pollatsek A (2003) The e-z reader model of eye-movement control in reading: Comparisons to other models. Behav Brain Sci 26(4):445–476
17. Salojärvi J, Puolamäki K, Kaski S (2005) Implicit relevance feedback from eye move- ments. Artificial Neural Networks: Biological Inspirations. In: International Confer- ence on Artificial Neural Networks, Lecture Notes in Computer Science. Springer- Verlag, pp 513–518
18. Starker I, Bolt R (1990) A gaze-responsive self-disclosing display. In: Proceedings of the SIGCHI conference on Human factors in computing systems: Empowering people, pp 3–10
Informatik_Spektrum_33_3_2010 281
Using Eye-Tracking with Dynamic Areas of Interest for Analyzing Interactive Information Retrieval
ABSTRACT
Vu T. Tran Information Engineering University of Duisburg-Essen Duisburg, Germany
vtran@is.inf.uni-due.de
Norbert Fuhr Information Engineering University of Duisburg-Essen Duisburg, Germany
norbert.fuhr@uni-due.de
how the first two parameters can be estimated in our setting, while the third one still is an open research issue.
By applying economic theory to IIR Azzopardi modelled the process of interaction between a user and a system and constructed a cost function to measure the user effort [1]. Cutrell and Guang [4] used eye-tracking to explore the ef- fects of different presentations of search results. Joachims et al. [5] examined the reliability of click-through data for implicit relevance feedback of a web search engine by com- paring them to eye-tracking data.
2. EXPERIMENTAL DESIGN
For our experiments, we used a collection based on a crawl of 2.7 million records from the book database of the online bookseller Amazon.com. As test subjects, we recruited 12 students of computer science, cognitive and communication science and related fields. After an introduction into the system, users had to work on three tasks, with a time limit of 20 minutes per task. Due to space limitations, we only discuss the results of the ‘complex’ task type here, where users were asked to find books about one of the following topics: 1) Trustworthy books about 9/11 terrorist attacks, 2) Controversial books discussing the climate change, 3) Highly acclaimed novels about racial discrimination.
The user interface of our system consists of four major areas: a query input field, a list of result items, a detail area showing all available data about the currently selected document, and a basket where users should place docu- ments they deemed relevant. We wanted to collect eye- tracking data for each specific result list item, even when the user is scrolling down in this list; however, standard eye-tracking software allows only for the monitoring of static ‘areas of interest’ (AOI). Thus, we developed a framework called AOILog which automatically keeps track of position, visibility and size of all user interface objects at any point in time. Combining this information with the eye-tracking data, we always know at which object the user is currently looking.1
3. RESULTS AND ANALYSIS
Our analysis is based on the combination of logging and eye-tracking data. For the latter, as in other studies, we focus on the so-called fixations, and also consider them only if they last for at least 80 ms, since this is the minimum time required for reading anything on the screen [2].
Corresponding to the four areas of the user interface, we can distinguish four types of user actions: formulating quer- ies, looking at a result item, regarding document details, and
1AOILog is an open source software; it can be easily inte- grated into any user interface based on Java Swing.
Based on a new framework for capturing dynamic areas of interest in eye-tracking, we model the user search process as a Markov-chain. The analysis indicates possible system improvements and yields parameter estimates for the Inter- active Probability Ranking Principle (IPRP).
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval—Search Process
General Terms
User Study, Interactive Retrieval
1. INTRODUCTION
Although interactive retrieval systems are a commodity today, the theoretic foundation for this type of systems is rather scarce. A first attempt in this direction was the IPRP formulated in [3]. However, there has been little work build- ing upon this framework. Although there have been many user studies of search behaviour, none of them provides re- sults that could be used for improving the functional level of an IR system.
In this paper, we present a new methodology for analysing interactive IR, extending current approaches by separating between eye-tracking and action level, and by considering dynamic areas of interest for the former. An example study demonstrates the feasibility of our approach, yielding sug- gestions for system improvements as well as parameter esti- mates for the IPRP.
In the following, after giving a survey over related work, we describe our test setting, and then analyse the experi- mental results, before we come to the conclusion.
The IPRP assumes that a user moves between situations si, in each of which the system presents a list of choices, about which the user has to decide, and the first accepted choice moves the user to a new situation. Each choice cij is associated with three parameters: the effort eij for con- sidering this choice, the acceptance probability pij , and the benefit aij resulting from the acceptance. Below, we show
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$10.00.

  Query 4,9 sec
3 %
Result
Item
2,3 sec
87 %
15 %
Basket 1,7 sec
Detail 15,3 sec
decrease, thus leading to a reduction of the overall ef- fort). Furthermore, we also see that click-through data is a poor indicator of relevance.
4. As mentioned above, users often compare the current document with entries from the basket. This could be supported by a similarity search function, which show the current document and similar one from the basket side-by-side.
Now we analyse this data with regard to the IPRP. The timings correspond to the effort eij for evaluating a choice cij, while the transition probabilities give the chances pij of accepting it. As a possible approach for quantifying the benefit aij of a decision, we can regard the time needed for finding the first (next) relevant document. For that, we compute the expected time for reaching the basket. Here we can apply the method for computing “first passage times” in Markov networks, which leads to a linear equation system. As results, we get 127.9 s for the query, 123.0 s for the result list and 109.5 s for the details stage. The benefits can then be defined as the time differences between the two situations of a transition invoked by accepting a choice. As could be expected, the biggest benefits are achieved when moving to the basket—but the corresponding acceptance probabilities are low. On the other hand, there are also choices with negative benefits (when going back to query reformulation or from detail to result). While the order of the benefits seems ok, negative benefits are in contradiction to the IPRP, which says that the corresponding choices are useless and thus should be avoided. This is a more general problem of parameter estimation for the IPRP: when reformulating the query, users do not really go back to the initial situation, they submit an improved query. For considering that, we would need a more complex Markov model, and a much larger number of observations.
4. CONCLUSION
In this paper, we extended current methodologies for ana- lyzing interactive IR by separating between eye-tracking and action level, and by implementing dynamic areas of interest. Our example study shows the findings point to possible sys- tem improvements, and that click-through rates are a poor indicator of relevance. Finally, we can immediately derive parameters for the IPRP, although open problems remain.
Acknowledgement
This work was supported by the Germen Science Foundation under grant no. FU 205/24-1.
5. REFERENCES
[1] L. Azzopardi. The economics in interactive information retrieval. SIGIR ’11, pages 15–24, New York, NY, USA, 2011. ACM.
[2] G. Buscher, A. Dengel, and L. van Elst. Eye movements as implicit relevance feedback. In CHI ’08, pages 2991–2996, New York, NY, USA, 2008. ACM.
[3] N. Fuhr. A probability ranking principle for interactive information retrieval. Information Retrieval, 11(3):251–265, 2008.
[4] Z. Guan and E. Cutrell. An eye tracking study of the effect of target rank on web search. In CHI ’07, pages 417–420, New York, NY, USA, 2007. ACM.
[5] T. Joachims, L. Granka, B. Pan, H. Hembrooke,
F. Radlinski, and G. Gay. Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search. ACM Trans. Inf. Syst., 25(2), 2007.
  2%
74 %
9 %
24 %
   100 %
85 %
 1%
    Figure 1: Transition probabilities and user efforts
looking at the basket (which can be viewed as a more strate- gic action, where the user is thinking about the continuation of her search). Thus, a one-to-one mapping of eye-tracking data and user actions seems to be straightforward. However, a closer analysis showed that this is too simplistic.
First, we noted that users were frequently looking back and forth between the details and the basket. The behaviour was due to the fact that the book database often contains very similar entries (e.g., different editions of a book), thus forcing users to check if the current result item was substan- tially different from the books already placed in the basket. Occasionally, this also happened for items in the result list. In a similar way, when formulating a new query, users did not only look at the query field, but also checked the details of the current document as well as the items in the basket.
In order to deal with these problems, we separate between two levels, the eye-tracking level and the action level. Then we define a mapping from the former to the latter, which also considers the logging data. By default, the area the user looks at defines the current action, with the following exceptions:
• When the user looks from a result item to the basket and back, without moving the item to the basket, this is counted as part of regarding the item.
• The same holds for looking back and forth between details and the basket.
• Query formulation starts when the user’s gaze wanders from the details or the basket to the query field for the first time, even when it returns to the basket/details several times before the query is actually submitted.
Applying these rules resulted in the transition probabil- ities and average times spent for the different actions dis- played in Figure 1. From this data, a number of observations can be made:
1. Query formulation is roughly the same time as looking at two result items.
2. Retrieval quality is surprisingly low—only about 3 % of the items in the result list are relevant. This is mainly due to the complexity of the retrieval tasks.
3. Since only 24 % of the details regarded are judged as relevant, the quality of the result entries could be im- proved, in order to increase this rate (while the prob- ability of the transition from result to details would
Using Eye-Tracking with Dynamic Areas of Interest for Analyzing Interactive Information Retrieval
ABSTRACT
Vu T. Tran Information Engineering University of Duisburg-Essen Duisburg, Germany
vtran@is.inf.uni-due.de
Norbert Fuhr Information Engineering University of Duisburg-Essen Duisburg, Germany
norbert.fuhr@uni-due.de
how the first two parameters can be estimated in our setting, while the third one still is an open research issue.
By applying economic theory to IIR Azzopardi modelled the process of interaction between a user and a system and constructed a cost function to measure the user effort [1]. Cutrell and Guang [4] used eye-tracking to explore the ef- fects of different presentations of search results. Joachims et al. [5] examined the reliability of click-through data for implicit relevance feedback of a web search engine by com- paring them to eye-tracking data.
2. EXPERIMENTAL DESIGN
For our experiments, we used a collection based on a crawl of 2.7 million records from the book database of the online bookseller Amazon.com. As test subjects, we recruited 12 students of computer science, cognitive and communication science and related fields. After an introduction into the system, users had to work on three tasks, with a time limit of 20 minutes per task. Due to space limitations, we only discuss the results of the ‘complex’ task type here, where users were asked to find books about one of the following topics: 1) Trustworthy books about 9/11 terrorist attacks, 2) Controversial books discussing the climate change, 3) Highly acclaimed novels about racial discrimination.
The user interface of our system consists of four major areas: a query input field, a list of result items, a detail area showing all available data about the currently selected document, and a basket where users should place docu- ments they deemed relevant. We wanted to collect eye- tracking data for each specific result list item, even when the user is scrolling down in this list; however, standard eye-tracking software allows only for the monitoring of static ‘areas of interest’ (AOI). Thus, we developed a framework called AOILog which automatically keeps track of position, visibility and size of all user interface objects at any point in time. Combining this information with the eye-tracking data, we always know at which object the user is currently looking.1
3. RESULTS AND ANALYSIS
Our analysis is based on the combination of logging and eye-tracking data. For the latter, as in other studies, we focus on the so-called fixations, and also consider them only if they last for at least 80 ms, since this is the minimum time required for reading anything on the screen [2].
Corresponding to the four areas of the user interface, we can distinguish four types of user actions: formulating quer- ies, looking at a result item, regarding document details, and
1AOILog is an open source software; it can be easily inte- grated into any user interface based on Java Swing.
Based on a new framework for capturing dynamic areas of interest in eye-tracking, we model the user search process as a Markov-chain. The analysis indicates possible system improvements and yields parameter estimates for the Inter- active Probability Ranking Principle (IPRP).
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval—Search Process
General Terms
User Study, Interactive Retrieval
1. INTRODUCTION
Although interactive retrieval systems are a commodity today, the theoretic foundation for this type of systems is rather scarce. A first attempt in this direction was the IPRP formulated in [3]. However, there has been little work build- ing upon this framework. Although there have been many user studies of search behaviour, none of them provides re- sults that could be used for improving the functional level of an IR system.
In this paper, we present a new methodology for analysing interactive IR, extending current approaches by separating between eye-tracking and action level, and by considering dynamic areas of interest for the former. An example study demonstrates the feasibility of our approach, yielding sug- gestions for system improvements as well as parameter esti- mates for the IPRP.
In the following, after giving a survey over related work, we describe our test setting, and then analyse the experi- mental results, before we come to the conclusion.
The IPRP assumes that a user moves between situations si, in each of which the system presents a list of choices, about which the user has to decide, and the first accepted choice moves the user to a new situation. Each choice cij is associated with three parameters: the effort eij for con- sidering this choice, the acceptance probability pij , and the benefit aij resulting from the acceptance. Below, we show
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$10.00.

  Query 4,9 sec
3 %
Result
Item
2,3 sec
87 %
15 %
Basket 1,7 sec
Detail 15,3 sec
decrease, thus leading to a reduction of the overall ef- fort). Furthermore, we also see that click-through data is a poor indicator of relevance.
4. As mentioned above, users often compare the current document with entries from the basket. This could be supported by a similarity search function, which show the current document and similar one from the basket side-by-side.
Now we analyse this data with regard to the IPRP. The timings correspond to the effort eij for evaluating a choice cij, while the transition probabilities give the chances pij of accepting it. As a possible approach for quantifying the benefit aij of a decision, we can regard the time needed for finding the first (next) relevant document. For that, we compute the expected time for reaching the basket. Here we can apply the method for computing “first passage times” in Markov networks, which leads to a linear equation system. As results, we get 127.9 s for the query, 123.0 s for the result list and 109.5 s for the details stage. The benefits can then be defined as the time differences between the two situations of a transition invoked by accepting a choice. As could be expected, the biggest benefits are achieved when moving to the basket—but the corresponding acceptance probabilities are low. On the other hand, there are also choices with negative benefits (when going back to query reformulation or from detail to result). While the order of the benefits seems ok, negative benefits are in contradiction to the IPRP, which says that the corresponding choices are useless and thus should be avoided. This is a more general problem of parameter estimation for the IPRP: when reformulating the query, users do not really go back to the initial situation, they submit an improved query. For considering that, we would need a more complex Markov model, and a much larger number of observations.
4. CONCLUSION
In this paper, we extended current methodologies for ana- lyzing interactive IR by separating between eye-tracking and action level, and by implementing dynamic areas of interest. Our example study shows the findings point to possible sys- tem improvements, and that click-through rates are a poor indicator of relevance. Finally, we can immediately derive parameters for the IPRP, although open problems remain.
Acknowledgement
This work was supported by the Germen Science Foundation under grant no. FU 205/24-1.
5. REFERENCES
[1] L. Azzopardi. The economics in interactive information retrieval. SIGIR ’11, pages 15–24, New York, NY, USA, 2011. ACM.
[2] G. Buscher, A. Dengel, and L. van Elst. Eye movements as implicit relevance feedback. In CHI ’08, pages 2991–2996, New York, NY, USA, 2008. ACM.
[3] N. Fuhr. A probability ranking principle for interactive information retrieval. Information Retrieval, 11(3):251–265, 2008.
[4] Z. Guan and E. Cutrell. An eye tracking study of the effect of target rank on web search. In CHI ’07, pages 417–420, New York, NY, USA, 2007. ACM.
[5] T. Joachims, L. Granka, B. Pan, H. Hembrooke,
F. Radlinski, and G. Gay. Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search. ACM Trans. Inf. Syst., 25(2), 2007.
  2%
74 %
9 %
24 %
   100 %
85 %
 1%
    Figure 1: Transition probabilities and user efforts
looking at the basket (which can be viewed as a more strate- gic action, where the user is thinking about the continuation of her search). Thus, a one-to-one mapping of eye-tracking data and user actions seems to be straightforward. However, a closer analysis showed that this is too simplistic.
First, we noted that users were frequently looking back and forth between the details and the basket. The behaviour was due to the fact that the book database often contains very similar entries (e.g., different editions of a book), thus forcing users to check if the current result item was substan- tially different from the books already placed in the basket. Occasionally, this also happened for items in the result list. In a similar way, when formulating a new query, users did not only look at the query field, but also checked the details of the current document as well as the items in the basket.
In order to deal with these problems, we separate between two levels, the eye-tracking level and the action level. Then we define a mapping from the former to the latter, which also considers the logging data. By default, the area the user looks at defines the current action, with the following exceptions:
• When the user looks from a result item to the basket and back, without moving the item to the basket, this is counted as part of regarding the item.
• The same holds for looking back and forth between details and the basket.
• Query formulation starts when the user’s gaze wanders from the details or the basket to the query field for the first time, even when it returns to the basket/details several times before the query is actually submitted.
Applying these rules resulted in the transition probabil- ities and average times spent for the different actions dis- played in Figure 1. From this data, a number of observations can be made:
1. Query formulation is roughly the same time as looking at two result items.
2. Retrieval quality is surprisingly low—only about 3 % of the items in the result list are relevant. This is mainly due to the complexity of the retrieval tasks.
3. Since only 24 % of the details regarded are judged as relevant, the quality of the result entries could be im- proved, in order to increase this rate (while the prob- ability of the transition from result to details would
  CHI 2011 • Work-in-Progress
May 7–12, 2011 • Vancouver, BC, Canada
Vidhya Navalpakkam
Abstract
Yahoo! Research
Santa Clara, CA nvidhya@yahoo-inc.com
We analyze gaze patterns to study how users in online reading environments cope with visual distraction, and we report gaze markers that identify reading difficulties due to distraction. The amount of visual distraction is varied from none, medium to high by presenting irrelevant graphics beside the reading content in one of 3 conditions: no graphic, static or animated graphics. We find that under highly-distracting conditions, a struggling reader puts more effort into the text — she takes a longer time to comprehend the text, performs more fixations on the text and frequently revisits previously read content. Furthermore, she reports an unpleasant reading experience. Interestingly, we find that whether the user is distracted and struggles or not can be predicted from gaze patterns alone with up to 80% accuracy and up to 15% better than with non- gaze based features. This suggests that gaze patterns can be used to detect key events such as user struggle/frustration while reading.
Justin M. Rao
Yahoo! Research Santa Clara, CA jmrao@yahoo-inc.com
Malcolm Slaney
Yahoo! Research Santa Clara, CA
malcolm@ieee.org
Copyright is held by the author/owner(s).
CHI 2011, May 7–12, 2011, Vancouver, BC, Canada. ACM 978-1-4503-0268-5/11/05.
Visual distraction, animated graphics, eye-tracking, reading, user experience, gaze patterns
Using Gaze Patterns to Study and Predict Reading Struggles due to Distraction
Keywords
ACM Classification Keywords
H.5.2 Information Interfaces and Presentation: User interfaces Evaluation/ methodology.
1705
 CHI 2011 • Work-in-Progress
May 7–12, 2011 • Vancouver, BC, Canada
Figure 1: Distraction conditions and sample displays used in the experiment. The animated graphic was similar to the static graphic in color, shape etc., but was blinking and moving randomly.
Previous studies have analyzed gaze patterns during reading (for a review, see [12]). They show that gaze pauses at some words for 100-500ms (fixations) and jumps between a few words (saccades). While most of these saccades are forward (eye moves left to right on a line, and down to the next line), occasionally, there are backward saccades (regressions). These are thought to reflect confusion and difficulty in comprehension [11, 13]. Gaze patterns are known to vary depending on the age [1], skill-level [10, 13] and reading task difficulty [6, 7]. We extend these studies by testing how visual distraction due to graphics — a
An innovation of our study is that we use gaze patterns as a behavioral measurement tool and a predictive tool of subjective user experience. Most previous studies measure the effect of age, skill, difficulty level etc. on gaze patterns while reading. Here, we attempt to go beyond measurement of reading behavior to ask whether we can predict the reader’s mental state, such as whether she struggles and finds the reading experience unpleasant. Identifying these gaze-based markers allows for one to build a user-interface that is informed by objective signals of user experience. Examples of such a corrective approach include gaze- based reading assistants that infer reading difficulty from long pauses on words, for example, and offer remedies such as presenting the meaning of the word to facilitate reading [3, 14]. To our knowledge, no one has used gaze patterns to measure and predict the
General Terms
widely prevalent factor in online reading environments — affects gaze patterns as well as the reader’s mental state.
Human Factors, Experimentation, Measurement, Performance
Introduction
Most previous work on the effect of distraction on gaze patterns has used the reading-with-text-distraction paradigm [5], using distractors of different color, text style (e.g., italic vs. upright) [8], or semantic meaning than the words in the text [9]. These studies differ from ours in three important ways — they did not study distraction due to graphics, did not vary the distraction- level systematically, nor did they study how distraction affects the reader’s mental state.
Online environments present several obstacles to readers in the form of visual distraction, including pop- up windows, email alerts, IM, ads, pictures, videos etc. This raises the important question of how visual distraction affects a reader's attention and mental state, and bears consequences for the web and other online reading environments. Indeed, the method in which many websites are monetized, through presenting ads alongside content, fundamentally distracts the user. In this eye-tracking study we address the question of how such distraction impacts the user. To do so, we vary the amount of visual distraction from none, medium to high by presenting no graphics, static or animated irrelevant graphics alongside the reading content (see Figure 1). We study the extent to which distraction affects reading time, comprehension, gaze patterns and the reader's experience, measured by the reported interestingness and pleasantness of reading experience. Our analysis shows how users are distracted by animated graphics, and which gaze markers are important predictors of a user struggling to read.
In a design similar to ours, [2] measured how pictures affect gaze patterns while reading. The study used relevant pictures and ads (both static) and found that the total dwell time is longer and regressions are higher for relevant pictures (presumably in trying to relate the picture to the text). As we are interested in studying the effects of visual distraction, we use irrelevant graphics that are static (less distracting) or animated (more distracting).
1706
 CHI 2011 • Work-in-Progress
May 7–12, 2011 • Vancouver, BC, Canada
Figure 2: Panels A-C show the heatmaps for the different conditions. Guess which is what? The answer is provided at the end of this paragraph. The hotspots (page regions that are most viewed by users) are shown in red, and the coolspots are in blue.
Apparatus: Werecordedsubjects'gazepatterns during task performance using a Tobii 1750 eye tracker (50Hz sampling frequency), with a 17" LCD monitor, set at resolution 1024x768, at roughly 85 cm viewing distance. We collected a log of eye and mouse movement.
Figure 2 shows the user interface and heat maps of fixations. It is immediately evident that the conditions differed significantly, with the difference mainly occurring on the text regions of the UI. Figures 3 and 4 show the metrics that differed significantly due to distraction (as computed by one-way repeated measures ANOVA). These are detailed below.
Answer: the conditions are static, animated and no graphics respectively.
Participants: There were 20 subjects aged 19-60, with normal or corrected vision. Subjects were paid $1 for every correct answer (5 questions per essay x 3 essays). During data cleaning, 3 subjects were excluded for the following reasons: poor calibration (2
We first report the results from the non-gaze metrics. Subjects reported all the essays to be equally interesting (mean rating of 4.0 out of 5, all stories
user’s distraction level due to irrelevant graphics, such as widely prevalent on the web.
subjects did not maintain their head in the correct position), or outliers in fixation duration or number of fixations (3 standard deviations, 1 subject).
Here, we use gaze patterns to show how people cope with visual distraction while reading: they struggle to ignore the distraction and work hard to focus on the text. This is reflected in more time on text, increased fixation duration, more fixations, more revisits to previously read content and poor pleasantness ratings. We show that the user's mental state — the pleasantness ratings that indicate whether the user struggles or not — can be predicted reasonably well by looking at the gaze patterns alone, demonstrating the utility of gaze as a measurement and predictive tool for understanding reading behavior.
Procedure: The study began with a 5-point calibration procedure followed by the task-instruction screen. This was followed by one practice essay paired with animated graphics to help subjects familiarize themselves with the task, types of graphics and the format of the questions in the reading-comprehension test. Next, subjects saw the 3 essays randomly paired with no, static or animated graphics. At the conclusion of the study, subjects were paid based on their performance.
Experiment
Design: The experiment consisted of a within-subject randomized design with 3 factors: no, medium and high distraction created by presenting no, static and animated graphics. Each subject saw 3 essays (from the Test of English Language Fluency), that were randomly paired with one of the graphic types. Each essay consisted of 300-400 words, followed by 5 factual/theme-based multiple-choice questions, and 2 subjective questions where subjects were asked to rate their user experience on a scale of 1-5 for interestingness and pleasantness (e.g., 1 for least pleasant and 5 for most pleasant).
Results
Gaze-based measurement of distraction
We considered 14 metrics in total. The non-gaze metrics are the reading time and comprehension score. The gaze-based metrics are the number of fixations on the graphic, fixation duration on the graphic, time to first visit the graphic, total time on the graphic, number of fixations on the text, fixation duration on the text, saccade length, re-reading, saccade length of re- reading and the total time on text. Apart from the above 12 objective metrics, we also considered the user's self-reported mental state in the form of pleasantness ratings and interestingness ratings of reading experience (on a rating scale of 1-5 for 1 being very bad and 5 being very good).
1707
 CHI 2011 • Work-in-Progress
May 7–12, 2011 • Vancouver, BC, Canada
Figure 3: Panels A-C show some metrics that differed significantly when the distraction level increased from none to medium to high.
Second, the animated graphics are seen much earlier than static or no graphics (within 10 seconds of page onset, compared to 30 seconds for the no and static graphic; Figure 4B). This is consistent with studies on attention from cognitive psychology which show that moving objects appear more salient and capture attention more rapidly than static objects [4, 15]. However, they are quickly rejected as irrelevant, and this is reflected by the small fixation duration on animated graphics (250ms compared to 450ms for no and static graphics, F(2,48)=3.27, p=0.05, one-way ANOVA; Figure 4C). Thus, the animated distractors capture overt attention, but do not sustain it.
The prediction accuracy using all 12 metrics was 87%. The most predictive metrics were the graphics type (82% accuracy), followed by two gaze-based metrics: the amount of re-reading (75%) and time to first fixation on the graphic (70%). The remaining metrics were less predictive (<70%). Combining the two gaze- based metrics lead to 80% prediction accuracy. In comparison, the non-gaze metrics (time on page and accuracy) were less predictive (74% accuracy).
within a half standard deviation of each other), regardless of the amount of distraction. There were no significant differences in accuracy of reading comprehension; subjects got an average of nearly 4 correct answers per essay across conditions. But there were differences in the reading time. When the distraction is absent (no graphics) or mild (static graphics), users report higher pleasantness levels and spend less time comprehending the text (Figures 3A and 3B). These differences are significant as given by a one-way repeated measures ANOVA test (F(2,48)=23.08, p=0.00 and F(2,48)=3.47, p=0.04 respectively).
The reason for the poor reading performance and experience could be due to covert attention on the distractors using peripheral vision. This is reflected in increased cognitive cost of processing the text, as reflected in longer time on text (and page) and more fixations on text. In particular, the “regressions”, revisits to previously read content, which we define as "re-reading," are 30% more frequent with animated graphics (Figure 4A). The difference is significant (F(2,48)=3.61, p=0.03, one-way ANOVA). No differences were observed in other metrics such as saccade length and saccade length of regressions. Our results show that users struggle to ignore the animated distractors and work hard to focus on the text.
The results from gaze patterns offer valuable insights on why the reading performance and experience lag in the animated graphics condition. First, the poor reading performance is not due to overt attention or fixations on the distractor. Although the animated distractors were always noticed and reported as distracting (based on verbal reports by subjects at the end of the study), they were barely fixated (<5% fixations on the graphics) and overt attention was mainly deployed on the text. There was little time spent on the graphics (<3 seconds) regardless of its type; total dwell time on the graphic did not significantly differ (F(2,48)=1.15, p=0.32, one-way ANOVA). The differences in total time on page are entirely driven by the time on text.
Gaze-based prediction of distraction
Can gaze patterns be used to predict whether the user struggles or not while reading? We fed the 12 metrics discussed earlier as input features to a decision-tree classifier, with the pleasantness of user experience (not pleasant: ratings<2.5, pleasant: ratings>=2.5) as the output variable. Specifically, there were 9 gaze-based metrics and 3 non-gaze metrics (reading comprehension accuracy, time on page, graphics type). To avoid overfitting, we computed the leave-one-out cross validation error as a function of tree height, and selected the tree height that minimized the cross- validation error.
To test whether gaze patterns are predictive simply because they co-vary with a strong predictor like
1708
 CHI 2011 • Work-in-Progress
May 7–12, 2011 • Vancouver, BC, Canada
Figure 4: Panels A-C show some gaze- based metrics that differed significantly when the distraction level increased from none to medium to high.
We thank Robert Dougherty, and the IE group at Yahoo! Research for their valuable feedback, especially Elizabeth Churchill. We thank Bob Moore and Prasad Kantamneni for helping setup the eye tracking study.
distractor type, we repeated the above analysis for the strong distractor condition (we focused on this condition due to the higher variability in subjects' experience; in the no/weak distractor condition, the experience was always pleasant, hence little or no variability). We find that non-gaze metrics are no longer informative about reading experience (60% accuracy), scarcely superior to chance. In contrast, gaze patterns continue to predict experience well (72% accuracy). Confirming the above findings, the most predictive gaze-based metrics were the amount of re- reading and time to first fixation on the graphic; these were sufficient to predict UX as well as all 9 gaze- metrics combined. These gaze based metrics can be used in future usability studies as objective quantifiers of subjective user experience. We view this as a promising advance, especially given that technological progress is making eye-tracking more accessible.
the text, significantly higher amount of revisits to previously read content and poor reading satisfaction levels reported by the user. In other words, the user struggles to cope with the strong visual distraction.
Discussion and Conclusion
An important contribution of this study is that we show that gaze patterns can be used as a predictive tool to detect distraction-induced struggles in reading. We trained a decision-tree classifier to predict pleasantness of reading experience from various gaze- and non-gaze metrics. We find that gaze-based prediction of user pleasantness level is up to 80% accurate and up to 15% better than non-gaze based prediction. The most predictive signals are the amount of revisits to previously read content and the time to first fixate the distractor. To summarize, we use gaze patterns to measure the effects of distraction, and more importantly predict distraction levels while reading. This suggests that gaze-tracking is useful beyond usability studies to predicting key events such as whether the user is frustrated or not.
We use gaze patterns to measure and predict distraction-induced struggles in reading. When the distraction is absent or mild (such as due to static graphics), we find that readers take less time comprehending the text and report a pleasant reading experience. In contrast, under high levels of distraction (such as due to salient, animated graphics), users behave differently — they take much longer to comprehend the text, report unpleasant reading experience, and their gaze patterns reveal important differences. For example, a strong visual distractor causes eye capture. Although users quickly reject it as irrelevant and deploy further eye fixations on the text, it continues to attract attention covertly through peripheral vision, which leads to an increased cost of processing the text. This is reflected in longer time to comprehend the text, higher number of eye fixations on
Potential applications of gaze-based prediction include developing reading assistants that take corrective measures to decrease visual distraction upon detecting reading struggle. On the web, this would translate to updating page and graphics properties (e.g., ads, pictures, videos) to enable better reading experience and avoid page abandonment due to reading struggle.
Acknowledgements
1709
CHI 2011 • Work-in-Progress
May 7–12, 2011 • Vancouver, BC, Canada
References
[8] S. Kemper and J. McDowd. Eye movements of young and older adults while reading with distraction. Psychology and aging, 21(1):32, 2006.
[1] L. Abel, B. Troost, and L. Dell’Osso. The effects of age on normal saccadic characteristics and their variability. Vision Research, 23(1):33–37, 1983.
[9] I. Mund, R. Bell, and A. Buchner. Age differences in reading with distraction: Sensory or inhibitory deficits? Psychology and aging, 2010.
[2] D. Beymer, P. Orton, and D. Russell. An eye tracking study of how pictures influence online reading. In Proceedings of the 11th IFIP TC 13 international Conference on Human-computer Interaction-Volume Part II, pages 456–460. Springer-Verlag, 2007.
[10] W. Murray and A. Kennedy. Spatial coding in the processing of anaphor by good and poor readers: Evidence from eye movement analyses. The Quarterly Journal of Experimental Psychology Section A, 40(4):693–718, 1988.
[3] R. Biedert, G. Buscher, and A. Dengel. The eyebook–using eye tracking to enhance the reading experience. Informatik-Spektrum, 33(3):272–281, 2010.
[11] A. Poole. Differences in Reading Strategy Use Among ESL College Students. Journal of College Reading and Learning, 36:1, 2005.
[4] R. Carmi and L. Itti. Visual causes versus correlates of attentional selection in dynamic scenes. Vision Research, 46(26):4333–4345, 2006.
[12] K. Rayner. Eye movements in reading and information processing: 20 years of research. Psychological bulletin, 124:372–422, 1998.
[5] S. Connelly, L. Hasher, and R. Zacks. Age and reading: The impact of distraction. Psychology and Aging, 6(4):533–541, 1991.
[13] K. Rayner and A. Well. Effects of contextual constraint on eye movements in reading: A further examination. Psychonomic Bulletin & Review, 1996.
[6] J. Henderson and F. Ferreira. Effects of foveal processing difficulty on the perceptual span in reading: Implications for attention and eye movement control. Journal of Experimental Psychology: Learning, Memory, and Cognition, 16(3):417–429, 1990.
[14] J. Sibert, M. Gokturk, and R. Lavine. The reading assistant: eye gaze triggered auditory prompting for reading remediation. In Proceedings of the 13th annual ACM symposium on User interface software and technology, pages 101–107. ACM, 2000.
[7] J. Jacobson and P. Dodwell. Saccadic eye movements during reading. Brain and Language, 1979.
[15] J. Theeuwes. Exogenous and endogenous control of attention: The effect of visual onsets and offsets. Perception & Psychophysics, 49(1):83, 1991.
Visual Analysis of Eye Tracking Data Michael Raschke, Tanja Blascheck, and Michael Burch
Abstract Eye tracking has become a valuable approach to evaluate visualization techniques in a user centered design process. Apart from just relying on task accuracies and completion times, eye movements can additionally be recorded to later study visual task solution strategies and the cognitive workload of study participants. During an eye tracking experiment many data sets are recorded. Standard techniques to analyze this eye tracking data are heat map and scan path visualizations. However, it still requires a high effort to analyze scan path trajectory data to find common task solution strategies among the study participants. In this chapter we discuss three existing methodologies for analyzing the vast amount of eye tracking data from a visualization and visual analytics perspective. These three approaches are a classical static visualization, visual analytics techniques and finally a software prototype, which helps the user to manage, view and analyze the recorded data in a simple interactive way.
1 Introduction
A key factor for the readability and thus, for the success of an information visualization is its ergonomics. To evaluate visualizations user experiments have to be performed to study the readability, efficiency and cognitive workload by controlled experiments, usability tests, longitudinal studies, heuristic evaluations, or cognitive walkthroughs [14, 22] with controlled experiments being the primary
M. Raschke ( ) • T. Blascheck
Institute for Visualization and Interactive Systems, University of Stuttgart, Stuttgart, Germany e-mail: michael.raschke@vis.uni-stuttgart.de; tanja.blascheck@vis.uni-stuttgart.de
M. Burch
Postdoctoral Researcher of Computer Science, Visualization Research Center (VISUS), University of Stuttgart, Stuttgart, Germany
e-mail: michael.burch@visus.uni-stuttgart.de
W. Huang (ed.), Handbook of Human Centric Visualization, 391 DOI 10.1007/978-1-4614-7485-2 15, © Springer ScienceCBusiness Media New York 2014

392 M. Raschke et al.
 Fig. 1 Most prominent visualization techniques for eye tracking data: Heat maps are time aggregated density based representations (left), scan paths are line based visualizations suffering from overdrawing and visual clutter (right)
method for evaluating visualizations [7]. Since the recording of eye movements became easier during the last decade, many user study designers additionally use eye tracking techniques.
Standard metrics to evaluate visualizations are accuracy rates and completion times. Accuracy rates show how many correct answers have been given by a participant while carrying out one or more given tasks. The completion times indicate the time participants needed to perform a given task. Both metrics can be analyzed by standard statistics techniques such as t-tests, ANOVA, and the like.
Additionally, eye tracking techniques provide information about eye movements of a participant during a user experiment. In most cases the participants’ fixation positions on the screen, the fixation durations and the scan path structure is of interest. Two very common visualization techniques for this spatio-temporal data are heat maps and scan paths, see Fig. 1. Both techniques are very common, but have some drawbacks concerning visual clutter (scan paths) and time aggregation (heat maps). Furthermore, detailed analysis of recorded eye tracking patterns requires a high effort. For example, a user experiment with 30 participants, three types of tasks and with 30 stimuli for each task leads to 2,700 scan paths in total. Each scan path typically consists of a high number of fixations. In this example, more then 10,000 extra data points have to be stored, formatted, and visualized to finally prove or disprove a hypothesis.
Visual Analysis of Eye Tracking Data 393
 Fig. 2 This chapter will present three visual analysis approaches to deal with the large amount of data recorded in an eye tracking experiment through classical static visualization, visual analytics, and a prototype to interactively manage, view, and analyze the recorded data
   Eye tracking techniques provide additional information about the eye move- ments of a participant during a user experiment besides the standard metrics accuracy rates and completion times. However, a detailed analysis of recorded eye tracking patterns requires a high effort. This chapter discusses three approaches, which deal with the visual analysis of a large amount of data recorded in an eye tracking experiment: a classical static visualization, a visual analytics approach and a prototype to interactively manage, view and analyze the recorded data.
 This chapter will discuss this issue and will present three approaches to deal with the large amount of data recorded in an eye tracking experiment (Sect. 3). These three approaches are (cf. Fig. 2): (1) a classical static visualization which shows important metrics of an eye tracking experiment at a glance (Sect. 4.1), (2) a visual analytics approach to analyze the statistic properties of the spatio temporal eye tracking data structure (Sect. 4.2), (3) a software prototype, which helps the user to manage, view and analyze the recorded data in a simple interactive way (Sect. 4.3). Finally, we will compare these three approaches with each other and with standard
394 M. Raschke et al.
techniques of eye tracking data analysis concerning the user centered design process of visualizations (Sect. 5). The next section will give an overview about eye tracking and existing visual analysis techniques for eye tracking data.
2 Background
2.1 Eye Tracking Experiments
Often, the purpose of visualizations is to present information in an easy, perceivable, and understandable way. To support successful designing process of visualizations or to prove the readability of a new visualization technique, evaluation experiments are carried out. Some user experiments are recording eye movements to study the task solution strategies or the cognitive workload of a visualization. An illustrative eye tracking study is described by Goldberg, which shows fundamental aspects of eye tracking experiments [8]. Further examples are the comparison of graph layouts [5], or a user study with cloud visualizations [13] to study the tag cloud perception and performance with respect to different user goals. One crucial step during the analysis of user study results is the comparison of scan paths. In many cases, the analysis aims at finding common eye movement patterns, which can be interpreted as common cognitive strategies to perform a given task. Thereby, eye tracking metrics such as completion times, fixations, fixation rates, pupil sizes and others can be used [11, 16]. For a detail description of eye tracking experiments, please read the chapter by Goldberg in this book.
2.2 Visualization of Eye Tracking Data
Visualizations are used to study eye tracking data to find interesting scan path structures. Today, there is already a large number of visualization techniques for representing eye tracking data. But only a small number of them have been integrated in existing eye tracking devices. Heat maps [4] or scan paths [6] are the most prominent ones, that focus on a direct exploration of the recorded eye tracking data, see Fig. 3.
But these simple visualization approaches also have some drawbacks: Heat maps are problematic, because they only show a density based representation of the time varying trajectory data. Such a diagram only shows the hot spots where participants have focused on more frequently and where not. There is no information about a sequential order of visited areas of interest (AOIs) or points of interest (POIs). Hence, such heat maps belong to the class of time aggregated diagrams. Scan paths on the other hand allow the exploration of the time varying behavior by using a line based trajectory representation where all single trajectories for each participant are drawn on top of each other. This approach soon leads to the drawback known as visual clutter [19] as many trajectories have to be plotted in one diagram to compare them.
Visual Analysis of Eye Tracking Data 395
 Fig. 3 Most prominent visualization techniques for eye tracking data: (a) Heat maps are time aggregated density based representations. (b) Scan paths are line based visualizations suffering from overdrawing and visual clutter
Gaze replays can be used in an animated fashion to show the sequence of eye fixations for a certain number of participants. The general drawback of animated diagrams is that the human’s short term memory can only remember a few time steps of this animated gaze replay. Static diagrams or multiple representations will do a better job in many cases [24].
Another technique is used by eSeeTrack which combines a timeline and a tree structured visual representation to extend current eye tracking visualizations by extracting patterns of sequential gaze orderings. Displaying these patterns does not depend on the number of fixations on a scene [23]. Aula et al. present a non- overlapping scan paths technique [3]. Further techniques to analyze eye movement patterns are transition matrices [9, 15] which are based on areas of interest on the stimulus.
2.3 Visual Analytics
Visual analytics approaches benefit from visualization, interaction, applied algo- rithms, data mining, human reasoning, and the like [12, 21]. Applying visual analytics methodologies to eye tracking data has been proposed by Andrienko et al. [1] with the goal to achieve better visualizations for common visual task solution strategies. In the work of Andrienko et al., the authors investigated techniques, succesfully applied to geographic movement data, to analyze vast amounts of eye tracking data recorded during eye tracking experiments.
Since eye tracking data has a spatio temporal structure combined with other meta data about the participants as well as task accuracies and completion times, a single visualization technique such as a heat map or a scan path cannot succeed as a stand alone tool. Visual analytics methods with several views on the same data can be combined, which is known as Multiple Coordinated Views [18]. Interactive features [25] such as brushing and linking can be integrated to inspect aspects
396 M. Raschke et al.
of a data set in different views linked together. The human user has to decide which views are displayed and which portions and aspects of the data are analyzed, preprocessed, and visualized. By using visual analytics, the viewer is hence, not depending on one single visualization technique.
3 Eye Tracking Data
Different data types have to be taken into account when designing and evaluating a user study. Psychology research differentiates between independent and dependent variables. These two types of evaluation variables are also valid for an eye tracking study. In eye tracking one can differentiate between the data which will be recorded by the eye tracker, the benchmarks completion times and accuracy rates, or meta data about the participant itself. This kind of data will be defined, discussed and classified in the following sections.
3.1 Variables
In psychological experiments one differentiates between independent and depended variables. Independent variables will be changed during the experiment to test a hypothesis. Depended variables will not be changed; they are used to investigate the effect the change of the independent variable has on them. Furthermore, there are extraneous variables which might also have an effect on the experiment and should be controlled. Some examples for extraneous variables are the age, gender, or the environmental testing situation. Those variables have to be controlled by either collecting the information, e.g., asking the participant about his or her age and gender, or by controlling it, e.g., all participants perform the experiment in the same room under the same conditions. Additionally, distractor variables are those, which should be removed or controlled to a high degree during an experiment because they can have an influence on the measured variables. For example, the behaviour of the operator can be a distractor variable because his or her behaviour varies over several run through of the experiment.
3.2 Recorded Data
What kind of data will be recorded during the experiment depends on the inves- tigated hypotheses. If a hypothesis is to evaluate two visualizations (independent variable), the most obvious choice is to record the completion times and accuracy rates of the participants (dependent variables). This data will have to be statistically analyzed to prove or disprove the hypothesis. However, this data does not provide
Visual Analysis of Eye Tracking Data 397
 Fig. 4 An eye tracking software records timestamps, fixations, pupil sizes and other information like events for key presses, mouse moves, and project management information such as stimuli file names and identifier for areas of interest
precise information why one visualization performs better than another one. To do so, “soft” metrics such as subjective affection about the visualizations could be recorded by questionnaires during the experiment. One great drawback of these metrics is that although they are quantitative, they represent a very subjective statement by the participants. A more precise metric, which can be used to answer the question why one visualization can be read better than another, are eye tracking metrics. They will be described in the next section.
3.3 Eye Tracking Data
In eye tracking experiments four main data types can be distinguished. A gaze point is a single eye gaze which has a specific position. Many gaze points will be grouped together as a fixation. Each fixation is defined by a two- or three-dimensional position, which is the averaged geometrical position of all gazes. Additionally, every fixation has a fixation duration at this position. If the eye tracker records pupil sizes, information about changes of the pupils is added to the fixation. Between every fixation the eyes change their focus. This focus change is called a saccade. During a saccade the vision is suppressed. The sequence of fixations and saccades are called a scan path (in some literature also called gaze path or gaze trajectory). Figure 4 shows an cutout of a CSV-file exported by the Tobii Studio 2.0 software.
3.4 Eye Tracking Trajectory Data
The analysis of eye tracking data is mostly concerned with obtaining insight from scan paths. In the context of this work, we will concentrate on two-dimensional scan paths. Thus, a scan path T of length n   1 can be expressed as a sequence of n fixations with a two-dimensional position pj
T WDp1 !p2 !:::!pn .pj WD.xj;yj/2N N;1 j  n/:
398 M. Raschke et al.
Each fixation position pj contains a fixation duration tdj 2 N; 1   j   n, expressing the time spent to inspect this point. The time it takes the eye to move from fixation point pj to pj C1 is denoted by tmj . The total time to inspect a stimulus can be expressed by
n n 1 XX
t WD tdj C tmj jD1 jD1
4 Three Visual Analysis Approaches of Eye Tracking Data
The standard analysis method for eye tracking data are visualizations. An advantage of visualizations is that they show data structures in an accessible fashion by exploiting the benefits of visual perception and the strengths of the human visual system. However, static visualizations, in particular line based diagrams, have drawbacks known as visual clutter. To overcome this problem, statistical and interactive techniques can be added to the visualizations. Statistical techniques are used in visual analytics approaches. This section contributes an example for three approaches: (1) the parallel scan path visualization technique, which visualizes eye movements of many participants in a single static visualization with a parallel layout containing various levels of detail, such as fixations, gaze durations, eye shift frequencies and time at a glance, (2) a visual analytics approach to analyze the statistic properties of spatio temporal eye tracking data, and finally (3) eTaddy, which is a software prototype, that helps users to view and analyze their recorded eye tracking data in a simple and interactive way.
4.1 Parallel Scan Path Visualization
The first approach for a visual eye tracking data analysis is the parallel scan path visualization technique (in the following “PSP”). A key feature of the PSP technique is the visualization of eye movements of many participants on a single screen in a parallel layout. The visualization presents various properties of scan paths, such as fixations, gaze durations and eye shift frequencies at one glance. We have developed the PSPs because we wanted to answer the following questions in our eye tracking experiments: Are there any general eye movement patterns when working with simple visualization techniques? Can different user groups be distinguished based on their eye movement patterns? Answering these questions can help to adapt and optimize the layout of visualizations to a specific user group. Our new visualization technique should have a structured layout of scan path lines, that could also be used for a larger number of scan paths, displaying various properties of scan paths, supporting quick detection of common eye movement patterns and studying
Visual Analysis of Eye Tracking Data 399
 Fig. 5 Parallel scan path visualizations map areas of interest, which have been defined on the stimulus, to vertical coordinate axes. The leftmost vertical axis indicates time
temporal properties of scan path. All this information should be viewable at a glance. More details about the PSP technique can be found in [17].
The PSP visualization technique is based on areas of interest and maps gaze durations and fixations to vertical axes. Figure 5 shows a sketch of this approach, where the three areas of interest AOI1, AOI2, and AOI3 are mapped to three coordinate axes. The leftmost axis indicates time, starting from the bottom with the start time of the eye tracking measurement. The orientation of the parallel scan path visualization is arbitrarily represented. In the following we use a vertical time axis from bottom (start of the scan path recording) to top (end of the scan path recording) depending on user preferences. The horizontal axis displays all selected areas of interest as independent values. We have developed three types of parallel scan path visualizations, which we will explain in more detail in the following:
1. Gaze duration sequence diagram: Shows only scan paths.
2. Fixationpointdiagram:Showsscanpathstogetherwithfixationsandfrequencies
of eye movement shifts between areas of interest.
   Key feature of the parallel scan path visualization is the visualization of eye movements of many participants in a single visualization with a parallel layout containing various levels of detail, such as fixations, gaze durations, eye shift frequencies and time.

400 M. Raschke et al.
 Fig. 6 Example stimulus (a) with areas of interest (b). Three visualization techniques have been developed which are using the same visualization concept with a parallel layout: gaze duration sequence diagram (c), fixation point diagram (d), and gaze duration distribution diagram (e). The scan paths of all three visualization techniques are plotted in (f)
3. Gaze duration distribution diagram: Shows scan paths together with information about frequencies of eye movement shifts between areas of interest and gaze durations in areas of interest.
4.1.1 Gaze Duration Sequence Diagram
Figure 6c shows a sketch of a gaze duration sequence diagram. This example shows the scan path of one participant. Every continuous line on a vertical axis represents a gaze duration inside an area of interest. Horizontal lines indicate a change of
Visual Analysis of Eye Tracking Data 401
attention from one AOI to another. Figure 6c shows a scan path starting in AOI1, moving to AOI4, back to AOI2 and so on. Figure 6a shows the stimulus together with its areas of interest (Fig. 6b). By means of the time axis both gaze duration parameters like start and end times and the temporal sequence of changes between areas of interest can be identified. Changes of participant’s attention can be studied by following the scan path line in the visualization.
4.1.2 Fixation Point Diagram
Fixation point diagrams additionally show single fixations. The scan path of one participant is shown in Fig. 6d. Now, every fixation on the stimulus inside an area of interest is additionally plotted as a filled circle. The gaze duration inside an area of interest is shown with a vertical continuous line on the corresponding area of interest axis. For each such group of fixations, the center of the respective lines are connected by ascending or descending lines. By using a fixation point diagram, characteristics of eye movements during a gaze duration such as the frequency and number of fixations can be studied.
4.1.3 Gaze Duration Distribution Diagram
Figure 6e shows a gaze duration distribution diagram. The gaze duration distribution diagram is based on eye tracking data visualizations for website analysis used in [10]. Scan paths through areas of interest are plotted similar to gaze duration sequence diagrams. However, this third type of a PSP visualization does not directly indicate the time of gaze durations. Now, time is shown by the midpoint of a gaze duration which is plotted as a filled circle. Lines between these circles display eye movements between areas of interest. A bar chart is overlaid showing the summation of the percentage of gaze durations in the areas of interest.
4.1.4 Implementation
All three visualization techniques have been implemented in a software tool together with classical heat map and scan paths, see Fig. 7. A simple project management supports users to select participants, participant groups, and areas of interest.1
1For further information about the software please contact the authors.

402 M. Raschke et al.
 Fig. 7 The Parallel scan path visualization approach (a) was implemented in a software tool together with heat map visualizations (b) and classical scan paths (c). A project management allows users to easily select participants and groups of participants. Various layout properties can be set via option dialogues (d)
4.2 Visual Analytics for Eye Tracking Analysis
Due to the progress in hardware technology much bigger datasets can be generated and stored than some years ago. This observation also holds for eye tracking data. Though there is no technical limitation for producing vast amounts of eye tracking data, the analysis of the data mountains is the more challenging part than its storage.
Visualization in general and visual analytics in particular have been applied as a promising example that exploit the perceptual abilities of the human user with the goal to find visual patterns very fast. This concept helps when the problem at hand cannot be specified purely algorithmically and its solution cannot be computed by a simple algorithm. Oftentimes, a mixture consisting of algorithms and visualizations are the medium of choice with the human user in-between.
In particular, visualization techniques have also been applied for visually exploring eye tracking data, i.e., gaze trajectories, and have also been integrated into eye tracking systems in form of simple heat maps and gaze plots. The spatio temporal nature of the data makes it difficult to find insight in such data by just using simple visualization techniques. More exactly, heat maps are time aggregated representations which do not allow to examine the sequential order of fixations. Gaze plots on the other hand are line based diagrams that suffer from overplotting and visual clutter. This means common visual task solution strategies cannot be examined by just inspecting a static gaze plot.
Visual Analysis of Eye Tracking Data 403
This is exactly the point where visual analytics comes into play as a means to uncover insight in the spatio temporal data. Andrienko et al. [1] proposed a visual analytics methodology for analyzing vast amounts of trajectory data. In this work the focus is on deriving common task solution strategies for a given static stimulus shown to a certain number of participants.
A rich source of visualization techniques exists for analyzing geographic movement data. Since such data has some commonalities with eye tracking trajectories some of the existing visual analytics techniques can be adapted to this novel domain. From about 30 generic methods relevant to movement data [2], 23 methods were available and tested. From these, only six methods have been found ineffective and the rest was judged as useful. Andrienko et al. [1] discusses the selected methods and the eye movement analysis procedures in which these methods are used. The methods and procedures combine computational techniques for data transformation and analysis, visual displays, and interactive operations. Figures 8a–f illustrate some of the applied visualization techniques in this tool.
4.3 eTaddy: Interactive Eye Tracking Data Analysis
The last approach, which we would like to present, uses interaction to support the user during the analysis of eye tracking data. eTaddy (eyeTracking Analysis, conDuction and Designtool for user studYs) is an integrative framework for the creation, conduction, and analysis of eye tracking user studies.2 Each step of a user study is represented with an own window in the framework. The data used in the framework, including eye tracking data, questionnaire answers, or participant data, is stored in a database. The framework can be extended via a plug-in system, to add own metrics, statistics, or visualizations. This framework allows users an efficient task oriented analysis process. In the following we will describe the architecture of eTaddy and the user interface.
2For further information about the software please contact the authors.
   Visualization in general and visual analytics in particular have been applied as a promising example that exploit the perceptual abilities of the human user with the goal to find visual patterns very fast. Andrienko et al. [1] proposed a visual analytics methodology that have originally used for geographic data for analyzing vast amounts of eye tracking data.

404 M. Raschke et al.
 Fig. 8 A small set of the visualization techniques provided by the visual analytics tool [1]: (a) Time varying distances to points of interest for all participants. (b) Line based trajectory visualization aggregated over time for a specific time interval. (c) Time aggregated representation showing number and direction of eye gazes. (d) Time varying histograms for fixation frequencies in specific areas of interest. (e) Time aggregated fixation numbers in areas of interest. (f) Barchart representation for different metrics in areas of interest
   eTaddy is an integrative framework to design, conduct and analyze eye tracking user studies. The framework supports the embedding of additional metrics, statistical tests, and visualizations via a plug-in system. Both data from the user experiment and data which is generated during the analysis process is stored in a database.
 4.3.1 Architecture
One drawback of commercial eye tracking software like BeeGaze (SMI) or Tobii Studio (Tobii) is the limited flexibility during the analysis process. Usually the data collected in the experiment is stored on the eye tracking computer. This means, that
Visual Analysis of Eye Tracking Data 405
only one person can conduct an eye tracking experiment at a time. Furthermore own visualizations and metrics cannot be added to the state-of-the-art eye tracking software suites. The data usually has to be exported and imported into statistic software like R or SPSS for the statistical evaluation of the hypotheses. To reduce the amount of time which is spent into this data management, we have implemented an integrative framework which is connected to a database for data storage.
The eTaddy framework consists of a plug-in system to include own metrics, visualizations or statistics and a database is connected for data storage. The plug-in system is implemented via an XML file, which will be parsed when starting the software in which new plug-ins can be added. The used database model defines classes for the user study itself, the different scenarios and tasks, answers from questionnaires, the results which are generated during the analysis process, and the eye tracking data from the eye tracker system. By using a database the complete analysis process can be done in the framework. There is no more need to import or export data, except of an initial import of the eye tracking data at the beginning.
4.3.2 User Interface
To analyze a user study, the first step using eTaddy is to create a new user study data set. The user can define information about the eye tracking system and can create scenarios of the user study. Each group of tasks from a scenario represents one hypothesis and consists of different stimuli. Besides the creation of scenarios the user can define the study procedure the participant will have to perform. This study procedure includes questionnaires, task descriptions, a vision test, or other text documents, which are needed for the user study conduction. The questionnaires can be created with eTaddy by using an integrated questionnaire editor.
Two separate computer screens can be used for the conduction of the user exper- iment: The first computer screen presents information for the moderator, the second computer screen presents information and stimuli for the participant. The participant will be presented tutorials, questionnaires, or vision tests consecutively. The main eye tracking recording is done with an standard eye tracking software. However, the recorded data is later used in the analysis process in eTaddy.
Figure 9 shows a screen shot of the implementation. To analyze the results of the eye tracking experiment the user first has to import the eye tracking data from the eye tracker software into the database (1). Then, all stimuli and participants will be displayed and the user can choose one or multiple stimuli and participants for the analysis (2). Next, the user can choose between different metrics, statistic analysis techniques and visualizations (3). The solution of such a calculation or visualization is displayed in the main panel of the window (4). This part is called the history and each calculation or visualization is displayed in a so called history node which contains information about the scenario, stimulus, chosen calculation, participants, time stamp and solution, respectively the visualization. Furthermore, eTaddy provides visualizations for the questionnaires and comes with a printing feature for a user study report.
406 M. Raschke et al.
 Fig. 9 Screen shot of the eTaddy prototype (in german). At first the eye tracking data has to be imported (1). Secondly, the user chooses one or multiple stimuli and participants (2). Then, she can choose between different metrics, statistics and visualizations (3). eTaddy presents the results of the calculations or of the visualizations in its main panel (4)
5 Discussion
Figure 10 shows an overview of the three visual analysis methodologies for eye tracking data, which have been presented in this chapter. This overview can help users to find the most appropriate visual analysis technique for their experimental results and research questions. Users start from the top of the decision tree diagram with the question whether they are interested in an overview about the recorded eye tracking data. If they are, they can use heat map visualizations or the presented approaches used in eTaddy. If they are interested in statistical information about fixations, fixation durations, frequencies and other metrics they also can use eTaddy. Next, the user would like to study the order in which items on the stimulus have been focused on. If areas of interest are not available, they can use traditional scan path visualizations. If areas of interest are available they can continue their analysis with methods from visual analytics as presented. If additionally a large number of participants has to be analyzed, they can either also use visual analytics techniques, eTaddy for an easy management of the recorded data, or the PSP visualizations. If they are interested in studying fixations, they can use fixation point diagrams. If they want to visualize gaze durations, they can proceed their analysis with gaze duration sequence diagrams or with gaze duration distribution diagrams in the case, if they wish to overlay summarized eye tracking information in the diagram.
Visual Analysis of Eye Tracking Data 407
 Fig. 10 Overview about the three presented approaches together with the classical scan path and heat map visualization techniques. This diagram can help users to plan their analysis process
6 Conclusion
In this chapter we have presented three approaches for a visual analysis on eye tracking data. Our three approaches deal with the large amount of data which is recorded during an eye tracking experiments. The parallel scan path visualization technique has the key feature, that it visualizes eye movements of many subjects in a single visualization with a parallel layout containing various levels of detail, such as fixations, gaze durations, eye shift frequencies and time. We have presented the visual analytics approach by [1] to use visual analytics techniques from geographical data for the analysis of eye tracking data. This is possible, because eye tracking data and geographic data have both a spatial temporal structure. The last approach, which we have presented, is based on the Information-Seeking mantra [20] and allows the user to analyze eye tracking data by the three steps overview, filtering and details-on-demand.
Depending on the user study tasks, one or more of these techniques can be used to efficiently analyze data from eye tracking experiment and thus, allows researchers to evaluate their new visualization techniques from a user centered perspective.
408 M. Raschke et al.
References
1. Andrienko, G., Andrienko, N., Burch, M., Weiskopf, D.: Visual analytics methodology for eye movement studies. Visualization and Computer Graphics, IEEE Transactions on 18(12), 2889–2898 (2012)
2. Andrienko, G.L., Andrienko, N.V., Bak, P., Keim, D.A., Kisilevich, S., Wrobel, S.: A con- ceptual framework and taxonomy of techniques for analyzing movement. Journal of Visual Languages and Computing 22(3), 213–232 (2011)
3. Aula, A., Majaranta, P., Raiha, K.J.: Eye-tracking reveals the personal styles for search result evaluation. Ifip International Federation For Information Processing 3585, 1058–1061 (2005)
4. Bojko, A.: Informative or misleading? Heatmaps deconstructed. In: International Conference on Human-Computer Interaction, pp. 30–39 (2009)
5. Burch, M., Konevtsova, N., Heinrich, J., Hoeferlin, M., Weiskopf, D.: Evaluation of traditional, orthogonal, and radial tree diagrams by an eye tracking study. Visualization and Computer Graphics, IEEE Transactions on 17(12), 2440–2448 (2011)
6. C ̧o ̈ltekin, A., Fabrikant, S.I., Lacayo, M.: Exploring the efficiency of users’ visual analytics strategies based on sequence analysis of eye movement recordings. International Journal of Geographical Information Science 24(10), 1559–1575 (2010)
7. Chen, C., Yu, Y.: Empirical studies of information visualization: a meta-analysis. Int. J. Hum.- Comput. Stud. 53(5), 851–866 (2000)
8. Goldberg, J.H., Helfman, J.I.: Comparing information graphics: a critical look at eye tracking. In: Proceedings of the 3rd BELIV’10 Workshop: BEyond time and errors: novel evaLuation methods for Information Visualization, BELIV ’10, pp. 71–78. ACM, New York, NY, USA (2010)
9. Goldberg, J.H., Kotval, X.P.: Computer interface evaluation using eye movements: methods and constructs. International Journal of Industrial Ergonomics 24(6), 631–645 (1999)
10. Guan, Z., Cutrell, E.: An eye tracking study of the effect of target rank on web search. In: Proceedings of the SIGCHI conference on Human factors in computing systems, CHI ’07, pp. 417–420. ACM, New York, NY, USA (2007)
11. Jacob, R.J.K., Karn, K.S.: Eye Tracking in Human-Computer Interaction and Usability Research: Ready to Deliver the Promises. The Mind’s eye: Cognitive The Mind’s Eye: Cognitive and Applied Aspects of Eye Movement Research pp. 573–603 (2003)
12. Keim, D.A., Mansmann, F., Schneidewind, J., Thomas, J., Ziegler, H.: Visual analytics: Scope and challenges. In: Visual Data Mining, pp. 76–90 (2008)
13. Lohmann, S., Ziegler, J., Tetzlaff, L.: Comparison of tag cloud layouts: Task-related performance and visual exploration. In: Proceedings of the 12th IFIP TC 13 International Conference on Human-Computer Interaction: Part I, INTERACT ’09, pp. 392–404. Springer- Verlag, Berlin, Heidelberg (2009)
14. Plaisant, C.: The challenge of information visualization evaluation. In: Proceedings of the working conference on Advanced visual interfaces, AVI ’04, pp. 109–116. ACM, New York, NY, USA (2004)
15. Ponsoda, V., Scott, D., Findlay, J.: A probability vector and transition matrix analysis of eye movements during visual search. Acta Psychologica 88(2), 167–185 (1995)
16. Poole, A., Ball, L.J.: Eye Tracking in Human-Computer Interaction and Usability Research: Current Status and Future Prospects, vol. Encyclopedia of HCI. Pennsylvania: Idea Group, Inc (2005)
17. Raschke, M., Chen, X., Ertl, T.: Parallel scan-path visualization. In: Proceedings of the Symposium on Eye Tracking Research and Applications, ETRA ’12, pp. 165–168. ACM, New York, NY, USA (2012)
18. Roberts, J.C.: State of the art: Coordinated & multiple views in exploratory visualization. In: International Conference on Coordinated and Multiple Views in Exploratory Visualization, pp. 61–71 (2007)
Visual Analysis of Eye Tracking Data 409
19. Rosenholtz, R., Li, Y., Mansfield, J., Jin, Z.: Feature congestion: a measure of display clutter. In: Proceedings of the Conference on Human Factors in Computing Systems (CHI), pp.761–770 (2005)
20. Shneiderman, B.: The eyes have it: A task by data type taxonomy for information visualiza- tions. In: Proceedings of the 1996 IEEE Symposium on Visual Languages, pp. 336–. IEEE Computer Society, Washington, DC, USA (1996)
21. Thomas, J., Cook, K.: Illuminating the Path. IEEE Computer Society Press, Los Alamitos (2005)
22. Tory, M., Staub-French, S.: Qualitative analysis of visualization: a building design field study. In: Proceedings of the 2008 Workshop on BEyond time and errors: novel evaLuation methods for Information Visualization, BELIV ’08, pp. 7:1–7:8. ACM, New York, NY, USA (2008)
23. Tsang, H.Y., Tory, M., Swindells, C.: eSeeTrack–visualizing sequential fixation patterns. IEEE Transactions on Visualization and Computer Graphics 16(6), 953–62 (2010)
24. Tversky, B., Morrison, J.B., Be ́trancourt, M.: Animation: can it facilitate? International Journal of Human-Computer Studies 57(4), 247–262 (2002)
25. Yi, J.S., ah Kang, Y., Stasko, J.T., Jacko, J.A.: Toward a deeper understanding of the role of interaction in information visualization. IEEE Transactions on Visualization and Computer Graphics 13(6), 1224–1231 (2007)
 DOI: 10.1111/cgf.13079
COMPUTER GRAPHICS forum Volume 00 (2017), number 0 pp. 1–25
Visualization of Eye Tracking Data: A Taxonomy and Survey
Abstract
T. Blascheck, K. Kurzhals, M. Raschke, M. Burch, D. Weiskopf and T. Ertl
University of Stuttgart, Germany
{tanja.blascheck, kuno.kurzhals, michael.raschke, michael.burch, daniel.weiskopf, thomas.ertl}@vis.uni-stuttgart.de
 This survey provides an introduction into eye tracking visualization with an overview of existing techniques. Eye tracking is important for evaluating user behaviour. Analysing eye tracking data is typically done quantitatively, applying statistical methods. However, in recent years, researchers have been increasingly using qualitative and exploratory analysis methods based on visualization techniques. For this state-of-the-art report, we investigated about 110 research papers presenting visualization techniques for eye tracking data. We classified these visualization techniques and identified two main categories: point-based methods and methods based on areas of interest. Additionally, we conducted an expert review asking leading eye tracking experts how they apply visualization techniques in their analysis of eye tracking data. Based on the experts’ feedback, we identified challenges that have to be tackled in the future so that visualizations will become even more widely applied in eye tracking research.
Keywords: eye tracking, visualization, survey
ACM CCS: [Human-centred computing]: Visualization—Visualization techniques [Human-centred computing]:
Visualization—Visualization design and evaluation methods
1. Introduction
Eye tracking has become a widely used method to analyse user behaviour in marketing, neuroscience, human–computer interac- tion [Duc02], visualization research [TCSC13], reading, scene per- ception and visual search [Ray98]. Apart from measuring comple- tion times and accuracy rates during the performance of visual tasks in classical controlled user experiments, eye-tracking-based eval- uations provide additional information on how visual attention is distributed and changes for a presented stimulus. Eye tracking de- vices record gaze points of a participant as raw data. Afterward, these gaze points can be aggregated into fixations and saccades, the two major categories of eye movement. In dynamic stimuli, smooth pursuit is an additional data type used for analysis. Additionally, areas of interest (AOIs) can be defined to measure the distribution of attention between specific regions on a stimulus.
Due to the wide field of applications for eye tracking and var- ious kinds of research questions, different approaches have been developed to analyse eye tracking data, such as statistical evalua- tion, either descriptive or inferential [HNA*11], string-editing al- gorithms [PS00, DDJ*10], visualization-related methods and visual analytics techniques [AABW12]. Regardless of whether statistical
or visual methods are used for eye tracking data analysis, a large amount of data generated during eye tracking experiments has to be handled. For example, a user experiment with 30 participants, three types of tasks and 30 stimuli for each task leads to 2700 scan- paths in total. Each scanpath typically consists of a large number of fixations and each fixation aggregates gaze points. The num- ber of these gaze points depends on the recording rate of the eye tracking device. In this example, if we assume a recording rate of 60 Hz, an average fixation duration of 250 ms, and that each stimu- lus is viewed for 2 min, about 20 000 000 gaze points and 1 300 000 fixations have to be stored, formatted and analysed to support or reject one or more hypotheses. Besides analysing eye tracking data with respect to quantitative metrics such as fixation count, fixation duration, fixation distribution, saccadic amplitude and pupil size, semantic information about which areas on a stimulus were focused on provides additional information to understand viewing strategies of participants.
While statistical analysis provides quantitative results, visual- ization techniques allow researchers to consider other aspects of recorded eye tracking data in an exploratory and qualitative way. Visualization techniques help understand spatiotemporal aspects of eye tracking data and complex relationships within the data. Such
1
 ⃝c 2017 The Authors
Computer Graphics Forum ⃝c 2017 The Eurographics Association and John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.
2
Blascheck et al. / Visualization of Eye Tracking Data
 1950
1960
1970 1980 1990 2000 2010
8 6 4 2
Figure 1: Histogram of all publications of this survey listed in Table 1, introducing eye tracking data visualization techniques. The number of published journal articles, conference papers and books has strongly increased during the last decade.
an analysis aims at finding hypotheses that can be investigated with statistical methods later on. Due to an increasing complexity of tasks and stimuli in eye tracking experiments, we believe that visualiza- tion will play an increasingly important role in future eye tracking analyses. Hence, the contribution of this survey has several facets: First, based on a taxonomy of eye tracking terms, a classification system for stimuli, gaze data and visualization techniques is formu- lated. Second, we assign papers from the literature to these classes. Third, based on these results and an expert review we conducted, we identify challenges for the future development of eye tracking visualization techniques.
Evaluation has become an important part of visualization re- search. More evaluations also using eye tracking have been done. One way of analysing the vast amount of eye tracking data is to use visualization techniques. The number of published journal arti- cles, conference papers and books about eye tracking visualization techniques has strongly increased during the last decade. Figure 1 shows a histogram of all relevant publications with visualization techniques for eye tracking data that are included in this survey. Especially after 1995, the number of publications on eye track- ing visualization techniques has overall increased. However, we are still missing a comprehensive survey of visualization techniques for eye tracking data that structures and discusses these approaches. Eye tracking in general has a longer history than the visualization techniques covered in this survey. We refer the interested reader to other papers to learn more about the history of eye tracking and eye tracking equipment [Yar67, YS75, JK03, PB06, HNA*11].
This paper is an extended version of our EuroVis 2014 state-of- the-art report [BKR*14], adding an extended and updated body of surveyed literature. We also revised our taxonomy to include gaze data as another category of eye tracking data. The main part of this paper is a presentation of the different visualization techniques with respect to our classification system. Furthermore, we conducted an expert review asking about eye tracking visualization techniques and challenges for future work.
2. SystematicLiteratureResearch
Since eye tracking has a wide range of applications, the development of eye tracking visualization techniques has become an interdisci- plinary challenge that requires a comprehensive literature research across several disciplines. To satisfy this requirement, we reviewed the main visualization and visual analytics conferences (IEEE VIS, EuroVis, EuroVA), the ACM Digital Library and the IEEE Xplore Digital Library, as well as the top conference on eye tracking re- search (Eye Tracking Research & Application), and the main jour-
nals of eye tracking research (Journal of Eye Movement Research), usability research (Journal of Human Factors), psychology [Be- havior Research Methods (formerly Behavior Research Methods & Instrumentation and Behavior Research Methods, Instruments, & Computers)] and cognitive science (Cognitive Science Journal). We searched the conference proceedings, journals, and libraries us- ing a keyword search. Our main keywords were ‘eye tracking’ and ‘visualization’. Here, the main challenge was that many papers did not explicitly develop visualizations for eye tracking data. Rather, papers used or created visualizations for the eye tracking data in lack of methods explaining certain phenomena. We also looked at eye tracking studies in general in the mentioned data sources.
Based on the papers found, we systematically reviewed each pa- per assigning them to different categories. Our main categories were stimulus, participant, visualization, data, eye tracking vis, applica- tion, type and chart. These categories were further subdivided, for example, by classifying a stimulus as 2D or 3D, see Sections 3.2– 3.4 for detailed descriptions of the three different categories used. If possible, each paper was assigned a tag of each category. In order to classify novel techniques, we used the category type to distinguish papers introducing a novel technique or extension of an existing technique, from papers that only use an existing method or classify eye tracking data and visualizations in general. This literature re- search resulted in about 110 papers that deal with eye tracking data visualization.
The papers tagged with the type introduction are shown in Table 1. We chose as a primary category to differentiate the papers the data type a visualization technique represents. This resulted in three main classes: point-based, AOI-based and visualization tech- niques using both. Furthermore, we used the categories visualization and stimulus to further differentiate the techniques. Our additional subcategories were based on their data type, visualization type, num- ber of participants visualized, dimensionality of the visualization, context and interactivity. The stimulus category was subdivided us- ing information about stimulus type, content and dimensionality of the stimulus. A detailed description of this classification and each subcategory is given in the next section.
For this systematic and detailed literature research, we used the literature system SurVis [BKW16] to find, analyse and group papers.
3. Taxonomy
Before we present our taxonomy classifying visualization tech- niques, we first define the basic terminology related to eye tracking data (Section 3.1). The taxonomy is subdivided into three
⃝c 2017 The Authors Computer Graphics Forum ⃝c 2017 The Eurographics Association and John Wiley & Sons Ltd.
Blascheck et al. / Visualization of Eye Tracking Data 3 Table1: Listofallreferencesthatintroducedanewvisualizationtechnique,improvedanexistingoneoradaptedanexistingone.Thereferencesareclassified
into gaze data, visualization-related and stimulus-related categories described in Sections 3.2–3.4.
Reference Gaze Data Visualization Stimulus Grindingeretal.[GDS10] •◦◦◦•◦••◦◦•◦•••◦••◦ Goldberg&Helfman[GH10c] ••◦•◦◦••◦◦•◦••◦◦••◦ Bojko[Boj09] ◦•◦◦•◦••◦•◦◦••◦◦••◦ Velichkovsky&Hansen[VH96] ◦•◦◦•◦••◦•◦◦••◦◦••◦ Wooding[Woo02] ◦•◦◦•◦•••••◦••◦◦••◦ Latimer[Lat88] ◦•◦◦•◦•◦•◦•◦••◦◦••◦ Kurzhals&Weiskopf[KW13] ◦••◦••••••◦•◦◦•◦••◦ Palettaetal.[PSF∗13b] ◦•••◦◦•◦••◦•◦◦••◦◦• Noton&Stark[NS71b] ◦◦••◦◦••◦•◦◦••◦◦••◦ Ramlolletal.[RTSB04] ◦◦••◦◦••◦•◦◦•◦••◦◦• Mackworth&Mackworth[MM58] ◦◦••◦◦••◦•◦◦•◦•◦••◦ Yarbus[Yar67] ◦◦••◦◦••◦◦•◦••◦◦••◦ Lankford[Lan00] ◦◦••◦◦••••◦◦••••••◦ Burchetal.[BSRW14] ◦◦•••◦••◦•◦◦••••◦◦◦ Hembrookeetal.[HFG06] ◦◦•◦•◦••◦•◦◦••◦◦••◦ Kurzhalsetal.[KHH∗16] ◦◦•◦•◦••◦•◦•◦◦•◦••◦ Chenetal.[CAS13] ◦◦•◦•◦••◦◦•◦••◦◦••◦ Lietal.[LCK10] ◦◦•◦•◦•◦••◦•◦•◦•◦•◦ Hurteretal.[HEF∗14] ◦◦••••••◦•◦◦••••••◦ Dorretal.[DJB10] ◦◦••◦•◦•◦•◦◦•◦•◦••◦ Duchowskietal.[DPMO12] ◦◦•◦••◦•◦•◦◦•◦•◦••◦ Duchowski&McCormick[DM98] ◦◦•◦••◦◦••◦•◦◦•◦••◦
Stellmachetal.[SND10b] ••••◦◦••••••◦◦••◦◦• Weibeletal.[WFE∗12] •◦••◦•••◦•••◦◦••◦◦• Pfeiffer[Pfe12] ◦•••••◦◦••◦•◦◦••◦◦•
Špakov&Räihä[SR08] •◦◦•◦◦••◦•◦•◦•◦◦••◦ Beymer&Russel[BR05] •◦◦•◦◦••◦•◦•◦•◦◦••◦ Räihäetal.[RAM∗05] •◦◦•◦◦••◦•◦◦••◦◦••◦ Kimetal.[KDX∗12] •◦◦•◦◦••◦◦•◦••◦•◦•◦ Crowe&Narayanan[CN00] •◦◦•◦◦••◦◦•◦••◦◦••◦ Holsanova[Hol01] •◦◦•◦◦••◦◦•◦••◦◦••◦ Itohetal.[ITS00] •◦◦•◦◦••◦◦•◦•◦••◦◦• Pellacinietal.[PLG06] •◦◦••◦••◦◦•◦••◦◦••◦ Kurzhalsetal.[KHW14] •◦◦◦•◦••◦•◦•◦◦••◦•◦ Burchetal.[BKW13] •◦◦◦•◦••◦◦••◦•◦◦••◦ Raschkeetal.[RCE12] •◦◦◦•◦••◦◦•◦••◦◦••◦ Richardson&Dale[RD05] •◦◦◦•◦••◦◦•◦••◦◦••◦ Kurzhals&Weiskopf[KW15b] •◦◦•◦◦••◦•◦•◦◦•◦••◦ Tsangetal.[TTS10] ••◦◦•◦••◦◦••◦◦••◦◦• Blaschecketal.[BRE13] ◦•◦•◦◦••◦◦••◦•◦◦••◦ Goldberg&Kotval[GK99] ◦•◦•◦◦••◦◦•◦••◦•◦•◦ Egusaetal.[ETK∗08] ◦•◦•◦◦••◦◦•◦••◦◦••◦ Itohetal.[IHN98] ◦•◦•◦◦••◦◦•◦•◦••◦◦• Schulzetal.[SSF∗11] ◦•◦•◦◦••◦◦•◦◦•••◦◦• Chengetal.[CSS∗15] ◦•◦◦•◦••◦•◦◦••◦◦••◦ Westetal.[WHRK06] ◦•◦◦•◦••◦◦••◦•◦◦••◦ Goldberg&Helfman[GH10b] ◦•◦◦•◦••◦◦•◦••◦◦••◦ Siirtolaetal.[SLHR09] ◦•◦◦•◦••◦◦•◦••◦◦••◦ Kurzhals&Weiskopf[KW15a] ◦•◦•◦◦••◦•◦•◦◦•◦••◦ Baldaufetal.[BFH10] ◦◦••◦•◦◦••◦•◦◦••◦◦• Duchowskietal.[DMC∗02] ◦◦••◦◦•◦••◦◦•◦••◦◦•
⃝c 2017 The Authors Computer Graphics Forum ⃝c 2017 The Eurographics Association and John Wiley & Sons Ltd.
                          AOI-based Both
Point-based
Temporal Spatial Spatiotemporal Single part. Multiple part.
Animated Static
2D
3D
In-context
Not in-context Interactive Non-interactive
Static Dynamic Active content Passive content 2D
3D
4
Blascheck et al. / Visualization of Eye Tracking Data
 1
= Gaze point = Fixa on
= Saccade
= Dwell
= AOI
= Transi on
1
2
Figure 2: Gaze points are spatially and temporally aggregated into fixations. Saccades connect individual fixations. Successive fixations are a dwell, however, only if fixations are within an AOI. An AOI is a region of specific interest on a stimulus. A saccade from one AOI to another is called a transition. The complete sequence of fixations and saccades is called scanpath.
categories: categories related to stimuli of eye tracking experiments (Section 3.2), categories related to the gaze data (Section 3.3) and categories related to visualization techniques (Section 3.4).
Other taxonomy papers target areas different from ours, for example, taxonomies restricted to the stimulus alone [SNDC09], the dimensionality of visualizations [Spa08], the eye track- ing data [RTSB04] or the tasks for analysing eye tracking data [KBB*17]. Our taxonomy includes more categories to obtain a fine-grained categorization of visualization techniques focusing on visualization techniques for eye tracking data and not on analysis tasks.
3.1. Terminology
Eye tracking devices record gaze points indicating where a partic- ipant is looking on a stimulus. The recording rates depend on the characteristics of devices. State-of-the-art devices allow rates be- tween 60 and 500 Hz or more. The recording rate specifies how many gaze points are recorded per second. In most cases, the raw gaze data are further processed. Different data types, which are shown in Figure 2, are distinguished and detailed in the following. For each data type, different metrics can be used during an analysis. The most important metrics for each data type are explained briefly. A comprehensive collection of eye tracking metrics can be found in the book by Holmqvist et al. [HNA*11].
Fixation. A fixation is an aggregation of gaze points. Gaze points are aggregated based on a specified area and time span. This time span is usually between 200 and 300 ms [HNA*11]. Different algo- rithms exist for calculating fixations [SG00]. Common metrics for fixations are the fixation count (number of fixations), the fixation duration (in milliseconds) and the fixation position given as x-, y- and—if required—z-coordinates.
Saccade. A saccade describes rapid eye movements from one fixa- tion to another. Saccades typically last about 30–80 ms. During this time span, visual information is suppressed [HNA*11]. Typical met-
rics are saccadic amplitude (distance of a saccade), saccadic duration (in milliseconds) and saccadic velocity (in degrees per second).
Smooth pursuit. During the presentation of dynamic stimuli, smooth pursuits can occur. They happen unintentionally and only if participants follow a movement in the stimulus. The velocity of the eye during smooth pursuits is about 10–30 degrees per sec- ond [HNA*11].
Scanpath. A sequence of alternating fixations and saccades is called a scanpath. It can provide information about the search be- haviour of a participant. An ideal scanpath would be a straight line to a specified target [CHC10]. Deviance from this ideal scanpath can be interpreted as poor search [GK99]. Scanpath metrics include the convex hull (i.e. which area a scanpath covers), scanpath length (in pixels) and scanpath duration (in milliseconds).
Stimulus. A stimulus can be any visual content presented to partic- ipants during an eye tracking experiment. We differentiate static and dynamic stimuli, with either active or passive content. Usually, 2D stimuli are presented to participants, but 3D stimuli have recently become a focal point of research as well [Pfe12].
Area of interest. An AOI or region of interest (ROI) is a part of a stimulus that is of special importance, typically marking an object partially or completely. For dynamic stimuli, AOIs also have to be dynamic. AOIs can either be specified beforehand or after an eye tracking experiment. Usually, AOIs are created based on semantic information of the stimulus. A transition is a saccadic movement from one AOI to another, and a dwell is a temporal aggregation of fixations within an AOI. Typical metrics for AOIs are the transition count (number of transitions between AOIs), the dwell time within an AOI (in milliseconds) and the AOI hit, which defines whether a fixation is within an AOI or not.
3.2. Stimulus-relatedcategories
The first part of our taxonomy is based on a categorization of stimuli. The type of stimulus can have a great influence on the choice and
6
3
5
4
⃝c 2017 The Authors Computer Graphics Forum ⃝c 2017 The Eurographics Association and John Wiley & Sons Ltd.
7
design of an eye tracking visualization technique. Stimuli can be divided into static or dynamic, representing 2D or 3D content, with active or passive content. The viewing task would be another type for classification, but is not included in our taxonomy.
Staticversusdynamicstimuli. Staticstimulican,forexample,be text or pictures where the visible content does not change. Dynamic stimuli can be videos, interactive applications or real-world scenar- ios. Some visualizations are overlaid on top of the stimulus and can be used with static and dynamic stimuli alike. Other visualization techniques depend on the static nature of a stimulus and cannot be applied to dynamically changing content.
2D versus 3D stimuli. 2D stimuli are common and can, for exam- ple, be static or dynamic 2D visualizations, videos or Web pages. Stimuli representing 3D models or objects have become increas- ingly popular in recent years. Data of 3D stimuli are usually col- lected using head-mounted eye trackers in real- or virtual-world scenarios [PR14]. 3D stimuli can be represented as stereoscopic images on 3D screens.
Passive versus active stimulus content. The participant’s mode of interaction is an important factor for data visualization and how a graphical user interface for a visualization technique is designed. Participants can watch presented stimuli passively without interfer- ing actions. A stimulus can be either static or dynamic (i.e. a presen- tation of pictures or videos). A synchronization between recordings of different participants is possible with minor effort, allowing one to compare multiple participants and search for similarities or dif- ferences in their scanpaths. Participants can also actively influence a stimulus by their actions, here a stimulus becomes dynamic. For example, eye tracking of interactive applications provides individ- ual recordings that result from the active involvement of a par- ticipant in an experiment. Comparing these individually recorded data sets is non-trivial because the synchronization of the data is difficult [Duc07, BJK*16b].
Viewing task. Although the task has a significant influence on eye movements of participants [AHR98], we decided not to use the task as a categorization factor because many visualization techniques do not depend explicitly on a given task for a successful interpretation of the data.
3.3. Gazedata-relatedcategories
The next set of categories is related to the gaze data collected during eye tracking experiments. First, we distinguish between point-based and AOI-based data. Furthermore, data can be classified as temporal, spatial or spatiotemporal. Another categorization is if the data are 2D or 3D as well as how many participants’ data are represented.
Point-based versus AOI-based. Eye tracking data can be evalu- ated in a point-based or AOI-based fashion [AABW12]. Point-based evaluation focuses on overall eye movements and their spatial or temporal distribution. A semantic annotation of data is not required. Depending on the stimulus, a point-based evaluation may not be suf- ficient for specific analysis tasks (e.g. comparison of asynchronous
eye tracking data). Here, an annotation of identified AOIs on a stim- ulus can be used to apply AOI-based metrics. AOI-based metrics provide additional information about the transitions and relations between AOIs. We use this classification as the first level in our taxonomy to distinguish between different visualization techniques.
Temporal, spatial and spatiotemporal visualizations. We clas- sify gaze data as either temporal, spatial or spatiotemporal. The temporal dimension focuses on time and is usually visualized with a timeline as one axis. The spatial dimension of the data focuses on x-, y- and possibly z-coordinates of gaze data. For AOI-based visualization techniques, the spatial domain contains AOIs and their relation to each other. The third data type is a combination of both, called spatiotemporal. Here, temporal as well as spatial aspects of the data are considered jointly.
2Dversus3Ddata. Dependingonthedimensionalityofthegaze data, different aspects are important. 2D data are the most common type of data collected, typically being an x- and y-coordinate of the eye movements. For 3D data, the third dimension, being depth collected as a z-coordinate, is important. A challenge with 3D data is the mapping of fixations onto the correct geometrical model.
Single participant versus multiple participants. Another factor for eye tracking analysis is the number of participants represented with a visualization technique. In contrast to techniques that show only a single participant, visualizing data of multiple participants at the same time can help identify common viewing strategies. However, these representations might suffer from visual clutter if too much data are displayed at the same time [RLMJ05]. Here, strategies such as averaging or bundling of lines might be used to achieve better results [HFG06, HEF*14].
3.4. Visualization-related categories
There are several taxonomies for visualizations in general. Some taxonomies focus on data dimension or type [Chi00, TM04], in- teraction techniques [Shn96, YKSJ07], visualization tasks [BM13] or specific visualization types [LPP*06, CM97]. However, these taxonomies are either too general or too restricted for eye tracking visualizations.
First, we shortly introduce statistical graphics. Then, we distin- guish between animated and static, 2D and 3D, in-context and not in-context, as well as interactive and non-interactive visualizations. We also discuss the recent approach of visual analytics as one means of further evaluating eye tracking data.
Statisticalgraphics. Themostcommonlyusedvisualizationtech- niques for eye tracking data are statistical diagrams such as bar charts, line charts, box plots or scatter plots. Statistical graphics are relevant for descriptive statistics, and therefore, important and widely used for eye tracking data. However, visualization techniques of statistical graphics are typically generic methods that were not specifically designed for eye tracking. Therefore, we discuss them separately from specific visualization techniques in Section 4.
Blascheck et al. / Visualization of Eye Tracking Data 5
⃝c 2017 The Authors Computer Graphics Forum ⃝c 2017 The Eurographics Association and John Wiley & Sons Ltd.
6 Blascheck et al. / Visualization of Eye Tracking Data
Static versus animated visualizations. Static visualizations usu- ally employ a time-to-space mapping of the data to the visualization. For dynamic stimuli, creating a static visualization often requires predefined AOIs because a spatial linking to dynamically changing content is hard to achieve. Animated visualizations use a time- to-time mapping by sequentially presenting certain points in time from the data. This allows in-context visualizations with an overlay of the visualization on top of the stimulus, keeping stimulus data and the visualization in the same domain. However, this requires complex layout algorithms that have to follow aesthetic drawing criteria [Pur97] for each static image in a sequence as well as for animation [BBD09] to preserve the viewer’s mental map.
2D versus 3D visualizations. 2D visualizations represent data in a 2D space, for example, one spatial dimension and the temporal dimension, or both spatial dimensions at the same time. 3D visual- izations represent three dimensions: both spatial dimensions and the temporal dimension or all three spatial dimensions for a 3D stimulus. 3D visualizations can be helpful as in the case of (STCs) [LCK10, KW13]. In a STC, the two spatial dimensions of a 2D stimulus as well as the temporal dimension are visualized at the same time. However, the third dimension has to be handled with care because of perceptual issues related to 3D visualizations, such as occlusion, making length judgments or reading text [Mun08]. In contrast, vi- sualizing 3D data in a 2D domain removes one dimension and leads to data loss; however, it can make an analysis easier. Example oper- ations for reducing one dimension are given in the state-of-the-art report by Bach et al. [BDA*14].
In-contextversusnotin-contextvisualizations. In-contextvisu- alizations link stimulus and visualization with each other, such as overlays over a stimulus or AOIs with thumbnail images. Visual- ization methods that do not include the stimulus in a visualization are not in-context visualizations. This is often the case for AOI- based visualization techniques. Not in-context visualizations have the problem that the spatial layout of AOIs is lost. However, if the relation between AOIs is more important than the spatial layout, losing this information is an acceptable trade-off.
Interactive versus non-interactive visualizations. Non- interactive visualizations usually represent data with a fixed set of parameters. The analyst has limited options to influence these parameters, usually because they have been pre-defined to create a static image (e.g. an attention map). In contrast, interactive visualizations allow the analyst to explore the data beyond what is represented at the beginning. For example, the analyst can navigate through time, zoom and filter data and obtain information about the data for specific parameter settings that can be adapted individually.
Visual analytics. When visualization techniques are not able to handle large eye tracking data, the emerging discipline of visual analytics can be an option for exploratory data analysis [TC05]. Machine-based analysis techniques such as methods from data min- ing or knowledge discovery in databases are combined with interac- tive visualizations and the perceptual abilities of a human viewer. A number of visual analytics systems were developed for the analysis of eye tracking data. Andrienko et al. [AABW12] discuss which and how existing visual analytics approaches can be applied to eye
tracking data; the authors focus on how techniques from geographic information science can be adapted for analysing spatiotemporal eye tracking data.
3.5. Classification structure
Based on the above categories, we coarsely divide the papers of eye tracking visualizations into two main subsets that differentiate whether a visualization technique is for point-based or AOI-based analysis. On the second level, we further segment the visualization techniques according to temporal, spatial and spatiotemporal aspects of the visualizations.
Table 1 provides an overview of all eye tracking papers that intro- duced a new visualization technique, improved an existing one or adapted an existing one for its application to eye tracking data. The table classifies the visualization techniques based on the two levels mentioned above. The upper part contains point-based visualization techniques described in Section 5 and the lower part of the table shows AOI-based visualization techniques described in Section 6. The middle part of the table contains papers with both point-based and AOI-based techniques. These papers are described in both sec- tions, focusing on the respective aspect. The individual sections on point-based and AOI-based visualization techniques are further subdivided into temporal, spatial and spatiotemporal visualization techniques. The first column of the table shows this subdivision.
In the case of point-based visualization techniques (Section 5), the temporal approaches are timeline visualizations, spatial approaches are attention maps and spatiotemporal approaches are subdivided into scanpath and STC visualizations, as they represent two dif- ferent concepts. Section 6 discusses AOI-based techniques: first temporal approaches in the form of timeline visualizations, then 3D visualization, and last spatial approaches in the form of relational visualization techniques.
Furthermore, the table displays the other categorizing factors de- scribed in the taxonomy: gaze-based categories, visualization cate- gories and stimulus categories.
4. Statistical Graphics
Before we describe the eye tracking visualization techniques based on our classification, this section summarizes statistical graphics that are commonly used to visualize eye tracking data, but that were not especially developed for it. Figure 3 shows examples of a bar chart (Figure 3a) and line chart (Figure 3b) both representing the same data set: two participants and their corresponding fixation counts within AOIs.
One of the first papers in eye tracking research uses line charts to study eye movement characteristics of children in a TV viewing sce- nario [GWG*64]. The authors present summarized time durations of different types of eye movements. Atkins and Zheng [AJTZ12] quantify the difference between doing and watching a manual task. To this end, they present results of recorded fixations in two line charts showing values for the x- and y-locations of fixations. Based on this visualization, the authors then discuss saccadic properties. Smith and Mital [SM13] use line charts to present values of mean
⃝c 2017 The Authors Computer Graphics Forum ⃝c 2017 The Eurographics Association and John Wiley & Sons Ltd.
bar chart and (b) line chart.
fixation durations, mean saccadic amplitudes and other metrics over time. They also use line charts to separately show these metrics for dynamic and static stimuli.
Bar charts are mainly employed to display the histogram of an eye tracking metric. Convertino et al. [CCY*03] plot the percentage of fixation durations for four different combinations of visualiza- tion techniques onto a bar chart. Thereby, the authors compare the usability of parallel coordinates [Ins85] with other types of visu- alizations. To evaluate different methods of image fusion, Dixon et al. [DLN*06] present eye location accuracy with bar charts.
Scatter plots are commonly used to plot data in a 2D Cartesian diagram. This type of diagram shows relations between two values of a sample. Just and Carpenter [JC76] plot the relation between response latency and angular disparity. Berg et al. [BBM*09] com- pare human vision and monkey vision. The authors present scatter plots of amplitude and velocity measurements of saccadic move- ments for both species. Additionally, they compare visual saliency between humans and monkeys and show results also using a scatter plot. Anderson et al. [ABL*13] represent the recurrence of fixations in a scatter plot. The authors plot the fixation numbers onto the x- and y-coordinates and mark if two fixations occur in the same spatial radius. Recurrence analysis indicates which parts of a scanpath oc- cur repeatedly, highlighting which areas of a stimulus were looked at multiple times.
Box plots are a popular visualization technique to present statis- tical distributions. Hornof and Halverson [HH02] analyse absolute, horizontal and vertical deviation of fixations for participants from their experiment to monitor the deterioration of the calibration of the eye tracker. Dorr et al. [DMGB10] investigate how similar eye movement patterns of different subjects are when viewing dynamic natural scenes. To compare eye movements, they employ normal- ized scanpath saliency scores and show a box plot of the computed scores for different movies.
Star plots are another type of statistical graphics. Goldberg and Helfman [GH10c] use them to understand angular properties of scanpaths and Nakayama and Hayashi [NH10] apply them to study angular properties of fixation positions.
5. Point-Based Visualization Techniques
This section comprises all visualization techniques that use spa- tial and temporal information of recorded data points (e.g. x- and y-coordinates of fixations along with temporal information) for vi- sualization. Therefore, a definition of AOIs is not required. These visualization techniques can be used to analyse temporal changes in the position of data points, distribution of attention, scanpaths or spatiotemporal structures of eye tracking data. As eye tracking data are not continuous (due to saccades), the spatiotemporal character- istic of such data differs from that in other typical applications such as in geospatial data visualization.
5.1. Timelinevisualizations
Timelines are an approach to visualize temporal data. A point- based timeline visualization represents time on one axis of a coordinate system and eye tracking data on the other axis. Such plots are usually represented in 2D space. The coordi- nates for x, y or z of fixations can either be displayed to- gether [KFN14] or the fixation position can be split into its co- ordinates and each one can be represented individually, as shown in Figure 4. If the fixation position is split and the x-coordinate is shown, time is depicted on the y-axis. To show the y- or z-coordinate of a fixations’ position, time is represented on the x-axis [GH10c]. This can be done for static or dynamic stimuli with active or passive content and for one or multiple participants [GDS10] representing fixation data independent of a stimulus. Such timeline visualiza- tions reduce crossings and overlaps of saccade lines. Furthermore, this visualization technique allows a visual scanpath comparison to measure similarity of aggregated scanpaths [GDS10]. Addition- ally, a general overview of scanning tendencies can be seen such as downward and upward scanning, or horizontal shifts [GH10c]. However, a separation into coordinates makes it difficult to perceive the combined behaviour in the two or three spatial dimensions.
Gaze stripes [KHH*16] combine point-based information with the stimulus context. The position of gaze points is employed to cut out thumbnails from the stimulus and stack them on separate timelines for each participant. This technique allows a comparison of
Blascheck et al. / Visualization of Eye Tracking Data 7
 (a) Bar chart
Figure 3: Examples of statistical graphics depicting identical eye tracking data of two participants showing the fixation count per AOI: (a)
⃝c 2017 The Authors Computer Graphics Forum ⃝c 2017 The Eurographics Association and John Wiley & Sons Ltd.
(b) Line chart
8 Blascheck et al. / Visualization of Eye Tracking Data
 Figure 4: Temporal evolution of scanpaths of multiple participants. Time is represented on the x-axis, and vertical fixation position on the y-axis. Colour represents each individual scanpath. Figure based on Grindinger et al. [GDS10].
multiple participants without defining AOIs and partially preserves the context of a static or dynamic stimulus (cf. Figure 5).
5.2. Attentionmaps
For spatial visualization techniques, marking fixation positions as an overlay on a stimulus is one of the simplest visualization techniques for recorded eye movements and was applied to dynamic stimuli as early as in 1958 by Mackworth and Mackworth [MM58]. This combined visualization of fixation data from different participants and a stimulus is denoted by bee swarm [Tob08]. It is usually pre- sented as an animation to show temporal changes of fixations either by representing each participant with an individual colour [Tob08] or by using an animated cursor following the eye tracking data of one participant [GSGM10]. An aggregation of fixations over time and/or participants is known as attention map, fixation map or heat map and can be found in numerous publications as summarizing
illustrations of gaze data. An attention map is typically a static and in-context representation. The main purpose of attention maps is to obtain an overview of eye movements and identify regions on a stimulus that attracted much attention; the latter is often used to determine AOIs. There are several papers describing how to create attention maps (e.g. [SM07, Bli10]).
Bojko [Boj09] introduces different types of attention maps de- pending on the data used, for example, fixation count, absolute fixation duration, relative fixation duration or participant percent- age attention maps. Each type has its benefits depending on the data needed for an evaluation. In her paper, guidelines for using attention maps are described to avoid misuse and misinterpretation of them. A review of attention maps is given by Holmqvist et al. [HNA*11], who further recommend that they should be used with care.
Classical attention maps are visualized as luminance maps [VH96], 3D landscapes [Lat88, Woo02, HRO11], 2D to- pographic maps with contour lines [GWP07, DMGB10] or with a colour coding [Boj09, DPMO12]. To emphasize attended re- gions, alternative visualization techniques use filtering approaches to reduce sharpness and colour saturation [DJB10] in unattended regions, by showing only fixations that were recurrent or deter- ministic [ABL*13], by showing only first fixations [NH08] or by representing reading speed of participants [BDEB12].
For dynamic stimuli (e.g. videos), not only the spatial, but also the temporal component of data is visualized by applying dynamically changing attention maps, for example, to identify attentional syn- chrony between multiple participants [MSHH11]. Attention maps for dynamic stimuli bear the problem that an aggregation of the data cannot be represented statically due to the changing stimulus. Figure 6(a) shows a traditional attention map of a video with a car, driving from the right to the left of the screen, employing the last frame of the sequence as stimulus context. Participants were watch- ing the car while it was moving, leading to a distribution of attention along the trajectory of the car. In the attention map, it seems that the attention was on two persons in the background, leading to mis- interpretations. This problem is overcome by motion-compensated
 Figure 5: Gaze stripes represent time on the x-axis and participants on the y-axis. For each participant, thumbnail series are displayed, representing the temporal changes of gaze point positions in-context of the stimulus. Figure based on Kurzhals et al. [KHH*16].
⃝c 2017 The Authors Computer Graphics Forum ⃝c 2017 The Eurographics Association and John Wiley & Sons Ltd.
(b) Motion-compensated
Figure 6: A traditional, static attention map for a video scene (a) can lead to misinterpretations when objects move. The motion- compensated attention map (b) shows that most attention actually was on the moving car [KW13]. Figure reprinted with kind permis- sion from IEEE.
Figure 8: In a typical scanpath visualization, each fixation is in- dicated by a circle. Saccades between fixations are represented by connecting lines between these circles.
this leads to data loss due to the removal of one dimension. An- other possibility when multiple objects are shown in a 3D scene is to colour-code the complete object in the attention map, as shown in Figure 7(b) [SND10a]. A typical representation shows the atten- tion map on the 3D model itself [DMC*02, Pfe12, CL13, PSF*13a, PSF*13b], as shown in Figure 7(c). Creating attention maps for 3D stimuli is associated with the utilization of additional information about object positions in a stimulus or feature detection. Direct vol- ume rendering techniques can be used for rendering 3D attention maps [BCR12].
5.3. Scanpathvisualizations
A spatiotemporal visualization using scanpaths connects consec- utive fixations through saccade lines on a stimulus. Noton and Stark [NS71a, NS71b] used the word ‘scanpath’ to describe a fix- ation path of one subject when viewing a specific pattern. Today, a scanpath describes any sequence of saccades and fixations vi- sualized as a static overlay on a stimulus [HNA*11]. In a typical scanpath visualization, a circle represents each fixation, where the radius corresponds to the fixation duration and connecting lines rep- resent saccades between fixations [SPK86], see Figure 8. Changing the colour of fixation circles can represent additional information such as the speed of a participants’ gaze [Lan00]. A simple scan- path only shows recorded saccades without printing circles for each fixation [Yar67].
Many different approaches exist to show the position of fixations and their temporal information in a scanpath layout. However, only scanpath visualizations like the one shown in Figure 8 are widely used today. It is clear that this kind of visualization quickly produces
Blascheck et al. / Visualization of Eye Tracking Data 9
  (a) Traditional
 (a) Projected
(b) Object-based
(c) Surface-based
Figure7: Attentionmapsfor3Dstimuli:(a)theprojectedattention map, (b) object-based attention map and (c) surface-based attention map [SND10b]. Figure reprinted with kind permission from ACM.
attention maps [KW13] using optical flow [BSL*11, FBK15] in- formation between consecutive frames to adjust fixation data based on the moving object that was watched. The resulting attention map shows the highest values on the moving object that was actually watched (cf. Figure 6b).
When looking at 3D stimuli, the third dimension has to be in- cluded into an attention map. One possibility is to use a 2D rep- resentation of a 3D stimulus and show the attention map on this 2D projection [JN00, SND10a, SYKS14] (cf. Figure 7a). However,
⃝c 2017 The Authors Computer Graphics Forum ⃝c 2017 The Eurographics Association and John Wiley & Sons Ltd.
10
Blascheck et al. / Visualization of Eye Tracking Data
  (a) Standard
(c) Multicolor
(b) Light to Dark
(d) Thin to Thick
Figure9: Differenttypesofscaledtracesshowingascanpathwith- out fixation durations: (a) the normal scaled trace, (b) the line type being changed from light to dark, (c) multi-colours used for each saccade and (d) line thickness changing from thin to thick. Figure based on Goldberg and Helfman [GH10c].
visual clutter [RLMJ05], if several scanpaths are shown in one visualization to study eye movement behaviour. On the one hand, if scanpath shapes are different, lines and circles are plotted all across the visualization and it is difficult to find patterns. On the other hand, if scanpath shapes are similar, lines and circles may overlap.
Many approaches have been presented to overcome the prob- lem of visual clutter. One approach averages or bundles scan- paths [HFG06, CAS13, HEF*14]. Here, a crucial question is to find an adequate averaging or bundling criterion. This question of scanpath similarity has not been fully answered yet [DDJ*10]. An- other solution is to reduce the ink used in scanpath visualizations and to show a condensed version of a scanpath [GH10c] (cf. Figure 9). The advantage of this visualization technique is that less visual clut- ter is produced, since circles for representing fixations are missing. However, a drawback of this graphical representation is that it is still difficult to visually identify common scanpath patterns by com- paring line directions. Another solution is to break down the spatial information of fixations into their two dimensions [CPS06]. For ex- ample, vertical and horizontal components of saccades are shown separately on four sides of an attention map visualization [BSRW14] (cf. Figure 10). One advantage of this visualization technique is that the horizontal and vertical characteristics can be studied indepen- dently from each other. However, it demands more mental effort to combine the two separately presented views into one mental image. Another possibility is to show only a part of the scanpath at a time, for example, the subsequent 5 s of a selected timestamp or time frame [WFE*12].
Another challenge of scanpath visualization is how to show 3D fixation information. Basically, there are two approaches. The first one, shown in Figure 11, is to visualize scanpaths in the 3D domain of the stimulus [DMC*02, SND10b, TKS*10, Pfe12, PSF*13a]. The other one is to warp the stimulus into a 2D representation and to draw scanpath lines on this 2D image [RTSB04]. Besides the question how to adequately show 3D data on a 2D computer screen, which might lead to data loss, visualizations of scanpaths from 3D data have the same limitations as their 2D counterparts.
Figure 10: A saccade plot represents horizontal and vertical posi- tions of a saccade on the four sides of a stimulus. The position is represented by a dot and the colour coding is used to distinguish between different saccades. The distance of a dot from a stimulus represents the length of the saccade. Whether a dot is placed on the left, right, top or bottom depends on the position of the saccade in the stimulus. Figure based on Burch et al. [BSRW14].
(a) (b)
Figure 11: (a) The 3D scanpath is represented with spheres. (b) Cones show a 3D scanpath with viewing direction [SND10b]. Figure reprinted with kind permission from ACM.
5.4. Space-timecube
As an alternative spatiotemporal visualization approach, STC visual- izations are commonly used in various research fields, for example, in the geographic information sciences [Ha ̈g82, Kra03]. For an ap- plication to eye tracking data, the 2D spatial domain of a stimulus is extended by a third, temporal dimension. This representation pro- vides an overview of the complete data set and can be applied to static [LCK10] and dynamic stimuli [DM98, KW13]. With an STC, scanpaths, fixations and cluster information are visualized statically and allow a direct identification of interesting time spans that would require a sequential search in the data otherwise.
Figure 12 shows an STC with gaze data and identified clus- ters, with data from multiple participants, recorded from a dynamic stimulus. A sliding video plane along the temporal dimension is ap- plied to relate time spans with the dynamic content of the stimulus. Since this 3D visualization bears issues resulting from occlusion,
 ⃝c 2017 The Authors Computer Graphics Forum ⃝c 2017 The Eurographics Association and John Wiley & Sons Ltd.
Figure12: STCvisualizationofvideoeyetrackingdata.Atemporal dimension (green) extends the spatial dimension of the video to a static overview of the data, displaying gaze points and gaze clusters. Figure based on Kurzhals and Weiskopf [KW13].
distortion and inaccurate depth perception, 2D projections of the data can be applied to walls on the sides of an STC.
The main advantage of the STC in comparison to alternative visualization techniques is the direct overview of data from multi- ple participants that allows an efficient identification of important AOIs. To this point, an application of the STC to data from mul- tiple participants was applied to synchronizable stimuli only. The application of this visualization technique to asynchronous data (i.e. head-mounted eye tracking data) has not been investigated so far. It would bear additional issues with the spatiotemporal coherence between participants and an AOI-based analysis of this kind of data provides more effective approaches.
6. AOI-BasedVisualizationTechniques
Other than point-based visualization techniques, AOI-based visu- alization techniques employ additional information of the recorded fixation data. AOIs annotate regions or objects of interest on a stimulus. The annotation of AOIs in a static stimulus is often per- formed by defining bounding shapes around an area. Automatic fixation clustering algorithms are also a common approach to iden- tify AOIs [PS00, SD04]. With AOI information, various metrics can be applied to the data [PB06, JK03], depending on the analyst’s research questions. A simple in-context approach is an overlay of AOI boundaries on a stimulus with values of the used metric in each AOI (e.g. dwell time in an AOI [RVM12]). The techniques in this
(a) AOI timeline
(b) Scarf plot
Figure14: (a)AOItimelineswithattentionhistogramsand(b)scarf plots of 10 participants. For each participant, a timeline is shown with coloured time spans that correspond to the colours of visited AOIs. Black spaces indicate that no AOI was looked at. Figure based on Kurzhals et al. [KHW14].
section visualize mainly temporal aspects of the data, or relations between AOIs.
6.1. Timeline AOI visualizations
Similar to point-based data, timelines can be applied to show tempo- ral aspects of AOI-based data. As in the case of point-based timeline visualizations, time is again shown as one axis of a coordinate sys- tem. The other axis can represent AOIs or participants. Typically, timelines are represented independent from a stimulus and are 2D static visualizations.
If the second dimension describes the number of partici- pants, gazes in an AOI can be represented as colour-coded time spans [RD05, SND10b] (cf. Figure 13), also known as scarf plots [RD05, HNA*11, RHOL13, KHW14]. A scarf plot allows us to compare different participants as they are represented below each other. This technique is efficient to interpret and compare scan- paths, if the number of AOIs is not too large. Scarf plots are limited in the number of AOIs represented at the same time. AOIs have to be additionally labeled or aggregated into groups to show a large number.
Figure 14(b) shows a scarf plot representing scanpaths of mul- tiple participants. Scanpath similarity can be calculated and the
Blascheck et al. / Visualization of Eye Tracking Data
11
   Figure 13: Models of a 3D stimulus are represented by colour-coded time spans on a timeline depending on when and how long they were fixated [SND10b]. Figure reprinted with kind permission from ACM.
⃝c 2017 The Authors Computer Graphics Forum ⃝c 2017 The Eurographics Association and John Wiley & Sons Ltd.
12 Blascheck et al. / Visualization of Eye Tracking Data
  (a) AOIs (b) Time plot
Figure 15: A time plot (a) uses a horizontal timeline for each AOI, which is highlighted as rectangles on a stimulus (b). Each fixation is drawn as a circle at its point in time and depending on the AOI. The circle radius indicates the duration of a fixations. A line showing the path connects each fixation. Figure based on Ra ̈ iha ̈ et al. [RAM*05].
scarf plot can be clustered depicting different groups [KHW14]. Furthermore, it shows which AOIs have been looked at often or in- frequently. Displaying AOIs with a thumbnail image on the x-axis can be used to visualize participant groups instead of individual par- ticipants [TTS10]. This can be used for static or dynamic stimuli. Creating a representative scarf plot for all participants of one group of participants can condense the visualization even more.
In the case of AOIs represented on the second axis, either one participant, averaged information of multiple participants, or mul- tiple participants separately can be shown. In general, all of these techniques represent AOIs on separate timelines horizontally or ver- tically parallel to each other. This approach is often referred to as AOI timeline [HNA*11]. A representation with coloured time spans is also common [CN00, Hol01, HNA*11, WFE*12] (cf. Figure 14a). Alternatively, representations similar to those in scanpath visual- izations can be applied to display gaze durations in AOIs [ITS00, RAM*05, AMR05, KDX*12, RCE12] (cf. Figure 15). The visual- ization shown in Figure 16 can be used for visual scanpath compar- ison when applying statistical measures [RHB*14]. However, all of these techniques lead to visual clutter if multiple participants are shown on top of each other.
By averaging eye tracking data, we can represent data of multiple participants in the same visualization without causing visual clutter. AOI rivers [BKW13], as shown in Figure 17, represent the average time spent in each AOI and transitions from one AOI to another. This enables us to see the distribution amongst participants.
Radial timeline representations display one or multiple AOIs in the centre with time represented on the perimeter. If one AOI is depicted, data from multiple stimuli (e.g. different videos containing this specific AOI) can be shown in an AOI cloud [KW15b] (cf. Figure 18). However, the number of stimuli represented is limited due to the colour coding. Representing multiple AOIs in the circle centre shows which AOI has been visited at what point in time (cf. Figure 19). This technique does not scale if many AOIs are
Figure 16: A parallel scanpath visualization maps time onto the y-axis and AOIs of a stimulus onto the x-axis. For each participant, fixations within an AOI are displayed according to time as vertical line, and horizontally sloping lines represent saccades. Figure based on Raschke et al. [RCE12].
Figure 17: Based on ThemeRiver by Havre et al. [HHWN02], AOI rivers display the change of fixation frequencies for each AOI and transitions between AOIs. Time is represented on the x-axis, and each AOI is colour-coded. Figure based on Burch et al. [BKW13].
represented at the same time [PLG06]. For investigating individual participants in reading tasks, each word represents one AOI. Such a technique is helpful as it allows us to see backtracks (i.e. backward movements to re-read words) [BR05, SR08].
6.2. 3Dvisualizations
As described in the previous section, AOI-based visualization techniques can be used to discard the spatial component of the data for an analysis of temporal aspects. In this case, it is possible to apply these techniques to either 2D or 3D stimuli. However, a subset of visualization techniques for recordings from 3D stimuli (e.g. real-world recordings from a head-mounted eye tracker) explicitly include the 3D spatial component of the stimulus for
 ⃝c 2017 The Authors Computer Graphics Forum ⃝c 2017 The Eurographics Association and John Wiley & Sons Ltd.
Blascheck et al. / Visualization of Eye Tracking Data 13
  Figure 18: AOIs that appear in multiple videos are represented as circles. The timelines of the videos are depicted as segments on the perimeter of each circle. This enables an analyst to assess when and how long a participant was looking at a specific AOI in each video. Figure based on Kurzhals and Weiskopf [KW15b].
Figure 20: 3D AOI on a virtual supermarket shelf. Copyright by Pfeiffer et al. [PRPL16].
tic [EDP*12] and geometry-based [PH10] estimation of the 3D point of regard. Approaches using 3D AOIs exist for real-world scenar- ios [PSF*13a], virtual reality [PR14] or videos [DM98]. Automatic calculation of AOIs in such dynamic stimuli can be achieved by clus- tering fixation data of individual participants [KW13], by averaging fixation data of multiple participants [Duc02] or by marking AOIs on an image and automatically detecting them in a video [PSF*13a].
3D AOIs can be overlaid on the virtual model [BFH10] or indi- vidual object itself [SND10a], as shown in Figure 20. Additional information about an AOI can then be added, for example, different AOIs can have different colour [BFH10], AOIs can be highlighted based on how much time was spent on them [SND10b], or the AOI can be highlighted each time a participant looked at it [PR14]. 3D AOIs are used in a multi-party communication tool by rotating persona each time a participant looks at one [Ver99] or in gaze- contingent displays [DM98].
6.3. Relational AOI visualizations
Relational visualization techniques use AOIs and show the relation- ship between them (i.e. how often attention changed between two AOIs). Different metrics are represented in the AOI context, for example, the transition percentage or transition probability between two AOIs. This can be achieved in-context or not in-context. AOI metrics can be encoded into the AOI on the stimulus. For example, Cheng et al. [CSS*15] encode reading-related information on top of AOIs: grey shading for the reading speed based on the ratio of saccade length and dwell time.
A common visualization technique to evaluate transitions be- tween AOIs is the transition matrix [GK99]. A transition matrix orders AOIs horizontally in rows and vertically in columns and each cell contains the number of transitions between two AOIs (cf. Figure 21). This matrix representation can be used to evaluate the search behaviour of participants. For example, a densely filled ma- trix indicates poor search behaviour since all AOIs were focused on
 (a) AOIs
(b) Search path
Figure 19: AOIs are placed in a circular layout in the middle of a search path (a). The size of each AOI circle corresponds to the number of visits to that AOI. The smaller circles inside an AOI represent the transition probability for this AOI from any other AOI. Each fixation is then placed next to its corresponding AOI, which leads to a circular looking search path (b) [LHB*08]. Figure reprinted with kind permission from Elsevier.
the representation. These techniques contain application scenarios that mainly provide an actively created content, for example, eye tracking in a virtual reality environment.
The main purpose of these methods is a 3D scene representation of a stimulus or at least 3D gaze position to apply standard meth- ods such as attention maps with 3D information. Pfeiffer [Pfe12] distinguishes two general approaches calculating a gaze position and there are multiple authors applying one or the other: holis-
⃝c 2017 The Authors Computer Graphics Forum ⃝c 2017 The Eurographics Association and John Wiley & Sons Ltd.
14
Blascheck et al. / Visualization of Eye Tracking Data
 AOI 1
AOI 2
AOI 3
 0.1
 0.82
 0.45
 0.51
 0.15
 0.73
 0.13
 0.22
 0.9
     Figure 21: A classical transition matrix orders AOIs horizontally in rows and vertically in columns. Each cell represents the number of transitions between two AOIs. The colour coding corresponds to the transition count.
for several times. Finding visual search patterns is improved when cells in the transition matrix are coloured to show the transition count [LPS*12, KSK13, BKR*16].
Furthermore, transition matrices can be used to compare multi- ple participants. The classical transition matrix only represents one participant. If all participants are shown in the same matrix, a vi- sual comparison of search strategies is possible. This is achieved by concatenating the matrices of all participants and assigning a value to each matching sequence between them, as shown in Fig- ure 22 [GH10b]. A similar visualization technique using matrices places AOIs horizontally as rows and, for example, the task or query of a study vertically as columns. Each cell then contains different metrics such as fixation duration or count [ETK*08, SLHR09]. A matrix representation mixes statistical measures with a visual rep- resentation. This representation has the advantage that it allows us to compare 2D data sets visually. However, if the number of AOIs is high, the matrix becomes large.
Well-established visualization techniques such as a directed graph or tree visualization are used to analyse the relation between AOIs. A directed graph shows transitions between AOIs (cf. Figures 23 and 24). Each node of the graph depicts one AOI and the links depict transitions. Node size can be varied or colour coding may be applied to represent different metrics such as fixation count or fixation duration. The thickness of a link depicts the number of tran- sitions between two AOIs, as shown in the example in Figure 23. Usually, this type of diagram represents only data of one partic- ipant. Averaged values of multiple participants can also be used. Most visualization techniques show the graph independently of the stimulus [IHN98, FJM05, TAK*05, SSF*11, BRE13, BKR*16]. Losing the topological information makes the graph harder to inter- pret. However, the transition information allows us to see in which order AOIs have been looked at or how often the participant returned to look at an AOI. One example of such a visualization technique is provided in Figure 24. Some examples also show the graph in- context [HHBL03, HC13, OGF14] (cf. Figure 23a). This enables an analyst to correlate the AOI transitions directly to the stimulus
Figure 22: The AOI sequences of multiple participants are con- catenated and shown in one matrix horizontally and vertically. A red line marks each matching sequence. Green lines show where the AOI sequence of one participant ends [GH10a]. Figure reprinted with kind permission from ACM.
content. Like the transition matrix, the graph representation does not scale to a large number of AOIs. Although the transition matrix scales better, AOI graphs are more useful for following a specific path if the nodes are properly ordered in the graphs [vLKS*11].
A tree visualization can be used to compare AOI sequences of participants. A node in the tree represents an AOI. Each branch in the tree represents a different AOI sequence, as shown in Figure 25. Many branches indicate many different search patterns [WHRK06, TTS10, RHOL13]. An AOI transition tree containing more informa- tion than the word tree (i.e. frequencies of transitions and transition patterns) can be created [KW15a]. The AOI transition tree repre- sents AOIs with thumbnails of video stimuli (cf. Figure 26). This visual comparison of scan patterns allows one to find participant groups. However, it can only be used for short sequences, because for long sequences, the difference between participants would be too large and each participant would become a single participant group.
7. ExpertReview
To identify challenges in eye tracking research, we conducted an online study asking the top 100 eye tracking researchers listed in Google Scholar about their experience with eye tracking visualiza- tions. We used researchers listed on Google Scholar, as we believe that those are the researchers with the most experience available. We received 34 filled out questionnaires. Our questionnaire contained 11 questions subdivided into three blocks: six questions about the research background and applied eye tracking equipment, three
AOI 1
AOI 2
AOI 3
0−0.2 transi ons 0.21−0.4 transi ons 0.41−0.6 transi ons 0.61−0.8 transi ons 0.81−1.0 transi ons
⃝c 2017 The Authors Computer Graphics Forum ⃝c 2017 The Eurographics Association and John Wiley & Sons Ltd.
(a) In-context
Blascheck et al. / Visualization of Eye Tracking Data 15
   (b) Not in context
Figure 23: Graph representations for transition frequencies be- tween AOIs. AOIs are represented in-context (a) as circles placed on the stimulus. The radius of the circle presents the proportional dwell time. Arrows depict transitions and the thickness shows the num- ber of transitions between two AOIs [HHBL03]. Figure reprinted with kind permission from Elsevier. AOIs are depicted not in-context (b) in a triangular layout with the most important AOI in the mid- dle [TAK*05]. Figure reprinted with kind permission from IEEE.
questions about the application of visualization for eye tracking analysis and two questions about open challenges in eye tracking re- search. Typically, questions were multiple choice, only one question had a text field. Some questions had an additional text field, where experts could add another answer not listed in the answers list.
The results of this questionnaire are depicted in Figure 27 and discussed in more detail in the following sections based on those three question categories.
7.1. Backgroundofexperts
To evaluate the background of the participating experts, we used the research fields described by Duchowski [Duc02]. Most researchers
Figure 24: The circular heat map transition diagram places AOIs on a circle, where the AOI circle segment is colour-coded based on the fixation count inside the AOI. The size of the circle segment indicates the total dwell time within an AOI. Arrows represent tran- sitions between AOIs, with the thickness displaying the number of transitions. Figure based on Blascheck et al. [BRE13].
Figure 25: A word tree represents the sequences of AOIs starting at a selected AOI. The font size corresponds to the number of se- quences. The sequence AOI-0, AOI-10 and AOI-11 shows a common sequence amongst participants.
work in psychology (15) and computer science (9) and most of them are professors or senior researchers (23). Two researchers added that they were a research team leader and a virtual reality specialist. Most of the experts (26) have been working in eye tracking research for over 6 years. The different problem types researchers investigate were chosen from Duchowski [Duc02]. Scene perception (17), us- ability (15) and visual search (15) are the problem types investigated most frequently.
Typical hardware equipment the researchers use is from the com- panies Tobii (23) or SMI (15), which mainly provide attention maps and scanpaths as visualizations for the eye tracking data. Some added that they use eye trackers custom-built (2) or built by them- selves (2). For analysing eye tracking data, the experts stated what type of software they use. The eye tracking softwares Tobii Studio (17) and SMI BeGaze (9) and statistical programs such as SPSS (14), Matlab (13), and R (12) are used most often. Again, some experts stated that they use customly developed systems (3).
⃝c 2017 The Authors Computer Graphics Forum ⃝c 2017 The Eurographics Association and John Wiley & Sons Ltd.
16 Blascheck et al. / Visualization of Eye Tracking Data
 Figure 26: An AOI transition tree displays AOIs on the y-axis and their transition sequence on the x-axis. Data from multiple partic- ipants are aggregated. This technique is used for dynamic stimuli, by taking video shots and the defined AOIs per shot, displaying them as an icicle plot. Figure based on Kurzhals and Weiskopf [KW15a].
7.2. Eyetrackingandvisualization
The visualization-related questions consisted of two questions where the experts had to rate different statements (cf. Tables 2 and 3). All experts agree that statistical analysis is important. An exploratory approach was rated as being important by 28 of 34 experts, which is a good sign for the acceptance of eye tracking visualizations. Almost all experts (28) agree that visualization is important and 31 of our experts also use visualization to present their data.
When asking researchers what type of visualizations they use, most of them answered that they use scanpaths (27) or attention maps (24). The other answers can be seen in Figure 28. Some experts added additional visualizations:
r Post-hoc visualizations based on single-subject research meth- r ods.
r Gaze replays.
r Custom clustering software for dynamic images.
r Movies and animations. r Cross-recurrence maps. r Time series.
Custom interactive stimuli visualizations.
These results let us conclude that scanpaths and attention maps are still the most used visualizations. However, from the individual comments, we can see that some of the more advanced visualization techniques mentioned in Sections 5 and 6 are being used as well. Overall, we believe that researchers are moving into the direction of using more eye tracking visualization techniques for analysis, but user friendly and wider availability of these techniques would further improve their usage.
7.3. Challengesineyetrackingresearch
The last set of questions was concerned with challenges in eye tracking visualization research. First, the experts were asked to rate different statements whether they agree or disagree with these chal- lenges. Second, the experts could state their own challenges they encounter during their research. Table 4 shows statements and an- swers of the experts. If we add up the answers for ‘strongly agree’ and ‘agree’, we find that the most important challenge is to analyse more eye tracking data from dynamic stimuli (30). Second, the ex- perts think that approaches for automatically detecting patterns are needed (29).
Last, we asked the experts: ‘Do you consider other future chal- lenges as important for the field? Please name them and explain briefly’. The experts stated many different challenges they see in general in eye tracking research. As this survey is focused on visual- ization and eye tracking, we sum up the answers related to this topic.
One expert mentioned that ‘a major problem with eye tracking over the past 15 years is that some visualizations have been so
Table 2: Experts had to rate two statements about statistical and exploratory analysis of eye tracking data. Strongly
Strongly disagree
– –
Not important
3 1
 Statement
I consider statistical analysis as important for analysing eye tracking data.
I consider an exploratory approach as important when analysing eye tracking data.
Table 3: Experts had to rate two statements about eye tracking and visualization.
agree Agree Disagree
29 4 – 13 15 4
No answer
1 2
No answer
3 2
   Question
How important are visualizations during your analysis?
How important are visualizations for presenting your study results?
Very important Important
18 10 23 8
  ⃝c 2017 The Authors Computer Graphics Forum ⃝c 2017 The Eurographics Association and John Wiley & Sons Ltd.
   User Experience Marke ng Informa on Science Informa on and Library Sciences Communica on Biomedical Engineering Adver sement Human Factors Computer Science Psychology
What is your main research field?
0 2 4 6 8 10 12 14 16
(a) (b)
(c)
Blascheck et al. / Visualization of Eye Tracking Data
17
VR Specialist Research Team Lead PhD / MSc Student Postdoc. Fellow / Junior Res. Professor / Senior Res.
What is your research posi on?
0 5 10 15 20 25
How many years have you been working with eye tracking?
> 10 years
6-10 years
1-5 years
0 2 4 6 8 10 12 14 16
  What type of problems do you inves gate using eye tracking?
     Visual A en on and Interac on Television/Video Point of Purch. Display Eval. Oculomotor Control Neurodevelopmental Disorders Interac on with Techn. EM Modeling Examples Expert/Novice Differences Collabora on Decision Making Avia on Auditory Language Proc. A en onal Neuroscience Accessibility Ad Placement Driving Cogni ve Modeling Print Adver sing EM and Brain Imaging Reading Visual Inspec on Natural Tasks Web Pages Usability Visual Search Scene Percep on
0 5 10 15 20
  What so ware do you use for analyzing your eye tracking data?
     iMo ons A en on Tool eyetracking.com Other stat so ware SAS C++ Mirametrix SW GazeTag Ergoneers D-Lab Python MS Excel E-Prime Ogama Custom Developed SR Research Self-Developed SMI BeGaze R Matlab SPSS Tobii Studio
0 5 10 15 20
  What hardware do you use to collect eye tracking data?
       Smart Eye Posi ve Science Ober NAC Gazepoint Facelab Eyemouse DPI Arrington LC Technologies Interac ve Minds Ergoneers Mirametrix ISCAN Self-made Custom-built ASL The Eye Tribe SR Research SMI Tobii
0 5 10 15 20 25
(d) (e)
Figure 27: Overview of the results from the online questionnaire.
(f)
influential that some researchers think that they are enough. Heat maps were a real problem to science in the 2004–2008 period. Ac- tually, this only reflects the fact that eye tracking has become so common—so many eye trackers have been sold—and the scientific training in eye movement-based research has not been available to all new researchers. Manufacturers who are eager to sell tend to promise that visualizations will solve the researchers problems, when in most cases, they do not’. This is also closely related to a statement a second expert made, who claims that ‘Researchers should be trained in how to use visualizations’. We agree that re- searchers have to be trained before using visualizations. They should also be guided in choosing an appropriate visualization for their task and data. This also requires that visualizations should be developed that can be used intuitively and are user friendly. With this survey, we want to provide a step into this direction. We have summarized which visualization techniques are available and what benefits and drawbacks they have. This may be a first indicator for researchers to decide which visualization to use to answer a research question.
A different challenge is a ‘combination of quantitative results with qualitative visualizations’. So far, most visualization techniques only display the eye tracking data. However, more visualization
frameworks are developed where quantitative data are integrated and can be analysed in combination [KHW14, BJK*16b, RSW*16]. Additionally, this also requires ‘being able to standardize the visu- alization options [ . . . ] if we are to ensure comparable results. For example, more work on 3rd-party tools that can display eye track- ing data from a range of trackers would be helpful (e.g. Ogama1)’. We see this statement as an appeal to make tools more publicly available.
A visualization-specific challenge is to find ‘[ . . . ] a way to ag- gregate and analyse scan paths over a large sample [ . . . ]’. With this survey, we have shown other ways to analyse scanpath data using one of the many other visualization techniques presented in this paper.
8. DiscussionofFutureResearchDirections
Numerous visualization techniques have been developed to visualize different aspects of eye tracking data. Table 1 shows an overview of
1http://www.ogama.net/, last accessed on November 3, 2016
 ⃝c 2017 The Authors Computer Graphics Forum ⃝c 2017 The Eurographics Association and John Wiley & Sons Ltd.
18 Blascheck et al. / Visualization of Eye Tracking Data Table 4: Experts had to rate two statements about the challenges of eye tracking analysis.
 Statement
Approaches for automatically detecting patterns are needed.
Eye tracking data from dynamic stimuli (e.g. videos) should be analysed more. Techniques for analysing a high number of AOIs (>10 AOIs) are necessary. Visualizations should scale for a high number of participants.
More visualization techniques for eye tracking data are necessary, for example,
interactive visualizations or spatiotemporal visualizations.
Eye tracking data from interactive stimuli (e.g. Web pages or interactive
visualizations) should be analysed more.
Smooth pursuit from dynamic stimuli should be handled separately by
visualization techniques.
Eye tracking should be combined with other sensors such as EEG or skin
resistance.
Strongly
Agree Agree Disagree
19 10 1 17 13 – 16 9 2 14 13 4 14 12 5
14 10 3 9 14 – 8 16 5
Strongly
disagree No answer
– 4 – 4 – 7 – 3 – 3
– 7 – 11 – 5
    If you are using visualiza ons, what visualiza ons are you using?
       Bee Swarm
Line / Bar Chart
A en on Map
Scan Path
0 5 10 15 20 25 30
Figure 28: Results for application of visualizations in eye tracking research.
existing approaches: visualizations for point-based data, AOI-based data and visualizations for both data types. From this overview, we can recognize that some aspects of eye tracking analysis have been investigated to a lesser degree than others.
Table 1 shows that the number of spatiotemporal visualization techniques for AOI-based analysis is small. Usually, if time is visu- alized in AOI-based methods, a timeline is used along with one other data dimension. Therefore, animation is not needed. This might also explain the lack of in-context visualization techniques for AOI- based analysis. In case of point-based eye tracking visualizations, fixation data are often overlaid on a stimulus or presented as a 3D visualization in case of dynamic stimuli. For not in-context AOI- based visualization techniques, a presentation of eye tracking data is more abstract. This abstraction is intended to help analysts con- centrate on specific questions. However, the mental map of the eye tracking data on the stimulus is lost.
In general, for AOI-based methods, one limitation is the number of AOIs that can be represented at once. Most of the visualization techniques do not scale well, if there are many AOIs. Progress
has been made to overcome this problem by using hierarchical AOIs [BJK*16a]. Our experts also named some challenges with AOIs in general. For example, two experts mentioned the need for small and precise AOIs (e.g. one word in regular text size). We see this as one area for future research.
We found that the number of visualization techniques for the cate- gory ‘dynamic stimuli’ increased within the last 5 years. Our experts also think that this is an important topic for future work, especially how to do a ‘gaze analysis in temporally moving scenes using fully automatic techniques’. This is also in line with other research agen- das about visualizing eye tracking data [RTSB04, SNDC09]. Appli- cation areas for dynamic stimuli are, for example, video data or data coming from a head-mounted eye tracker. Only a few approaches deal with the question of how 3D eye tracking data can be visual- ized. This requires a ‘greater emphasis on mobile eye-trackers’ as well as ‘being able to aggregate data from eye tracking glasses’, as our experts mentioned. This question is important in case of user experiments that record stereoscopic fixation data. Also, separate handling of smooth pursuit from dynamic stimuli has mostly been neglected. Including smooth pursuit information in scanpath repre- sentations, for example, could provide valuable information about attentional synchrony of multiple viewers.
Furthermore, we think that multimodal data visualization tech- niques will become more important due to the interdisciplinary character of eye tracking research. Some of our experts named this as a challenges as well. In this context, one expert especially mentioned an ‘easy integration of eye tracking with other record- ing equipment with full online synchronization’. For example, eye tracking data could be combined with other sensor information coming from EEG devices or skin-resistance measurements. One approach is already included in the commercial software BeGaze by SMI [SMI14]. Blascheck et al. [BJK*16b, BJK*16a] developed a system for analysing eye tracking data in combination with in- teraction and think-aloud data. However, there are still many open questions for new visualization techniques in this direction.
Another challenge in eye tracking are stimuli with active con- tent, where participants can influence a stimulus during a study. For
⃝c 2017 The Authors Computer Graphics Forum ⃝c 2017 The Eurographics Association and John Wiley & Sons Ltd.
example, in studies where usability of Web pages is investigated, participants may click on links and navigate to different sites. The question is how participants can be compared. Often, a representa- tive stimulus is created and fixations of participants are manually mapped to a stimulus. However, this might lead to erroneous data due to ambiguity or inaccuracies an annotator introduces. A tool called Gaze Tracker [Lan00] records complete Web page informa- tion and scroll behaviour automatically. Yet, this approach can only be used for Web pages or graphical user interfaces. However, we believe that further visualization techniques for this type of stimulus will have to be developed in the future.
Our literature review showed that there is a large number of vi- sualization techniques for analysing eye tracking data. However, it is not always apparent which visualization technique works best for which type of analysis. This question cannot be answered com- pletely and to a full extent. Choosing an appropriate visualization technique depends on different factors. Therefore, we have classi- fied the presented techniques based on our taxonomy. Yet, we have let out analysis tasks. Kurzhals et al. [KBB*17] present a task tax- onomy for eye tracking visualizations that is a first step into the direction of suggesting visualizations based on tasks. For example, a common analysis task is to compare scanpaths of participants. Comparing scanpaths of multiple participants can help find regular- ities or similar patterns between different participants. Many of the presented visualization techniques can be used for scanpath com- parison [WHRK06, Coc09, GH10b, TTS10, RHOL13, RHB*14, EYH15]. This is just one example of how the analysis task influ- ences the type of visualization technique for an evaluation.
Finding patterns in eye tracking data is a general goal of eye tracking analysis as also mentioned by Stellmach et al. [SNDC09]. Often not one visualization technique alone is sufficient to find those patterns and analyse the eye tracking data. Rather, multiple visualization techniques have to be used in combination. An in- teraction of an analyst with a visualization techniques can further improve analysis results. The column for the category ‘interactive visualization’ shows that there are not many visualizations for inter- active analysis of eye tracking data. Most visualizations follow the paradigm of static visualization and provide little support for inter- action. For this reason, we motivate to allow more interaction with visualization techniques for eye tracking data [RSW*16]. Ramloll et al. [RTSB04] have already called for more interaction, for exam- ple, by using brushing or focus-and-context techniques. Interactions allow an analyst to investigate data using filtering techniques or se- lecting participants. In the end, specific hypothesis can be tested by collecting new data and testing for statistical significance. There- fore, we propose combining those approaches: using visualizations, statistics and interaction together. This is to some extent done by visual analytics and could therefore also be applied to eye track- ing analysis as already proposed by Andrienko et al. [AABW12] and to some extend implemented by Blascheck et al. [BJK*16a, BJK*16b].
Two other areas of research our experts named seem interesting in the context of eye tracking and visualization. First, one experts men- tioned to use visualizations ‘not just as a means for the researcher to understand the data, but as a specific independent variable in which participants are shown the eye movement patterns of either themselves, or from other observers, and how they can (or can-
not) understand and exploit this information’. This comment is es- pecially interesting, when analysing newly developed visualization techniques to find out if analysts understand the new techniques. Ad- ditionally, one expert noted to analyse saccadic behaviour in more detail. This is an interesting new direction, as only few visualization techniques considered saccades for their main data type so far.
9. Conclusion
In this state-of-the-art report, we have reviewed existing papers describing eye tracking visualizations. To classify the different ap- proaches, we have created a taxonomy that divides the papers into point-based and AOI-based visualization techniques as well as into visualizations that use both types of data. Furthermore, we have clas- sified the papers on a second level according to the data represented: temporal, spatial or spatiotemporal. An overview of all visualization techniques was given as well as a detailed description of the differ- ent visualization techniques. Experts analysed challenges and use of eye tracking visualizations. Based on the results of our expert re- view, we presented future directions for research. Our survey aimed at helping researchers get an overview of eye tracking visualization. Based on our categorization, we think that researchers now have a first indicator how to choose an appropriate visualization technique for their research questions.
Acknowledgements
This work was funded by the German Research Foundation (DFG) as part of SFB/Transregio 161.
References
[AABW12] ANDRIENKO G., ANDRIENKO N., BURCH M., WEISKOPF D.: Visual analytics methodology for eye movement studies. IEEE Transactions on Visualization and Computer Graphics 18, 12 (2012), 2889–2898.
[ABL*13] ANDERSON N., BISCHOF W., LAIDLAW K., RISKO E., KING- STONE A.: Recurrence quantification analysis of eye movements. Behavior Research Methods 45, 3 (2013), 842–856.
[AHR98]AALTONEN A., HYRSKYKARI A., RA ̈IHA ̈ K.-J.: 101 spots, or how do users read menus? In Proceedings of the SIGCHI Con- ference on Human Factors in Computing Systems (1998), ACM, pp. 132–139.
[AJTZ12] ATKINS S., JIANG X., TIEN G., ZHENG B.: Saccadic delays on targets while watching videos. In Proceedings of the Symposium on Eye Tracking Research & Applications (2012), ACM, pp. 405–408.
[AMR05] AULA A., MAJARANTA P., RA ̈ IHA ̈ K.-J.: Eye-tracking reveals the personal styles for search result evaluation. In Proceedings of the Human-Computer Interaction-INTERACT (2005), Springer, pp. 1058–1061.
[BBD09] BECK F., BURCH M., DIEHL S.: Towards an aesthetic dimensions framework for dynamic graph visualisations. In
Blascheck et al. / Visualization of Eye Tracking Data 19
⃝c 2017 The Authors Computer Graphics Forum ⃝c 2017 The Eurographics Association and John Wiley & Sons Ltd.
20 Blascheck et al. / Visualization of Eye Tracking Data
Proceedings of the International Conference on Information Vi- sualization (2009), IEEE Computer Society Press, pp. 592–597.
[BBM*09] BERG D. J., BOEHNKE S. E., MARINO R. A., MUNOZ D. P., ITTI L.: Free viewing of dynamic stimuli by humans and monkeys. Journal of Vision 9, 5 (2009), 1–15.
[BCR12] BERISTAIN A., CONGOTE J., RUIZ O.: Volume visual atten- tion maps (VVAM) in ray-casting rendering. Studies in Health Technology and Informatics 173 (2012), 53–57.
[BDA*14] BACH B., DRAGICEVIC P., ARCHAMBAULT D., HURTER C., CARPENDALE S.: A review of temporal data visualizations based on space-time cube operations. In Proceedings of the EuroVis - STARs (2014), R. Borgo, R. Maciejewski and I. Viola (Eds.), The Eurographics Association, pp. 1–19.
[BDEB12] BIEDERT R., DENGEL A., ELSHAMY M., BUSCHER G.: To- wards robust gaze-based objective quality measures for text. In Proceedings of the Symposium on Eye Tracking Research & Ap- plications (2012), ACM, pp. 201–204.
[BFH10] BALDAUF M., FRO ̈HLICH P., HUTTER S.: KIBITZER: A wear- able system for eye-gaze-based mobile urban exploration. In Proceedings of the Augmented Human International Conference (2010), ACM, pp. 1–5.
[BJK*16a] BLASCHECK T., JOHN M., KOCH S., BRUDER L., ERTL T.: Triangulating user behavior using eye movement, interaction, and think aloud data. In Proceedings of the Symposium on Eye Tracking Research & Applications (2016), ACM, pp. 175–182.
[BJK*16b] BLASCHECK T., JOHN M., KURZHALS K., KOCH S., ERTL T.: VA2: A visual analytics approach for evaluating visual analytics applications. IEEE Transactions on Visualization and Computer Graphics 22, 1 (2016), 61–70.
[BKR*14] BLASCHECK T., KURZHALS K., RASCHKE M., BURCH M., WEISKOPF D., ERTL T.: State-of-the-art of visualization for eye tracking data. In Proceedings of EuroVis - STARs, R. Borgo, R. Maciejewski and I. Viola (Eds.), (2014), The Eurographics Association, pp. 63–82.
[BKR*16] BLASCHECK T., KURZHALS K., RASCHKE M., STROHMAIER S., WEISKOPF D., ERTL T.: AOI hierarchies for visual explo- ration of fixation sequences. In Proceedings of the Symposium on Eye Tracking Research & Applications (2016), ACM, pp. 111– 118.
[BKW13] BURCH M., KULL A., WEISKOPF D.: AOI rivers for visual- izing dynamic eye gaze frequencies. Computer Graphics Forum 32, 3 (2013), 281–290.
[BKW16] BECK F., KOCH S., WEISKOPF D.: Visual analysis and dis- semination of scientific literature collections with SurVis. IEEE Transactions on Visualization and Computer Graphics 22 (2016), 180–189.
[Bli10] BLIGNAUT P.: Visual span and other parameters for the gen- eration of heatmaps. In Proceedings of the Symposium on Eye Tracking Research & Applications (2010), ACM, pp. 125–128.
[BM13] BREHMER M., MUNZNER T.: A multi-level typology of ab- stract visualization tasks. IEEE Transactions on Visualization and Computer Graphics 19, 12 (2013), 2376–2385.
[Boj09] BOJKO A.: Informative or misleading? Heatmaps decon- structed. In Proceedings of the Human-Computer Interaction- INTERACT (2009), J. Jacko (Ed.), Springer, pp. 30–39.
[BR05] BEYMER D., RUSSELL D. M.: WebGazeAnalyzer: A system for capturing and analyzing web reading behavior using eye gaze. In CHI Extended Abstracts on Human Factors in Computing Systems (2005), ACM, pp. 1913–1916.
[BRE13] BLASCHECK T., RASCHKE M., ERTL T.: Circular heat map transition diagram. In Proceedings of the Conference on Eye Tracking South Africa (2013), ACM, pp. 58–61.
[BSL*11] BAKER S., SCHARSTEIN D., LEWIS J. P., ROTH S., BLACK M. J., SZELISKI R.: A database and evaluation methodology for optical flow. International Journal of Computer Vision 92, 1 (2011), 1–31.
[BSRW14] BURCH M., SCHMAUDER H., RASCHKE M., WEISKOPF D.: Saccade plots. In Proceedings of the Symposium on Eye Tracking Research & Applications (2014), ACM, pp. 307– 310.
[CAS13] CHEN M., ALVES N., SOL R.: Combining spatial and tem- poral information of eye movement in goal-oriented tasks. In Proceedings of the International Conference on Human Fac- tors in Computing and Informatics (2013), Springer, pp. 827– 830.
[CCY*03] CONVERTINO G., CHEN J., YOST B., RYU Y.-S., NORTH C.: Exploring context switching and cognition in dual-view coordi- nated visualizations. In Proceedings of the International Confer- ence on Coordinated and Multiple Views in Exploratory Visual- ization (2003), pp. 55–62.
[CHC10] CONVERSY S., HURTER C., CHATTY S.: A descriptive model of visual scanning. In Proceedings of the BELIV Workshop: Be- yond Time and Errors - Novel Evaluation Methods for Visualiza- tion (2010), ACM, pp. 35–42.
[Chi00] CHI E. H.: A taxonomy of visualization techniques using the data state reference model. In Proceedings of the IEEE Sym- posium on Information Visualization (2000), IEEE Computer Society Press, pp. 69–75.
[CL13] CHEN M., LIM V.: Tracking eyes in service prototyping. In Proceedings of the Human-Computer Interaction-INTERACT (2013), Springer, pp. 264–271.
[CM97] CARD S., MACKINLAY J.: The structure of the information vi- sualization design space. In Proceedings of the IEEE Symposium on Information Visualization (1997), IEEE Computer Society Press, pp. 92–99.
[CN00] CROWE E. C., NARAYANAN N. H.: Comparing interfaces based on what users watch and do. In Proceedings of the Symposium on Eye Tracking Research & Applications (2000), ACM, pp. 29–36.
⃝c 2017 The Authors Computer Graphics Forum ⃝c 2017 The Eurographics Association and John Wiley & Sons Ltd.
[Coc09] COCO M. I.: The statistical challenge of scan-path analysis. In Proceedings of the Conference on Human System Interactions (2009), IEEE Computer Society Press, pp. 372–375.
[CPS06] CROFT J., PITTMAN D., SCIALFA C.: Gaze behavior of spotters during an air-to-ground search. In Proceedings of the Symposium on Eye Tracking Research & Applications (2006), ACM, pp. 163–179.
[CSS*15] CHENG S., SUN Z., SUN L., YEE K., DEY A. K.: Gaze-based annotations for reading comprehension. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (2015), ACM, pp. 1569–1572.
[DDJ*10] DUCHOWSKI A., DRIVER J., JOLAOSO S., TAN W., RAMEY B. N., ROBBINS A.: Scan path comparison revisited. In Proceedings of the Symposium on Eye Tracking Research & Applications (2010), ACM, pp. 219–226.
[DJB10] DORR M., JARODZKA H., BARTH E.: Space-variant spatio- temporal filtering of video for gaze visualization and perceptual learning. In Proceedings of the Symposium on Eye Tracking Re- search & Applications (2010), ACM, pp. 307–314.
[DLN*06] DIXON T. D., LI J., NOYES J., TROSCIANKO T., NIKOLOV S., LEWIS J., CANGA E., BULL D., CANAGARAJAH C.: Scan path analysis of fused multi-sensor images with luminance change: A pilot study. In Proceedings of the International Conference on Information Fusion (2006), IEEE Computer Society Press, pp. 1–8.
[DM98] DUCHOWSKI A., MCCORMICK B. H.: Gaze-contingent video resolution degradation. In Proceedings of the Society of Photo- graphic Instrumentation Engineers (1998), SPIE, pp. 318–329.
[DMC*02] DUCHOWSKI A., MEDLIN E., COURINA N., GRAMOPADHYE A., MELLOY B., NAIR S.: 3D eye movement analysis for VR visual inspection training. In Proceedings of the Symposium on Eye Tracking Research & Applications (2002), ACM, pp. 103–155.
[DMGB10] DORR M., MARTINETZ T., GEGENFURTNER K. R., BARTH E.: Variability of eye movement when viewing dynamic natural scenes. Journal of Vision 10, 10 (2010), 1–17.
[DPMO12] DUCHOWSKI A., PRICE M. M., MEYER M., ORERO P.: Ag- gregate gaze visualization with real-time heatmaps. In Proceed- ings of the Symposium on Eye Tracking Research & Applications (2012), ACM, pp. 13–20.
[Duc02] DUCHOWSKI A.: A breadth-first survey of eye-tracking appli- cations. Behavior Research Methods, Instruments, & Computers 34, 4 (2002), 455–470.
[Duc07] DUCHOWSKI A.: Eye Tracking Methodology: Theory and Practice (2nd edition). Springer, London, 2007.
[EDP*12] ESSIG K., DORNBUSCH D., PRINZHORN D., RITTER H., MAY- COCK J., SCHACK T.: Automatic analysis of 3D gaze coordinates on scene objects using data from eye-tracking and motion-capture systems. In Proceedings of the Symposium on Eye Tracking Re- search & Applications (2012), ACM, pp. 37–44.
[ETK*08] EGUSA Y., TERAI H., KANDO N., TAKAKU M., SAITO H., MIWA M.: Visualization of user eye movement for search result pages. In Proceedings of the International Workshop on Evalu- ating Information Access (2008), National Institute Informatics, pp. 42–46.
[EYH15] ERASLAN S., YESILADA Y., HARPER S.: Eye tracking scan- path analysis techniques on web pages: A survey, evaluation and comparison. Journal of Eye Movement Research 9, 1 (2015), 1–19.
[FBK15] FORTUN D., BOUTHEMY P., KERVRANN C.: Optical flow mod- eling and computation: A survey. Computer Vision and Image Understanding 134 (2015), 1–21.
[FJM05] FITTS P., JONES R., MILTON J.: Eye movement of aircraft pi- lots during instrument-landing approaches. In Ergonomics: Psy- chological Mechanisms and Models in Ergonomics, N. Moray (Ed.), (London and New York, 2005), Taylor & Francis, pp. 56– 66.
[GDS10] GRINDINGER T., DUCHOWSKI A., SAWYER M.: Group-wise similarity and classification of aggregate scanpaths. In Proceed- ings of the Symposium on Eye Tracking Research & Applications (2010), ACM, pp. 101–104.
[GH10a] GOLDBERG J. H., HELFMAN J. I.: Comparing information graphics: A critical look at eye tracking. In Proceedings of the BELIV Workshop: Beyond Time and Errors - Novel Evaluation Methods for Visualization (2010), ACM, pp. 71–78.
[GH10b] GOLDBERG J. H., HELFMAN J. I.: Scanpath clustering and aggregation. In Proceedings of the Symposium on Eye Tracking Research & Applications (2010), ACM, pp. 227–234.
[GH10c] GOLDBERG J. H., HELFMAN J. I.: Visual scanpath representa- tion. In Proceedings of the Symposium on Eye Tracking Research & Applications (2010), ACM, pp. 203–210.
[GK99] GOLDBERG J. H., KOTVAL X. P.: Computer interface eval- uation using eye movements: Methods and constructs. Inter- national Journal of Industrial Ergonomics 24, 6 (1999), 631– 645.
[GSGM10] GOGGINS S., SCHMIDT M., GUAJARDO J., MOORE J.: Assess- ing multiple perspectives in three dimensional virtual worlds: Eye tracking and all views qualitative analysis (AVQA). In Proceed- ings of the Hawaii International Conference on System Sciences (2010), IEEE Computer Society Press, pp. 1–10.
[GWG*64] GUBA E., WOLF W., GROOT S. d., KNEMEYER M., ATTA R. V., LIGHT L.: Eye movement and TV viewing in children. AV Communication Review 12, 4 (1964), 386–401.
[GWP07] GOLDSTEIN R., WOODS R., PELI E.: Where people look when watching movies: Do all viewers look at the same place? Computers in Biology and Medicine 37, 7 (2007), 957–964.
[Ha ̈g82] HA ̈GERSTRAND T.: Diorama, path and project. Tijdschrift voor Economische en Sociale Geografie 73, 6 (1982), 323– 339.
Blascheck et al. / Visualization of Eye Tracking Data 21
⃝c 2017 The Authors Computer Graphics Forum ⃝c 2017 The Eurographics Association and John Wiley & Sons Ltd.
22 Blascheck et al. / Visualization of Eye Tracking Data
[HC13] HOOGE I., CAMPS G.: Scan path entropy and arrow plots: Capturing scanning behavior of multiple observers. Frontiers in Psychology 4, 996 (2013), 1–10.
[HEF*14] HURTER C., ERSOY O., FABRIKANT S., KLEIN T., TELEA A.: Bundled visualization of dynamic graph and trail data. IEEE Transactions on Visualization and Computer Graphics 20, 8 (2014), 1141–1157.
[HFG06] HEMBROOKE H., FEUSNER M., GAY G.: Averaging scan pat- terns and what they can tell us. In Proceedings of the Symposium on Eye Tracking Research & Applications (2006), ACM, pp. 41– 41.
[HH02] HORNOF A. J., HALVERSON T.: Cleaning up systematic error in eye-tracking data by using required fixation locations. Behavior Research Methods, Instruments, & Computers 34, 4 (2002), 592– 604.
[HHBL03] HOLMQVIST K., HOLSANOVA J., BARTHELSON M., LUNDQVIST D.: Reading or scanning? A study of newspaper and net paper reading. In The Mind’s Eye: Cognitive and Applied Aspects of Eye Movement Research, J. Hyo ̈na ̈, R. Radach and H. Deubel (Eds.), (Amsterdam, 2003), Elsevier Science BV, pp. 657–670.
[HHWN02] HAVRE S., HETZLER E., WHITNEY P., NOWELL L.: The- meRiver: Visualizing thematic changes in large document collec- tions. IEEE Transactions on Visualization and Computer Graph- ics 8, 1 (2002), 9–20.
[HNA*11] HOLMQVIST K., NYSTRO ̈M M., ANDERSSON R., DEWHURST R., JARODZKA H., VAN DE WEIJER J.: Eye Tracking: A Compre- hensive Guide to Methods and Measures (1st edition). Oxford University Press, New York, 2011.
[Hol01] HOLSANOVA J.: Picture Viewing and Picture Description: Two Windows on the Mind. PhD thesis, Lund University, 2001.
[HRO11] HUTCHERSON D., RANDALL R., OUZTS A.: Shelf real estate: Package spacing in a retail environment, 2011. http:// andrewd.ces.clemson.edu/courses/cpsc412/fall11/teams/reports/ group9.pdf, Last accessed on October 1, 2016.
[IHN98] ITOH K., HANSEN J. P., NIELSEN F.: Cognitive modelling of a ship navigator based on protocol and eye-movement analysis. Le Travail Humain 61, 2 (1998), 99–127.
[Ins85] INSELBERG A.: The plane with parallel coordinates. The Vi- sual Computer 1, 2 (1985), 69–91.
[ITS00] ITOH K., TANAKA H., SEKI M.: Eye-movement analysis of track monitoring patterns of night train operators: Effects of ge- ographic knowledge and fatigue. In Proceedings of the Human Factors and Ergonomics Society Annual Meeting (2000), SAGE Publications, pp. 360–363.
[JC76] JUST M. A., CARPENTER P. A.: Eye fixations and cognitive processes. Cognitive Psychology 8, 4 (1976), 441–480.
[JK03] JACOB R., KARN K.: Eye tracking in human-computer interac- tion and usability research: Ready to deliver the promises. In The
Mind’s Eye: Cognitive and Applied Aspects of Eye Movement Re- search, J. Hyo ̈na ̈, R. Radach and H. Deubel (Eds.), (Amsterdam, 2003), Elsevier Science BV, pp. 573–605.
[JN00] JONES M. G., NIKOLOV S.: Volume visualization via region en- hancement around an observer’s fixation point. In Proceedings of the International Conference on Advances in Medical Signal and Information Processing (2000), IEEE Computer Society Press, pp. 305–312.
[KBB*17] KURZHALS K., BURCH M., BLASCHECK T., ANDRIENKO G., ANDRIENKO N., WEISKOPF D.: A task-based view on the visual analysis of eye tracking data. In Proceedings of the Eye Track- ing and Visualization: Foundations, Techniques, and Applica- tions (ETVIS 2015) (2017), M. Burch, L. Chuang, B. Fisher, A. Schmidt and D. Weiskopf (Eds.), Springer, pp. 3–22.
[KDX*12] KIM S.-H., DONG Z., XIAN H., UPATISING B., YI J. S.: Does an eye tracker tell the truth about visualizations? Findings while investigating visualizations for decision making. IEEE Transac- tions on Visualization and Computer Graphics 18, 12 (2012), 2421–2430.
[KFN14] KRASSANAKIS V., FILIPPAKOPOULOU V., NAKOS B.: EyeMMV toolbox: An eye movement post-analysis tool based on a two-step spatial dispersion threshold for fixation identification. Journal of Eye Movement Research 7, 1 (2014), 1–10.
[KHH*16] KURZHALS K., HLAWATSCH M., HEIMERL F., BURCH M., ERTL T., WEISKOPF D.: Gaze Stripes: Image-based visualization of eye tracking data. IEEE Transactions on Visualization and Computer Graphics 22, 1 (2016), 1005–1014.
[KHW14] KURZHALS K., HEIMERL F., WEISKOPF D.: ISeeCube: Visual analysis of gaze data for video. In Proceedings of the Symposium on Eye Tracking Research & Applications (2014), ACM, pp. 43– 50.
[Kra03] KRAAK M.-J.: The space-time cube revisited from a geovisu- alization perspective. In Proceedings of the International Carto- graphic Conference (2003), The International Cartographic As- sociation (ICA), pp. 1988–1995.
[KSK13] KREJTZ I., SZARKOWSKA A., KREJTZ K.: The effects of shot changes on eye movement in subtitling. Journal of Eye Movement Research 6, 3 (2013), 1–12.
[KW13] KURZHALS K., WEISKOPF D.: Space-time visual analytics of eye-tracking data for dynamic stimuli. IEEE Transactions on Visualization and Computer Graphics 19, 12 (2013), 2129–2138.
[KW15a] KURZHALS K., WEISKOPF D.: AOI transition trees. In Pro- ceedings of the Graphics Interface Conference (2015), ACM, pp. 41–48.
[KW15b] KURZHALS K., WEISKOPF D.: Eye tracking for personal vi- sual analytics. Computer Graphics and Applications 35, 4 (2015), 64–72.
[Lan00] LANKFORD C.: Gazetracker: Software designed to facilitate eye movement analysis. In Proceedings of the Symposium on
⃝c 2017 The Authors Computer Graphics Forum ⃝c 2017 The Eurographics Association and John Wiley & Sons Ltd.
Eye Tracking Research & Applications (2000), ACM, pp. 51– 55.
[Lat88] LATIMER C. R.: Eye-movement data: Cumulative fixation time and cluster analysis. Behavior Research Methods, Instru- ments, & Computers 20, 5 (1988), 437–470.
[LCK10] LI X., CO ̈LTEKIN A., KRAAK M.-J.: Visual exploration of eye movement data using the space-time-cube. In Geographic Information Science, S. Fabrikan, T. Reichenbacher, M. Kreveld and S. Christoph (Eds.), (Berlin Heidelberg, 2010), Springer, pp. 295–309.
[LHB*08] LORIGO L., HARIDASAN M., BRYNJARSDO ́TTIR H., XIA L., JOACHIMS T., GAY G., GRANKA L., PELLACINI F., PAN B.: Eye track- ing and online search: Lessons learned and challenges ahead. Journal of the American Society for Information Science and Technology 59, 7 (2008), 1041–1052.
[LPP*06] LEE B., PLAISANT C., PARR C. S., FEKETE J.-D., HENRY N.: Task taxonomy for graph visualization. In Proceedings of the AVI Workshop on BEyond Time and Errors: Novel Evalua- tion Methods for Information Visualization (2006), ACM, pp. 1– 5.
[LPS*12] LI R., PELZ J., SHI P., OVESDOTTER Alm C., HAAKE A.: Learning eye movement patterns for characterization of percep- tual expertise. In Proceedings of the Symposium on Eye Tracking Research & Applications (2012), ACM, pp. 393–396.
[MM58] MACKWORTH J. F., MACKWORTH N. H.: Eye fixations recorded on changing visual scenes by the television eye-marker. Journal of the Optical Society of America 48, 7 (1958), 439– 444.
[MSHH11] MITAL P., SMITH T., HILL R., HENDERSON J.: Cluster analy- sis of gaze during dynamic scene viewing is predicted by motion. Cognitive Computation 3, 1 (2011), 5–24.
[Mun08] MUNZNER T.: Process and pitfalls in writing informa- tion visualization research papers. In Information Visualization: Human-Centered Issues and Perspectives, A. Kerren, J. Stasko, J.-D. Fekete and C. North (Eds.), (Boca Raton, London, New York, 2008), Springer, pp. 134–153.
[NH08] NYSTRO ̈ M M., HOLMQVIST K.: Semantic override of low-level features in image viewing – both initially and overall. Journal of Eye Movement Research 2, 2 (2008), 1–11.
[NH10] NAKAYAMA M., HAYASHI Y.: Estimation of viewer’s response for contextual understanding of tasks using features of eye- movements. In Proceedings of the Symposium on Eye Tracking Research & Applications (2010), ACM, pp. 53–56.
[NS71a] NOTON D., STARK L.: Scanpaths in eye movements during pattern perception. Science 171, 3968 (1971), 308–311.
[NS71b] NOTON D., STARK L.: Scanpaths in saccadic eye movements while viewing and recognizing patterns. Vision Research 11, 9 (1971), 929–942.
[OGF14] OPACH T., GOLEBIOWSKA I., FABRIKANT S.: How do people view multi-component animated maps? The Cartographic Jour- nal 51, 4 (2014), 330–342.
[PB06] POOLE A., BALL L.: Eye tracking in human-computer interac- tion and usability research: current status and future prospects. In The Mind’ s Eye: Cognitive and Applied Aspects of Eye Movement Research, J. Hyo ̈na ̈, R. Radach and H. Deubel (Eds.), (Amster- dam, 2006), Elsevier Science BV, pp. 211–219.
[Pfe12] PFEIFFER T.: Measuring and visualizing attention in space with 3D attention volumes. In Proceedings of the Symposium on Eye Tracking Research & Applications (2012), ACM, pp. 29–36.
[PH10] PAPENMEIER F., HUFF M.: DynAOI: A tool for matching eye-movement data with dynamic areas of interest in anima- tions and movies. Behavior Research Methods 42, 1 (2010), 179– 187.
[PLG06] PELLACINI F., LORIGO L., GAY G.: Visualizing Paths in Con- text. Tech. Rep. TR2006-580, Department of Computer Science, Dartmouth College, 2006.
[PR14] PFEIFFER T., RENNER P.: EyeSee3D: A low-cost approach for analyzing mobile 3D eye tracking data using computer vision and augmented reality technology. In Proceedings of the Symposium on Eye Tracking Research & Applications (2014), ACM, pp. 369–376.
[PRPL16] PFEIFFER T., RENNER P., PFEIFFER-LESSMANN N.: EyeSee3D 2.0: Model-based real-time analysis of mobile eye-tracking in static and dynamic three-dimensional scenes. In Proceedings of the Symposium on Eye Tracking Research & Applications (2016), ACM, pp. 189–196.
[PS00] PRIVITERA C., STARK L.: Algorithms for defining visual regions-of-interest: Comparison with eye fixations. IEEE Trans- actions on Pattern Analysis and Machine Intelligence 22, 9 (2000), 970–982.
[PSF*13a] PALETTA L., SANTNER K., FRITZ G., HOFMANN A., LODRON G., THALLINGER G., MAYER H.: A computer vision system for attention mapping in SLAM based 3D models. In ArXiv e-prints (2013), pp. 1–8.
[PSF*13b] PALETTA L., SANTNER K., FRITZ G., MAYER H., SCHRAM- MEL J.: 3D attention: Measurement of visual saliency using eye tracking glasses. In Proceedings of CHI Extended Abstracts on Human Factors in Computing Systems (2013), ACM, pp. 199– 204.
[Pur97] PURCHASE H. C.: Which aesthetic has the greatest effect on human understanding? In Proceedings of Graph Drawing (1997), Springer, pp. 248–261.
[RAM*05] RA ̈IHA ̈ K.-J., AULA A., MAJARANTA P., RANTALA H., KOIVUNEN K.: Static visualization of temporal eye-tracking data. In Proceedings of Human-Computer Interaction-INTERACT (2005), M. F. Costabile and F. Paterno` (Eds.), vol. 3585, Springer, pp. 946–949.
Blascheck et al. / Visualization of Eye Tracking Data 23
⃝c 2017 The Authors Computer Graphics Forum ⃝c 2017 The Eurographics Association and John Wiley & Sons Ltd.
24 Blascheck et al. / Visualization of Eye Tracking Data
[Ray98] RAYNER K.: Eye movements in reading and information processing: 20 years of research. Psychological Bulletin 124, 3 (1998), 372–422.
[RCE12] RASCHKE M., CHEN X., ERTL T.: Parallel scan-path vi- sualization. In Proceedings of the Symposium on Eye Tracking Research & Applications (2012), ACM, pp. 165–168.
[RD05] RICHARDSON D. C., DALE R.: Looking to understand: The coupling between speakers’ and listeners’ eye movements and its relationship to discourse comprehension. Cognitive Science 29, 6 (2005), 1045–1060.
[RHB*14] RASCHKE M., HERR D., BLASCHECK T., BURCH M., SCHRAUF M., WILLMANN S., ERTL T.: A visual approach for scan path com- parison. In Proceedings of the Symposium on Eye Tracking Re- search & Applications (2014), ACM, pp. 135–142.
[RHOL13] RISTOVSKI G., HUNTER M., OLK B., LINSEN L.: EyeC: Coordinated views for interactive visual data exploration of eye- tracking data. In Proceedings of the International Conference on Information Visualization (2013), IEEE Computer Society Press, p. 239.
[RLMJ05] ROSENHOLTZ R., LI Y., MANSFIELD J., JIN Z.: Feature con- gestion: A measure of display clutter. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (2005), ACM, pp. 761–770.
[RSW*16] RASCHKE M., SCHMITZ B., WO ̈ RNER M., ERTL T., DIEDERICHS F.: Application design for an eye tracking analysis based on visual analytics. In Proceedings of the Symposium on Eye Tracking Research & Applications (2016), ACM, pp. 311– 312.
[RTSB04] RAMLOLL R., TREPAGNIER C., SEBRECHTS M., BEEDASY J.: Gaze data visualization tools: Opportunities and challenges. In Proceedings of the International Conference on Information Vi- sualization (2004), IEEE Computer Society Press, pp. 173–180.
[RVM12] RODRIGUES R., VELOSO A., MEALHA O.: A television news graphical layout analysis method using eye tracking. In Proceed- ings of the International Conference on Information Visualization (2012), IEEE Computer Society Press, pp. 357–362.
[SD04] SANTELLA A., DECARLO D.: Robust clustering of eye move- ment recordings for quantification of visual interest. In Proceed- ings of the Symposium on Eye Tracking Research & Applications (2004), ACM, pp. 27–34.
[SG00] SALVUCCI D. D., GOLDBERG J. H.: Identifying fixations and saccades in eye-tracking protocols. In Proceedings of the Sym- posium on Eye Tracking Research & Applications (2000), ACM, pp. 71–78.
[Shn96] SHNEIDERMAN B.: The eyes have it: A task by data type taxonomy for information visualizations. In Proceedings of the IEEE Symposium on Visual Languages (1996), IEEE Computer Society Press, pp. 336–343.
[SLHR09] SIIRTOLA H., LAIVO T., HEIMONEN T., RA ̈IHA ̈ K.-J.: Vi- sual perception of parallel coordinate visualizations. In Proceed- ings of the International Conference on Information Visualization (2009), IEEE Computer Society Press, pp. 3–9.
[SM07] SPAKOV O., MINIOTAS D.: Visualization of eye gaze data using heat maps. Electronics and Electrical Engineering 2, 74 (2007), 55–58.
[SM13] SMITH T. J., MITAL P. K.: Attentional synchrony and the influence of viewing task on gaze behavior in static and dynamic scenes. Journal of Vision 13, 8 (2013), 1–24.
[SMI14]SMI: SMI begaze eye tracking analysis software, 2014. http://www.smivision.com/en/gaze-and-eye-tracking- systems/products/begaze-analysis-software.html, Last accessed on October 1, 2016.
[SND10a] STELLMACH S., NACKE L., DACHSELT R.: 3D attentional maps: Aggregated gaze visualizations in three-dimensional vir- tual environments. In Proceedings of the International Confer- ence on Advanced Visual Interfaces (2010), ACM, pp. 345– 348.
[SND10b] STELLMACH S., NACKE L., DACHSELT R.: Advanced gaze vi- sualizations for three-dimensional virtual environments. In Pro- ceedings of the Symposium on Eye Tracking Research & Appli- cations (2010), ACM, pp. 109–112.
[SNDC09] STELLMACH S., NACKE L., DACHSELT R., CRAIG L.: Trends and techniques in visual gaze analysis. In ArXiv e-prints (2009), pp. 89–93.
[Spa08] SPAKOV O.: iComponent - Device-independent Platform for Analyzing Eye Movement Data and Developing Eye-Based Ap- plications. PhD thesis, University of Tampere, 2008.
[SPK86] SCINTO L., PILLALAMARRI R., KARSH R.: Cognitive strategies for visual search. Acta Psychologica 62, 3 (1986), 263–292.
[SR08] SPAKOV O., RA ̈IHA ̈ K.-J.: KiEV: A tool for visualization of reading and writing processes in translation of text. In Proceed- ings of the Symposium on Eye Tracking Research & Applications (2008), ACM, pp. 107–110.
[SSF*11] SCHULZ C. M., SCHNEIDER E., FRITZ L., VOCKEROTH J., HAPFELMEIER A., BRANDT T., KOCHS E. F., SCHNEIDER G.: Visual at- tention of anaesthetists during simulated critical incidents. British Journal of Anaesthesia 106, 6 (2011), 807–813.
[SYKS14] SONG H., YUN J., KIM B., SEO J.: GazeVis: Interactive 3D gaze visualization for contiguous cross-sectional medical images. IEEE Transactions on Visualization and Computer Graphics 20, 5 (2014), 726–739.
[TAK*05] TORY M., ATKINS S., KIRKPATRICK A., NICOLAOU M., YANG G.-Z.: Eyegaze analysis of displays with combined 2D and 3D views. In Proceedings of the IEEE Symposium on Information Visualization (2005), IEEE Computer Society Press, pp. 519– 526.
⃝c 2017 The Authors Computer Graphics Forum ⃝c 2017 The Eurographics Association and John Wiley & Sons Ltd.
[TC05] THOMAS J., COOK K.: Illuminating the Path. IEEE Computer Society Press, Los Alamitos, 2005.
[TCSC13] TOKER D., CONATI C., STEICHEN B., CARENINI G.: Individual user characteristics and information visualization: Connecting the dots through eye tracking. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (2013), ACM, pp. 295–304.
[TKS*10] TAKEMURA K., KOHASHI Y., SUENAGA T., TAKAMATSU J., OGASAWARA T.: Estimating 3D point-of-regard and visualizing gaze trajectories under natural head movement. In Proceedings of the Symposium on Eye Tracking Research & Applications (2010), ACM, pp. 157–160.
[TM04] TORY M., MO ̈LLER T.: Rethinking visualization: A high- level taxonomy. In IEEE Symposium on Information Visualiza- tion (2004), IEEE Computer Society Press, pp. 151–158.
[Tob08] Tobii Technology AB: Tobii Studio user’s manual version 3.4.5, 2008. http://www.tobiipro.com/siteassets/tobii-pro/user- manuals/tobii-pro-studio-user-manual.pdf/?v=3.4.5, Last ac- cessed on October 1, 2016.
[TTS10] TSANG H. Y., TORY M., SWINDELLS C.: eSeeTrack - Visual- izing sequential fixation patterns. IEEE Transactions on Visual- ization and Computer Graphics 16, 6 (2010), 953–962.
[Ver99] VERTEGAAL R.: The GAZE groupware system: Mediating joint attention in multiparty communication and collaboration. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (1999), ACM, pp. 294–301.
[VH96] VELICHKOVSKY B. M., HANSEN J. P.: New technological win- dows into mind: There is more in eyes and brains for human-
computer interaction. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (1996), ACM, pp. 496– 503.
[vLKS*11] VON Landesberger T., KUIJPER A., SCHRECK T., KOHLHAM- MER J., VAN WIJK J., FEKETE J.-D., FELLNER D.: Visual analysis of large graphs: State-of-the-art and future research challenges. Computer Graphics Forum 30, 6 (2011), 1719–1749.
[WFE*12] WEIBEL N., FOUSE A., EMMENEGGER C., KIMMICH S., HUTCHINS E.: Let’s look at the cockpit: Exploring mobile eye- tracking for observational research on the flight deck. In Proceed- ings of the Symposium on Eye Tracking Research & Applications (2012), ACM, pp. 107–114.
[WHRK06] WEST J., HAAKE A., ROZANSKI E. P., KARN K.: eyePat- terns: Software for identifying patterns and similarities across fixation sequences. In Proceedings of the Symposium on Eye Tracking Research & Applications (2006), ACM, pp. 149–154.
[Woo02] WOODING D. S.: Fixation maps: Quantifying eye- movement traces. In Proceedings of the Symposium on Eye Track- ing Research & Applications (2002), ACM, pp. 31–36.
[Yar67] YARBUS A. L.: Eye Movements and Vision. Plenum Press, 1967.
[YKSJ07] YI J. S., KANG Y.-a., STASKO J., JACKO J.: Toward a deeper understanding of the role of interaction in information visualiza- tion. IEEE Transactions on Visualization and Computer Graphics 13, 6 (2007), 1224–1231.
[YS75] YOUNG L. R., SHEENA D.: Survey of eye movement recording methods. Behavior Research Methods & Instrumentation 7, 5 (1975), 397–429.
Blascheck et al. / Visualization of Eye Tracking Data 25
⃝c 2017 The Authors Computer Graphics Forum ⃝c 2017 The Eurographics Association and John Wiley & Sons Ltd.
SEQIT: Visualizing Sequences of Interest in Eye Tracking Data
Mike Wu
Abstract— Eye tracking is becoming widely used in HCI and many other fields to study user behaviour. Eye tracking data explains user attention pattern in great details and is often large in volume. This complex nature makes analyzing it a challenge. Various visualizations have been designed to aid the data analysis, but none of them focuses sequential patterns in eye gaze, which can reveal insights in user behaviours. SEQIT is a visualization system designed for sequence analysis of eye tracking data. Using defined areas of interest (AOI), SEQIT aggregates fixations into AOI visits and presents sequences of AOI visits in a timeline. SEQIT supports comparison between multiple sequences and exploration of sequence patterns associated with user characteristics. The interface of SEQIT is cleanly designed, fast, and responsive.
   1 INTRODUCTION
Eye trackers are increasingly present in academic research and com- mercial applications [1]. Researchers in human-computer interaction use eye tracking to study the usability of computer interfaces, as it can provide data that describe user behaviour and reveal usability prob- lems that traditional methods might have missed [18]. It has also been used in various medical exams, e.g., for detecting schizophrenia [10], and in developmental psychology to study the perceptual and cognitive abilities of children [8].
Analyzing eye tracking data often involves examining a set of quan- titative metrics, including fixation count, saccade amplitude, pupil size, etc. While statistical analysis is suitable for generating these metrics, visualization techniques can reveal spatial-temporal patterns and trends in the data that can then be verified through statistical test- ing [1]. Thus, the tasks performed on eye tracking visualizations in- clude understanding overall distribution of fixations and identifying sequential patterns of user’s eye movement. To draw conclusions about the behaviours of a group of users requires comparing the pat- terns and finding common ones among users. There has not been a tool that supports these analyses.
In this paper, I present SEQIT, a visualization system that facili- tates discovery and analysis of sequences of interest (SEQIT) in eye tracking data. This system focuses on finding sequence patterns that correlate with certain user characteristics in the eye gaze dataset. The system leverages areas of interest (AOI) defined on the interface and aggregates fixation sequences into AOI visit sequences.
Eye tracking datasets are often large in volume: an HCI experiment often involves tens of participants and hundreds of trials, which result in hundreds of thousands of fixations [1]. On a standard desktop mon- itor with high definition resolution, the design of SEQIT can support visualizing around one hundred sequences at a time and comparing around ten user characteristic attributes.
I begin with a survey of related work in eye tracking visualization and sequence analysis in Section 2. Next in Section 3, I describe the task and data abstractions in designing the visualization. In Section 4, I propose the solution, and the resulting system is presented in Sec- tion 4. Finally, I discuss the strengths and weaknesses of the system and future work in Section 7, and Section 8 summarizes the contribu- tions of this work.
2 RELATED WORK
Because of its spatial and temporal properties, eye tracking data is of- ten analyzed through visualization, and there is a wide range of visu- alization tools that support different aspect of the anlaysis. Sequence analyses of eye gaze have also been done through statistical testing and visualizations.
• MikeWuiswiththeDepartmentofComputerScienceattheUniversityof British Columbia. E-mail: mikewu@cs.ubc.ca.
2.1 Eye Tracking Visualization
Two traditional approaches are plotting the scan path and the heatmap of fixations. Scan path suffer from visual clutter easily as the amount of data visualized increases; heatmap is visually appealing, but it lacks temporal information associated with each fixation [19]. The tempo- ral information, in terms of the sequence of user’s eye movement, can often reveal insights such as the efficiency of the arrangement of ele- ments in the interface and user’s strategies when processing the visual information [18].
Both ISeeCube [13] and the EyeC [20] present the AOI sequence in the form of scarf plots in a timeline, which SEQIT also shares but uses with added purposes.
There has been an increasing number of eye-tracking visualizations developed in recent years [1]. Blascheck et al. surveyed the field and found that many of the systems lack the support for interactive analy- sis [1]. They classified these systems according to a set of categories: SEQIT is both point- and AOI-based and facilitates interactive spatial- temporal analysis with multiple comparisons – there is no existing sys- tem that falls into this set of categories.
2.2 Eye Gaze Sequence Analysis and Visualization
Researchers have made a number of attempts at identifying patterns in eye gaze sequences. West et al.’s eyePatterns tool extract patterns in the AOI sequence using text processing techniques. The tool an- alyzes two types of sequences: expanded and collapsed. In the ex- panded sequences, each element in the AOI sequence is associated with a fixation, whereas in the collapsed sequence, consecutive fixa- tions within the same AOI are “collapsed” into one element in the AOI sequence. This collapsing technique inspired the concept of AOI vis- its in SEQIT, which is described later in Section 4.1. The shortcoming of eyePatterns tool is the removal of durations. Fixation durations are not considered when forming the sequence, so the significance of each fixation are equalized regardless of its duration. The number of fixa- tions that collapsed into one element in the collapsed sequence is also ignored in the analysis.
Statistical analyses using sequential pattern mining have identified, in AOI sequences, patterns that are different between user groups de- fined by their cognitive abilities and task groups defined by task dif- ficulty [22]. These analyses also do not take into account temporal information, such as durations and temporal position of the pattern within the trial.
Transitions between AOIs are sequence patterns of length two, and there are various form of visualization that encodes transition frequen- cies. Blacscheck et al., Holmqvist et al., Tory et al. all visualize transi- tion between AOIs with arrows [2, 9, 23]. They encodes the frequency of transitions with the width of the arrow. None of these systems sup- port sequence patterns longer than two.
Sequence patterns with length longer than two have been visualized in tree structures, where each branch of the tree represents a sequence pattern [20]. Tsang et al. encodes the frequency of the pattern as the size of the node [24]. This tree representation is the only known

 Fig. 1. An example of the task given in the experiment [5]. Eye tracking data from this experiment is used in this paper to illustrate SEQIT.
 visualization of sequences, but these implementations also lack the temporal information such as duration as the statistical analyses of sequences do.
3 TASK AND DATA ABSTRACTIONS
This system is designed to support exploring sequence patterns in eye gaze. Any sequences of interest are generally unknown a priori, so the task is to explore and discover the patterns. Sequences of interest are those that uncover user behaviours, either in general or associated with certain groups of users. Thus, another task is to find correlations between sequence patterns and user characteristics.
As a full analysis tool, it also provides the opportunity for general inspection of the data, in terms of data quality and overall trend. Low data quality in eye tracking may be present in the form of missing data and/or offset in the fixation coordinates resulted from incorrect cali- bration of the eye tracker. Overall trend concerns with the distribution of fixations, the length of the trials, etc.
Eye tracking data can be abstracted as time-series data with spatial properties. Each fixation occurs in a time interval and at a spatial location. Two consecutive fixations are linked by a saccade, which is a rapid movement of the eye.
In interface analyses with eye tracking, meaningful regions in the visual stimulus are often defined as areas of interest (AOI) to assist the analysis of the interface by providing the semantics to the spatial locations of the fixations.
Task and data abstractions are summarized in Table 2.
3.1 Example Data
In this paper, I use the dataset from an experiment conducted with eye tracking as the example [5]. There are 62 participants in the study, each participant performed 80 tasks. In each task, participants were asked to answer a question based on the information presented in a bar chart (Figure 1). User characteristic data recorded in this experi- ment include three cognitive ability measures: perceptual speed, ver- bal working memory, and visual working memory. Each measure is a quantitative value computed from the respective cognitive test. There are five AOIs defined on the interface (Figure 2).
4 SOLUTION
SEQIT uses aggregation and linked multiform views among other vi- sualization idioms. Eye tracking data is presented in multiform views: individual fixations are shown only as details on demand; for overview, I choose to reduce the complexity by aggregating fixations. The views in the interface are connected by linked highlighting and shared colour encoding, and I use interactive components to support exploration and discovery of sequence patterns. The solution is summarized in Table 2.
4.1 AOI Visits
AOI visits are derived from the aggregation of fixations. Successive fixations within the same AOI are grouped as an AOI visit. The du-
Fig. 2. Five AOIs are defined on the interface in the example dataset.
Table 1. Examples of AOI visit. Each row represents a fixation point in the data. The first five data points correspond to two AOI visits.
ration of the AOI visit is the summed durations of individual fixations in the AOI visit. Table 1 gives an example of the relation between fixation and AOI visit. Each AOI is assigned a colour to be applied consistently throughout the interface for representing the AOI.
4.2 Interface Panels
SEQIT consists of four closely connected panels: the review panel, the timeline panel, the user characteristics panel, and the sequence tool panel.
4.2.1 Review Panel
The review panel contains the visual stimulus as the background and overlays additional data. The visual stimulus in the example dataset is a screenshot of the visualization task given to the experiment par- ticipants (Figure 1). Data that can be displayed on top of the visual stimulus include fixations and saccades, a heatmap of fixations, and regions of the AOIs. The visual stimulus background provides the spatial context for the overlaid data to allow easy reference.
   Timestamp
Fixation AOI
  Fixation Duration
 AOI Visit
 AOI Visit Duration
  0
Question
  200
  Question
   650
   200
Question
  250
  450
Question
  200
  650
Legend
  350
 Legend
  600
   1000
Legend
  250
  1250
Label
  300
 ...
 ...

Table 2. System analysis based on the framework by Munzner [14].
4.2.2 Timeline Panel
The timeline panel shows the eye-tracking data in the form of se- quences of AOI visits. The panel shows the data of one task in the experiment at a time, and the eye gaze sequence of each participant is displayed as a row in the timeline panel. Each sequence is visually encoded in a series of horizontally stacked bars to represent the AOI visits, with the length of each bar denoting the duration of that AOI visit, and its colour corresponding to the AOI of the visit. Thus, the timeline panel contains multiple rows of AOI visit sequences, aligned on the left at trial start time.
When users hover over a row in the timeline, a fisheye distortion is applied to zoom into this particular row, and the colours of other rows are faded out to further highlight the selected row. At the same time, the fixations and saccades associated with the selected trial are drawn in the review panel on top of the visual stimulus.
The timeline can be presented in two time scales: absolute and rel- ative. In absolute time scale, the actual times of the trials are mapped consistently across all of the rows, so that the total length of a bar represents the total duration of the trial. In relative time scale, the proportion of trial completed acts as the time scale, under which all bars are of the same length and aligned at the rightmost point, which denotes 100% of trial completed.
4.2.3 User Characteristics Panel
The user characteristics panel presents the cognitive ability measures of the participants. Each measure form a bar chart, with the bar for a participant aligned with the corresponding row in the timeline. The bar chart of each measure can be sorted by its value, and the ordering of the rows in the timeline is updated accordingly at the same time. Sorting allows users of SEQIT to find correlations between user characteristics as well as between any user characteristic and sequence patterns in the timeline.
4.2.4 Sequence Tool Panel
Using the sequence tool, users can create sequence patterns and visu- alize the occurrences of these patterns on the timeline. After clicking on the “New sequence” button, users can select an AOI in the review panel to form a sequence pattern. Created sequences are saved in a list in this panel.
4.3 Interface Interactions
The four panels are connected through linked highlighting. When users hovering over the region of an AOI in the review panel, the re- gion is highlighted and labeled, while in the timeline view, every bar that represents a visit to this AOI is also highlighted. Similarly, in the sequence tool panel, when users hovering over a saved sequence pattern, any occurrence of such pattern is highlighted in the timeline view.
5 IMPLEMENTATION
The visualization is implemented as a web application. It is written in HTML, CSS, and Javascript, and I built it with a number of Javscript libraries. D3.js [4] is the core library for drawing geometric compo- nents and applying animated transition on various actions. The fisheye distortion effect in the timeline panel and user characteristics panel is implemented with D3’s fisheye plugin. The animation for sorting the timeline rows is inspired by Mike Bostock’s sortable bar chart exam- ple [3]. The heatmap of fixations is created with Heatmap.js [25]. The
tooltips used in SEQIT are those from d3.tip [17]. The toggle buttons are Bootstrap Toggle [11]. The user characteristics icons are from the Font Awesome icon library [6]. The overall layout in the application is built with the responsive bootstrap framework [16].
There are two main SVG objects, one forms the review panel and the other combines both the user characteristics panel and the timeline panel. I used D3.js to draw the individual components such as the fixation points and the saccades in the review panel, the AOI visits in the timeline panel, and the bar charts in the user characteristics panel. The actions of the buttons and controls are implemented in Javascript with the help of jQuery [12].
I designed and implemented the algorithms and the logics behind the linked highlighting described in Section 4.3. I also wrote scripts in python to transform data outputted from the eye tracker to JSON format to be read as input by SEQIT, and once it is read, I store the data in structures that allow easy and efficient access.
6 RESULTS
Figure 3 shows SEQIT in action. The review panel is in the upper-left portion of the interface. The visual stimulus, in this case a screenshot of the experiment task, is shown as the background. A toggle in the top-right corner of the review panel shows and hides the heatmap.
Below the review panel is the sequence tool panel, which displays sequence patterns that are created. The sequences are colored based on the AOI colours. The “New sequence” button is located in the top-right corner of the sequence tool panel. Hovering over any created pattern highlights the occurrences of the pattern in the timeline (Figure 5).
To the right of these two panels is the user characteristics panel. The three cognitive ability measures in the dataset are presented as bar charts in the three columns. Sorting controls are located in the header of each column, clicking on which would toggle between ascending and descending sort by each measure, and the state of the toggle would change to reflect the current sorting behaviour.
To the right of the user characteristics panel is the timeline panel, where each row is aligned with the bar of the user’s characteristics. The AOI visit sequences are presented as stacked bars according to the description in Section 4.2.2. A legend for the AOI colour coding is positioned at the top of the timeline panel, and a toggle for switching between absolute and relative time scale is to the right of the legend in the top-right corner of the timeline panel. Figure 3 shows the timeline in absolute time scale, and Figure 4 shows it in relative scale.
The fisheye distortion when users hovering over the timeline is de- picted in Figure 4, where rows that are not selected are faded out, and the review panel shows the fixations and the saccades associated with the selected trial.
6.1 Scenarios of Use
Jim is a HCI researcher who just conducted a user study for his newly designed user interface, measured the cognitive abilities of the partici- pants, and collected eye tracking data while the participants were per- forming tasks on the interface. He has exported the data from the eye tracker, transformed it to the format supported by SEQIT, and loaded it into the system.
6.2 Scenario 1: Data Inspection
Jim first takes a holistic look at the timeline view. In a number of rows, he finds large white spaces among the otherwise colourful AOI visits. These white spaces indicate gaps in the fixation data. He then hovers over these rows individually and examine the fixation and sac- cade plots shown in the review panel. He notes down any trials with potentially low data quality for future crosschecking with other trials performed by those users.
6.3 Scenario 2: Overall Trends
Jim sorts the user characteristic and the timeline panels by each of the cognitive ability measures he recorded in the study. He did not see any noticeable correlations between any pair of the measures, but he will confirm this observation later through statistical analysis.
   System
 SEQIT
  What: Data
 Time-series data with spatial property.
  What: Derived
 Aggregation of time-series data according to spatial semantic.
  Why: Task
 Inspect data and explore to discover sequences of interests.
  How: Encode
 Timeline with stacked bar charts. Color by spatial semantic.
  How: Facet
 Multiform with linked coloring and highlighting.
  How: Reduce
 Filtering.
  Scale
 User characteristics column: ten. Timeline rows: one hundred.

 Fig. 3. SEQIT, with heatmap toggled on, timeline in relative scale and sorted by perceptual speed. The review panel is in the top-left corder, below which is the sequence tool, and to their right are the user characteristics panel and the timeline panel.
 Fig. 4. Fisheye distortion is applied when users hover over a row in the timeline, which is in absolute time scale in this figure. Timeline rows that are not hovered are faded out. The fixations and saccades from the trial represented by the hovered row are displayed in the review panel.
 Fig. 5. Sequence pattern creation process. Two AOIs, Question and Legend, have been added to the sequence pattern, as shown in the sequence tool panel in the bottom-right corner. The cursor is hovering over the Question AOI, so every occurrence of the pattern Question-Legend-Question is highlighted in the timeline panel. Note the uneven distribution of the pattern towards the lower half of the timeline that is associated with participants with low verbal working memory capacity.
He sets the timeline to absolute scale and looks to see if the length of the sequences is correlated with any of the user characteristics. He noticed that users with high perceptual speed tend to finish the task faster than those with low perceptual speed.
Then, he sets the timeline to relative scale, hovers over each AOI in the review panel, and examines visits to this AOI in the timeline in terms of location, duration, and frequency to understand the user behaviour in general. He tries sorting the timeline by each user char- acteristic to see if there are any trends related to these measures.
6.4 Scenario 3: Sequence Pattern Discovery
Jim wants to explore some patterns in the AOI visit sequences, so he goes to the sequence tool panel. After pressing the “New sequence” button, he clicks on the Question AOI, the Legend AOI, and again the Question AOI in the review panel sequentially to define the pattern, which is being added to the list of patterns in the sequence tool panel (Figure 5). At each step during this process, he examines the occur- rences of the pattern he has created thus far in the timeline. Finally, he clicks on the green checkmark button to complete the creation of this pattern.
He repeats this process to create a number of other sequence pat- terns. He sorts the timeline by each user characteristic and hovers over each pattern to look for trend in the occurrences of these patterns in terms of location, duration, and frequency. He finds that the Question- Legend-Question pattern occur more frequently for users with low ver- bal working memory capacity, so he notes it down for future analysis across all tasks in the experiment.
7 DISCUSSION AND FUTURE WORK
SEQIT aims to aid the exploration and analysis of eye tracking data, which is often large in volume, so the system employs the “overview first, then details-on-demand” mantra [21] as well as focus+context idioms. SEQIT presents the eye tracking data in both the fixation level as well as the aggregated AOI-visit level. The system supports tasks in different stages of the analysis workflow, while giving the flexibility of controls to various parameters.
The interface is carefully laid out and elements nicely align with each other to produce a clean design overall. The web application is
built with the bootstrap framework, so layout of the content is adapted to the screen size – most of the features are fully functional on a mobile device. Navigations are clear and easy to use, and icons are used whenever possible to increase the recognizability of the controls. The colours that represent the AOIs are distinct and used consistently across the interface.
Currently, there are certain limitations to the system. First, it sup- ports analysis of only one task at a time, so users cannot compare trends across all tasks. Second, the AOIs must be pre-defined and loaded into the system, that is, dynamic defining AOI is not yet possi- ble. Third, more detailed information can be encoded into the timeline rows to take advantage of the fisheye zooming, which might have a relatively low utility value in the current stage.
For future work, I will resolve the limitations described above among other improvements. I will add the support for comparison across multiple tasks, in which the timeline panel will show all of the tasks performed by one user or a condensed representation of every task by each users. Custom AOIs can be dynamically defined by se- lecting any regions in the review panel. I will design and implement easy-to-use export tools to save any findings made in SEQIT. Details of the interface can be further fine-tuned, for example, I will add a time axis to the timeline and tooltips for the user characteristic icons, and I will try lowering the saturation in the AOI colours used in the time- line to improve its visual appearance. The style of the saccade can be optimized for easy tracing based on the results in two comparison studies [7, 15].
Developing this visualization allows me to understand my data in a much clearer way. By laying down the sequences in a timeline, I am able to see the gaps and the mix of short and long fixations in the sequences. These findings force me to rethink the process of eye tracking data analyses in the future.
8 CONCLUSIONS
I presented a visualization system, named SEQIT, for sequence anal- ysis of eye tracking data. It supports tasks at various stages of data analysis: the timeline gives an overview of the AOI visits, the se- quence tool supports defining and visualizing specific sequence pat- terns, and the review panel shows the fixations and saccades of the
selected trial. The interface is cleanly designed, fast, responsive, and easy to use. The system offers various controls, such as a toggle to display the time-series data in different scales. Users can perform the intended task of discovering sequences of interests easily in SEQIT. SEQIT with the example dataset is open sourced1 and available for viewing online2; a number of sequences of interest have already been discovered through the use of SEQIT.
ACKNOWLEDGMENTS
The author wish to thank Tamara Munzner for her mentoring and feedback throughout this project and the Information Visualization course, Kailun Zhang and Antoine Ponsard for their inputs on the de- sign of the system, Cristina Conati, Giuseppe Caranini, Dereck Toker, Matthew Gingerich, and Se ́bastien Lalle ́ for their guidance in the re- search project that inspired SEQIT.
REFERENCES
[1] T. Blascheck, K. Kurzhals, M. Raschke, M. Burch, D. Weiskopf, and T. Ertl. State-of-the-art of visualization for eye tracking data. In Proc. EuroVis, 2014.
[2] T. Blascheck, M. Raschke, and T. Ertl. Circular heat map transition di- agram. In Proc. Conf. Eye Tracking South Africa (ETSA), pages 58–61, 2013.
[3] M.Bostock.Sortablebarchart.
[4] M. Bostock, V. Ogievetsky, and J. Heer. D3: Data-driven docu-
ments. IEEE Trans. Visualization and Computer Graphics (Proc. Info-
Vis), 17(12):2301–2309, 2011.
[5] G. Carenini, C. Conati, E. Hoque, B. Steichen, D. Toker, and J. Enns.
Highlighting interventions and user differences: informing adaptive in- formation visualization support. In Proc. Conf. Human Factors in Com- puting Systems (CHI), pages 1835–1844, 2014.
[6] D. Gandy. Font Awesome.
[7] J. H. Goldberg and J. I. Helfman. Visual scanpath representation. In
Proc. Sympos. Eye Tracking Research and Applications (ETRA), pages
203–210, 2010.
[8] G. Gredeba ̈ck, S. Johnson, and C. von Hofsten. Eye tracking in infancy
research. Developmental Neuropsychology, 35(1):1–19, 2009.
[9] K. Holmqvist, J. Holsanova, M. Barthelson, and D. Lundqvist. Read- ing or scanning? a study of newspaper and net paper reading. Mind,
2(3):657–670, 2003.
[10] P.S.Holzman,L.R.Proctor,andD.W.Hughes.Eye-trackingpatternsin
schizophrenia. Science, 181(4095):179–181, 1973.
[11] M. Hur. Bootstrap Toggle.
[12] jQuery Foundation. jQuery.
[13] K. Kurzhals, F. Heimerl, and D. Weiskopf. ISeeCube: visual analysis of
gaze data for video. In Proc. Symp. Eye Tracking Research and Applica-
tions (ETRA), pages 43–50, 2014.
[14] T. Munzer. Visualization Analysis and Design. CRC Press, 1st edition,
2014.
[15] R. Netzel, M. Burch, and D. Weiskopf. Comparative eye tracking study
on node-link visualizations of trajectories. 20(12):2221–2230, 2014.
[16] M.OttoandJ.Thornton.Bootstrap.
[17] J.Palmer.d3-tip:Tooltipsford3.jsvisualizations.
[18] A. Poole and L. J. Ball. Eye tracking in HCI and usability research.
Encyclopedia of human computer interaction, 1:211–219, 2006.
[19] M. Raschke, T. Blascheck, and M. Burch. Visual analysis of eye track- ing data. In Handbook of Human Centric Visualization, pages 391–409.
2014.
[20] G.Ristovski,M.Hunter,B.Olk,andL.Linsen.EyeC:Coordinatedviews
for interactive visual exploration of eye-tracking data. In Int. Conf. Infor-
mation Visualisation (IV), pages 239–248, 2013.
[21] B. Shneiderman. The eyes have it: A task by data type taxonomy for
information visualizations. In Proc. IEEE Symp. Visual Languages, pages
336–343, 1996.
[22] B.Steichen,M.M.Wu,D.Toker,C.Conati,andG.Carenini.Te,Te,Hi,
Hi: Eye gaze sequence analysis for informing user-adaptive information visualizations. In Proc. Conf. User Modeling, Adaptation and Personal- ization (UMAP), pages 183–194, 2014.
1 https://github.com/m- wu/cs547 2 http://cs.ubc.ca/ ̃mikewu/cs547
[23] M. Tory, M. S. Atkins, A. E. Kirkpatrick, M. Nicolaou, and G.-Z. Yang. Eyegaze analysis of displays with combined 2D and 3D views. In IEEE Visualization (VIS), pages 519–526, 2005.
[24] H.Y.Tsang,M.Tory,andC.Swindells.eSeeTrack–visualizingsequen- tial fixation patterns. IEEE Trans. Visualization and Computer Graphics, 16(6):953–962, 2010.
[25] P.Wied.heapmap.js:Dynamicheatmapsfortheweb.
    See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/305996566 ScanGraph: A novel scanpath comparison
method using graph cliques visualization
Article in Journal of Eye Movement Research · August 2016 DOI: 10.16910/jemr.9.4.5
CITATIONS READS
2 53
2 authors:
Jitka Doležalová
Palacký University of Olomouc
7 PUBLICATIONS 12 CITATIONS SEE PROFILE
Stanislav Popelka
Palacký University of Olomouc
27 PUBLICATIONS 101 CITATIONS SEE PROFILE
       All content following this page was uploaded by Stanislav Popelka on 05 September 2016.
The user has requested enhancement of the downloaded file. All in-text references underlined in blue are added to the original document and are linked to publications on ResearchGate, letting you access and read them immediately.
Journal of Eye Movement Research 9(4):5, 1-13.
  ScanGraph: A Novel Scanpath Comparison Method Using Visualisation of Graph Cliques
  Jitka Dolezalova Palacký University Olomouc, Olomouc, Czech Republic
Stanislav Popelka Palacký University Olomouc, Olomouc, Czech Republic
 The article describes a new tool for analysing eye-movement data. Many different ap- proaches to scanpath comparison exist. One of the most frequently used approaches is String Edit Distance, where gaze trajectories are replaced by sequences of visited Areas of Interest. In cartographic literature, the most commonly used software for scanpath comparison is eyePatterns. During an analysis of eyePatterns functionality, we found that the tree-graph visualisation of its results is not reliable. Thus, we decided to develop a new tool called ScanGraph. Its computational algorithms are modified to work better with sequences of dif- ferent length. The output is visualised as a simple graph, and similar groups of sequences are displayed as cliques of this graph. This article describes ScanGraph’s functionality on a simple cartographic eye-tracking study example. Differences in the reading strategy of a simple map between cartographic experts and novices were investigated. The paper should aid researchers who would like to analyse the differences between groups of participants, and who would like to use our tool, available at www.eyetracking.upol.cz/scangraph.
    Keywords: eye movement, eye tracking, scanpath, scanpath comparison, string edit distance, usability, cartography, application
 Introduction
This article describes a new tool for scanpath comparison using visualisation of graph cliques. This tool will allow to find similarities between participants’ process of presented stimuli observation. With information about their personal characteristics (age, sex, knowledge, etc.) it is possible to reveal if these groups are using a similar strategy. The output of the tool is a simple graph. In this graph cliques are identified. A clique is a subset of vertices in a graph where all vertices are connected by an edge with all of the others from that subset. These cliques represents participants with similar sequences of visited Areas of Interest (similar approaches to observing stimuli). The advantage over other scanpath comparison techniques is
that the visualisation highlights only those participants that are similar according to the user-defined value of the degree of similarity.
In the introduction, the history and background of scanpath comparison is described with an emphasis on the most frequently used method – String Edit Distance. Also, the software eyePatterns is mentioned. During analysis of the eyePatterns outputs, it was found that results were not reliable. Its weaknesses are described in the first part of the methods section. Based on these findings, we decided to develop a new tool called ScanGraph, which calculates the similarities between scanpaths and visualises results in the form of graph cliques. The basic theory of simple graphs and cliques is also described in the methods section. The results section contains detailed information about ScanGraph. The functionality of the application is presented practically on an example of a model case study from the field of cartography. The results provide brief information about the model case study, and then an example of practical use of ScanGraph is presented. In the discussion section, the limitations of ScanGraph are described together with future proposals how to eliminate them.
 Received April 4, 2016; Published August 5, 2016.
Citation: Dolezalova, J. & Popelka, S. (2016). ScanGraph: A novel scanpath comparison method using visualisation of graph cliques. Journal of Eye Movement Research, 9(4):5, 1-13.
Digital Object Identifier: 10.16910/jemr.9.4.5
ISSN: 1995-8692
This article is licensed under a Creative Commons Attribution 4.0
International license.
   1
Journal of Eye Movement Research Dolezalova, J. & Popelka, S. (2016) 9(4):5, 1-13. ScanGraph: A Novel Scanpath Comparison Method Using Visualisation of Graph Cliques
During analysis of eye-tracking data using an average of eye-movement measures as fixation counts and dura- tions, eye-movement behaviour unfolding a particular se- quence over time is ignored. This sequence is a rich source of information (Anderson, Anderson, Kingstone, & Bischof, 2014). To analyse sequences of eye-move- ments, a large number of methods comparing scanpaths has been developed. These methods are collectively known as scanpath comparison.
The beginnings of interest in distinctive scanning pat- terns can be found in the study of Noton and Stark (1971), who reported a qualitative similarity in eye-movements when people viewed line drawings on multiple occasions. This observation was used to support the “Scanpath The- ory”, which proposed that visual features were encoded and stored alongside a motor memory of the scanpath made during perception. When a picture is seen again, it is recognised by executing the stored scanpath and matching the sequential features (Foulsham et al., 2012). The scan- path comprises sequences of alternating saccades and fix- ations that repeat themselves when a respondent is viewing stimuli. Scanpath comparison methods can be divided into six groups (String Edit Distance, ScanMatch, Sample- based Measures, Linear Distance, MultiMatch and Cross- recurrence Quantification Analysis). An overview of these methods and their comparison is described in Anderson et al. (2014).
One of the most frequently used methods is String Edit Distance, which is used to measure the dissimilarity of character strings. As Duchowski et al. (2010) mentions, scanpath comparison based on the String Edit Distance in- troduced by Privitera and Stark (2000) was one of the first methods to quantitatively compare not only the loci of fix- ations but also their order.
When using String Edit Distance, the grid or Areas of Interest (AOI) have to be marked in the stimulus. The gaze trajectory (scanpath) is then replaced by a character string representing the sequence of fixations with characters for AOIs they hit. Only 10 percent of the scanpath duration is taken up by the collective duration of saccadic eye-move- ments. Fixations in the created Areas of Interest took 90 percent of the total viewing period (Bahill & Stark, 1979). A sequence of transformations (insertions, deletions, and substitutions) is used to transform one string to another. Their similarity is represented as the number of transfor- mation steps between two analysed strings (Anderson
et al., 2014). Foulsham et al. (2012) pointed to the disad- vantage of String Edit Distance, which is reducing dis- tances to binary classification (because of the necessity of dividing stimuli on a grid or creating Areas of Interest). For some applications, as in cartography, this disadvantage can be turned into an advantage – for example, when ana- lysing the behaviour of respondents to map composition elements.
One of the most used metrics calculating the distance between sequences is called Levenshtein distance, named after the Russian scientist Vladimir Levenshtein (Levenshtein, 1966). The Levenshtein distance between two strings         ... | |;         ... | | of length | | a | | (let us denote      ,   ) is the lowest number of deletions, insertions, or substitutions required to transform the source string into a target string. For example, the dis- tance between sequences “gravitation” and “gravidity” is equal to 5 (three changes and two deletions). Hence,      ,    ∈   ,      ,      0 if and only if the strings are equal and      ,      max | |, | |  if and only if there is any correspondence between the strings. The value of the Levenshtein distance increases with larger differ- ences between the strings. The Levenshtein method was the first used for searchpath and scanpath analysis in the study of Choi, Mosley, and Stark (1995) and Stark and Choi (1996).
The other possible metric is called the Needleman-Wun- sch algorithm with its scoring system. The Needleman- Wunsch algorithm (let us denote its value     ,  ) searchesforconcordantelementsbetweentwostrings
    ... | |;         ... | | of the length | | a | |. The basic scoring system used for our needs is given by Match reward   1, Gap cost   0 and Mismatch penalty    1. For example, the distance between “gravitation” and “gra- vidity” is equal to 6 (six matches). Hence,     ,    ∈   ,     ,     min | |,| | ,when isasubsetof or is a subset of  . The value of     ,    increases with the similarity between the strings.
In our issue, we want to count the distances between each pair of sequences from a certain set. Both of these met- rics properly work when all of the compared strings have the same length. But when the lengths of the sequences are not equal, a serious problem arises. Let us show an example with Levenshtein distance. Let        ,        ,                  ,               . The distances are      ,   3,     ,    7,thus      ,
          2
Journal of Eye Movement Research Dolezalova, J. & Popelka, S. (2016) 9(4):5, 1-13. ScanGraph: A Novel Scanpath Comparison Method Using Visualisation of Graph Cliques
     ,   . But whereas the sequences   and   have similar parts, the sequences   and   are totally different.
String Edit Distance measurement was used for the evaluation of web page design (Josephson & Holmes, 2002). Areas of Interest were marked on the web page, and alphabetic code was assigned to each of them. Then, the eye-path sequence for each subject’s viewing of each web page by recording the sequence of fixations was created. Sequences were compared with the use of the Optimal Matching Analysis tool (Chan, 1995). Fabrikant, Rebich- Hespanha, Andrienko, Andrienko, and Montello (2008) analysed eye-movement data recorded in controlled exper- iments on small-multiple map (a series of similar maps us- ing the same scale, allowing them to be easily compared) displays with the use of ClustalG software (Wilson, Harvey, & Thompson, 1999). Clustal software packages are widely used for analysing gene sequences in DNA and proteins. ClustalG was developed specifically to analyse social-science data. Based on the results of visual geoana- lytical approaches with sequence alignment analysis tech- niques, it was found that small-multiple displays cannot generally be computationally or informationally equiva- lent to non-interactive animations (animations which can- not be controlled – merely the playback of the video).
In 2006, West, Haake, Rozanski, and Karn (2006) in- troduced the software eyePatterns – software that uses well-established sequence analysis algorithms designed primarily to aid eye-movement researchers in comparing sequence patterns within and across experimental groups of subjects. Apart from String Edit Distance, eyePatterns also integrates transition frequency analysis, clustering, se- quence alignment, and pattern discovery.
For research in the field of cognitive cartography, String Edit Distance is the most important feature. Based on eye-movement data, this method can answer questions such as “How is it possible that one person orientates themselves in a map very quickly, while it takes others a long time?”, “Is there a difference in map reading be- tween men and women?”, or “Do all people look at maps the same way?” In some cartographic studies, sequence alignment methods were also used for non-eye-movement data. For example Joh, Arentze, Hofman, and Timmermans (2002) developed a new measure for similar- ity between activity patterns in activity-travel patterns data. Shoval and Isaacson (2007) used sequence alignment for analysing sequential aspects within the temporal and spatial dimensions of human activities.
String Edit Distance in eyePatterns was used, for ex- ample, in the study by Coltekin, Fabrikant, and Lacayo (2010), who analysed dynamic visual analytics displays. Levenshtein (Levenshtein, 1966) and Needleman-Wunsch (Needleman & Wunsch, 1970) algorithms implemented in eyePatterns were used to generate a distance matrix. Data were visualised in eyePatterns with a tree-graph con- structed by a hierarchical clustering algorithm. In this tree- graph, clusters of participants were identified visually. To- gether with Path Similarity Analysis (Andrienko, Andrienko, Burch, & Weiskopf, 2012) eyePatterns was also used in the author’s study by Popelka, Dvorsky, Brychtova & Hanzelka (2013). The aim of the study was to identify the typology of map readers (common behav- ioural characteristics identical or similar between more in- dividuals) based on their eye-movements while solving ge- ographical tasks with the use of a map.
Methods
eyePatterns and its disadvantages
West et al. (2006) states that eyePatterns uses hierar- chical clustering for calculating sequence similarity. Clus- tering partitions data into subsets of items that share simi- lar traits. Agglomerative hierarchical clustering builds a hierarchy of clusters, beginning with the two most closely related sequences, and ending with the most distant sequence or cluster. The hierarchy tree can be visualised, exposing outlying and the most similar sequences (West et al., 2006).
When we analysed outputs from eyePatterns, we found that sequences with the lowest value of Levenshtein dis- tance have (correctly) the closest possible distance (two edges between them), because the algorithm starts with them. These two sequences now make a cluster and dis- tances in the matrix are recalculated using this cluster in- stead of the original nodes (sequences). eyePatterns uses the average of distances between the pair of sequences making a new cluster. Due to this clustering, the distances between nodes in the tree-graph are distorted. The distance is now calculated for the average value for the whole clus- ter. The problem is that the distance between particular nodes inside the cluster towards the other node can be lower than the distance between this node and the average value for the whole cluster. From the tree-graph, it is not possible to distinguish in which cases the distances be- tween original sequences were used and where the average
            3
Journal of Eye Movement Research Dolezalova, J. & Popelka, S. (2016) 9(4):5, 1-13. ScanGraph: A Novel Scanpath Comparison Method Using Visualisation of Graph Cliques
 for the cluster was used. Hence, tree-graph visualisation doesn’t correspond to the statement used in the eyePatterns interface – “The fewer branches that are between two se- quences, the more similar those sequences are”, as is illus- trated below in Figure 1.
Twenty scanpath strings (non-collapsed, marked as S1 – S20) from the stimulus used in model case study (see section “Model Case Study”) were used to highlight the inaccuracy/errors of similarity calculation in eyePat- terns. The tree-graph in Figure 1 displays the output of Le- venshtein distance (“default scoring scheme”) calculation from eyePatterns. Blue labels S1-S10 display participants GIS1-GIS10 (cartographers). Red labels S11-S20 stand for participants NOGIS1-NOGIS10 (non-cartographers).
Figure 1. Output of eyePatterns – a tree-graph constructed by a hierarchical clustering algorithm
Figure 2 displays the tree-graph from Figure 1 with four highlighted sequences (participants). The closest highlighted pair in the tree-graph contains sequences S1 and S17. This should mean that the sequences are very similar. However, the Levenshtein distance between these two sequences was 13. Compare with the pair S12 and S14, lying on the opposite sides of the tree-graph. This should mean that the sequences differ a lot. The Le- venshtein distance of these two sequences is only 4 – which means that only four changes are necessary to change one sequence to another. A similar situation is visible from the dendrogram (Figure 3) displaying the same data.
Figure 2. Tree-graph from eyePatterns with highlighted inaccuracies
Figure 3. Tree-graph from eyePatterns redrawn to a dendrogram
Trajectories represented by sequences S1 and S17 were displayed in an OGAMA Scanpath module (Figure 4). It is evident that these two trajectories are very different as it is also obvious from their sequences (S1=EAAAAA AAAAAAAAAAAAACCC, S17=AAAAACCBBCCCC AAAA). Participant S1 (blue line) performed fewer fixa- tions in the map. He also visited AOI E (Map title) and B (Map of Alaska), while no fixations from participant S17 (red line) were recorded there.
  4
Journal of Eye Movement Research 9(4):5, 1-13.
Dolezalova, J. & Popelka, S. (2016) ScanGraph: A Novel Scanpath Comparison Method Using Visualisation of Graph Cliques
A binary relation R on A is set to be a relation of equiv- alence if it is reflexive, symmetric, and transitive.
A partition of a set   is by definition a union of subsets    that cover   but do not intersect each other:     ⋃       ,    ∩      ∅, ∀ ,   ∈  1, ... ,   . Given a relation of equivalence, we denote by       the class of equivalence of an element  :            ∈   ∶     . Two elements have the same class if and only if they are in relation:               ⇔    . This is a direct consequence of transitivity and symmetry. Hence, given a relation of equivalence on a set  , its classes of equivalence form a partition of the set  .
A binary relation   on a set   is set to be a relation of tolerance if it is reflexive and symmetric.
The notion of tolerance relation is an explication of similarity or closeness.
As an analogy of equivalence classes and partitions, here we have tolerance classes and coverings. A set   ⊂   is called a tolerance preclass if it holds that for all  ,   ∈  ,   and   are tolerant, i.e.    . A maximum preclass is called a tolerance class. So two tolerance classes can
have common elements.
Given a non-empty set  , a collection ∏       of non-
empty subsets of   such that ⋃ ∈    is called a covering of  . Given a tolerance relation on a set  , the collection of its tolerance classes forms a covering of  .
Every partition is a covering; not every covering is a partition (Chajda, 2005).
Simple graphs
A graph       ,    is defined as a structure of two finite sets   and  . The elements of   are called vertices, and the elements of   are called edges. Each edge has a set of one or two vertices associated with it, which are called its endpoints.
An edge is said to join its endpoints. A vertex joined by an edge to a vertex   is said to be a neighbour of  .
A proper edge is an edge that joins two distinct vertices. A self-loop is an edge that joins a single endpoint to itself. A multi-edge is a collection of two or more edges hav-
ing identical endpoints.
A simple graph has neither self-loops nor multi-edges
(Gross & Yellen, 2005).
When we use the term graph without a modifier, we
mean a simple graph.
 Figure 4. Comparison of trajectories S1 and S17, which were selected by eyePatterns as very similar
After discovering the inaccuracy of eyePatterns, we de- cided to develop our own application called ScanGraph for finding similar sequences in eye-tracking data. Compared with eyePatterns, where all sequences were included in the tree-graph, ScanGraph highlights only those sequences that are similar according to pre-set parameters. The result is displayed as a simple graph and similar groups are dis- played as cliques of this graph.
Novel approach – ScanGraph
Our aim was to create a new tool that will work on the principle of binary relation. The task of finding groups of similar elements is equivalent to the task of seeking toler- ance classes of a tolerance relation. This is also equivalent to the problem of finding cliques in a simple graph and can be easily and clearly visualised. The necessary terms are defined below.
Binary relations
A binary relation between two sets   and   is a subset of the Cartesian product      . A binary relation on set   isasubsetof   .
When an element   ∈   is in a relation to an element   ∈  wewrite   .
Given a binary relation   on a set   we have the fol- lowing definitions:
A relation   on a set   is called reflexive if and only if     for every element   ∈  .
A binary relation   on a set   is called symmetric if and only if for any   and   in  , whenever    , then    .
A binary relation   on a set   is called transitive, if and only if for any  ,   and   in  , whenever    ,    , then    .
 5
Journal of Eye Movement Research 9(4):5, 1-13.
Cliques
A subset   of      is called a clique if every pair of vertices in   is joined by at least one edge, and no proper superset of   has this property.
Thus, a clique of a graph   is a maximal subset of mu- tually adjacent vertices in  .
The clique number of a graph   is the number      of vertices in the largest clique in  .
A complete graph is a simple graph such that every pair of vertices is joined by an edge (Gross & Yellen, 2005).
Results
ScanGraph Application
The application was created using PhP and C# (Backend) and D3.js (Frontend). Its interface can be seen in Figure 5. When the web page www.eyetracking.upol.cz/scangraph/ is loaded, only the left column (1) is displayed. The user can select input data or try the functionality with a prede- fined source of data (1a). The application works with data exported from the application OGAMA (Voßkühler, Nordmeier, Kuchinke, & Jacobs, 2008). OGAMA
Dolezalova, J. & Popelka, S. (2016) ScanGraph: A Novel Scanpath Comparison Method Using Visualisation of Graph Cliques
(OpenGazeAndMouseAnalyzer) is an open-source appli- cation design to record and analyse eye and mouse move- ment data. It allows Levenshtein distances between se- quences to be calculated, but the output is just a matrix with distance values. It is not possible to find groups of similar participants. Sequence similarity measures from OGAMA can be exported to a text file – and this text file can be imported directly to ScanGraph. This is one of the advantages over eyePatterns, which needs data in a spe- cific format prepared in a text file or table processor as ne- cessity.
Then, the user can specify the method of computation (1b) and select between collapsed or original strings (1c). In collapsed strings, there are no successive characters (AOIs) in the sequence. In the last step, the user can dis- play an advised graph (1d) or construct a graph according to parameter   (1e) or percentage of edges (1f) (see be- low).
After clicking on the button “Advised graph” or “Com- pute graph”, elements 2   7 are added to the display. In element 2, the points (vertices) representing all se- quences of participants from the input dataset are shown. Different colours represent the affiliation to the category (e.g. male/female, expert/novice, etc.) The table on the left
     Figure 5. Interface of the ScanGraph application
6
Journal of Eye Movement Research Dolezalova, J. & Popelka, S. (2016) 9(4):5, 1-13. ScanGraph: A Novel Scanpath Comparison Method Using Visualisation of Graph Cliques
(3) contains 100 pre-computed values of parameter  . In addition to parameter values, the number of edges is vis- ible. The last column contains the percentage of edges from the complete graph. The user can click into the table to display the particular graph.
Cliques with two and more vertices found in these graphs are listed on the right side of the page (4). Colour points in front of the participant’s name represents their group. An explanation of colours can be found in the upper part (4a). After clicking on any group of similar sequences in this section of the page, the clique is highlighted in the graph, and strings are also shown in the bottom part of the page (5). The user can visually inspect the sequence of the characters in the string. The overview of settings is shown in the upper part of the page (6), and the user can add labels with subject names to the graph (6a).
By clicking on the icon (7a), the user can download the table with all matrices (original matrix, modified matrix, and adjacency matrix), listed similarity groups with their character strings, input data, and overview of the settings. Clicking on (7b) allows a permanent link to the displayed graph to be created.
Computations and Visualisation
At first, the distance matrix (original matrix           ) is constructed according to the Levenshtein or Needleman- Wunsch algorithm. Each element of the matrix     is a dis- tance between sequences   a  . The distances, however, are poorly comparable between themselves because of the dif- ferent lengths of the sequences, as mentioned above. Ac- cording to this, the value of a distance     is divided by the higher value of the length of sequences   and  . It is the highest distance the two sequences could have (in the case of Levenshtein) or the value of the greatest similarity the two sequences could have (in the case of Needleman-Wun- sch) (modified matrix          ). Obviously, now the new elements could have a value from the interval 〈0,1〉 and still apply the higher the value is (in the case of Le- venshtein), or the lower the value is (in the case of Needle- man-Wunsch) the more different the strings are. The next step is up to the user.
The first option is to select the Advised graph button. This button returns a graph with 5% of the possible edges and a corresponding value of parameter   (see below). This graph is user-friendly and according to our experi-
ences has a very high interpretive value about any similar- ities. This option is recommended for users with no expe- rience with ScanGraph. The second option is to construct a user-defined graph. The graph is created according to pa- rameter   or percentage of edges.
The parameter   takes its value from the interval 〈0,1〉 and represents the degree of similarity. The higher the value of  , the higher the similarity of the given sequences. Obviously,       1       applies in the case of Le- venshtein distance and           in the case of the Needle- man-Wunsch algorithm ∀ ,   ∈  1, ... ,    of the modified matrix, where   is the number of participants. The value of   constructs a new matrix (adjacency matrix           ) according to these conditions:
     1, if     , 0, otherwise.
Hence, the adjacency matrix represents a simple graph, which is displayed. The second option is to set a percent- age of edges. This number takes a value from the interval   0,100  . The algorithm finds a value of the parameter   for which the graph will have a given percentage of edges (eventually rounded to the nearest lower value) and displays the graph and parameter  .
Besides the graph itself, a table with three columns is displayed. Parameter   with its 100 possible values and number of edges, and the percentage of the corresponding graph.
Each graph is represented by its adjacency matrix. Us- ing the matrix, all cliques contained in the graph can be found. Each clique represents a group of sequences which has the same or higher degree of similarity than the given parameter  .
The maximal clique problem (finding all maximal cliques in a graph) is an NP-complete decision problem.
Definitions of the decision problem according to (Gross & Yellen, 2005) follows.
A decision problem is a problem that requires only a yes or no answer regarding whether some element of its domain has a particular property.
A decision problem belongs to the class P if there is a polynomial-time algorithm to solve the problem.
 7
Journal of Eye Movement Research Dolezalova, J. & Popelka, S. (2016) 9(4):5, 1-13. ScanGraph: A Novel Scanpath Comparison Method Using Visualisation of Graph Cliques
A decision problem belongs to the class NP if there is a way to provide evidence of the correctness of a yes answer so that it can be confirmed by a polynomial-time algorithm.
A decision problem   is polynomially reducible to   if there is a polynomial-time transformation of each instance    of problem   to an instance    of problem   such that instances    and    have the same answer.
A decision problem is    hard if every problem in class    is polynomially reducible to it.
An   -hard problem   is   -complete if   is in class   .
The algorithm default used by ScanGraph is based on an exhaustive algorithm and finds an optimal solution with all cliques with two or more vertices in the given graph. When the computational time is too large, ScanGraph uses a heuristic algorithm.
Figure 6 displays an example of the influence of the pa- rameter   value. On each image, the largest clique is marked. When the parameter is set to 0, the graph is al- ways a complete graph. As the value of a parameter is in- creased, vertices very different from each other are dropped from the largest clique. When the value of a parameter is set to 0.5, the first vertex is out of the graph – it is not similar to any other in the dataset. The largest value of the parameter in the figure is 0.9 and only two vertices are making a clique. The character strings of these vertices were exactly the same, so in this case, it was needless to include an image with the graph with parameter 1, because it will be exactly the same as the previous image.
Model Case Study
The functionality of the developed ScanGraph tool was presented in an example of a case study comparing differ- ent map compositions. The data were recorded as a part of work by students. The aim of the study was to reveal whether cartographers and non-cartographers perceive maps differently.
The experiment contained a total of 18 stimuli. Six types of maps were created and each of them was pre- sented with three different map compositions. The distri- bution of map elements (map, legend, title, imprint, addi- tion map) were placed at various positions in the stimuli.
A total number of 20 respondents participated in this eye-tracking study. Half of them were selected from a group of undergraduate students who had already at- tended a cartography course. The rest of them were se- lected from among students in fields of study other than cartography. The differences between cartographers and non-cartographers were investigated. For the case study, an eye-tracking device SMI RED 250 was used, and data were recorded with a frequency of 60Hz. Eye positions were recorded every 16ms. Eyes move in a number of dif- ferent ways, simultaneously responding to commands from a number of different brain areas. One of the most important types of eye movement is no movement at all, but rather the ability to keep the eye trained on a fixed spot in the world. This is known as fixation. To get from one fixation to the next, the eyes make rapid, ballistic move- ments known as saccades (Hammoud & Mulligan, 2008). Plenty of algorithms for fixation detection exist, but the
               Figure 6. Example of the influence of parameter p value
8
Journal of Eye Movement Research Dolezalova, J. & Popelka, S. (2016) 9(4):5, 1-13. ScanGraph: A Novel Scanpath Comparison Method Using Visualisation of Graph Cliques
 most used for low-speed data (up to 250 Hz) is I-DT. I-DT takes into account the close spatial proximity of eye posi- tion points during eye movement trace (Salvucci & Goldberg, 2000).
For the case study, the software OGAMA was used with an ID-T algorithm for fixation detection. In OGAMA, the most important parameters for fixation detection are “Maximum distance” and “Minimum number of samples”. Thresholds in OGAMA were set to 15px (distance) and 10 samples. More information about this setting is de- scribed in (Popelka & Doležalová, 2015).
Results of the Model Case Study
Recorded eye-movement data were visualised using the Sequence Chart method available in SMI BeGaze. The Sequence Chart shows the temporal sequence of the visited Areas of Interest. Figure 7 shows a Sequence Chart for all respondents for three different map compositions. Areas of Interest were marked around all map composition ele- ments. The colour of the stripes under the maps represents a distribution of respondent attention between these AOIs. From a visual analysis of the Sequence Chart, a difference between the group of cartographers and non-cartographers can be seen. The most prominent difference is in the map element title (blue).
A fixation cross preceded each stimulus, so AOI repre- senting the map field is always viewed in the first 500ms. Beyond this time, most cartography students automatically read the title of the map, or rather, noted fixations repre- senting it in AOI. Non-cartographers did not do so. It is ev- ident especially in the first column, where the stimulus was an “ideal” map composition. In the following columns, the composition did not obey cartographic rules. Despite this fact, students of cartography were trying to find the title of the map.
The Sequence Chart is illustrative and easy to interpret, but a deeper analysis of differences between the strategies of participant groups needs a more sophisticated method of analysis. In this paper, data from this short study were used for demonstrating the use and possibilities of the developed ScanGraph web application. More specifically, eye-move- ment data recorded during observation of map composition #1 (first column of Figure 7) were used.
Figure 7. Sequence chart visualisation of participant read- ing strategy of three different map compositions. Map composi-
tion #1 was used for a model case study.
Demonstration of using the ScanGraph Application
The ScanGraph application was designed to work with data exported from the open-source application OGAMA (Voßkühler et al., 2008) – An open source software de- signed to analyse eye and mouse movements in slideshow study designs. OGAMA contains a tool called “Levenshtein Distance Calculation”, which is capable of computing Le- venshtein distances between the trajectories of participants. Sequence similarities can be calculated based on the regular grid or user defined Areas of Interest.
The output of this tool is a matrix of similarities between sequences and also the list of scanpath strings for each par- ticipant. When using ScanGraph, only the strings are im- portant. The values of similarity calculated in OGAMA are not used, because the Levenshtein algorithm was modified to take into account different lengths of strings.
The first step of data analysis with ScanGraph is the creation of Areas of Interest above analysed stimuli. In our case, map composition #1 from Figure 7 was used, and Ar- eas of Interest were marked around map composition ele- ments (see Figure 8).
            9
Journal of Eye Movement Research Dolezalova, J. & Popelka, S. (2016) 9(4):5, 1-13. ScanGraph: A Novel Scanpath Comparison Method Using Visualisation of Graph Cliques
   Figure 8. Areas of Interest marked in the stimulus with Map composition #1
In the next step, the Scanpath module in OGAMA was used to display the trajectories of participants and scanpath strings. The text file with sequence similarities can be exported from OGAMA and directly used as input data in ScanGraph.
Figure 9. An environment of OGAMA’s Scanpath module. Levenshtein distances between selected participants (and set of
AOIs) are calculated and can be exported as a text file. Only Subject names (1), Scanpath strings (2), and affiliation to sub- ject groups (3) is used in ScanGraph.
The exported text file was then opened with ScanGraph. The user can choose between the Levenshtein and Needleman-Wunsch algorithm and construct a graph.
In this particular example, the order of visited areas as well as their number of fixations was investigated. There- fore, the original (non-collapsed) data were analysed with the Levenshtein algorithm. The user can modify the value of parameter   or percentage of edges. In our case, we started with the parameter value 0.8, and two cliques were found (Figure 10).
Figure 10. Output of ScanGraph showing two cliques based onparameter    0.8.
Figure 11 shows the trajectories of participants making similar groups with parameter     0.8 (Figure 10). The larger one contained three participants from the group of non-cartographers. All of them spent their whole observa- tion time in the AOI A (containing the map field). The other clique comprised two participant sequences and is displayed in shades of blue. Both participants performed the same number of fixations (17), mainly in the map field (AOI A) and map title (AOI E). Participant GIS6 (dark blue) made an extra fixation in the map legend.
Figure 11. Trajectories of five participants making two cliques(basedonparameter    0.8).Threeparticipantsdis-
played in shades of red spent the whole time in the map field. Participants GIS6 and NOGIS4 (shades of blue) visited the map field and map title.
When the value of the parameter was decreased to 0.75, a total of six cliques was found in the data (Figure 12). The group of three similar participants was (obviously) pre- served, but it was extended by participant NOGIS4 (in the case of the first clique), respectively GIS2 (in the case of the second clique). This means that participants NOGIS4
  10
Journal of Eye Movement Research Dolezalova, J. & Popelka, S. (2016) 9(4):5, 1-13. ScanGraph: A Novel Scanpath Comparison Method Using Visualisation of Graph Cliques
and GIS2 are both similar to the group of three participants from Figure 10 (NOGIS2, NOGIS5, and NOGIS8), but are not similar to each other.
advantage in using String Edit Distance based on Areas of Interest marked around map composition elements.
Until now, scanpath comparison in cartography was performed by using the eyePatterns application (West et al., 2006), which offers a variety of functions, but in car- tography, only evaluating similarity between sequences was previously used. We have found that the implementa- tion of Levenshtein and Needleman-Wunsch algorithms in eyePatterns is correct, but it is not appropriate for compar- ing strings with different length. Visualisation of results via tree-graphs is inaccurate and misleading, so we de- cided to develop our own tool – ScanGraph. We believe that our tool offers more useful results than eyePatterns in this specific functionality, but the variety of functionality (i.e. search for patterns) in eyePatterns can still be used for some applications.
The case study presented in this article demonstrated the use of the ScanGraph application. The goal of the case study was not to find anything important from the trajec- tories of participants but to present ScanGraph functional- ity. For that reason, only one stimulus observed by 20 par- ticipants was investigated.
During the development of ScanGraph, we ran into several problems, but most of them were solved or by- passed. As was mentioned above, the exhaustive algorithm is   -complete. Due to this, with an increasing number of edges, the computational time increases non-polynomi- ally. In that case, a heuristic algorithm is used. The heuris- tic algorithm might not find an optimal solution, i.e. all maximal cliques (Vecerka, 2007). However, the graphs where a solution could be found by an exhaustive algo- rithm have a higher interpretative value for the experiment.
ScanGraph is still under development. The next step will be to add a matrix of differences between AOIs. For some experiments, it may be important to define the cost of transitions between each pair of AOI separately. For ex- ample, transition from AOI A to AOI B (e.g. map vs. leg- end) could mean a more important change than transition from AOI C to AOI D (e.g. two columns of the legend), so the Levenshtein distance should be different. For this case, the user will define his own matrix of differences between AOIs.
Another possible improvement could be the computa- tion of similarities between participants for a whole exper- iment. The user will upload a compressed file containing character strings from all stimuli of the experiment. The
   Figure 12. ScanGraph output showing two cliques based on parameter     0.75.
The trajectories from Figure 12 are displayed in Figure 13. Trajectories of participants NOGIS2, NOGIS5, and NOGIS8 shown in shades of red are again displayed in shades of red. The blue trajectory represents participant NOGIS4, and the green belongs to participant GIS2. Both trajectories are similar to the red ones, but they are not sim- ilar to each other.
 Figure 13. Trajectories of five participants making two cliques(basedonparameter    0.75).ParticipantsNOGIS2,
NOGIS5, and NOGIS8 are displayed in shades of red. Partici- pants NOGIS4 (blue line) and GIS2 (green line) are both simi- lar to the red ones, but not to each other.
Discussion
As already mentioned in the introduction, plenty of scanpath comparison methods exist (Anderson et al., 2014). For cartographic research (and not only), there is an
  11
Journal of Eye Movement Research Dolezalova, J. & Popelka, S. (2016) 9(4):5, 1-13. ScanGraph: A Novel Scanpath Comparison Method Using Visualisation of Graph Cliques
global similarity between all participants throughout all stimuli will be again displayed as a graph.
Conclusion
This article describes the possibilities of a newly devel- oped application for scanpath comparison called ScanGraph. The application performs scanpath compari- son based on the String Edit Distance method, and its out- put is a graph. Groups of similar sequences/participants are displayed as cliques of this graph.
ScanGraph can be used for all studies where differ- ences between gaze movements in different groups of par- ticipants are investigated. ScanGraph works with an ex- ported file from the open-source application OGAMA. To use ScanGraph, it is necessary to create Areas of Interest around specific parts of analysed stimuli. OGAMA allows a text file containing character strings representing the or- der of visited AOIs to be exported. This file can be directly imported to ScanGraph, which is freely available at www.eyetracking.upol.cz/scangraph. In the ScanGraph web environment, the user can display groups of partici- pants with similar character strings – participants with a similar strategy. The user can calculate an advised graph (containing 5% of edges) or a user defined graph based on parameter (percentage of similarity) or percentage of edges. Groups of similar participants are marked in this graph and the user can quickly inspect their character strings.
Until now, the eyePatterns application was commonly used for this purpose. We have found that the eyePatterns output, a tree-graph showing similarity between all se- quences, does not reflect the similarity measured by the al- gorithms used. From the tree-graph, similar groups can only be found visually, which is very inaccurate. Our ap- proach does not connect all the sequences (participants), but created groups of similar participants correspond to the computations. The user knows that an identified group is similar according to a given parameter – which is not pos- sible in eyePatterns. The algorithms for calculating simi- larity were modified and work better with strings of differ- ent length.
The functionality of ScanGraph was presented in an example of a simple cartographic case study in detail. This paper can serve as a user manual for ScanGraph.
Acknowledgements
This paper is supported by the projects of Operational Program Education for Competitiveness – European So- cial Fund (projects CZ.1.07/2.3.00/20.0170), of the Minis- try of Education, Youth, and Sports of the Czech Republic and the student project IGA_PrF_2016_008 of Palacky University.
The authors declare that there is no conflict of interest regarding the publication of this paper.
References
Anderson, N. C., Anderson, F., Kingstone, A., & Bischof, W. F. (2014). A comparison of scanpath comparison methods. Behavior research methods, 1-16.
Andrienko, G., Andrienko, N., Burch, M., & Weiskopf, D. (2012). Visual Analytics Methodology for Eye Movement Studies. Visualization and Computer Graphics, IEEE Transactions on, 18(12), 2889- 2898.
Bahill, A. T., & Stark, L. (1979). The trajectories of saccadic eye movements. Scientific American, 240(1), 108-117.
Coltekin, A., Fabrikant, S., & Lacayo, M. (2010). Exploring the efficiency of users' visual analytics strategies based on sequence analysis of eye movement recordings. International Journal of Geographical Information Science, 24(10), 1559-1575.
Duchowski, A. T., Driver, J., Jolaoso, S., Tan, W., Ramey, B. N., & Robbins, A. (2010). Scanpath comparison revisited. Paper presented at the Proceedings of the 2010 Symposium on Eye- Tracking Research & Applications.
Fabrikant, S. I., Rebich-Hespanha, S., Andrienko, N., Andrienko, G., & Montello, D. R. (2008). Novel method to measure inference affordance in static small-multiple map displays representing dynamic processes. Cartographic Journal, The, 45(3), 201-215.
Foulsham, T., Dewhurst, R., Nyström, M., Jarodzka, H., Johansson, R., Underwood, G., & Holmqvist, K. (2012). Comparing scanpaths during scene encoding and recognition: A multi-dimensional approach.
Gross, J. L., & Yellen, J. (2005). Graph theory and its applications: CRC press.
                          12
Journal of Eye Movement Research Dolezalova, J. & Popelka, S. (2016) 9(4):5, 1-13. ScanGraph: A Novel Scanpath Comparison Method Using Visualisation of Graph Cliques
Hammoud, R. I., & Mulligan, J. B. (2008). Introduction to Eye Monitoring Passive Eye Monitoring (pp. 1- 19): Springer.
Chajda, I. (2005). Úvod do algebry (Vol. 1). Olomouc: Vydavatelství Univerzity Palackého.
Chan, T. W. (1995). Optimal matching analysis: a methodological note on studying career mobility. Work and occupations, 22(4), 467-490.
Choi, Y. S., Mosley, A. D., & Stark, L. W. (1995). String editing analysis of human visual-search. Optometry and Vision Science, 72(7), 439-451. doi:10.1097/00006324-199507000-00003
Joh, C.-H., Arentze, T., Hofman, F., & Timmermans, H. (2002). Activity pattern similarity: a multidimensional sequence alignment method. Transportation Research Part B: Methodological, 36(5), 385-403.
Josephson, S., & Holmes, M. E. (2002). Visual attention to repeated internet images: testing the scanpath theory on the world wide web. Paper presented at the Proceedings of the 2002 symposium on Eye tracking research & applications.
Levenshtein, V. I. (1966). Binary codes capable of correcting deletions, insertions, and reversals. Soviet physics doklady, 10(8), 707-710.
Needleman, S. B., & Wunsch, C. D. (1970). A general method applicable to the search for similarities in the amino acid sequence of two proteins. Journal of molecular biology, 48(3), 443-453.
Noton, D., & Stark, L. (1971). Scanpaths in saccadic eye movements while viewing and recognizing patterns. Vision Research, 11(9), 929-IN928. doi:http://dx.doi.org/10.1016/0042-6989(71)902 13-6
Popelka, S., & Doležalová, J. (2015). Non-photorealistic 3D Visualization in City Maps: An Eye-Tracking Study Modern Trends in Cartography (pp. 357- 367): Springer.
Popelka, S., Dvorsky, J., Brychtova, A., & Hanzelka, J. (2013). User typology based on eye-movement paths Geoconference on Informatics, Geoinformatics and Remote Sensing - Conference Proceedings, Vol I (pp. 1041-1048).
Privitera, C. M., & Stark, L. W. (2000). Algorithms for defining visual regions-of-interest: Comparison with eye fixations. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 22(9), 970- 982.
Salvucci, D. D., & Goldberg, J. H. (2000). Identifying fixations and saccades in eye-tracking protocols. Paper presented at the Proceedings of the 2000 symposium on Eye tracking research & applications.
Shoval, N., & Isaacson, M. (2007). Sequence alignment as a method for human activity analysis in space and time. Annals of the Association of american Geographers, 97(2), 282-297.
Stark, L. W., & Choi, Y. S. (1996). Experimental metaphysics: The scanpath as an epistemological mechanism. Advances in psychology, 116, 3-69.
Vecerka, A. (2007). Grafy a grafové algoritmy. Univerzita Palackého Prırodovedecká Fakulta Katedra Informatiky, Olomouc.
Voßkühler, A., Nordmeier, V., Kuchinke, L., & Jacobs, A. M. (2008). OGAMA (Open Gaze and Mouse Analyzer): open-source software designed to analyze eye and mouse movements in slideshow study designs. Behavior research methods, 40(4), 1150-1162.
West, J. M., Haake, A. R., Rozanski, E. P., & Karn, K. S. (2006). eyePatterns: software for identifying patterns and similarities across fixation sequences. Paper presented at the Proceedings of the 2006 symposium on Eye tracking research & applications.
Wilson, C., Harvey, A., & Thompson, J. (1999). ClustalG: Software for analysis of activities and sequential events. Paper presented at the IATUR Conference Proceedings.
Behav Res (2012) 44:420–438 DOI 10.3758/s13428-011-0156-y
EyeMap: a software system for visualizing and analyzing eye movement data in reading
 Siliang Tang & Ronan G. Reilly & Christian Vorstius
Published online: 13 October 2011 # Psychonomic Society, Inc. 2011
Abstract We have developed EyeMap, a freely available software system for visualizing and analyzing eye movement data specifically in the area of reading research. As compared with similar systems, including commercial ones, EyeMap has more advanced features for text stimulus presentation, interest area extraction, eye movement data visualization, and experimental variable calculation. It is unique in supporting binocular data analysis for unicode, proportional, and nonpropor- tional fonts and spaced and unspaced scripts. Conse- quently, it is well suited for research on a wide range of writing systems. To date, it has been used with English, German, Thai, Korean, and Chinese. EyeMap is plat- form independent and can also work on mobile devices. An important contribution of the EyeMap project is a device-independent XML data format for describing data from a wide range of reading experiments. An online version of EyeMap allows researchers to analyze and visualize reading data through a standard Web browser. This facility could, for example, serve as a front-end for online eye movement data corpora.
Keywords Eye movements . Reading . Data visualization
The latest version of EyeMap can be retrieved from sourceforge.net/ projects/openeyemap/files. The online system is available at http:// eyemaponline.tk.
S. Tang : R. G. Reilly (*)
Department of Computer Science, NUI Maynooth, Maynooth, Co.Kildare, Ireland
e-mail: ronan.reilly@nuim.ie
C. Vorstius
Department of Psychology and Florida Center for Reading Research, Florida State University,
Tallahassee, FL, USA
Over the last 2 decades, one of the most successful tools in the study of human perception and cognition has been the measurement and analysis of eye movements (for recent overviews, see Duchowski, 2002; Hyöna, Radach, & Deubel, 2003; van Gompel, Fischer, Murray, & Hill, 2007). An area that has particularly profited from recent technical and methodological advances is research on reading. It has become a central field in cognitive science (Radach & Kennedy, 2004; Rayner, 1998) and has included the development of advanced computational models based on eye movement data (e.g., Engbert, Nuthmann, Richter, & Kliegl, 2005; Reichle, Pollatsek, Fisher, & Rayner, 1998; Reilly & Radach, 2006). At the same time, eye tracking during reading is also beginning to provide a major arena for applied research in fields as diverse as clinical neuroscience (Schattka, Radach, & Huber, 2010), training (Lehtimaki & Reilly, 2005), and elementary school education (Radach, Schmitten, Glover, & Huestegge, 2009).
Eye-tracking techniques have gradually improved during the last few decades. Many commercial trackers have emerged that use video oculography, which is a noninvasive and relatively accurate eye-tracking technology. However, many trackers are general purpose and have either low spatial accuracy (greater than 0.2°) or a low sampling rate (less than 250 Hz), which makes them unsuitable for modern reading research. A few, such as the SR Research EyeLink series (http://www.sr-research.com) and the IVIEW X series from SMI (http://www.smivision.com), have sufficient spatial resolution to support letter-level spatial accuracy, while at the same time providing sufficient temporal resolution for millisecond-level viewing time analyses (see http://www. eyemovementresearch.com/eye%20trackers.htm for a com- prehensive overview of available hardware).
A similar situation pertains for eye movement data analysis software in reading. The main manufacturers

Behav Res (2012) 44:420–438
421
 provide proprietary software that delivers good solutions for a range of data analysis tasks. Typical examples are the Data ViewerTM software from SR Research, the BeGazeTM Analysis Software from Sensomotoric Instruments (SMI), and the Studio Software SuiteTM from Tobii (http://www. tobii.com). Programs like these are integrated with system- specific software packages for stimulus presentation and measurement control. They offer a range of options that are very useful for many applications. However, these currently available software options also have a number of substantial limitations:
1. They tend to have few functions for processing reading data and generating useful variables (measurements on eye movement events in specific areas of interest, such as letters and words).
2. Most have limited options for text presentation and analysis. Some cannot load text material (users have first to convert the text into an image). Some do not support Unicode and, therefore, have prob- lems dealing with writing systems other than Roman-based ones. Some do not readily support the editing of eye movement data. For example, the otherwise very advanced Data Viewer software by SR Research supports only manual drift correction for each text line and sometimes has problems identifying word boundaries.
3. The software is hardware specific, and the data are stored in a proprietary format. Consequently, users cannot work across platforms, which causes problems for data sharing and result comparison.
4. They are not freely available, are difficult to extend, have limited visualization capabilities, and can handle only a limited range of experimental paradigms. As a result, researchers often have to develop their own analysis software for specific experiments.
5. Most solutions are commercial software packages, and purchase and support can be quite expensive for academic researchers.
In order to resolve the above-mentioned problems and
produce a more effective tool that allows psychologists to gain more insight into their eye movement reading data, we developed EyeMap, a freely available software system for visualizing and analyzing eye movement data specifically for reading research.
EyeMap supports dynamical calculation and exporting of more than 100 different types of reading-related variables (see Appendix 1 for the current list of variables). It allows researchers to load reading materials from an HTML file, which is convenient for creating, presenting, modifying, and organizing different experimental text materials. Furthermore, Unicode text can also be handled, which means that all writing systems, even unspaced scripts
such as Thai and Chinese, can be presented and segmented properly. Moreover, as far as we are aware, it is the first data analysis software that can automatically and reliably find word boundaries for proportional and nonproportional fonts. During the analysis, these boundaries are marked as areas of interest (AoIs) automatically. Another advantage of EyeMap is that it can work on a wide range of platforms, even mobile devices. It also contributes an XML (eXten- sible Markup Language) based device-independent data format for structuring eye movement reading data. Another important contribution is that EyeMap provides a powerful user interface for visualizing binocular eye movement data. Direct data manipulation is also supported in the visualiza- tion mode. Furthermore, an EyeMap online version allows users to analyze their eye movement data through a standard Web browser, which is in keeping with the trend toward cloud computing environments.
Figure 1 presents all of the functional modules and associated data files involved in the EyeMap environment. In Fig. 2, we present an informal use-case analysis of the process of going from the binary formatted eyetracker data (top of figure) through to an exported CSV file (bottom of figure).
The gray rectangles in Fig. 2 represent processes carried out by EyeMap, the yellow ones represent EyeMap- generated data, and the green ones represent user-supplied data sources. Note that in the current version of the system, the Data Parser is a stand-alone module, separate from the main EyeMap software. This permits its replacement when data files from different manufacturers are parsed.
It should also be noted that currently, EyeMap uses the results from the online saccade/fixation detection mechanism used by the specific eyetracker manufacturer. Therefore, differences in the often undocumented event detection algorithms used by different manufacturers currently carry over to EyeMap analyses. However, in principle, it should be possible to use a potentially more accurate postprocessing algorithm on the raw eye position samples extracted from the eye tracking record.
Isomorphic with the sequence of actions illustrated in Fig. 2, the workflow in EyeMap follows roughly four phases: data conversion, data integration, data visualization and editing, and data export and sharing. In the following sections, we will provide some more detail of these different phases.
Eye movement data conversion
Eye movement data are organized in many different ways, and each tracker manufacturer has its own data descrip- tions. In high-speed online trackers, which are deployed in most reading experiments, the raw data are stored in binary format, due to the requirement for fast data retrieval and storage. The lack of standardization of data format,

422
Behav Res (2012) 44:420–438
  Fig. 1 System overview
however, can be an impediment to data exchange between different research groups. Some tracker manu- facturers provide APIs or tools to export the binary data into ASCII, which is a convenient general format for data sharing but not sufficient for data processing, since the data in this format lack any structural or relational information. Some researchers have tried to develop device-independent data formats for a range of trackers. The first public device-independent data format was developed by Halverson and his colleagues (Halverson & Hornof, 2002). It has an XML-like structure, in which information such as experiment description, AoI informa- tion, and internal/external events are stored together. XML enforces a more formal, stricter data format that facilitates further data processing. Another XML-based generic data storage format was published as a deliverable from the EU-funded project COGAIN (Bates, Istance, & Spakov, 2005), which has become the de facto standard for gaze- based communication systems. However, since it is designed for gaze-based communication, it focuses mainly on hardware aspects so as to be compliant with as many trackers as possible, which is not necessarily a helpful option for reading research, because only a limited number of types of trackers can be used in modern reading research (see above). On the other hand, reading researchers are more interested in the cognitive processing occurring during fixation/saccade/blink/pupil events, which is at a greater level of detail than the gaze-level analyses afforded by relatively low-temporal-resolution trackers. Therefore,
COGAIN carries with it a considerable overhead, while still not being specific enough for reading research. For these reasons, we developed a novel XML-based device- independent data format designed specifically for reading experiments to facilitate the calculation of eye movement measures that are specific to reading.
A reading experiment is always trial based, and on each trial, subjects are required to read some predetermined text, usually a sentence or paragraph per screen. To describe a reading experiment with all the related events that can occur during the experiment, we need at least the following elements in our XML description (see Fig. 3 for a sample structure):
1. <root> </root>: All the information of the experiment is inside the root pair. It contains < trial > elements. And each data file has only one < root > block.
2. <trial> </trial>: Trial block contains all the information for a single experimental trial. Each block has <fix> and <sacc> elements. Each trial block has a unique number called “id,” which starts from 0, indicating the number of the trial in the current experiment. Each trial also has an <invalid> tag, which represents whether the current trial is valid or not.
3. <fix> </fix>: Fix block contains all the information for a fixation; each fixation block may have the following tags:
a. <eye>: the eye with which the current fixation event is associated;
b. <st>: start time-stamp in milliseconds;

Behav Res (2012) 44:420–438 Fig. 2 Use-case analysis of
EyeMap
423
  c. <et>: end time-stamp in milliseconds;
d. <dur>: duration (in milliseconds) of the current
fixation;
e. <x>/<y>: average x-, y-coordinates of the current
fixation;
f. <pupil>: average pupil size (diameter) in arbitrary
units for the current fixation;
g. <blink> *: duration (in milliseconds) of a blink if it
occurred prior to the current fixation;
h. <raw> *: the distance in pixels of the start (sx; sy) and end (ex; ey) x-, y-coordinates from the average
x-, y-coordinates of the current fixation;
i. <id>: a unique number starting from 0 (based on temporal order, where even numbers are used for
the right eye and odd for the left eye);
4. <sacc> block contains all the information for a saccade;
each saccade block will have the following tags:
a. <eye>: the eye with which the current fixation event is associated;
b. c. d. e. f. g.
h.
<st>: start time stamp in milliseconds;
<et>: end time stamp in milliseconds;
<dur>: duration (in milliseconds) of the fixation; <x>/<y>: x-, y-coordinates of the start position;
<tx>/<ty>: x-, y-coordinates of the end position; <ampl>: the total visual angle traversed by the saccade, reported by <ampl>, which can be divided by (<dur>/1000) to obtain the average velocity; <pv>: the peak values of gaze velocity in visual degrees per second;
i. <id>: a unique number starting from 0 (based on temporal order, even numbers for right eye, odd for left eye).
With respect to the odd/even ID coding scheme, this has proved especially useful for helping to synchronize data from the right and left eyes for studies of binocularity effects. The SR DataViewer, for example, provides separate data exports for the right and left eyes, but the task of synchronizing these data streams is nontrivial. The use of

 424
Behav Res (2012) 44:420–438
Fig. 3 An example of eye movement data XML
odd/even ID numbers makes the synchronization process a lot more manageable.
A data parser was developed to convert different data sources into the aforementioned XML format. The data parser is actually a small lexical analyzer generated by Lex and Yacc (Johnson, 1978; Lesk & Schmidt, 1975; Levine, Mason, & Brown, 1992). The parser works on lists of regular expressions that can identify useful components from the ASCII data file, recognize them as tokens, and then recombine the tokens into XML elements. Currently, the parser works on EyeLink (http:// www.sr-research.com) and Tobii (http://www.tobii.com) exported ASCII data formats. However, it would be quite straightforward to create a new parser for other eye- tracking devices, since users need only to create a new list of regular expressions based on the output specification of the new device.
A data structure for reading
In a typical reading experiment, parameters of interest are either properties of the stimulus materials (words,
characters, and their linguistic features) or measures of the ongoing oculomotor behavior during the dynamic process of acquiring this linguistic information. This produces two different perspectives on the experimental data: the text perspective and the event perspective. Moreover, reading researchers are usually interested only in some specific variables (or attributes) in any given experiment. Take some common variables in a text-reading experiment as an example: The variable launch distance is an event-based variable, which describes the distance in character spaces from the launch site of an incoming saccade to the beginning of the current word. In contrast, the variable gaze duration, defined as the sum of all fixations on the target prior to any fixation on another word, is considered a text-based variable in this classsification. In order to fulfill the data modification and variable calculation requirements of these two perspectives, the experimental data are organized as a tree structure containing six different levels (from the root to the leaves: experimental level, trial level, word level, gaze level, fixation level, and saccade level). The basic component of the experimental data tree is an abstract node class. The node class has the basic functions of constructing and
Behav Res (2012) 44:420–438
maintaining the relationships among the elements of the data
tree, which is the key to all the variable calculations. For a
425
node Np0 , if its ith child node is Ni, then the node Ni can be formalized as the following set:
  Ni 1⁄4 i;i0;Np;Ni 1;Niþ1;Aij 1⁄4 Ai1;Ai2;   ;Aij;Tin 1⁄4 ðNi1;Ni2    ;NinÞ  Ni 2 Tpn and Ni 1⁄4 Npi
 As is shown in the formula above, the node Ni satisfies the following properties:
1. Ni has an ID that indicates that it is the ith child of its current parent.
2. Ni has another ID i′ that indicates that it is the ith child of its old parent. If i ≠ i′, this means that the current parent node Np is not the original parent of Ni. In other words, node Ni is created by another node but adopted by Np. Since the node class allows multiple parents to connect to one node, the ID i′ can help a parent node to identify itself.
3. Ni has a reference to its current parent node Np.
4. Ni has a reference to its predecessor (Ni-1) and
successor (Ni+1).
5. Ni has a set of attributes Aij , which contains the
specific information of node Ni.
6. Ni contains a n-tuple Tin that records an ordered list of
child nodes.
From the vertical perspective, the node class builds a
bidirectional link between a parent node and its children, while from the horizontal perspective, the node class retains bidirectional references to children at the same level. The vertical perspective can simplify variable calculations. For example, once you reach the gaze node Gi, the gaze duration on Gi can simply be calculated by the following formula:
two different gazes. Thus, as is depicted in Fig. 4, the WORD5 node contains all the information (word boundary, word center, word frequency, etc.) for the word stinking. It has two child GAZE nodes. Three FIXATION nodes are allocated to two GAZE nodes on the basis of the gaze definition.
To optimize the loading speed, the data tree is not created at one time. Each time that a trial is presented/ modified, the corresponding trial branch is created and/or updated; thus, the information in the visualization is always up to date.
Data integration
In the current implementation of EyeMap, there are two sources of data that can be used to augment the eye movement data. A mandatory data source is the file detailing the text being read (i.e., text.html). Clearly, any eye movement data are uninterpretable without information about the words being viewed. The text information is provided in a HTML formatted file that also provides font type and size information. The latter plays an important role in word and letter segmentation.
An additional, optional facility is to synchronize speech data with eye movement data. Voice data are incorporated in two ways: as part of the visualization system and as part of the data export facility.
For visualization, the articulation start time and duration are extracted from a user-supplied voice.xml file for each word. When the user examines a word on the current trial, EyeMap will play the associated recording of that word.
For data analysis, the articulation start time is synchro- nized with the experimental timeline. It is therefore possible to associate when word articulation starts or ends with which fixation was being made at that time and what word or letter was being fixated. In this way, EyeMap can readily provide information about the eye voice span.
Data visualization and editing
The main job of this phase is to represent the experimental stimuli and visualize the eye movement data in different
Xn j1⁄41
This effectively involves the traversal of the gaze subtree and the accumulation of fixation durations in the process. A horizontal perspective, on the other hand, is extremely useful in generating word-level data matrices for output and analysis. Typically, in a reading exper- iment, the analysis focuses, in turn, on the fixation event level and on the objects of experimental interest, usually the words and sentences of the text. EyeMap allows the export of separate event-level and word-level data matrices.
Figure 4 is a fragment of a dynamic memory data structure created for processing a trial of a single line of text-reading data. Inside the green box, the word stinking has three right-eye fixations, and these fixations belong to
Gi
AigazeDur 1⁄4
Gij AijfixationDur :

426
Behav Res (2012) 44:420–438
  Fig. 4 Nodes in memory
ways, which facilitates the user in understanding and exploring the data from different perspectives. In a reading experiment, the primary stimulus is text. Figure 5 shows some sample experimental data using EyeMap with different combinations of Unicode, proportional, and non- proportional fonts and spaced and unspaced scripts. So far, EyeMap has been used for research on a wide range of writing systems, such as English, German, Thai, Korean, and Chinese.
As is shown in Fig. 5, EyeMap not only recreates the experimental stimulus, but also finds the word center (yellow line) and word boundary automatically, with and without punctuation (red vs. green boundary), and marks them as AoIs. As far as we are aware, EyeMap is the first software capable of reliably autosegmenting proportional fonts in this way. For example, the SR Data Viewer has problems handling proportional fonts. Figure 6 gives a sample comparison with EyeMap.
Fig. 5 Representing various texts with recorded eye move- ment data and word boundaries. a) multiline spaced English
text with proportional font and punctuation, b) single-line spaced Korean text with non- proportional font, c) single-line unspaced English text with non-proportional font, d) single- line unspaced Chinese text
with non-proportional font
a) multiline spaced English text with non-proportional font and punctuation b) single- line spaced Korean text with non-proportional font
c) single- line unspaced English text with non-proportional font
d) single- line unspaced Chinese text with non-proportional font

Behav Res (2012) 44:420–438
427
 Fig. 6 Comparison of SR Data Viewer and EyeMap seg- mentation algorithms
There appear to be several problems with SR Data Viewer’s autosegmentation algorithm. In Fig. 6, it treats the two words “of the” as one. Performance deteriorates further when the algorithm has to work with colored texts. Also, the algorithm always draws the word boundary through the middle of the space between the two words, while reading researchers normally treat the space preceding a word as part of that word. Segmenting letters for other than monospaced fonts is also highly unreliable. EyeMap, on the other hand, can detect the boundary of every letter in the word, even for text materials comprising mixed proportional font types with letter kerning.
EyeMap achieves this superior performance by taking advantage of the Flex implementation platform. It provides an application programmer interface (API) to a wide range of functions for segmenting paragraphs into lines, lines into words, and words into characters. Flex does all this on the basis of font metrics, rather than the analysis of an image, as is the case with Data Viewer. Note, however, that for word segmentation in writing systems that do not explicitly indicate word boundaries, EyeMap must rely on a user- specified separator incorporated in the HTML text file.
EyeMap offers one-dimensional (1-D), two-dimensional (2-D), and dynamic data visualizations (replays), which allow for a wide range of visualization. These visualizations
are implemented as viewers, representing a conceptual model of a particular visualization, with supporting panels. There are six different viewers, four of which are 2-D viewers. These are the fixation viewer, saccade viewer, playback viewer, and pure scan path viewer. The rest are 1- D charts, which comprise the pupil size viewer and an events viewer.
An example of a 2-D viewer is the fixation viewer shown in Fig. 7. Its contents are dynamically changed according to the selected trial. It contains a text background that shows the stimulus page of text for the current trial. Fixations are represented by semitransparent circles with different colors indicating the two eyes. When the user clicks the corresponding object (fixation dots, saccades, or words) inside the viewer, all the properties of that object are displayed. For example, when the word “see” is selected in Fig. 7, the word-level variables of “see” are calculated automatically and presented on the word properties list right of screen. The viewers also allow quick access to object data through pop-up tips. When the mouse cursor moves over an object, a tip with relevant information about the underlying object appears close to it.
Although the 2-D viewers implemented in EyeMap have some features in common, they plot different information on the text stimulus. Figure 8 demonstrates six available
 Fig.7 Azoomedfixationviewer
  Trial
Li
Fixation
Selected word
Pop-up tips
Fixation dots
   Word properties

428
Behav Res (2012) 44:420–438
 Fig. 8 Two-dimensional eye movement plot with a fixation dot with fixed circle radius,
b fixation durations as the circle radius, c pupil size as the circle radius, d gradated colored saccade lines with gaze point,
e word heatmap with word boundaries, and f pure scan path
 visualizations from the same text stimulus page. The snapshots a– c are taken from the fixation viewer. The fixation dot size in snapshots b and c corresponds to different measures, with Fig. 7b using fixation duration as the circle radius, while in Fig. 7c, the radius represents mean pupil size. Example d is taken from the saccade viewer, which connects gaze point (mean fixation position) with a gradually colored line indicating saccade distance and directions. Furthermore, snapshot e is a word heat map captured from a dynamic gaze replay animation in the playback viewer. Finally, snapshot f depicts a pure scan path representing real saccade trajectories.
EyeMap also utilizes different visual effects to enhance each viewer. For example, a drop-shadow effect can be activated in fixation viewers, which can be useful in some situations. For example, Fig. 9 shows some data collected from a patient with pure alexia, using a letter-by-letter
Fig. 9 Fixation dots with/ without drop shadow
reading strategy (unpublished data), where the drop-shadow very clearly indicates the temporal unfolding and spatial overlap of multiple fixations on the same word.
In the playback viewer, as can be seen from Fig. 10, when gaze animation is performed over the static text background, the semitransparent fixation dot has a “tail” to highlight the most recent fixations.
Time-correlated information, such as eye movement events, pupil dilation, mouse movements, keystrokes, stimuli changes, speech, and so forth, may be better represented in 1- D visualizations. Figures 11 and 12 give an example of pupil dilation versus time and eye movement events versus time in a 1-D chart, which we have implemented as a pupil viewer and an event viewer. In the event viewer, time range is controlled by a time axis bar; the user can control the observation time by moving the slider on the time axis or dragging the mouse around the chart.
a) b) c)
d) e) f)

Behav Res (2012) 44:420–438
429
  Fig. 10 Gaze playback with a virtual “tail”
In addition to visualizing reading data, direct data manipulation and modification through visualization are also supported by EyeMap. This function is extremely helpful in some cases, since fixation drift may occur during experimental trials despite good calibration, especially when working with children and patients. In a typical single-line reading experiment, a drift correction target dot may appear at the beginning of each trial, and the spatial reference grid for the calibration would be shifted accord- ingly. However, in a multiline text-reading experiment, as shown in Fig. 6, although the reading data are recorded by a relatively high-quality tracking system (e.g., EyeLink II at 500 Hz) with a drift correction before each trial, the data might not necessarily be clean, due to a small drift over time, especially in the vertical dimension. In such cases, manual drift correction can solve the problems of misalign- ment, when a few fixations cross over into the space assigned to the next line of text. As an illustration, Fig. 13 presents an example of how a near-ideal scan path can be produced with little effort in offline drift correction. Note that we recommend using this feature only for small corrections in real research studies so that natural variability of eye movement patterns is not distorted.
EyeMap offers a convenient edit mode, which allows users do correct fixation locations without having to use the keyboard. In edit mode, users can select multiple dots by dragging a rectangle and align them by selecting a corresponding option from the right-click mouse context menu. To move the selected fixation dots up and down as one group, users can use the mouse wheel directly. A double click on the view will simply submit all the intended modifications.
It is also possible to align globally, in the horizontal axis, all fixations with the drift correction dot. This is effective only for displays involving single lines of text. Its
Fig. 11 Mean pupil size variation versus time
advantage is that it operates on a complete data set instantaneously. However, if the experiment involves multiline displays of text, alignment has to be done manually, as described in the previous paragraph.
Ultimately, the whole point of a system like EyeMap is to generate a data matrix from an experiment that can be statistically analyzed. EyeMap provides an easy-to-use interface for the selection of appropriate variables at the fixation event and word level. Once a standard set of variables has been decided upon, these can be stored in the export.xml file and will be automatically loaded in future sessions. As was already mentioned, EyeMap provides over 100 variables of relevance to the researcher (see Appendix 1 for the current list). The user can see the full list along with a brief description in EyeMap’s variable viewer (see Fig. 14). The output data format is a CSV file with a header containing the exported variable names.
The fixation and word report output is dependent on the data tree module described in the data structure section. The exported variable names and their associated eye are listed in the user supplied export.xml file, which can also be created or modified in the EyeMap export.xml Editor (variables viewer). The export.xml file is loaded by EyeMap automatically. As is shown in Fig. 14, users can add variables to the export list by a drag-and-drop from the list of the available variables on the left. Variables can also be removed from the export list by similarly moving them to the Trash list on the right.
Data sharing is important in all lines of research. However, the current situation with oculomotor reading research is that most research groups compile their text materials and collect eye movement data for their own purposes, with limited data sharing. Large amounts of similar data are collected. In recent years, some research groups have started to create eye movement data corpora based on text-reading studies that have been made available to fellow researchers (Kennedy & Pynte, 2005; Kliegl, Nuthmann, & Engbert, 2006). A long-term goal should be to develop online databases for all the researchers to share freely their experimental setups, methodologies, and data. As a significant step toward this goal, we have created an online version of EyeMap at http://eyemaponline.tk. The

430
Behav Res (2012) 44:420–438
 Fig. 12 Fixation events on words as a function of time
application is presented in Fig. 15 as running within a Chrome browser. Users can access the full functionality of EyeMap through all standard Web browsers, which dem- onstrates that EyeMap can serve as a useful front-end tool for presenting text corpora and supporting the analysis of large sets of reading data. The current online version is not connected to any database as yet, so users currently cannot change the default data source. However, in the near future, the online system will allow users to upload and select their data source from a set of data corpora. This type of functionality is our ultimate goal and will motivate the design principle of future EyeMap versions.
Implementation and testing
To satisfy the requirement of platform independence, the system was built using several interpreted languages. Interpreted languages give applications some additional flexibility over compiled implementations, such as platform independence, dynamic typing, and smaller executable program size, to list only a few. The data parser, which converts EyeLink EDF data file to a device-independent XML data format, is written in Java, built using JDK (Java Development Kit) 6, and packaged into an executable jar file. EyeMap itself is written in Adobe FLEX and built using Adobe Flash builder 4.0. Adobe Flex is a software development kit (SDK) released by Adobe Systems for the development and deployment of cross-platform rich Inter- net applications based on the Adobe Flash platform. Since
Fig. 13 Eye scan path before and after the manual drift correction
Adobe Flash platform was originally designed to add animation, video, and interactivity to Web pages, FLEX brings many advantages to developers wanting to create user-friendly applications with many different modes of data visualizations, as well as integrated audio and animation, which fully satisfy the design demands of EyeMap. Although maintained by a commercial software company, a free FLEX SDK is available for download, thus making source code written in FLEX freely available.
In addition to manually checking the mapping of fixation location to word and letter locations, we compared the variables created in EyeMap with the analysis output from DataViewer. For this validity check, we randomly picked a subject data set from a current sentence-reading study consisting of 132 trials.
For the EyeMap analysis, the EDF output was converted into XML using EyeMap’s edf-asc2xml converter. Word regions were created automatically, on the basis of the information in the text.html file. Fixation and Word Reports were exported, and the resulting CSV files were read into SPSS.
For the DataViewer analysis, EDF data were imported into DataViewer. Since the autosegmentation function in DataViewer did not create word boundaries immediately following the last pixel of the last letter in a word, interest areas had to be defined manually for each trial. Fixation, Saccade, and Interest Area Reports were exported, and resulting Excel files were read into SPSS. Variables from both outputs were compared and showed exact matches. The table in Appendix 2 presents results for some key variables for different word lengths for this one subject.

Behav Res (2012) 44:420–438
431
 Fig. 14 Creating an export variable list by drag-and-drop in the export.xml editor
 Related work
Prior to EyeMap, the best-known and most widely used eye movement data analysis tool for reading and related applica- tions was probably the EyeLink DataViewerTM (SR Research Ltd.). The EyeLink family of eye-tracking systems use high- speed cameras with extremely accurate spatial resolution, making them very suitable for reading research. The SR Data Viewer is commercial software, allowing the user to view, filter,
and process EyeLink data. In visualization, it provides several different data-viewing modes such as eye event position and scan path visualization and temporal playback of recording with gaze position overlay. In data analysis, it can generate variables including “interest area dwell time” and fixation progression, first-pass and regressive fixation measures, and so forth. Therefore, DataViewer is a powerful and professional tool for both usability studies and reading research. However, when directly compared, EyeMap offers both comparable
 Fig. 15 Online version of EyeMap running in a Chrome browser

432
Behav Res (2012) 44:420–438
 features and several distinct additional ones. First, from the visualization point of view, EyeMap is fully compatible with the EyeLink data format and has visualization modes compa- rable to those provided by DataViewer. Moreover, EyeMap has more precise AoI extraction from text. On the other hand, from an analysis aspect, although DataViewer provides many export variables, most of them are too general for reading psychology or psycholinguistic purposes. In contrast, EyeMap can generate a large number of specific oculomotor measures, representing virtually all common analysis parameters in current basic and applied research (Inhoff & Radach, 1998). Another important difference is that Data Viewer cannot combine binocular data. Left-eye and right-eye data have to be analyzed separately. EyeMap, however, can generate binocular data reports. Data Viewer is also limited to running on Windows and Mac platforms, while EyeMap runs on any currently available platform. It should be noted that EyeMap is not intended to fully replace more comprehensive software solutions such as Data ViewerTM, BeGazeTM, and Studio SoftwareTM, but it can provide a very useful additional tool for the analysis of reading data.
Conclusion
The key innovation in EyeMap involves the development of a freely available analysis and visualization platform specifically focused on eye movement data from reading studies. This
focus has allowed for the development of (1) an XML-based open standard for the representation of data from a range of eye-tracking platforms; (2) the implementation of full Unicode support, significantly improving the ease with which different writing systems can be studied; (3) the development of robust techniques for text handling, such as the automatic segmenta- tion of text in both proportional and nonproportional fonts into areas of interest; (4) the development of support for integrating a speech stream with the eye movement record; and (5) a facility for analyzing binocular data in a more integrated way than has heretofore been possible.
While the component technologies of the system are not new, their combination represents a unique and powerful reading analysis platform.
Author Note The development of this software was supported by a John and Pat Hume Postgraduate Scholarship from NUI Maynooth and by grants from the U.S. Department of Education, Institute for Education Sciences, Award R305F100027, Reading for Understand- ing Research Initiative, and from the German Science Foundation (DFG), HU 292/9-1,2 and GU 1177/1-1.
We thank Ralph Radach and Michael Mayer from the Department of Psychology, Florida State University, and Steson Ho from the Department of Psychology, University of Sydney, for valuable advice and for rigorously testing the software. Thanks are also due Jochen Laubrock and an anonymous reviewer for very helpful comments on an earlier draft.
The software homepage is located at eyemap.tk, while the user manual and installation packages can be retrieved freely from sourceforge.net/projects/openeyemap/files.
 Appendix 1
Table 1 EyeMap variables
No. Variable Name Level
1 EXP Expt
2 List Expt
3 Subject Expt
4 Var1 Expt
5 Var2 Expt
6 LineCount_T Trial
7 SentenceCount_T Trial
8 TrialNum Trial
9 TrialProperty Trial
10 WordCount_T Trial
11 Fixated Word
12 FixCount_W Word
13 GazeCount_W Word
14 LineNum_T Word
Description
  Exptal name abbreviation, taken from 3rd–5th digit of filename. Optional, taken from 6th digit of filename.
Subject code, taken from first two digits of filename.
Optional, taken from 7th digit of filename.
Optional, taken from 8th digit of filename. Total number of lines for the trial.
Total number of sentences for the trial. Trial number.
Properties defined in the trial.csv file.
Total number of words for the trial.
1 if the word was fixated, 0 if not.
Total number of fixations on the word. Total number of gazes (passes) on the word. Line number in the current trial.

Behav Res (2012) 44:420–438 433 Table 1 (continued)
  No. Variable Name
15 BlinkCount_W
16 SentenceNum_T
17 TVDur_sac
18 TVDur
19 Word
20 WordBlinkDur
21 WordCount_L
22 WordCount_S
23 WordLen_punct
24 WordLen
25 WordLocX
26 WordLocY
27 WordNum_E
28 WordNum_L
29 WordNum_S
30 WordNum_T
31 WordProperty
32 FixCount_G
33 GazeBlinkDur
34 GazeDur_sac
35 GazeDur
36 GazeNum_W
37 GazePupil
38 BlinkCount_G
39 Refixated
40 RefixCount
41 Blink
42 BlinkDur
43 FixDur
44 FixLocX
45 FixLocXBeg
46 FixLocXEnd
47 FixLocY
48 FixLocYBeg
49 FixLocYEnd
50 FixNum_E
51 FixNum_G
52 FixNum_S
53 FixNum_T
54 FixNum_W
55 FixPupil
56 FixStartTime
57 LandPos_NP
58 LandPos
59 LandPosDec_NP
Level Description
Word Number of blinks on the word.
Word Sentence number in the current trial.
Word Total viewing time (TVD), or the sum of all fixations on the word+saccades.
Word Total viewing time (TVD), or the sum of all fixations on the word.
Word Word, without punctuation.
Word Duration of the blinks in the word.
Word Total number of words on the line.
Word Total number of words in the sentence.
Word Word Length, in letter, including punctuation.
Word Word length, in letters.
Word x-pixel location of the word (upper left corner).
Word y-pixel location of the word (upper left corner).
Word Word number for the experiment.
Word Word number on the line.
Word Word number in the sentence.
Word Word number for the trial.
Word Properties defined in the word.csv file.
Gaze Total number of fixation for the current gaze (pass).
Gaze Duration of the blinks in the gaze.
Gaze Gaze duration (GD) of the current gaze (pass), including internal saccades, plus outgoing. Gaze Gaze duration (GD) of the current gaze (pass).
Gaze Number of the current gaze (Pass) on the word.
Gaze Mean pupil diameter for the entire gaze.
Gaze Number of blinks on the gaze.
Gaze 1, if FixNum_G
Gaze Total number of refixations on the word in the current gaze
Fixation If 1, then blink before; if 1, then blink after the current fixation.
Fixation Duration of the blink.
Fixation Duration of the current fixation.
Fixation x-pixel average fixation location of the current fixation.
Fixation x-pixel location of the current fixation at the beginning of that fixation.
Fixation x-pixel location of the current fixation at the end of that fixation.
Fixation y-pixel Average fixation location of the current fixation.
Fixation y-pixel location of the current fixation at the beginning of that fixation.
Fixation y-pixel location of the current fixation at the end of that fixation.
Fixation Fixation number, in the experiment.
Fixation Number of the current fixation in the current gaze (pass).
Fixation Fixation number in the sentence.
Fixation Fixation number for the trial.
Fixation Number of the current fixation on the words.
Fixation Mean pupil diameter for the fixation.
Fixation Time stamp of the start of the current fixation (ms).
Fixation (monospaced) Landing position in the word, described in letter units (space before word is 0). Fixation Landing position in the word, described in letter units (space before word is 0).
Fixation (monospaced) Landing position in the word, described in letter units including decimal places (space before word is 0).

434
Behav Res (2012) 44:420–438
 Table 1 (continued) No. Variable Name
60 LandPosDec
61 LaunchDistBeg_NP
62 LaunchDistBeg
63 LaunchDistBegDec_NP
64 LaunchDistBegDec
65 LaunchDistCent_NP
66 LaunchDistCent
67 LaunchDistCentDec_NP
68 LaunchDistCentDec
69 LaunchDistEnd_NP
70 LaunchDistEnd
71 LaunchDistEndDec_NP
72 LaunchDistEndDec
73 LineSwitch
74 NxtFixDur
75 NxtLandPos_NP
76 NxtLandPosDec_NP
77 NxtWordFix
78 PreFixDur
79 PreLandPos_NP
80 PreLandPosDec_NP
81 PreWordFix
82 Refixation
83 RepairTime
84 ReReading
85 G1F1
86 G1Fn
87 GnFn
88 SacInAmp
89 SacInAmpX
90 SacInAmpY
91 SacInDur
92 SacInInter
93 SacInLocBegX
94 SacInLocBegY
95 SacInLocEndX
96 SacInLocEndY
97 SacInNWonP
98 SacInProg
99 SacInStartTime
100 SacInVel
101 SacInVel_max
102 SacInVelX
103 SacInVelY
104 SacInWord
Level
Fixation Fixation Fixation
Fixation Fixation Fixation Fixation Fixation Fixation Fixation Fixation Fixation Fixation Fixation Fixation Fixation Fixation Fixation Fixation Fixation Fixation Fixation Fixation Fixation Fixation Fixation Fixation Fixation Saccade Saccade Saccade Saccade Saccade Saccade Saccade Saccade Saccade Saccade Saccade Saccade Saccade Saccade Saccade Saccade Saccade
Description
Landing position in the word, described in letter units including decimal places.
(monospaced) Launch distance from the beginning of the current word.
Launch distance from the beginning of the current word starting at letter zero, which is the space in his own mind...
(monospaced) Launch distance from the beginning of the current word including decimal places. Launch distance from the beginning of the current word including decimal places. (monospaced)Launch distance from the center of the current word.
Launch distance from the center of the current word.
(monospaced) Launch distance from the center of the current word including decimal places. Launch distance from the center of the current word including decimal places.
(monospaced) Launch distance from the end of the current word.
Launch distance from the end of the current word.
(monospaced) Launch distance from the end of the current word including decimal places. Launch distance from the end of the current word including decimal places.
If the current fixation is on a different line than the previous fixation.
Duration of the next fixation.
(monospaced) Landing position of the next fixation in characters.
(monospaced) Landing position of the next fixation including decimal places. Outgoing saccade amplitude in word units, zero if last fixation with the same word. Duration of the previous fixation.
(monospaced) Landing position of the previous fixation in characters. (monospaced) Landing position of the previous fixation including decimal places. Incoming saccade amplitude in word units, zero if last fixation with the same word. If 1, then multiple fixations on the word, if 0, only 1 or none).
Total rereading time on word.
1 if there was a prior fixation on the line to the right of the current word; else, 0. The first fixation in the 1st gaze on the word.
The last fixation in the 1st gaze on the word.
The last fixation in the last gaze on the word.
Amplitude of the incoming saccade, in letters.
Amplitude of the incoming saccade, x-axis.
Amplitude of the incoming saccade, y-axis.
Duration of the incoming saccade.
If 0, then intraword saccade; if 1, then interword saccade.
Beginning location of the incoming saccade, x-axis.
Beginning location of the incoming saccade, y-axis.
End location of the outgoing saccade, x-axis.
End location of the outgoing saccade, y-axis.
Target NWonP of incoming saccade
If 1, then incoming saccade is progressive; if 0, then regressive.
Time stamp of the start of the incoming saccade (ms).
Mean incoming saccade velocity, x-coordinate
Maximum incoming saccade velocity, x-coordinate
Average incoming saccade velocity, x-axis.
Average incoming saccade velocity, y-axis.
Word from which the current saccade has exited.

Behav Res (2012) 44:420–438 Table 1 (continued)
No. Variable Name
105 SacInXY
106 SacOutAmp
107 SacOutAmpX
108 SacOutAmpY
109 SacOutDur
110 SacOutInter
111 SacOutLocBegX
112 SacOutLocBegY
113 SacOutLocEndX
114 SacOutLocEndY
115 SacOutNWonP
116 SacOutProg
117 SacOutStartTime
118 SacOutVel
119 SacOutVel_max
120 SacOutVelX
121 SacOutVelY
122 SacOutWord
123 SacOutXY
124 ArticDur
125 VLauoff10
126 VLauon10
127 VoffFix
128 VoiceOffPos_NP
129 VoiceOffPosDec_NP
130 VoiceOnPos_NP
131 VoiceOnPosDec
132 VoiceOnPosDec_NP
133 VoiceOffPos
134 VoiceOffPosDec
135 VoiceOnPos
136 VoiceOffTrialTime
137 VoiceOnTrialTime
138 VoiceOffWord
139 VoiceOnWord
140 VonFix
141 FalseStartFix
142 FalseStartTrialTime
143 FalseStartWord
144 FalseStartPos_NP
145 FalseStartPos
146 FalseStartPosDec_NP
147 FalseStartPosDec
148 MSGxInc
149 MSGxName
150 MSGxStartTime
435
  Level
Saccade Saccade Saccade Saccade Saccade Saccade Saccade Saccade Saccade Saccade Saccade Saccade Saccade Saccade Saccade Saccade Saccade Saccade Saccade Voice Voice Voice Voice Voice Voice Voice Voice Voice Voice Voice Voice Voice Voice Voice Voice Voice Voice Voice Voice Voice Voice Voice Voice Message Message Message
Description
Saccade amplitude of the incoming saccade, in Euclidian units. Amplitude of outgoing saccade
Amplitude of the outgoing saccade, x-plane.
Amplitude of the outgoing saccade, y-plane.
 Duration of the outgoing saccade.
If 0, then intraword saccade; if 1, then interword saccade.
Beginning location of the outgoing saccade, x-axis.
Beginning location of the outgoing saccade, y-axis.
End location of the outgoing saccade, x-axis.
End location of the outgoing saccade, y-axis.
Target NWonP of outgoing saccade
If 1, then outgoing saccade is progressive; if 0, then regressive.
Time stamp of the start of the outgoing saccade (ms).
Mean outgoing saccade velocity, x-coordinate
Maximal outgoing saccade velocity, x-coordinate
Mean outgoing saccade velocity, x-coordinate
Mean outgoing saccade velocity, x-coordinate
Word to which the current saccade is directed.
Saccade amplitude of the outgoing saccade, in Euclidian units.
Articulation duration of the current word.
Distance from voice offset fixation to the first character of the word that is being spoken. Distance from voice onset fixation to the first character of the word that is being spoken.
Voice offset fixation.
(monospaced)Letter position at time of the current word's voice offset.
(monospaced)Letter position at time of the current word's voice offset, including decimal places. (monospaced)Letter position at time of the current word's voice onset.
Letter position at time of the current word's voice onset, including decimal places. (monospaced)Letter position at time of the current word's voice onset, including decimal places. Letter position at time of the current word's voice offset.
Letter position at time of the current word's voice offset, including decimal places.
Letter position of the current word's voice onset.
Trial-based time of the current word's voice offset (ms).
Trial-based time of the current word's voice onset (ms).
Fixated word at the time of the current word's voiced offset.
Fixated word at the time of the current word's voiced onset.
Voice onset fixation.
False start fixation
Trial-based time of the current word's voice false start (ms).
Fixated word at the time of the current word's voiced false start.
(monospaced)Letter position of the current word's voice false start.
Letter position of the current word's voice false start.
(monospaced)Letter position at time of the current word's voice false start, including decimal places. Letter position of the current word's voice false start, including decimal places.
Increment number of a message.
Message name.
Time stamp (EDF time) of the message.

436 Behav Res (2012) 44:420–438
 Appendix 2
The table below displays a sample of typical reading variables for a randomly selected subject broken down by word length. See Appendix 1 for a definition of the variables
Table 2 Comparison of variable values from SR DataViewer and EyeMap
R_WordLen TVDur FixDurG1F1 GazeDurG1F1 FixCountG1F1
EM DV EM DV EM DV EM DV
SacInAmpG1F1 EM DV 2.1800 2.1800
         1 Mean 169.8750 169.88 169.88 169.88 169.88 169.88 1.00 1.00 N8888888888
Std. Deviation Minimum Maximum
2 Mean N
Std. Deviation Minimum Maximum
3 Mean N
Std. Deviation Minimum Maximum
4 Mean N
Std. Deviation Minimum Maximum
5 Mean N
Std. Deviation Minimum Maximum
6 Mean N
Std. Deviation Minimum Maximum
7 Mean N
Std. Deviation Minimum Maximum
8 Mean N
55.43964 55.440 55.440 92.00 92 92 274.00 274 274 233.8624 233.86 175.16 109 109 109 123.80574 123.806 51.712 55.00 55 55 779.00 779 348 246.8095 246.89 177.77 357 358 357 134.29925 134.120 62.533 41.00 41 35 792.00 792 568 327.0598 327.06 186.34 184 184 184 183.97019 183.970 66.425 36.00 36 36 1064.00 1064 435 355.2072 355.21 188.04 251 251 251 198.98327 198.983 73.162 41.00 41 41 1238.00 1238 582 381.8077 381.81 202.38 208 208 208 232.77356 232.774 66.675 72.00 72 69 1424.00 1424 576 384.5181 384.52 189.67 193 193 193 261.16047 261.160 63.041 103.00 103 36 2294.00 2294 386 439.4660 439.47 206.14 103 103 103
55.440 55.440 92 92
274 274 175.16 182.79 109 109 51.712 59.337 55 55
348 396 178.04 183.12 358 357 62.661 71.094 35 35
568 648 186.34 219.65 184 184 66.425 97.558 36 36
435 660 188.04 230.80 251 251 73.162 116.029 41 41
582 773 202.38 243.40 208 208 66.675 117.727 69 69
576 953 189.67 220.17 193 193 63.041 88.451 36 43 386 697 206.14 268.72 103 103
55.440 .000 .000 92 1 1 274 1 1 182.79 1.05 1.05 109 109 109 59.337 .210 .210 55 1 1 396 2 2 183.38 1.04 1.04 358 357 358 71.164 .202 .202 35 1 1 648 3 3 219.65 1.18 1.18 184 184 184 97.558 .416 .416 36 1 1 660 3 3 230.80 1.22 1.22 251 251 251 116.029 .471 .471 41 1 1 773 4 4 243.40 1.27 1.27 208 208 208 117.727 .571 .571 69 1 1 953 5 5 220.17 1.23 1.23 193 193 193 88.451 .481 .481 43 1 1 697 4 4 268.72 1.36 1.36 103 103 103
.60444 .60444 1.15 1.15 2.95 2.95 2.2926 2.2926 84 84 2.52116 2.52116 .02 .02 20.11 20.11 2.3706 2.3790 318 319 1.97661 1.96595 .00 .02 20.84 20.84 2.3732 2.3732 170 170 2.87832 2.87832 .06 .06 34.34 34.34 2.1608 2.1608 242 242 .95467 .95467 .02 .02 11.02 11.02 2.1655 2.1655 195 195 .79746 .79746 .01 .01 5.92 5.92 2.2822 2.2822 188 188 1.34691 1.34691 .03 .03 12.22 12.22 2.3063 2.3063 102 102

Behav Res (2012) 44:420–438
437
 Table 2 (continued)
R_WordLen TVDur
GazeDurG1F1 FixCountG1F1 EM DV EM DV
185.240 185.240 .827 .827 49 49 1 1 1203 1203 6 6 249.13 249.13 1.24 1.24 55 55 55 55 125.783 125.783 .508 .508 65 65 1 1 701 701 3 3 331.45 331.45 1.73 1.73 33 33 33 33 220.611 220.611 1.039 1.039 41 41 1 1 967 967 5 5 353.60 353.60 1.67 1.67 15 15 15 15 281.706 281.706 .976 .976 118 118 1 1 1231 1231 4 4 291.56 291.56 1.56 1.56
SacInAmpG1F1 EM DV
.76345 .76345 .08 .08 5.15 5.15 2.2500 2.2500 54 54 .79442 .79442 .28 .28 4.63 4.63 2.2447 2.2447 30 30 1.20611 1.20611 .05 .05 7.54 7.54 2.3673 2.3673 15 15 1.07526 1.07526 .08 .08 4.21 4.21 2.2956 2.2956
 FixDurG1F1 EM DV EM DV
        Std. Deviation 296.67407 Minimum 49.00 Maximum 1705.00
9 Mean 420.9273 N 55
Std. Deviation 236.45623 Minimum 65.00 Maximum 1006.00
10 Mean 535.1212 N 33
Std. Deviation 308.24268 Minimum 151.00 Maximum 1242.00
11 Mean 625.7333 N 15
Std. Deviation 430.32238 Minimum 191.00 Maximum 1474.00
296.674 70.985 70.985 49 49 49 1705 404 404 420.93 211.49 211.49 55 55 55 236.456 82.988 82.988 65 62 62 1006 510 510 535.12 197.76 197.76 33 33 33 308.243 84.904 84.904 151 41 41 1242 445 445 625.73 217.53 217.53 15 15 15 430.322 58.300 58.300 191 118 118 1474 378 378 467.78 201.33 201.33
12 Mean 467.7778 N9999999999
Std. Deviation Minimum Maximum
234.58835 234.588 81.185 143.00 143 79 910.00 910 357 1209.5000 1209.50 166.50
81.185 177.291 79 79
357 606 166.50 867.00
177.291 .726
79 1
606 3
867.00 5.00 5.00
13 Mean N22222222
Std. Deviation Minimum Maximum
Total Mean N
Std. Deviation Minimum Maximum
References
89.80256 89.803 48.790 1146.00 1146 132 1273.00 1273 201 340.5468 340.50 189.19 1527 1528 1527 225.34808 225.280 67.405 36.00 36 35 2294.00 2294 582
48.790 4.243 132 864 201 870 189.25 222.73 1528 1527 67.420 118.958 35 35
582 1231
4.243 1.414 1.414 864 4 4 870 6 6 222.76 1.20 1.20 1528 1527 1528 118.927 .522 .522 35 1 1 1231 6 6
2.2769 2.2788 1415 1416 1.68552 1.68305 .00 .01 34.34 34.34
.726 .79128 .79128 1 1.52 1.52
3 3.81 3.81
 Bates, R. Istance, H., & Spakov, O. (2005) D2.2 Requirements for the common format of eye movement data. Communication by Gaze Interaction (COGAIN), IST-20030511598: Deliverable 2.2. Avail- able at http://www.cogain.org/results/reports/COGAIN-D2.2.pdf
Duchowski, A. T. (2002). A breadth-first survey of eye-tracking applications. Behavior Research Methods, Instruments, & Computers, 34, 455–470.
Engbert, R., Nuthmann, A., Richter, E., & Kliegl, R. (2005). SWIFT: A dynamical model of saccade generation during reading. Psychological Review, 112, 777–813.
Halverson, T., & Hornof, A. (2002). VizFix software requirements specification. Eugene: University of Oregon, Computer and Information Science. Retrieved August 3, 2011, from http:// www.cs.uoregon.edu/research/cm-hci/VizFix/VizFixSRS.pdf
Hyöna, J., Radach, R., & Deubel, H. (Eds.). (2003). The mind's eye: Cognitive and applied aspects of eye movement research. Oxford: Elsevier Science.
Inhoff, A. W., & Radach, R. (1998). Definition and computation of oculomotor measures in the study of cognitive processes. In G. Underwood (Ed.), Eye guidance in reading and scene perception (pp. 29–54). Oxford: Elsevier.
Johnson, S. C. (1978). YACC: Yet another compiler-compiler. Murray Hill, NJ: Bell Laboratories.

438
Behav Res (2012) 44:420–438
 Kennedy, A., & Pynte, J. (2005). Parafoveal-on-foveal effects in normal reading. Vision Research, 45, 153–168.
Kliegl, R., Nuthmann, A., & Engbert, R. (2006). Tracking the mind during reading: The influence of past, present, and future words on fixation durations. Journal of Experimental Psychology. General, 135, 12–35.
Lehtimaki, T., & Reilly, R. G. (2005). Improving eye movement control in young readers. Aritifical Intelligence Review, 24, 477–488. Lesk, M. E., & Schmidt, E. B. (1975). Lex: A lexical analyzer
generator. Murray Hill, NJ: Bell Laboratories.
Levine, J., Mason, T., & Brown, D. (1992). Lex & Yacc (2nd ed.).
Sebastopol, CA: O'Reilly Media.
Radach, R., & Kennedy, A. (2004). Theoretical perspectives on eye
movements in reading: Past controversies, current issues, and an agenda for the future. European Journal of Cognitive Psychology, 16, 3–26.
Radach, R., Schmitten, C., Glover, L., & Huestegge, L. (2009). How children read for comprehension: Eye movements in developing
readers. In R. K. Wagner, C. Schatschneider, & C. Phythian- Sence (Eds.), Beyond decoding: The biological and behavioral foundations of reading comprehension (pp. 75–106). New York: Guildford Press.
Rayner, K. (1998). Eye movements in reading and information processing: 20 years of research. Psychological Bulletin, 124, 372–422.
Reichle, E. D., Pollatsek, A., Fisher, D. L., & Rayner, K. (1998). Toward a model of eye movement control in reading. Psychological Review, 105, 125–157.
Reilly, R. G., & Radach, R. (2006). Some empirical tests of an interactive activation model of eye movement control in reading. Journal of Cognitive Systems Research, 7, 34–55.
Schattka, K., Radach, R., & Huber, W. (2010). Eye movement correlates of acquired central dyslexia. Neuropsychologia, 48, 2959–2973.
van Gompel, R., Fischer, M., Murray, W., & Hill, R. (Eds.). (2007). Eye movements: A window on mind and brain. Oxford: Elsevier Science.
AdeLE (Adaptive e-Learning with Eye-Tracking): Theoretical Background, System Architecture and Application Scenarios
(Extended version of the paper published in the proceedings of the I-KNOW ‘04, Graz, Austria, 2004)
Christian Gütl 1,3 [cguetl@iicm.edu and cguetl@acm.org], Maja Pivec 2,4 [maja.pivec@fh-joannuem.at and mpivec@izit.si], Christian Trummer 2 [christian.trummer@fh-joannuem.at], Victor Manuel García-Barrios 1,5 [vgarcia@iicm.edu and victor.barrios@campus02.at], Felix Mödritscher 1,5 [fmoedrit@iicm.edu and felix.moedritscher@campus02.at], Juergen Pripfl2 [juergen.pripfl@fh-joanneum.at], Martin Umgeher2 [martin.umgeher@fh-joanneum.at]
(1) Institute for Information Systems and Computer Media (IICM), Faculty of Computer Science, Graz University of Technology, Austria [http://www.iicm.edu]
(2) Department of Information Design, University of Applied Sciences (FH JOANNEUM), Graz, Austria [http://www.fh-joanneum.a]
(3) Infodelio Information Systems [http://www.infodelio.com], Guetl IT R&C [http://www.guetl.com]
and Internet Studio-Isser [http://www.studio-isser.com]
(4) Institute for Information Technology (IZIT), Maribor, Slovenia [http://www.izit.si]
(5) Degree Program in IT and IT-Marketing at Graz University of Applied Sciences CAMPUS 02, Graz, Austria [http://www.campus02.at]
Abstract
Due to the rapidly growing amount of knowledge, a stronger need emerges for efficient and improved knowledge acquisition strategies. E-learning can be very helpful for different learning activities in various learning environments. However, in order to support different teaching and learning paradigms, e-learning should deal with more than simply reading online lessons. Therefore, content as well as communication and collaboration have to be supported in a highly personalised manner by e-learning systems. Though, tracking and grasping the user behaviour in real time remains the most challenging task to retrieve an appropriate and fine- grained user profile as well as to provide personalised learning content. In this paper we present AdeLE, a technology-based solution of an enhanced adaptive e-learning framework, which comprises novel solution approaches for fine-grained user profiles by exploiting real time eye-tracking and content-tracking analysis as well as a dynamic background library. Based on the global objectives of an enhanced e-learning environment, the system architecture of AdeLE and the methods used in order to gain fine-grained user information by real time eye-tracking are addressed. Furthermore, various scenarios in different application domains are illustrated.
Keywords
e-learning, adaptive knowledge transfer, dynamic background library, user profiling, eye tracking, AdeLE
1. Introduction
It is well documented in renowned research publications and studies that the increasing amount of knowledge and therefore the serious demands on knowledge acquisition for students and employees requires improved and efficient learning activities. Furthermore, people have to be supported and assisted during their life-long learning activities. Traditional learning methods and environments do not meet the contemporary needs of our information society any more. Thus, technology-based solutions have been increasingly established to overcome these problems. As a result, e-learning can be identified as one of the emerging areas in the last few years, as shown by means of concrete numbers in the (IDC 2003) study. About 934 Mio US$ were invested in e-learning worldwide in the year 2003 and the European market is meant to be the best one. Though, a lot of failures and only a few - in most cases locally restricted - success stories can be identified (see also Baumgartner 2003).
In this paper, we introduce our new solution approach, named AdeLE, which is the abbreviation for ‘Adaptive e-Learning with Eye-Tracking’ and comprises a technology-based solution in the field of adaptive e- learning exploiting novel methods of resolution for fine-grained user profiling based on real-time eye-tracking and content tracking information. By applying fine-grained user profiles and real-time behavioural data about users, a highly personalised information provision and a wide range of new applications are facilitated. Given that the objectives of technology-based educational environments and their impact on individuals are linked with

complex and context-dependent constraints and conditions, we are conscious that the solution framework presented in this paper may only be valid for specific sub-fields of e-learning.
In concrete, this paper represents an extended version of ‘AdeLE: A Framework for Adaptive E-Learning through Eye Tracking’, presented at the IKNOW 04 (see García-Barrios et al. 2004), whereupon the scope of this version is extended by and focused onto a practical discussion of real-time eye-tracking and a description of practical application domains. Thus, the reminder of the paper is organised as follows: Section 2. Motivation, describes key issues and requirements for the proposed system, in Section 3. Related Work, a selection of related research topics is depicted, Section 4. Real-time Eye-tracking, discusses specific features of eye-tracking technology and the application of its real-time application for the AdeLE framework, Section 5. Architectural Overview of the AdeLE Framework, provides a technical insight into the modules of the AdeLE architecture, and in Section 6. Application Scenarios, examples of application areas for our solution approach are discussed.
2. Motivation
Our previous experiences in the subject of e-learning (e.g. Dietinger 2003; García-Barrios et al. 2002; Pivec 2000) have shown that learners tend to stick to distinct learning methods and teachers favour various teaching methods. Consequently, e-learning involves more than simply reading online lessons.
Adaptive E-learning
E-learning is a large and complex field of research encompassing a variety of learning and teaching paradigms, such as constructivistic, serial, symmetric (Jain et al. 2002), cognitive, face-to-face, discovery, and managed learning (Lennon and Maurer 2003). In addition, various levels of pre-knowledge as well as progress and difficulties on lessons for individual learners have to be considered. As stated in (Jain et al. 2002), e- learning, even if standardised, tends to produce asymmetrical learning, as its tools reach out to a dispersed audience where individuals may arrive at different stages at different times, even if along a common learning trajectory. Furthermore, the personalisation of instructions and interactivity improves the knowledge acquisition process. For example, a learner in a classroom setting asks significantly less questions, whereas in an individual tutoring setting, a learner may ask or is required to answer a lot of questions in any learning session. Thus, the achievement of individually tutored learners’ performance, as measured by test scores, may significantly exceed that of classroom colleagues; for details see (ADL 2001) and (Bloom 1984). These are some of the reasons for considering personalisation and adaptivity to be one of the key issues in modern e-learning environments.
Key Issues and Objectives
Following the observations and findings stated so far, it is worth mentioning that in order to offer the appropriate learning method and to compile the relevant learning assets and learning sequences for any individual learner, the gathering and analysis of fine-grained user profiling information is a serious technological challenge. Furthermore, just providing the technical solution of delivering learning content from static learning repositories will not meet all requirements for supplying teachers and learners with personalised and timely information. In particular, the knowledge transfer process defines the key issue within the context of technology- based instructional environments and it can be interpreted as a holistic phenomenon composed of two related streams: the teaching process (knowledge generation and delivery as well as assessment of knowledge acquisition) and the learning process (knowledge acquisition). Both of them have to be supported by a future- oriented approach.
Thus, the main objectives of an innovative solution for an e-learning system are:
(1) personalised retrieval, management and presentation of relevant and timely information for learning
activities,
(2) support of various learning and teaching paradigms, and
(3) improved knowledge of the users' behaviour in the field of human-computer interaction in general as
well as related to the displayed learning contents in order to gain new insights and input for (1) and
(2) as well.
Considering the above depicted aspects, we believe that a more extensive solution framework is needed,
which allows the binding together of effective modern technologies and solution approaches in order to enhance the adaptation of knowledge provisioning and to increase the effectiveness of personalisation. This notion builds the basis of our solution approach, the AdeLE framework. Unlike common methodologies of tracking page views and mouse clicks, we advocate a combination of fine-grained real time eye-tracking and content-tracking operations for the user profiling as well as complementing the data stream by interactive system-user dialogs and online progress testing. Furthermore, in the field of information acquisition for the teaching and learning activities, we propose besides static learning assets also a highly dynamic, task-specific and personalised information retrieval component, which we call dynamic background library.
Going the right way?
To conclude this chapter, let us state that we are conscious and aware of the fact that personalisation and adaptation should not be considered as universal goals or solutions for effective e-learning. E-learning
environments, adaptive systems and personalisation techniques have a lot of proven weaknesses and have been (often justly) criticised by experts of different fields.
Although we think that it is not necessary to repeat all drawbacks and problems of personalisation extensively in this paper, consider the following few examples of critical aspects regarding the application of personalised e-learning. From the knowledge delivery viewpoint, there are some cases where teachers want students to use exactly the same learning material in order to provide a common experience and thus, to foster community-based learning. From the viewpoint of knowledge acquisition and the support of different learning styles as well as considering some findings coming from other research fields, developers of adaptive systems should (a) exercise care in not allowing an information overload on screen in order to improve the effectiveness of working memory (for details about cognitive load theory see e.g. (Cooper 1998; Feinberg and Murphy 2000), (b) enable the scrutability of the user profile in order to give learners the overview and control of adaptational parameters, and (c) consider privacy and security issues e.g. due to the intrusive character of devices such as an eye-tracker system (see also Czarkowski and Kay 2003).
3. Related Research Fields
The purpose of this chapter is, on the one hand, to show that the ideas behind personalised e-learning are not as new as supposed. On the other hand, this fact underscores that the field is broad and has a high potential of interest and for research. Thus, in accordance to the basic issues behind the idea of the AdeLE solution framework, a selection of some interesting subjects related to the proposed system is discussed as follows.
Adaptive E-learning, Standardisation and Learning Repositories
The utilisation of adaptive multimedia systems as improved learning environments are well documented in research work (Brusilovsky 1998a; Brusilovsky 1998b; Hothi et al. 1998; Seeberg 2003), implementation concepts and systems can be found, for example in (Beaumont et al. 1995; Hockemeyer 1997; Boyle et al. 1998; ARIADNE 2003; García-Barrios 2001b). Tracking the behaviour of users and analysing their learning progress are not new research issues, but were demonstrated in classic systems already in the 1950’s. Well known early systems are CLASS and PLATO (Crowell 1967; Modesitt 1974). In the present, novel user modelling techniques are important as they allow systems to personalise the human-system interaction processes (Conlan et al. 2002).
Well-defined learner model standards and specifications, like PAPI (Public and Private Information for Learner - IEEE), IMS LIP (IMS Learner Information Package) or GESTALT (Getting Educational Systems Talking Across Leading-edge Technologies) already exist (PAPI 2000; IMS LIP 2001; Gestalt 1999). Furthermore, e-learning capabilities that enable interoperability, accessibility, structuring, packaging and reusability of Web-based learning content are also finding standardisation through advanced specification initiatives, such as SCORM (Sharable Content Object Reference Model). Also regarding courseware objects other well-known standards find broad application, as for example LOM (Learning Object Metadata) in order to support semantic enrichment, or IMS Simple Sequencing in order to define conditional interrelationships of different successive nodes along an adaptable learning path (LOM 2002; IMS 2003).
The examples shown in the previous paragraphs point out that adaptivity and personalisation constitute broad research fields with a relatively long history and a large number of significant results (see also Brusilovsky 1998b; Hothi et al. 1998; Beaumont et al. 1995). However, much work is still to be done towards existing or emerging issues and challenges, especially more generic solutions are needed.
Further, a lot of work is already done regarding the topic of learning repositories. In the field of static learning assets there exist various initiatives as listed in (NLII 2004), web information retrieval for assisting learning activities are discussed for example in (Liaw et al. 2003), and the novel idea of the dynamic background libraries is well documented in (Dietinger et al. 1999) and (Guetl 2002). Though, to our knowledge no integrative solution combining static and dynamic learning content in e-learning systems is available so far.
Eye-Tracking Research Field
The field of eye-tracking research is indeed an old but at present a very active discipline. Nowadays, eye- and gaze-tracking systems are found in several research fields, like scan-path testing on the WWW (Josephson et al. 2002), research in the area of aviation (Merchant 2001) or of driving environments (Hayhoe et al. 2002), computational studies about visual cognition (see Rao et al. 1997; Zhai 2003), design implications in web search tasks (Goldberg et al. 2002), human computer interaction (Ivory et al. 2001), and many others. Quite recently eye-tracking vendors began to implement real time eye-tracking analysis, but there is still a lack of integration in profiling systems and exploiting the data flow for personalised content compilation.
To sum it up, let us state that according to our extensive literature survey in the context of adaptive e- learning, (a) great research work has been done, and (b) several prototype solutions, tools and systems already exist. However, these are just isolated solutions, which cannot entirely satisfy the teachers’ and learners’ needs in order to improve a personalised semantic knowledge transfer. Thus, the AdeLE framework, which will be introduced in following sections, covers new ideas by means of providing a modular and flexible architecture for adaptive multimedia learning systems in order to enhance the adaptive semantic knowledge transfer process as well as by combining real time eye-tracking and content-tracking in order to enhance fine-grained user profiling.
4. Real-time Eye-tracking Information
One of the main aims within the scope of the AdeLE project is to observe users’ learning activities in real-time by monitoring the major possible number of behavioural aspects and personal traits. Learner profile information of special interest for AdeLE are (a) personal characteristics, such as cognitive or learning styles, (b) momentary states, like tiredness or mental effort (see García-Barrios et al. 2004), as well as (c) other indicators during the learning process, such as objects and areas of focus, time spent on objects, frequency of visits, and sequences in which learning content is consumed (see Preis and Mueller 2003). The advantage of interpreting these data in real time lies in gaining prompt information about the user’s state. By exploiting eye-tracking data combined with other user behavioural traits linked with the content provided, a fine-grained learner profile can be tracked by the system and applied e.g. for personalisation of learning content and navigation. Furthermore, based on this fine-grained learner information, the target of AdeLE is to gain an insight into the learning strategies that users apply when using an e-learning platform and to be able to detect patterns indicative of disorientation or other suboptimal learning strategies. At this point, it should be also mentioned that in order to interpret efficiently behavioural indicators, it is important to not rely exclusively on eye-tracking data, but to supplement it with information gained by direct and constant user feedback, i.e. learners must ‘on screen’ not only be informed of why/how adaptation happens but also get free control of the adaptation procedures.
Eye-Tracking Technology and E-learning
In terms of eye-tracking technology, eye movements, scanning patterns and pupil diameter are indicators of thought and mental processing involved during visual information extraction (Rayner 1998; Kahneman 1966). Thus, real-time information of the precise position of gaze and of pupil diameter can be used for supporting and guiding learners through their learning journey. Very roughly, eye movements can be divided into two components: fixations, i.e. periods of time with relatively stable eye movements where visual information is processed, and saccades, which are defined as rapid eye movements that bring a new part of the visual scene into focus. However, more important indicators can be gained by analysing both components together with other derived parameters. Finding out reliable parameters is one of the emerging aims of the AdeLE project. To emphasise the complex situation in this research field consider the following problems and prospects in context.
Gaze duration (i.e. time spent on an object) and fixations are not indicative of attention per se, because one can also pay attention to objects that do not lie in the centre of the focused region. Nevertheless, by considering other indicators, such as saccadic velocity, blink velocity and rate as well as eyelid’s degree of openness, a better and more meaningful approximation can be gained. Saccadic velocity, for example, is said to decrease with increasing tiredness and to increase with increasing task difficulty (Fritz et al. 1992). Further, blink rate, decreasing blink velocity and decreasing degree of openness may be indicators for increasing tiredness (Galley 2001). Thus, if tiredness is identified, it should be possible through adaptive e-learning mechanisms to suggest optimised strategies such as the best time to take a break. At this point it is reasonable to also emphasise that the user should always retain the final say over whether to accept or reject the system’s suggestions (we call that controlled adaptation or non-intrusive adaptivity).
AdeLE’s Eye-Tracking System
At the present there basically exist two types of eye-tracking systems on the market: outside-in systems and inside-out systems. Outside-in systems are characterised by the fact that one or more cameras record the eye of the participant and trace the gaze in a scene through imaging algorithms. The cameras are positioned in front of the participant. One of the advantages of these systems is given by the fact that the camera can be integrated into the monitor, and therefore remains basically invisible (i.e. a relatively non-intrusive monitoring is possible). Inside-out systems are characterised by a special device that the participant has to wear on the head. The image of the eye is led into a mini-camera by using mirrors. This mini-camera records the eye and the actual line of vision is found out through imaging algorithms. More characteristics of both systems along with advantages and disadvantages related to the requirements of the AdeLE project are outlined in detail in (Pivec et. al 2004). For further detailed information about general eye-tracker characteristics refer to (Jacob 1995) and (Galley 2001).
For the purposes of the AdeLE project outside-in systems seem to be more suitable, hence they are less intrusive for the learner and support tracking of the user by regular e-learning lessons. Based on the project objectives and the requirements for the eye-tracking system, the AdeLE team decided to utilise Tobii 1750. Figure 1 on the next page shows the utilisation of the eye-tracker system within the AdeLE project, on the left side a user is being monitored on real-time by the eye-tracker during a learning session and on the right side the monitored results are examined. The Tobii 1750 is integrated into a 17” TFT monitor and can therefore be used for many forms of eye-tracking studies with stimuli like Web sites, slide shows, videos and text documents. Further, the Tobii 1750 does not show any problems with its functional re-acquisition from extreme head- motions. Another advantage is given by its high tracking quality, i.e. it can be used by young or old people, by persons with dark or bright eyes, by users with different ethnical-dependent anatomic eye types, by people with glasses or contact lenses as well as under varying environmental light conditions. More technical advantages of the system can be summarised as follows: a) high accuracy (0.5 degrees accuracy, bias error), b) compensation of unparalleled quality of head-motion and drift reduction, and c) binocular tracking with a frequency of 50 Hz. Further, the system provides a well-designed programming interface with which its automatic functionality can
be configured, enabling no additional manual adjustments of parameters on the device. This interface is utilised to integrate the eye-tracking system into the AdeLE framework, as described in the next section.
Figure 1: Utilisation of the Tobii 1750 Eye-Tracking system.
5. Architectural Overview of AdeLE Framework
This section gives an overview over the functional and logical architecture of the AdeLE framework, as is illustrated in Figure 2 below. The main functional components are mainly divided into the following three logical parts: Core Module, User-centred Modules and Lecturer-centred Modules.
    User
    Content
Browser
Eye Tracking
Monitor
Content Tracking
Browser
         Content Communication Collaboration
Interactive Dialog
Browser
      User Profiling Database
User Information
Collaborative Filtering Database
Statistics
 Adaptive Semantic Knowledge Transfer
   Content Communication Collaboration
Course Ware
Multimedia Knowledge Units
Course Topics
Knowledge Topic Structure
    Content
Browser
     Course Creation and Maintenance
Course Creator, Lecturer
Background Knowledge
Dynamic Background Library
       Figure 2: The architecture of the AdeLE framework.
Core Module
The Adaptive Semantic Knowledge Transfer Module (ASKTM) represents the core module of the framework. From a global point of view the ASKTM coordinates all the surrounding modules and sends and
requests information to and from them. It compiles pieces of content and meta-information for delivery to the learners. Separate interfaces are provided for the other two groups of users: course creators (authors) and lecturers (teachers, trainers or tutors). For media and platform-independence, the information is provided in an XML schema and can be transformed into various formats. The process of content delivery is depicted in the upper left and lower left parts of Figure 2.
User-centred Modules
The upper right part of Figure 2 shows the user-centred modules for advanced user profiling. The core functionality for gaining enhanced and more precise user information is located in the combination of the Eye Tracking Module (ETM) and the Content Tracking Module (CTM). ETM in combination with CTM provides real-time fine-grained data regarding the user’s reading and learning behaviour. The ETM also gives the system hints about concentration, excitement or tiredness of the learner, and consequently, inferring criteria to monitor efficacy aspects to the knowledge assimilation process.
The entire set of information of user interaction and behaviour is supplied to the User Information Module (UIM), which is in charge of managing the user modelling and profiling issues. In order to support user- controlled adaptation, the Interactive Dialog Module (IDM) allows learners to set and change user profile settings actively. Further, the system also can proactively force user interaction. For example, the latter module can be used to verify and if necessary adjust any automatically inferred user information. If tiredness is suspected, the IDM may also be used to suggest a short break or provide a relaxation exercise to the user. We have chosen the term ‘user’ at the upper-side of the framework, because lecturers may also interact with the system in order to achieve a direct tutoring intervention in the adaptation processes of the e-learning environment. The implementation of this feature is intended to allow overriding of automatically generated system decisions, e.g. to support hybrid learning techniques.
The UIM encompasses three user information databases of different granularity: the User Profiling Database (UPD), Collaborative Filtering Database (CFD) and Statistics Database (SD). The UPD holds fine- grained information about a wide range of user interactions (e.g. sequences of scanned and viewed pieces of information) and more abstracted values of user behaviour types (e.g. level of gained expertise in certain subtopics). Similar user profiles or user behaviour types are grouped and managed in the CFD (i.e. supporting stereotyped user-modelling). Through collaborative filtering, the system can proactively suggest particular pieces of information in proper media by exploiting the collective knowledge of user groups and their behaviour. Finally, the SD manages abstracted information in a user-independent level. Course creators and administrators may use valuable information (e.g. identified problematic areas of courseware sections) without violating the privacy of individual learners. The learning process will be improved, because the system will create or deliver adapted content by means of tracked statistical data (e.g. by delivering more images/tables for learners that have problems with large and complicated texts).
Lecturer-centred Modules
The lower-right section of Figure 2 shows the lecturer-centred modules of the AdeLE framework that are responsible for the course creation process. The Course Creation and Maintenance Module (CCMM) represents the core module for the entire course management and controls the Courseware Module (CM), the Course Topics Module (CTM) and the Background Knowledge Module (BKM). Course creators and lecturers can set up and maintain courses as well as request statistics about their courses. The CM manages pieces of information in different media types and an extensive set of metadata. This module (CM) can either store pieces of information locally or just manage metadata and include remotely located sources by caching them.
On the one hand, the CTM manages course content by just defining subsections using meta-descriptors, i.e. course creators only predefine subtopics and their relations at the time of course generation. During the learning process, users get dynamically proper and most recent pieces of information out from the pool of the CM. On the other hand, the CTM permits to manage a course topics structure and a thesaurus for providing automatically relations between subsections. The BKM dynamically provides additional information within the learning process and helps course creators to keep pace with most recent information.
Different Viewpoints on the AdeLE Framework
From the point of view of learners, the AdeLE framework provides an adaptive e-learning system with personalised views of the learning material, i.e. the content is adapted in accordance to pre-knowledge and learning progress, preferred media types, etc. Furthermore, real time eye-tracking can help to identify areas of understanding difficulty and enable the provision of selective additional information or explanation. A smart progress profiling keeps pace with learners’ system interaction and may assist them at further learning sessions. In addition, learners can get a wide range of dynamic background information.
From the point of view of lecturers, the framework offers a wide range of helpful and smart features for courseware generation and maintenance. Assuming that courseware modules follow a separation of form and content, or at least follow a consistent style guide, it allows lecturers to create courseware by simply defining meta-descriptions of subtopics and their relations. Course authors can also create their own multimedia knowledge units applicable for course delivery and share them. In the processes of creating new knowledge units or updating information as well as defining background information for the learners, the dynamic background library assists the course creators and lecturers. The concept of dynamic background libraries is well documented
in (Guetl 2002) and a prototype implementation has been developed by (García-Barrios 2001b). Statistical information (e.g. identified courseware pieces with understanding difficulties, subjects of most/weak interest) supports the maintenance of the courseware.
Based on the architectural overview of the AdeLE framework, a wide variety of applications scenarios are facilitated. However, in the first stage of the AdeLE project, research work is focused on the utilisation of real time eye-tracking and content tracking information. In order to get a view on the practical application of the AdeLE framework in this field of research, some application scenarios are shown in the following section.
6. Application Scenarios
By merging eye-tracking technology with proper content presentation the goal is to identify, evaluate and develop methods of adaptive instruction for personalised e-learning. Currently, the research efforts of the AdeLE team concentrate on three application areas, which are discussed in the following paragraphs.
Individual Learning Strategies
The first application scenario deals with the development of methods to extract individual learning strategies from the learner’s gaze behaviour and adapt against the identified learning style. Comprehensive reviews of cognitive psychology research indicate that people exhibit significant individual differences in how they learn (Schmeck 1988; Glaser 1984; Robertson 1985; Honey 1986; Leutner 1998). A simple example being individuals who have a strong visual memory but weaker verbal processing will find text based material harder to process than individuals who have stronger verbal skills. In the traditional classroom environment a teacher has the chance to adapt or explain material to suit individuals’ needs. In e-learning environments where a teacher is frequently not present, pedagogical material is nowadays more uniformly presented. In this environment information about the learner’s gaze behaviour would be a great opportunity to optimise material to an individual’s needs. For example, if somebody prefers text and ignores pictures the amount of pictures presented could be reduced, and vice versa.
Additional Context Specific Information
The second application scenario is defined by the use of the information gained from the specific content accessed by the user (specific words, paragraphs, areas of pictures, tables, and the like) in order to provide additional context specific information. For example, an animated picture could accompany textual information, whereas the integration of the picture proceeds in relation to the words or paragraphs accessed by the user (see Figure 3 below and Figure 4 on the next page).
Figure 3: In an e-learning course concerned with Alexander the Great’s Conquest of Persia, a map of
Alexander’s advance in the region is shown. The map content is updated in correspondence to the text paragraph currently read by the learner. In the example, the second paragraph (“Granikos”) is being read and the map shows the journey of Alexander from Macedonia to Granikos (green, yellow and red areas indicate fixations and gaze duration).

 Figure 4: The reader has advanced to the fourth and fifth paragraph (“Gaugamela”) and is automatically shown the corresponding map containing the passage from Issos to Gaugamela.
The AdeLE research team also aims at the possible application of the system in the field of high sensible knowledge transfer, such as training sessions in nuclear power plants, in the field of aviation techniques or in military, where it is essential that each section of a content unit has to be read by the learner. By means of the real-time user behaviour, unseen sections of content units provided to the learner are identified by the system and again will be supplied to the user at the following content units. Of course, this method can not reflect information about the knowledge acquisition and about pre-knowledge. However, information about content sections skipped by the learner are utilised to compile specific assessment tests to check the learner’s knowledge about these particular concepts.
Appropriate Intervention Strategies
The third interesting application area of the AdeLE research work is based on developing and testing appropriate intervention strategies when the learner is found to be stuck. The e-learning environment might intervene when a learner is not focused on a relevant part of the computer screen, or is focused completely outside the task area for a certain period of time, or the eye gaze is sufficiently quick/jerky for a given period of time. Just to give one example, in case of knowledge acquisition problems for a particular content section more detailed content or background information can be provided to the learner.
Other Application Areas
Besides the above-depicted issues, innovative solutions as well as an improved and more profound understanding are expected in the following areas:
• Improved knowledge of the users' behaviour in the field of human-computer interaction in general as well as related to the displayed learning contents
• Improved and detailed course progress tracking
• Novel possibilities for identifying the most suitable media and content presentation within knowledge
transfer environments
• Identification of problematic areas in the content flow and/or content structuring
7. Conclusions and Future Work
Evidently, the price of an advanced eye-tracking system plays a decisive role in the application possibilities of the AdeLE solution approach. Nevertheless, existing systems show that an eye-tracking device can be integrated into a standard monitor. Due to the continuing trend of rapid technical progress, it may be expected that in the next few years it would be possible to build a low-cost but high-quality eye-tracking system based on standard hardware components, which would be suitable for real-time analysis of eye-tracking information as described in this paper. Some results of the AdeLE project may contribute to find new ways of making advanced adaptive environments for teaching and learning feasible and affordable for institutions in a relative near future.
Potential target groups that could benefit from the presented ongoing research and proposed innovations based on eye-tracking supported real-time data capturing and adaptation-based systems are identified as follows:
• Various end-users
Support of 100% knowledge acquisition in application fields such as aviation, traffic, different complex procedures, risk management, decision support, research on learning, and others.
• E-learning platform and knowledge management platform developers
Inclusion of these innovative approaches and provision of better adaptive/adaptable platforms
• Content publishers
Improvement of content structuring, development of user-centred contents and of contents supporting various learning stiles.
• Eye-tracking system producers
Development of low cost eye-tracking systems, which are possible to apply in a standard computer working place.
Acknowledgements
The AdeLE project is partially funded by the Austrian ministries BMVIT and BMBWK, by the FHplus impulse programme. The support of the following institutions and individuals is gratefully acknowledged: Department of Information Design, Graz University of Applied Sciences (FH JOANNEUM); Institute for Information Systems and Computer Media (IICM), Faculty of Computer Science at Graz University of Technology; especially Hermann Maurer and Karl Stocker. Further information about the AdeLE project can be found at http://adele.fh- joanneum.at.
References
AdeLE (2005). Adaptive e-Learning with Eye Tracking; project Web site. http://adele.fh-joanneum.at, last visit: 2005-03-01.
ADL (2001). Advanced Distributed Learning: Sharable Content Object Reference Model Version 1.2: The SCORM Overview, Advanced Distributed Learning, USA, 2001. http://www.adlnet.org/, last visit: 2004-11-20.
ARIADNE (2003). Ariadne Foundation; official website of Ariadne Foundation for the European Knowledge Pool. http://www.ariadne-eu.org, last visit: 2003-11-28.
Baumgartner, P. (2003). E-Learning an Hochschulen: Didaktik, Modelle, Strategien und Perspektiven; In Proceedings of OCG e-Future, 2003 (in German).
Beaumont, I., Brusilovsky, P. (1995). Adaptive Educational Hypermedia: From Ideas to Real Systems; in Proceedings of ED- MEDIA 95, Graz, Austria, 1995, p. 93-98.
Bloom, B. S. (1984). The 2 Sigma Problem: The Search for Methods of Group Instruction as Effective as One-to-One Tutoring; Educational Researcher, 13 (6), 1984, p. 4-16.
Boyle, C.F., Encarnacion, A.O. (1998). MetaDoc: An Adaptive Hypertext Reading System; Adaptive Hypertext and Hypermedia; Editors: Brusilovsky, Kobsa, Vassileva; Kluwers Academic Publishers, NL, 1998.
Brusilovsky, P. (1998). Adaptive Educational Systems on the World-Wide-Web: A Review of Available Technologies; WWW-Based Tutoring Workshop, 4th Conference on Intelligent Tutoring Systems, USA, 1998.
Brusilovsky, P. (1998). Methods and Techniques of Adaptive Hypermedia. Adaptive Hypertext and Hypermedia; Editors: Brusilovsky, Kobsa, Vassileva; Kluwers Academic Publishers, NL, 1998.
Conlan, O., Dagger, D., Wade, V. (2002). Towards a Standards-based Approach to e-Learning Personalization using Reusable Learning Objects: E-Learn, 2002, p. 210-217.
Cooper, G. (1998). Research into Cognitive Load Theory and Instructional Design at UNSW; Research report at School of
Education Studies, University of New South Wales, Sydney, Australia, 1998, http://education.arts.unsw.edu.au/CLT_NET_Aug_97.HTML, last visit: 2005-08-02.
Crowell, F. A., Traegde, S. C. (1967). The role of computers in instructional systems: past and future; Proceedings of the 1967 22nd national conference, Washington, D.C., United States, 1967, p. 417-425.
Czarkowski, M., Kay, J. (2003). Challenges of Scrutable Adaptivity; In Proceedings from the 11th International Conference in Artificial Intelligence in Education (AIED); Sydney, Australia, 2003.
Dietinger, T. (2003). Aspects of E-Learning Environments: PhD work, Institute for Information Systems and Computer Media (IICM), Faculty of Computer Science at Graz University of Technology, Austria, 2003.
Dietinger, T., Guetl, C., Knögler, B., Neussl, D., Schmaranz, K. (1999). Dynamic Background Libraries - New Developments In Distance Education Using HIKS (Hierarchical Interactive Knowledge System), J.UCS 5,1, 1999. http://www2.iicm.edu/cguetl/papers/dynamicbacklib, last visit: 2004-10-29.
Feinberg, S., Murphy, M. (2000). Applying cognitive load theory to the design of web-based instruction; In Proceedings of IPCC/SIGDOC '00 (IEEE professional communication society international professional communication conference and Proceedings of the 18th annual ACM international conference on Computer documentation), IEEE Educational Activities Department, MA – USA, 2000, p. 353-360.
Fritz, A., Galley, N., Groetzner, Ch. (2002). Zum Zusammenhang von Leistung, Aktivierung und Motivation bei Kindern mit unterschiedlichen Hirnfunktionsstörungen; Zeitschrift für Neuropsychologie, 1, Heft 1, 1992, 79-92 (in German).
Galley, N. (2001). Physiologische Grundlagen, Meßmethoden und Indikatorfunktion der okulomotorischen Aktivität; In Frank Rösler (ed.): Enzyklopädie der Psychologie, 4, Grundlagen und Methoden der Psychophysiologie, 2001, 237-315 (in German).
García-Barrios, V. M.; Gütl, C.; Preis, A.; Andrews, K.; Pivec, M.; Mödritscher, F.; Trummer, C. (2004). AdELE: A Framework for Adaptive E-Learning through Eye Tracking; in Proceedings of I-KNOW ’04, Graz, Austria, 2004, p. 609-616.
García-Barrios, V. M., Gütl, C., Pivec, M. (2004). Semantic Knowledge Factory: A New Way of Cognition Improvement for the Knowledge Management Process; Proceedings of SITE2002, Nashville, USA, 2002.
García-Barrios, V. M. (2001). Dynamic Background Library: Learning on demand using Hyperwave eLearning Suite and xFIND; Project work at Institute for Information Systems and Computer Media (IICM), Graz University of Technology, Austria, 2001. http://www2.iicm.edu/cguetl/education/projects/vgarcia/Project.pdf, last visit: 2004-11-15.
Gestalt (1999). GESTALT: Getting Educational Systems Talking Across Leading-Edge Technologies; Work Package 3, D0301 Design and Specification of the RDS (Resource Discovery Service), 1999. http://www.fdgroup.co.uk/gestalt, last visit: 2004-10-29.
Glaser, R. (1984). Education and Thinking: The Role of Knowledge; American Psychologist 39, p. 93-104, 1984.
Goldberg, J. H., Stimson, M. J., Lewenstein, M., Scott. N., Wichansky, A. M. (2002). Eye Tracking in Web Search Tasks: Design Implications; Proc. Symposium on ETRA2002, New Orleans, Louisiana, USA, 2002, p. 51-58.
Guetl, C. (2002). Approaches to Modern Knowledge Discovery for the Internet. An Approach to the Information Gathering and Organizing System xFIND (Extended Framework for INformation Discovery); PhD work at Institute for Information Processing and Computer supported new Media (IICM), Graz University of Technology, Austria, 2001. http://www.iicm.edu/thesis/cguetl_diss/diss_cguetl.pdf, last visit: 2003-08-05.
Hockemeyer, C. (1997). RATH, A Relational Adaptive Tutoring Hypertext WWW – Environment; Institute for Psychology at Karl-Franzens University of Graz, Austria, 1997.
Hayhoe, M. M., Ballard, D. H., Triesch, J., Shinoda, H., Aivar, P., Sullivan, B. (2002). Vision in Natural and Virtual Environments; Proc. Symposium on ETRA2002, New Orleans, Louisiana, USA (2002), 7-13.
Honey, P. (1986). The Manual of Learning Styles; Maidenhead, Berks, Peter Honey, 1986.
Hothi, J., Hall, W. (1998). An Evaluation of Adapted Hypermedia Techniques Using Static User Modelling; Proceedings of the 2nd Workshop on Adaptive Hypertext and Hypermedia of the Hypertext'98, Pittsburg, USA, 1998.
IDC (2003). U.S. Corporate and Government eLearning Forecase. 2002-2007; 2003, last visit: 2004-01-29, retrieved from http://www.idc.com/, last visit: 2004-11-18.
IMS LIP (2001). IMS Learner Information Package (IMS LIP); Version 1.0 IMS Learner Information Package Specification, IMS Global Learning Consortium Inc., (March 2001). http://www.imsglobal.org/profiles/index.cfm, last visit: 2004-11-05.
IMS (2003). Global Learning Consortium: Overview of Specifications; 2003. http://www.imsglobal.org/overview.cfm, last visit: 2004-11-04.
Ivory, M. Y., Hearst, M. A. (2001). The State of the Art in Automating Usability Evaluation of User Interfaces; ACM Computing Surveys (CSUR), ACM Press, 33, 4, 2001, p. 470-516.
Jain, L. C., Howlett, R. J., Ischalkaranje, N. S., Tonfoni, G. (2002). Virtual Environments for Teaching & Learning; World Scientific Publishing Co. Pte. Ltd.; Jain. L. C. (ed), Series of Innovative Intelligence, Vol. 1, Preface, 2002.
Jacob, R. J. K. (1995). Eye tracking in advanced interface design; In Advanced Interface Design and Virtual Environments, Oxford University Press, Oxford, 1995, 258-288.
Josephson, S., Holmes, M. E. (2002). Visual Attention to Repeated Internet Images: Testing the Scanpath Theory on the World Wide Web; Proceedings of the symposium on ETRA2002, New Orleans, Louisiana, 2002, p. 43-49.
Kahneman, D. & Beatty, J. (1966). Pupil diameter and load on memory; Science, 154, p. 1583-1585, 1966.
Lennon, J. ; Maurer, H. (2003). Why it is Difficult to Introduce e-Learning into Schools And Some New Solutions; In Journal of Universal Computer Science, Vol. 9, No. 10, 2003, p. 1244-1257. www.jucs.org/jucs_9_10/why_it_is_difficult, last visit: 2004-11-10.
Leutner, D.; Plass, J.L. (1998). Measuring Learning Styles with Questionnaires Versus Direct Observation of Preferential Choice Behaviour in Authentic Learning Situations; The Visualizer/Verbalizer Behaviour Observation Scale (VV-BOS), Computers in Human Behaviour 14(4), p.p. 543-557, 1998.
Liaw, S.S.; Ting, I.H.; Tsai, Y.C. (2003). Developing a conceptual model for designing a Web assisted information retrieval system; In Proceedings of the 2003 International Conference on Computer-Assisted Instruction (ICCAI2003).
LOM (2003). Learning Object Metadata; Learning Technology Standards Committee of IEEE, Working Group 12 (WG12), 2002, http://ltsc.ieee.org/wg12, last visit: 2004-11-14.
Merchant, S. (2001). Eye Movement Research in Aviation and Commercially Available Eye Trackers Today; Eye Movement Summary - Assessing Human Visual Performance, Course at Department of Industrial Engineering, University of Iowa, USA, 2001. http://arrow.win.ecn.uiowa.edu/56245/FinalEyeTrackingReportAug17.pdf, last visit: 2004-10-25.
Modesitt, K. L. (1974). An excellent mixture for PSI: Computer science, PLATO, knowledge levels; Proceedings of the 1974 annual conference, ACM Press, 1974, p. 89-94.
NLII (2004). Learning Object Repositories; National Learning Infrastructure Initiative, Washington, USA, 2004. http://elearning.utsa.edu/guides/LO-repositories.htm, last visit 2004-10-25.
PAPI (2000). Public and Private Information; IEEE 2000, Draft Standard for Learning Technology - Public and Private Information (PAPI) for Learner, IEEE P1484.2/D6, 2000. http://ltsc.ieee.org/, last visit: 2004-11-10.
Pivec, M. (2000). Knowledge Transfer in On-line Learning Environments; PhD work at Graz University of Technology, Austria, 2000.
Pivec, M., Preis, M. A., García-Barrios, V. M., Gütl, Ch., Müller, H., Trummer, Ch., Mödritscher, F. (2004). Adaptive Knowledge Transfer in E-Learning Settings on the Basis of Eye Tracking and Dynamic Background Library; EDEN 2004 Annual Conference, Budapest, Hungary, 16-19 June, 2004
Preis, A. M., Mueller, H. (2003). EyeTracking in Usability Research & Consulting: What Do the Eyes Reveal about Websites & their Users; European Congress of Psychology: Psychology in Dialogue with Related Disciplines, Austria, 2003, p164.
Rao, R. P. N., Zelinsky, G. J., Hayhoe, M. M., Ballard, D. H. (1997). Eye Movements in Visual Congnition: A Computational Study; Technical Report - National Resource Laboratory for the Study of Brain and Behavior, Department of Computer Science, University of Rochester, USA, 1997.
Rayner, K. (1998). Eye Movements in Reading and Information Processing; 20 Years of Research. Psychological Bulletin, 124(3): p. 372-422, 1998.
Robertson, I. T. (1985). Human Information-Processing Strategies and Style; Behaviour and Information Technology 4(1), p. 19-29, 1985.
Schmeck, R.R. (1988). Learning Strategies and Learning Styles; New York, Plenum Press, 1988.
Seeberg, C. (2003). Life Long Learning; Modulare Wissensbasen für elektronische Lernumgebungen; Springer, Heidelberg / New York, 2003.
Zhai, S. (2003). What's in the Eyes for Attentive Input; Communications of the ACM (Association for Computing Machinery), 46, 3, 2003, p. 34-39. http://doi.acm.org/10.1145/636772.636795, last visit: 2004-10-29.
An Interactive Model-Based Environment for Eye-Movement Protocol Analysis and Visualization Dario D. Salvucci
ABSTRACT
This paper describes EyeTracer, an interactive environment for manipulating, viewing, and analyzing eye-movement protocols. EyeTracer augments the typical functionality of such systems by incorporating model-based tracing algorithms that interpret protocols with respect to the predictions of a cognitive process model. These algorithms provide robust strategy classification and fixation assignment that help to alleviate common difficulties with eye-movement data, such as equipment noise and individual variability. Using the tracing algorithms for analysis and visualization, EyeTracer facilitates both exploratory analysis for initial understanding of behavior and confirmatory analysis for model evaluation and refinement.
Keywords
Eye movements, protocol analysis, visualization, tracing.
1. INTRODUCTION
Our eyes reveal a great deal about us; whether or not our eyes are "windows to the soul," as the common saying goes, they are certainly windows to the mind. Eye movements make up an extremely informative component of human observable behavior that, along with other components like gesture and speech, allow us to infer what people are thinking based on their actions. Because of their informativeness, eye movements have enjoyed burgeoning attention in recent years as a tool for studying human behavior. For instance, researchers have studied eye movements to understand cognition and behavior in skills such as menu selection [2], reading [9], driving [11], and mathematics [7]. Researchers have also studied eye movements to infer intent in real-time user interfaces [5, 6, 8, 14].
Eye movements provide a wealth of information regarding how people acquire and process information. Eye movements are especially convenient because data can be collected at a fine
temporal grain size and subjects need little instruction and training to produce informative data. However, eye movements are also very time-consuming and tedious to analyze. Like verbal protocols, several trials of even a simple task can generate enormous sets of eye-movement data, all of which must be coded into some more manageable form for analysis. In addition, eye-movement protocols typically include a great deal of equipment noise and human variability. For large data sets with hundreds or thousands of trial protocols, it is simply implausible for humans to code the data consistently, accurately, and in a reasonable amount of time. Automated methods that assist in protocol analysis allow investigators to analyze larger, more complex data sets in a consistent, detailed manner that would otherwise be impossible.
This paper describes a system, EyeTracer, that provides model- based automated analysis and visualization of eye-movement protocols. At its core, the system incorporates several algorithms that analyze eye movements by means of tracing – relating protocols to the sequential predictions of a cognitive process model. One algorithm uses sequence matching to align strings of fixations with the targets to which they correspond. Two other algorithms use hidden Markov models (HMMs) [12] to determine the most probable interpretation of a protocol given a probabilistic model of behavior. These tracing algorithms utilize a number of fixation-identification algorithms that separate fixations (pauses during which encoding takes place) from saccades (rapid eye movements from one fixation to the next). All these algorithms have been shown to significantly alleviate typical noise and variability in eye-movement protocols and provide robust analysis in the face of such difficulties [15].
EyeTracer is the first system designed specifically for the automated tracing of eye-movement protocols. However, EyeTracer resembles several past and present protocol analysis systems, including MacSHAPA [18], PAS [21], SAPA [1], and Soar/MT [13]. Some systems (e.g., MacSHAPA) emphasize exploratory analysis of protocols with the goals of better understanding behavior and forming an initial cognitive model of this behavior. Other systems (e.g., PAS, Soar/MT) emphasize confirmatory analysis with the goals of comparing observed behavior to model predictions and thereby refining the model. Critical to both types of analysis is the ability to visualize protocols in some convenient form; for instance, Soar/MT provides a spreadsheet in which verbal protocols can
Nissan Cambridge Basic Research Four Cambridge Center Cambridge, MA 02142 USA +1 617 374 9669
dario@cbr.com
 Salvucci, D. D. (2000). An interactive model-based environment for eye-movement protocol analysis and visualization. In Proceedings of the Eye Tracking Research and Applications Symposium (pp. 57-63). New York: ACM Press.
be visually aligned with model predictions to make the trace
explicit. EyeTracer provides a number of ways to visualize
protocols for both exploratory and confirmatory analysis, thus
enabling a user to better understand behavior, construct a
prototype model, and subsequently refine the model based on
observed-predicted mismatches. EyeTracer runs on the Apple
Macintosh platform and is implemented in Common Lisp for
1
given the model, thus assigning points to either the fixation or saccade state.
• Dispersion-threshold identification (I-DT): a method that utilizes the fact that fixation points, because of their low velocity, tend to cluster closely together [22]. I-DT iterates through the protocol and groups consecutive points that lie within a given dispersion (i.e., maximum separation). It also incorporates a duration threshold of approximately 100 ms to guarantee fixations of a minimum duration.
• Region-of-interest identification (I-ROI): a method that identifies fixations within given rectangular target areas representing relevant units of visual information [3]. I-ROI simply groups consecutive data points within a single target area and labels these points as fixations.
Further details can be found in a companion paper [17].
While the above methods implemented in EyeTracer are representative of the most common identification algorithms in previous work, there are of course numerous other possible methods for fixation identification. For instance, Tole and Young [20] applied digital filters to perform identification on- line in real-time. Also, Karsh and Breitenbach [10] utilized a complex algorithm most similar to I-DT in their detailed exposition on identification. EyeTracer’s modular implementation allows for easy augmentation of the system to include other such methods.
2.2 Tracing
Tracing is the process of interpreting protocols by mapping observed protocols to the sequential predictions of a cognitive process model. This process is the central focus of the EyeTracer system, allowing for rigorous automated analysis for both exploratory and confirmatory analysis. EyeTracer includes three tracing algorithms that allow the user to trade off tracing speed and accuracy as desired for particular applications. This section provides a brief overview of these three algorithms; interested readers can refer to [15, 16] for more detailed descriptions.
To illustrate the tracing process, Figure 1 shows a sample protocol taken from an “eye-typing” task in which users type words by fixating the letters of the word on an on-screen keyboard [14]. The figure displays a protocol taken from a user typing the word SQUARE; thus, our model of the task would predict consecutive fixations on each letter of the word, as shown. Tracing determines the best mapping from the observed protocol to the predicted fixations, shown in the figure as dotted lines. Note that eye-movement protocols often contain off-center or extraneous fixations caused by equipment noise and individual variability; thus the tracing process must be robust to alleviate such problems.
2.2.1 Tracing Inputs and Outputs
The tracing algorithms have three inputs: an eye-movement protocol, a set of target areas, and a cognitive process model. The eye-movement protocol is a sequence of "point-of-regard" data points as collected by an eye tracker. The set of target areas defines rectangular regions that model where fixations for the various possible targets may occur; for instance, in the eye-
the MCL environment.
The paper begins with a description of the analysis and visualization aspects of the EyeTracer system. It then provides an overview of two sample applications in which EyeTracer has proven successful at coding of experiment protocols and building an intelligent gaze-based interface.
2. PROTOCOL ANALYSIS
EyeTracer addresses two main problems in the analysis of eye- movement protocols: the identification of fixations and saccades in a raw eye-movement protocol, and tracing or assignment of fixations to the visual targets to which they correspond. We now consider each problem in detail and briefly describe the algorithms as implemented in EyeTracer.
2.1 Fixation Identification
Fixation identification takes a raw eye-movement protocol and translates it into a sequence of fixations and saccades. Typically, eye-movement data as collected by eye-tracking equipment comprise a sequence of <x,y> data points sampled at a high frequency (e.g., 60-1000 times per second). While there are certain benefits to dealing directly with the raw data, we are often most interested in where and when the fixations occur in the protocol. To this end, fixation identification labels each point in the raw protocol as either a fixation or saccade and collapses consecutive points into either a single fixation or saccade. Thus, the output of fixation identification is (typically) a sequence of <x,y,t,d> fixations where x and y describe the location of the fixation, t describes its onset time, and d describes its duration.
EyeTracer includes four algorithms for fixation identification:
• V elocity-threshold identification (I-VT): a velocity-based method that separates fixation and saccade points based on their point-to-point velocities [4, 19]. I-VT classifies each data point as a fixation or saccade according to the velocity at that point: low velocities correspond to fixations, and high velocities to saccades.
• Hidden Markov model identification (I-HMM): a method based on probabilistic hidden Markov models [12] that determine the most likely identifications for a given protocol [15, 16]. I-HMM constructs a two-state HMM in which one state represents low-velocity fixations and the other state represents high-velocity saccades. It then interprets a raw eye-movement protocol through a process of decoding [12] that maximizes the probability of the data
EyeTracer is publicly available on the World Wide Web at < http://www.cbr.com/~dario/EyeTracer/ >.
 1
                                                                                                                                                                                                                                                                                                                                                                                                                                                  Model Predictions
Figure 1: Sample trace of an “eye-typing” protocol representing a user typing the word SQUARE.
typing screen in Figure 2, each letter has a target area around its respective key. The cognitive process model is expressed as a regular grammar comprising grammar rules with non-terminals and terminals. The non-terminals represent intended goals while the terminals represent predicted fixations. In addition, the grammar can incorporate probabilities associated with each rule that represents its likelihood of firing. For example, Table 1 shows a sample model grammar for the eye-typing task that models the intent to type one of three words "RAT", "TRAP", and "PART". The non-terminal type fixates the letters in a word and sets a new non-terminal end. This non-terminal produces the fixation on the Space box to end the typing of the word. The rule probabilities for type are uniform such that all rules fire with equal likelihood. The model grammar can be written directly or derived from a more sophisticated modeling paradigm, such as a production-system cognitive architecture.
Table 1: Sample eye-typing grammar.
Tracing produces two outputs. First, it generates a trace, or alignment, from the observed eye-movement protocol to the predictions of the cognitive process model. Second, it generates a “goodness-of-fit” score given as the mismatch between model and data. Both the trace and the mismatch score
can be used to evaluate the model or compare multiple models, thus facilitating subsequent model refinement.
2.2.2 Target Tracing
Target tracing uses a common sequence-matching algorithm to align observed and predicted fixations [15]. First, target tracing uses one of the fixation-identification algorithms to generate a sequence of observed fixations from the raw eye- movement protocol. Next, it determines all possible sequences of predicted fixations from the given process model; for instance, the model grammar in Table 1 would produce three sequences for each of the three words it embodies. Finally, target tracing matches the observed sequence with each predicted sequence and determines which predicted sequence produces the best match; the best match is defined as the alignment that requires the fewest insertions, deletions, and substitutions from one sequence to the other. The determined predicted sequence and its alignment with the observed sequence is the final result of the tracing process.
2.2.3 Fixation Tracing
Fixation tracing utilizes hidden Markov models [12] to interpret eye-movement protocols. Hidden Markov models are powerful statistical tools that have been extensively employed for speech and handwriting recognition. To trace a sequence of identified fixations, fixation tracing requires the construction of a tracer HMM that embodies the given process model, as shown in Figure 2. Each “state” in this HMM is actually a smaller fixation HMM that represents a fixation on the predicted target area; each state of these sub-HMMs incorporates probability distributions for the <x,y> location of observed fixations, centered around the center of the
S
Q
U
A
R
E
       Rule Prob.
           type‡R A T end 1/3 type‡T R A P end 1/3 type‡P A R T end 1/3 end ‡ Space 1

predicted target area. The fixation HMMs are then combined into a tracer HMM that encapsulates the predictions of the model grammar, shown in the figure.
2.3 Discussion
While the above identification and tracing algorithms can be very powerful in particular contexts, as we will see shortly, it is important to note two assumptions that limit their usefulness in other contexts. First, the algorithms assume that the user can identify visual targets and target areas needed for interpretation. This task may be difficult for complex scenes or even single complex objects (e.g., faces) for which visual targets may not be obvious. Second, the algorithms assume that the visual screen is static — that is, does not change during visual scanning. While there may be ways to extend the algorithms to dynamic environments, this avenue of research as yet remains unexplored.
3. PROTOCOL VISUALIZATION
Regardless of the type of data analysis, visualization of protocols is essential to assist in understanding behavior and developing cognitive models. EyeTracer provides a number of ways to visualize, or view, eye-movement protocols. When a user selects a set of protocols to view (i.e., a particular subject and day), EyeTracer opens a Viewer window that allows two main types of visualizations. First, the user can replay a protocol in real time or any factor thereof. The replay can also include other actions such as mouse movements or typing into an editable text box.
Second, EyeTracer can draw a static display representing the protocol in one of several ways. Sample protocols for various types of displays are shown in Figure 3. If no model is used, EyeTracer defaults to (a) a velocity-based highlighting where points with smaller velocities are drawn larger to emphasize low-velocity fixations. If a model is specified, EyeTracer uses the selected tracing method to find fixations and emphasize fixation points. The tracing methods include six combinations of fixation identification and tracing algorithms: target tracing with the four identification algorithms I-HMM, I-VT, I-DT, and I-TA; fixation tracing with I-HMM; and point tracing. The user may opt not to trace the protocol, in which case EyeTracer performs fixation identification only and does not trace the protocol. The system can display such protocols in several ways depending on selected options; these ways include views (b) with only fixation highlighting and (c) with highlighting, numbering, and the associated targets (i.e., the nearest targets). When the user opts for tracing, EyeTracer traces the protocol with the given process model and (d) displays predicted fixations in blue with their associated targets and unpredicted fixations in red with no associated target; the system also outputs the protocol trace to the output window for inspection.
The various ways of viewing protocols makes EyeTracer an excellent tool for exploratory and confirmatory analysis with several levels of post-processing. Velocity-based highlighting adds only minimal processing to the protocol and thus presents an unbiased picture of subject behavior. Fixation highlighting adds slightly more processing (i.e., fixation identification) but improves the clarity of the protocol and further emphasizes fixations. Fixation numbering and associations also facilitate viewing by providing sequential information and naive target assignments, respectively, to the
                      R
A
T
                                                              T
R
A
P
Space
                                                               P
A
R
T
                            R
Figure 2: Tracer HMM for the Table 1 grammar. The bottom portion of the figure shows the fixation HMM for the target area R; the top portion shows how such fixation HMMs are combined into the final tracer HMM.
Given the constructed tracer HMM, fixation tracing interprets the observed fixation sequence using the standard HMM decoding process [12]. This process determines the alignment of observed fixations to HMM states that maximizes the probability of the sequence given the HMM. The process results in a robust trace of the protocol that alleviates noise and variability through its probabilistic interpretation.
2.2.4 Point Tracing
Point tracing, like fixation tracing, traces protocols using hidden Markov models. However, instead of tracing the sequence of identified fixations, point tracing interprets the raw eye-movement data directly. To this end, the tracer HMMs constructed for point tracing comprise states that predict the location and velocity of raw sampled points rather than the location of fixations, as in fixation tracing. Point tracing then uses the same decoding process to find the most probable alignment of the protocol to the HMM. While point tracing can sometimes produce more accurate interpretations than fixation tracing, the latter is often much more efficient especially for large, complex process models.
      (a)
(b)
(c)
(d)
Figure 3: Visualizations of a sample protocol with (a) velocity-based highlighting, (b) fixation highlighting, (c) numbered fixations with target associations, and (d) traced fixations with blue shading and associations for predicted fixations and red shading for unpredicted fixations (color indicated by text and arrows).
                           (red)
     (blue)
   fixations. Tracing provides a view biased to the specific process model used, which assists greatly in finding mismatches between observed behavior and model predictions.
4. SAMPLEAPPLICATIONS
EyeTracer has been employed in several real-world domains applications. The following exposition summarizes two applications: the coding of experimental protocols and real- time interpretation in a gaze-based user interface.
4.1 Experiment Protocol Coding
The first application involved the coding of eye-movement protocols from psychological experiments [16]. Coding is the process of classifying a protocol (or parts of a protocol) as one of a set number of predicted strategies. The coding of eye- movement protocols is typically very tedious and time-
consuming because of the size and complexity of typical data sets. Coding protocols by hand is thus generally infeasible except for only a small set of protocols. This study explored how tracing can provide robust and consistent automated coding for even very large eye-movement data sets.
The coding study used an equation-solving task (as in Figure 3) in which undergraduates solved equations of a very specific form. In the experiment, we instructed subjects to execute particular strategies when solving these problems; the instructions included the order in which to encode the equation items and compute results. These instructions gave us “correct” interpretations in that we knew (with reasonable certainty) what strategy each protocol represented. The protocols were then given to the tracing algorithms and to two human experts accustomed to examining eye-movement protocols.
Table 2 shows the percent agreement between the tracing algorithms, the human experts, and the “correct” interpretations as defined by the instructed strategies. When examining the agreement with the correct interpretations, we see that the tracing algorithms produced interpretations that were more accurate than those of the human experts. Interestingly, the interpretations of the human experts generally agreed more with those of the automated algorithms than with each other. In addition, the tracing algorithms required approximately an order of magnitude less time for their interpretations (approximately 1 second per protocol) than the human experts (approximately 1 minute per protocol). Thus, the tracing algorithms analyze eye movements as well as or better than human experts in much less time.
Table 2: Percent agreement between target tracing (TT), fixation tracing (FT), point tracing (PT), human experts 1 and 2 (H1 and H2), and correct interpretations.
information, the full grammar incorporates full sequential information, and the remaining grammars fall in between.
Table 3 shows the ratio of correct interpretations for fixation tracing using the four model grammars and three vocabulary sizes: 12, 100, and 1000 words. The simple grammar did extremely poorly, interpreting almost none of the protocols correctly. The 1st-order and 2nd-order grammars produced better results because of their increased predictive power. The full grammar produced excellent results even for the largest vocabulary. However, the accuracy of the better grammars came at the cost of decreased speed. Table 4 shows the amount of time, in ms, needed to interpret one protocol. As grammars incorporate more information and as vocabulary size grows, interpretation time increases steadily. Fortunately, several techniques such as beam decoding can nicely reduce these times; space constraints preclude a detailed description. In summary, the tracing algorithms provide a wide variety of speed-accuracy tradeoffs that an interface developer can utilize for particular machine setups and applications.
Table 3: Ratio of correct interpretations for all grammars and vocabularies.
Table 4: Time to interpret one protocol for all grammars and vocabularies, in ms.
5. CONCLUSIONS
EyeTracer, unlike previous systems for eye-movement data analysis, makes use of model-based tracing algorithms that allow for analysis and visualization with respect to a cognitive process model. This aspect of EyeTracer facilitates both exploratory and confirmatory analysis: exploratory analysis with no assumed model for initial understanding of behavior and for model prototyping; and confirmatory analysis for model comparison and refinement. EyeTracer has been applied to the domains of equation solving, reading, and eye typing with good success [15], and further applications to other domains are forthcoming.
             TT FT PT
          H1 H2
         Correct
     TT FT PT
    – 84.4 84.4 – 93.7
–
 81.2 90.6 81.2 84.4 78.1 90.6
    87.5 93.7 93.7
         H1 H2
                    – 81.2 –
          78.1 90.6
          Words
            Simple 1st- 2nd- Full order order
          12 100 1000
        .00 .43 .00 .10 .00 .04
.77 .99 .48 .99 .14 .95
          4.2 Gaze-BasedUserInterfaces
The second application involved the study of a gaze-based interface in which users type words by looking at on-screen letters, as described earlier [14]. In the experimental task, users read the word to be typed, eye-typed the words with the eye movements, and finally fixated the space bar to produce the output. Earlier eye-based interfaces have achieved reasonable success, particularly for users with physical disabilities [5]. However, they typically incorporate two restrictions that limit their usability. First, they require large spacing between items in the visual scene to reduce off-center fixations. Second, they require long dwell thresholds—minimum time to actuate a command—to reduce incidental fixations. This study examined how the data degrade when these restrictions are removed. A sample eye-typing screen and protocol are shown in Figure 1.
To interpret the eye-typing data, we utilized four model grammars that incorporate increasing amounts of sequential information based on a given vocabulary. The simple grammar simply maps fixations to the nearest letter; this grammar has been used in almost all previous eye-based interfaces. The 1st- order grammar contains the probabilities for 1st-order transitions from letter to letter; for instance, the probability of a transition from “Q” to “U” is high while “Q” to “J” is low. The 2nd-order grammar contains the probabilities for 2nd-order transitions from letters pairs to another letter. The full grammar contains each word in the vocabulary in their entirety. Note that the simple grammar incorporates no sequential
     Words
            Simple 1st- 2nd- Full order order
          12 100 1000
        48 57 98 124 50 83 407 867 49 81 1005 9104

ACKNOWLEDGMENTS
This work was completed in part at Carnegie Mellon University under a National Science Foundation Graduate Fellowship awarded to Dario Salvucci and Office of Naval Research grant N00014-95-10223 awarded to John R. Anderson. I thank John Anderson for his tireless support and the ACT-R research group for many helpful questions and comments.
REFERENCES
[1] Bhaskar, R., & Simon, H. A. (1977). Problem solving in semantically rich domains: An example from engineering thermodynamics. Cognitive Science, 1, 193-215.
[2] Byrne, M. D., Anderson, J. A., Douglass, S., & Matessa, M. (1999). Eye tracking the visual search of click-down menus. In Human Factors in Computing Systems: CHI 99 Conference Proceedings. New York: ACM Press.
[3] DenBuurman, R., Boersma, T., & Gerrisen, J. F. (1981). Eye movements and the perceptual span in reading. Reading Research Quarterly, 16, 227-235.
[4] Erkelens, C. J., & Vogels, I. M. L. C. (1995). The initial direction and landing position of saccades. In J. M. Findlay, R. Walker, & R. W. Kentridge (Eds.), Eye Movement Research: Mechanisms, Processes, and Applications (pp. 133-144). New York: Elsevier Science Publishing.
[5] Gips, J. (1998). On building intelligence into EagleEyes. In V. O. Mittal, H. A. Yanco, J. Aronis, & R. Simpson (Eds.), Assistive Technology and Artificial Intelligence (pp. 50-58). Berlin: Springer-Verlag.
[6] Goldberg, J. H., & Schryver, J. C. (1995). Eye-gaze determination of user intent at the computer interface. In J. M. Findlay, R. Walker, & R. W. Kentridge (Eds.), Eye Movement Research: Mechanisms, Processes, and Applications (pp. 491-502). New York: Elsevier Science Publishing.
[7] Hegarty, M., Mayer, R. E., & Green, C. E. (1992). Comprehension of arithmetic word problems: Evidence from students’ eye fixations. Journal of Educational Psychology, 84, 76-84.
[8] Jacob, R. J. K. (1995). Eye tracking in advanced interface design. In W. Barfield & T. A. Furness (Eds.), Virtual Environments and Advanced Interface Design (pp. 258- 288). New York: Oxford University Press.
[9] Just, M. A., & Carpenter, P. A. (1980). A theory of reading: From eye fixations to comprehension. Psychological Review, 87, 329-354.
[10] Karsh, R., & Breitenbach, F. W. (1983). Looking at looking: The amorphous fixation measure. In R. Groner, C. Menz, D. F. Fisher, & R. A. Monty (Eds.), Eye Movements and Psychological Functions: International Views (pp. 53-64). Hillsdale, NJ: Erlbaum.
[11]McDowell, E. D., & Rockwell, T. H. (1978). An exploratory investigation of the stochastic nature of the drivers’ eye movements and their relationship to roadway geometry. In J. W. Senders, D. F. Fisher, & R. A. Monty (Eds.), Eye Movements and the Higher Psychological Processes (pp. 329-345). Hillsdale, NJ: Erlbaum.
[12] Rabiner, L. R. (1989). A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77, 257-286.
[13] Ritter, F. E., & Larkin, J. H. (1994). Developing process models as summaries of HCI action sequences. Human- Computer Interaction, 9, 345-383.
[14] Salvucci, D. D. (1999). Inferring intent in eye-movement interfaces: Tracing user actions with process models. In Human Factors in Computing Systems: CHI 99 Conference Proceedings (pp. 254-261). New York: ACM Press.
[15]Salvucci, D. D. (1999). Mapping eye movements to cognitive processes. Doctoral Dissertation, Department of Computer Science, Carnegie Mellon University.
[16] Salvucci, D. D., & Anderson, J. R. (1998). Tracing eye movement protocols with cognitive process models. In Proceedings of the Twentieth Annual Conference of the Cognitive Science Society (pp. 923-928). Hillsdale, NJ: Erlbaum.
[17] Salvucci, D. D., & Goldberg, J. H. (2000). Identifying fixations and saccades in eye-tracking protocols. To appear in the Proceedings of the Eye Tracking Research and Applications Symposium.
[18]Sanderson, P., Scott, J., Johnston, T., Mainzer, J., Watanabe, L., & James, J. (1994). MacSHAPA and the enterprise of exploratory sequential data analysis (ESDA). International Journal of Human-Computer Studies, 41 , 633-681.
[19] Sen, T., & Megaw, T. (1984). The effects of task variables and prolonged performance on saccadic eye movement parameters. In A. G. Gale & F. Johnson (Eds.), Theoretical and Applied Aspects of Eye Movement Research (pp. 103-111). Amsterdam: Elsevier.
[20] Tole, J. R., & Young, L. R. (1981). Digital filters for saccade and fixation detection. In D. F. Fisher, R. A. Monty, & J. W. Senders (Eds.), Eye Movements: Cognition and Visual Perception (pp. 247-256). Hillsdale, NJ: Erlbaum.
[21] Waterman, D. A., & Newell, A. (1971). Protocol analysis as a task for artificial intelligence. Artificial Intelligence, 2, 285-318.
[22] Widdel, H. (1984). Operational problems in analysing eye movements. In A. G. Gale & F. Johnson (Eds.), Theoretical and Applied Aspects of Eye Movement Research (pp. 21-29). New York: Elsevier.
Answering questions with line and bar graphs
Liang Dong, Robert St. Amant
Department of Computer Science, North Carolina State University
Abstract. This paper describes our work in progress toward a practical tool to improve the ability of data analysts to extract specific information from simple graphical presentations. Toward this goal, we have developed cognitive models that carry out the necessary perceptual and cognitive operations to answer specific questions about data presented either as a line graph or a bar graph, concerning point reading (quantity estimation or comparison to a known value), item comparison, and trend estimation. These models run in the SIMCog framework, which allows models to interact with a Web application running in a browser. The models are a reasonable match to human data gathered in a pilot study; generalization and model validation remain for future work.
Keywords: cognitive model, graphical presentation, eye tracking 1 Introduction
How well does a given graphical presentation of data convey specific information? Visual representations of data in some graphical form (we’ll refer to these in gen- eral as visualizations) have become a pervasive part of interactive online systms. Line graphs and bar graphs, some of the simplest presentations available, show trends in the economy, election results, and comparable information. Although there are many popular libraries like d3.js and chart.js to generate graphs for presentation on the Web, these tools generally do not extend to assisting in the choice of a given presentation technique to convey specific information. The ap- proach described in this paper is a first step toward producing recommendations to improve decisions about this choice.
It is well known that for specific tasks, some visualizations are superior to others; for example, bar graphs are superior to pie charts presenting the same in- formation, in terms of efficiency and accuracy in general. What has been explored only to a lesser extent, however, is how the relative effectiveness of different visu- alizations depends on the data being displayed. Our long-term goal is to develop an automated system that can give the designer of a visualization a recommen- dation for the most effective type of visualization for a given dataset. We are working with intelligence analysts who look at a variety of visualizations, often in a sensemaking context.
Our approach involves developing ACT-R cognitive models of common tasks required to extract information from simple visualizations. Currently, at this
stage in the project, our work is restricted to line graphs and bar graphs. From a practical perspective, we are also interested in making such results easily avail- able to people who build visualizations. Our models run in SIMCog-JS [3] and interact with Web browser-based software, as shown in Figure 1.
Fig. 1. Model execution environment
In the remainder of this paper we will briefly describe related work, then the path we have followed to the present. First is a user study, with gaze tracking, in which users answered specific questions, looking at different types of graphs. Second is our modeling work to reproduce user performance. Our results are preliminary, with models having been fit to user performance, for a small set of users; they have not yet been validated in a larger experiment on more users and visualizations generated from different datasets.
2 Related Work
Visualization design, even for simple cases such as line graphs and bar graphs, has seen continuing interest among psychologists, computer scientists, statisticians, and data analysts. For example, Cleveland’s work [1, 2] focuses on the graphical presentation of data for scientific or technical purposes, with an emphasis on accurately conveying large amounts of information so as as to make decoding easy and effective. Cleveland begins with basic principles of graph construction, looking at ways of making the data stand out.

Kosslyn [4], in contrast, targets the communicator-of-results rather than the pure statistician developing visualizations for expert colleagues or the artist pro- ducing visual effects that may create an emotional impact on the viewer. Tukey famously remarked that a good visualization “forces us to notice what we never expected to see;” Kosslyn’s focus is on clear communication of what the analyst has already noticed.
Researchers have examined principles of good data presentation from a cog- nitive perspective. Influential early work is by Lohse [5], whose UCIE (Under- standing Cognitive Information Engineering) system simulates graphical percep- tion for simple visualizations. UCIE predicts response time to answer a question posed to a graphic display from assumptions about the sequence of eye fixations, short-term memory capacity and duration limits, and the degree of difficulty to acquire information at each glance. An empirical study compared actual perfor- mance to UCIE predictions over a range of display types and question types. The results yielded some support for the cognitive model. UCIE simulates how people answer certain questions posed to bar graphs, line graphs, and tables. It can process three types of queries: point reading, comparisons, and trends. Point-reading questions refer to a single datapoint . Comparison questions refer to a pair of adjacent data points. Trend questions refer to a range of successive data points.
Peebles and Cheng [6] describe an experiment and eye movement study, the results of which show that optimal scan paths assumed in the task analysis approximate the detailed sequences of saccades made by individuals. Their re- search demonstrates the computational processing non-equivalence of two infor- mationally equivalent graphs and illustrates how the computational advantages of a representation can outweigh factors such as user unfamiliarity. Peebles and Cheng even describe ACT-R models of their tasks. The work described in this paper differs by considering a different type of graph; further, our modeling work is not as far advanced.
3 A Pilot User Study
We take line and bar graphs as representative of the type of familiar graphical information visualization relied on by analysts. Our review of the information visualization and graphical perception literatures did not lead to a set of tasks applicable to bar graphs, however. Using a generic set of primitive task opera- tors identified by Lohse [5], and based on general guidelines in the visualization literature, we developed a plausible, small set of tasks that can be carried out with a line graph or bar graph. The tasks can best be understood as answers to questions. Consider the sample line graph shown (colored lines only) on the left of Figure 2:
– Point reading: Is the value of Product B at time 4pm greater than 3?
– Item comparison: At time 3pm, is the value of Product A less than Product
C?
– Trend estimation: Is the trend of Product A from 5pm to 8pm up or down?
These tasks can be grouped or chained together to extract information rel- evant to an analysis from the display. More generally, point-reading questions refer to a single datapoint, requiring estimation and also potentially comparison with a fixed value, in a fixed location. Item comparison questions refer to pairs of data points, for comparison of their values. Trend questions refer to a range of successive data points.
We set up a pilot study with three participants. Each participant looked at a succession of questions of the type shown above, each question followed by a visualization. After the visualization was displayed, it was replaced by a set of choices for the answer. An Eye Tribe gaze tracking system was mounted below the monitor, to record data from which gaze fixations could be tracked. A sample set of fixations, with lines connecting them, is overlaid on the visualizations in Figure 2. In each round, participants answered three questions for each type of visualization, on different data; the rounds were repeated for a total of three rounds per participant.
Fig. 2. Visualizations for point reading questions, with fixations overlaid

For our three pilot study participants, the bar graph resulted in a lower mean task duration for point reading and item comparison, while the line graph was superior for identifying trends. These issues are discussed in more detail below.
Fig. 3. Mean task duration
4 Cognitive modeling
Our models were built in the ACT-R cognitive architecture. ACT-R is a high- level computational emulation of human cognitive processing, generally including representations of memory, attention, visual and motor processing, problem solv- ing, learning, and related phenomena—to a large extent, the phenomena that we are interested in with respect to analyst performance. A version of ACT-R runs in Java, on a server that can communicate with a client browser.
This arrangement is made possible by SIMCog-JS, a system due to Halver- son [3]. SIMCog-JS (Simplified Interfacing for Modeling Cognition - JavaScript) allows models to interact with browser-based software, while requiring little mod- ification to the task code. The modeler specifies how elements in the interface are translated into ACT-R chunks; the software allows keyboard and mouse in- teraction with JavaScript code, and it allows sending ACT-R commands from the external software. Our framework is based on SIMCog-JS, but we developed graphs based on d3.js and chart.js on the front end. The framework is shown in Figure 4.

 Fig. 4. Cognitive framework based on SIMCog-JS
The basic requirement for the cognitive models is to simulate patterns in how people read and understand line graphs and bar graphs, to derive answers to specific questions. Our pilot study shows preliminary evidence that people do follow general patterns. For types of graphs, fixations generally occur in the following sequence: Legend → X axis → Line or Bar → Y axis. Production rules in the model follow the same pattern, with state information recorded in the goal buffer.
The cognitive models we have developed are still relatively simple, but they roughly performance to answer questions about visualizations, as described in the previous section. The model is initialized with a set of chunks in declarative memory that represent state information (e.g., the first action that the model will take) and domain information (e.g., the mapping of characters such as 1, 2, 3... to an ordering of quantities). Given a task that concerns a specific item, the model begins by looking up and recording the color coding of the item in the legend to the visualization. Depending on the specific task, the model will find and attend to the locations of points in a line graph or values in a bar graph; it will find, attend, and read labels on the x-axis and y-axis as necessary for estimation and comparison.
One complication is that people’s scans are not “ideal;” sometimes their fixations revisit the legend, as if they are probabilistically unable to retrieve an association between the symbol named in the question and the color of a line or bar in the graph. For example, on the right of Figure 2 the fixation sequence revisits the legend after moving to the correct bar. To reproduce this behavior, we define several productions that can fire (their conditions will be satisfied
during the match) when the model finishes a specific step, such as reading the X axis. One production moves the gaze back to the legend, while another moves to the relevant line or bar. These two productions are randomly selected in the conflict resolution process; we use the utility parameter to tune the probabilities with which productions fire.
Means of task duration for the participants in the pilot study are shown on left of Figure 3 for the three types of questions. Means of model task durations, based on ten runs, are shown on the right. The results appear to be a reasonable match between the model and human performance. Several caveats apply, however: we have fitted these models to a small number of participants in the pilot study, who carried out a small number of repetitions for each type of question, to different datasets. The models have not yet been validated; a larger study with more participants and a greater number of datasets is needed.
We are currently refining the models to improve their match to observed performance—the general patterns, but “errors” as well. The overall goal, as described in the introduction, is to develop models that replicate human per- formance at a level detailed enough to produce recommendations for a given visualization, tied to the data being visualized.
5 Conclusion
In summary, we have developed cognitive models in ACT-R framework that can answer specific questions about a line graph or a bar graph. Our work can be divided into the following three parts.
For the user study, to better understand how people answer questions based on line graphs and bar graphs, we designed a small pilot study. An eye tracker was used to identify gaze fixations. This data provided a basis for modeling work; the experiment itself also provided a motivation for the integration of experiment and modeling software in a single environment (though this is incomplete.)
We developed ACT-R cognitive models to simulate the basic patterns of how people read line graphs and bar graphs to answer questions. The cognitive models can handle three types of questions: point reading, item comparisons, and trends. The models themselves are not novel; although they were developed within our lab, they replicate the structure of comparable models in the literature. We mainly used utility theory as represented in ACT-R to capture randomness in participant behavior, which we attribute to memory limitations.
As for the architecture, we employed the SIMCog framework (a solution allowing models to interact with web browser based software) to connect Web graphs and cognitive models. The server is built within the Java ACT-R task environment. We use d3.js library to generate graphs in the client; the server interprets JSON-RPC messages from the client about the current status and relays them to ACT-R model.
The paper is a first step in evaluating graph presentation, tied to specific datasets, in a real world Web application. The research is a work in progress, with several limitations. A formal experiment, with sufficient data for statistical
analysis, remains to be done. A continuation of user evaluation will also be needed to support model validation. In the future, we will also make the system suitable for other graphs like pie graphs, other types of histograms, and so on.
6 Acknowledgments
Thanks to Tim Halverson for his contribution and assistance with respect to SIMCog. This research was support in part by the National Science Foundation (IIS-1451172) and by the Laboratory for Analytic Science.
References
1. W. S. Cleveland. Visualizing Data Hobart Press, 1993.
2. W. S. Cleveland. The Elements of Graphing Data. Hobart Press, 1994.
3. T. Halverson, B. Reynolds, and L. Blaha. SIMCog-JS: Simplified Interfacing for
Modeling Cognition - JavaScript. Proceedings of the International Conference on
Cognitive Modeling, Groningen, The Netherlands, April 9-11, 39-44.
4. S. M. Kosslyn. Graph Design for the Eye and Mind. Oxford University Press, 2006.
5. G. L. Lohse. A cognitive model for understanding graphical perception Human-
Computer Interaction 8(4) (1993): 353-388.
6. D. Peebles and P. C.-H. Cheng. Modeling the effect of task and graphical represen-
tation on response latency in a graph reading task. Human Factors: The Journal of the Human Factors and Ergonomics Society 45(1) (2003): 28-46.
Tanja Blascheck Institute for Visualization and Interactive Systems, University of Stuttgart Universit‰tsstrafle 38 70569 Stuttgart
+49 (711) 685-88307
tanja.blascheck@vis.uni-stuttgart.de
ABSTRACT
Eye tracking experiments are the state-of-the-art technique to study questions of usability of graphical interfaces. Visualizations help to analyse eye tracking data by presenting it in a graphical way. In this paper we contribute a new visualization technique combining features of state-of-the-art visualizations for eye tracking data like heat maps, and transition matrices with a circular layout. The circular heat map transition diagram uses areas of interest (AOIs) and orders them alphabetically on a circular layout to show transitions between AOIs visually. The AOIs are colour coded segments on a circle where the colour is mapped with respect to the fixation count in each AOI. The segment size corresponds to the fixation duration within an AOI. Furthermore, the transitions between and within the AOIs of a participant are drawn as arrow lines. Key features of the circular heat map transition diagram are extraction of similar eye movement patterns of different participants, graphical representation of transitions between AOIs, finding an appropriate AOI sequence, and investigating inefficient search behaviour of participants. To be able to use the visualization technique in practice, we have implemented three variants, the AOI transition diagram, the AOI transition and completion time diagram, and the fixation transition diagram. We will show their application in an exemplary analysis of an eye tracking experiment.
Thomas Ertl
Institute for Visualization and Interactive Systems, University of Stuttgart Universit‰tsstrafle 38 70569 Stuttgart
+49 (711) 685-88331
thomas.ertl@vis.uni-stuttgart.de
Categories and Subject Descriptors [Visualization]: Visualization design and evaluation methods; Visualization techniques
[Human computer interaction (HCI)]:
and evaluation methods
General Terms
Measurement, Experimentation, Human Factors.
Keywords
Eyetracking, visualization, heat map, transition matrix.
1. INTRODUCTION
Visualizing eye tracking data can help to analyse the vast amount of data generated during eye tracking experiments. State-of-the-art visualization techniques like heat maps or scanpath visualizations are very helpful, but they suffer from time aggregation and visual clutter. Transition matrices are an additional useful technique to evaluate eye tracking data based
Copyright © 2013 by the Association for Computing Machinery, Inc
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1 (212) 869-0481 or e-mail permissions@acm.org.
ETSA ë13,Cape Town, South Africa, August 29ñ31, 2013. Copyright 2013 ACM 978-1-4503-2110-5/00/0010Ö.$15.00
on areas of interest (in the following AOI). The density of the transition matrix can give information about the efficiency of a search, as ìfrequent transitions from one region of a display to another indicates inefficient searchî [3]. This technique can help the analyst to find an appropriate sequence of AOIs which can later help to visualize AOIs or transitions with other techniques such as the parallel scanpath visualization [6].
This paper contributes a new visualization technique, the circular heat map transition diagram, combining the approach of transition matrices and a circular layout. This visualization technique presents transitions between AOIs in a graphical way to allow the extraction of eye movement patterns of users visually. The user can use the visualization technique to choose an appropriate AOI sequence to reduce transition crossings, and transitions, fixation count, and fixation duration is presented in a graphical way. This enables the user to make statements about the efficiency of a search strategy.
The AOIs in the circular heat map transition diagram are presented in alphabetical order as segments in a circular layout. The circular layout was chosen so each AOI has the same transition distance to achieve equal spacing. This type of layout is also space-saving as each AOI is represented only once in the diagram. The circle segments are a colour coded representation of the fixation count. The colour coding gradient uses a luminance scale map from the Colorbrewer website [1] as suggested by Duchowski et al. [2]. In this paper we will present three variants of the circular heat map transition diagram, the AOI transition diagram, the AOI transition and completion time diagram, and the fixation transition diagram representing all fixation transitions of one participant.
2. RELATED WORK
Different approaches of eye tracking visualizations have been developed to satisfy various kinds of experiments and research questions. Classical visualizations for eye tracking data are heat map and scanpath visualizations [4]. Another example is eSeeTrack by Tsang et al. that combines a timeline and a tree- structured visual representation to extend current eye tracking visualizations by extracting patterns of sequential gaze orderings [8]. The parallel scanpath visualization by Raschke et al. uses parallel coordinates to display fixations and transitions between AOIs [6]. A method, which can be classified in between visual and statistical analysis techniques, is presented by Ponsoda et al. The authors use transition matrices for analysing eye movement recordings during free viewing [5]. Goldberg and Kotval have shown the benefits and drawbacks of such transition matrices, to classify poor and good search behaviour of participants [3]. Our approach also uses transition matrices for the evaluation of eye tracking data, but represents the number of transitions in a graphical way for users to see transitions made by participants at one glance. Schulz et al. have used a similar visualization, ordering AOIs on a circle and indicating transitions between AOIs with lines. They used darkness of points and lines to indicate the frequency in an AOI and transition between AOIs [7].
HCI design
Proceedings of Eye Tracking South Africa, Cape Town, 29-31 August 2013
Circular Heat Map Transition Diagram
Michael Raschke Institute for Visualization and Interactive Systems, University of Stuttgart Universit‰tsstrafle 38 70569 Stuttgart
+49 (711) 685-88466
michael.raschke@vis.uni-stuttgart.de
58
 Proceedings of Eye Tracking South Africa, Cape Town, 29-31 August 2013
Figure 1. Heat map of participants performing the task: ìRead the value of bar number twelveî (left). AOIs used in the circular heat map transition diagram are defined based on the heat map (right).
3. CIRCULAR HEAT MAP TRANSITION
DIAGRAM
All three types of the circular heat map transition diagram are based on AOIs. AOIs have to be defined for the stimulus first, to use the circular heat map transition diagram. This is achieved by using the heat map of all participants (cf. Figure 1) as described by Holmqvist et al. [4]. After defining the AOIs the user can use the AOI transition diagram to get an overview about the transitions between AOIs, for example which AOIs have not been looked at, how often the AOIs have been focused on, and what transitions occurred. The AOI transition and completion time diagram can be used to study detailed information about the AOIs themselves, as the fixation duration is shown as well. The third type of diagram, the fixation transition diagram shows further details about the exact fixations of a participant between and within AOIs.
The diagrams are all based on a circular representation, where each segment of the circle represents the defined AOIs in alphabetical order. AOIs which have not been focused on will not be represented in the diagram. The colour of each AOI segment is based on a luminance colour scale [1], with dark red representing many fixations, orange representing a medium number of fixations, and yellow representing only a few fixations. The arrow lines indicate transitions of a participant between AOIs. They can either be aggregated transitions between AOIs, or all fixations of a participant. The arrow line thickness represents the number of transitions, by drawing thicker lines for more transitions between two AOIs. A double arrowhead represents transitions going in both directions. By hovering the mouse over an AOI segment or a transition detailed information about the fixation count, the fixation
duration, or the transition count is shown. The three different types of the circular heat map transition diagram are further explained in the following subsections.
3.1 AOI Transition Diagram
The AOI transition diagram in Figure 2 uses segments for the AOIs which have equal sizes. The transitions between the AOIs are aggregated for each AOI pair. The user can see which transitions appear more often using this visualization technique and the colour coding of each segment represents the aggregated number of fixations in each AOI. Transitions between AOIs participants have focused on can be studied in this type of visualization. This supports users to choose an appropriate AOI sequence for other visualization techniques. Furthermore, comparing the visualizations of different participants, the user can find similar or equal transition sequences which can be used in a pre-processing stage of scan path comparison methods [4]. Similar transitions between AOIs can indicate that participants used a similar strategy to solve a given task.
3.2 AOI Transition and Completion Time
Diagram
The AOI transition and completion time diagram (cf. Figure 3) has differently sized AOI segments. This visualization uses the results of the calculation of proportions between fixation duration of one OI and the completion time of a participant. The AOI transition and completion time diagram shows which AOI has been focused on the longest.
 Figure 2. The AOI transition diagram represents AOIs viewed by a participant with a color coding corresponding to the fixation count. Transitions between AOIs are represented by arrow lines where the thickness corresponds to the number of transitions. In this case participants one (left) and two (middle) have similar transitions, and participant three (right) only lacks a transition from AOI 3 to AOI 1.
59
Proceedings of Eye Tracking South Africa, Cape Town, 29-31 August 2013
 Figure 3. AOI transition and completion time diagram for three participants. The circle segments indicate the aggregated fixation duration within an AOI.
3.3 FixationTransitionDiagram
The fixation transition diagram (cf. Figure 4) shows all transitions of one participant. All transitions within an AOI and between AOIs are represented as arrow lines. The length of the arrow line within an area all of the same length. The first fixation of the participant is indicated by a blue rectangle. This visualization technique represents the complete transition sequence in the order the fixations appeared. This can help finding points in time where the eyes of the participant changed their focus from one AOI to another. The duration the participant fixated an AOI is also presented.
4. APPLICATION EXAMPLE
This section demonstrates how the circular heat map transition diagram can additionally be used with heat map visualizations, scanpath visualizations, and transition matrices to analyse results of eye tracking experiments. We use a small subset of results from a user study which was conducted to study visual reading processes of bar charts. Participants had to read the height of a bar in a bar chart diagram. Figure 1 shows one stimulus used in the study. The user study was conducted with ten participants with a mean age of 29.2 (min 23, max 58) with four women and six men using a Tobii T60XL eye tracker. In the following we will use the three new visualization techniques to study the behaviour of the participants. The main research question was: Are there general eye movement patterns when reading a bar chart?
Figure 2 shows the AOI transition diagram. On the left side participant one is shown with AOI 0 and AOI 3 having been focused on the most (dark red colour coding). AOI 2 has been focused on the second most (orange) and AOI 1 the least (yellow). Participant one had seven transitions back and forth
between AOI 0 and AOI 3 as well as five transitions between AOI 3 and AOI 2. There were also 3 transitions between AOI 0 and AOI 1 and only one transition from AOI 3 to AOI 1. The AOI transition diagram of participant two and three are shown in the middle and right side of Figure 2.
Participants one and two have a similar transition sequence structure. This can indicate a similar search pattern. Participant three only has a slight variation as the participant has no transition from AOI 3 to AOI 1. AOI 3 has been focused on the most by all three participants. The colour of the segments is coded in dark red. AOI 1 has been focused on the least by all three participants. The segments have the lightest colour coding. The search efficiency of participant three is the highest with only eight transitions.
The AOI transition and completion time diagram in Figure 3 shows that participant one has focused on AOI 3 the longest which is indicated by a slightly larger circle segment, although the colour coding of both AOI 3 and AOI 0 are the same. Participant three has focused on AOI 2 more than on AOI 0 which is also indicated by the larger segment of AOI 2.
The fixation transition diagrams (cf. Figure 4) show complete transition sequences of each participant. Participant one and two have a lot of transitions from AOI 3 to AOI 0 and back. Only a few are in AOI 2 and even less in AOI 1. Participant three has only few fixations and also very few transitions.
Using all three diagrams similar eye movement patterns between participant one and two can be found. Participant three only lacks one transition; therefore all three participants could be grouped into one user group. Furthermore, the diagrams indicate that the search behaviour of participant three was the most efficient as this participant only needed eight transitions
 Figure 4. Fixation transition diagram for three participants showing all fixations of the participants inside the AOIs as well as between AOIs. Participant three (right) has a lot less transitions than participants one (left) and two
(middle).
60
before giving the correct answer. Participants one and two did a lot of cross-checking to give the correct answer. This is indicated by the number of transitions between AOIs and can be seen in the fixation transition diagram showing all fixations. In addition an appropriate AOI sequence can be found, which would be AOI 1, AOI 0, AOI 3, and AOI 2.
5. DISCUSSION AND LIMITATIONS
The application example illustrates how the circular heat map transition diagram can additionally be used in eye tracking experiments to find similar search strategies of participants. The aggregation of the transitions between AOIs and the resulting transition path can be used to find user groups and general transition sequences of multiple users. Furthermore, the visualizations can help to find an appropriate AOI order, by sorting the AOIs based on the number of transitions between the AOIs. Another benefit of the circular heat map transition diagram is that the user can see at one glance which AOIs have been focused on the most and longest. Additionally the density of the transitions can be calculated.
The drawback of this visualization technique is its limitation to one participant. In future work we will investigate how eye tracking data of multiple participants can be aggregated in a useful manner without losing information about the eye tracking data. Visual clutter is another limitation of this visualization technique, especially for the fixation transition diagram. Since the AOIs are ordered in alphabetical order there are a lot of transition crossings which make it hard to follow the fixations properly. Hence, a preprocessing could be helpful to calculate the number of transitions between AOIs beforehand and sort them accordingly in the visualization.
6. CONCLUSION
This paper contributed the circular heat map transition diagram, a new visualization technique for eye tracking data. This visualization technique can additionally be used to analyse eye tracking data together with heat map and scanpath visualizations. The circular heat map transition diagram consists of three visualizations which can be applied sequentially to the data. The AOI transition diagram represents the AOIs with equal segment size, the AOI transition and completion time diagram sizes the segments for each AOI proportional to the fixation durations within an AOI and the completion time for the task. The fixation transition diagram represents all fixations within and between the AOIs in their sequential order. All three visualization techniques use a colour coding based on the fixation count with a red luminance scale. Finally, we have demonstrated the circular heat map transition diagram in an

exemplary eye tracking analysis in addition to heat map and scanpath visualizations and have discussed its limitations.
Limitations of this visualization technique are lack of the stimulus in the visualization, its limitation to one participant per diagram, and visual clutter in the fixation transition diagram.
Future work will be to extend the circular heat map transition diagram to show fixations of multiple users. Furthermore, a user study will be conducted to evaluate the usefulness, benefits, and limitations of the technique.
7. REFERENCES
[1] Brewer, C., Harrower, M., Woodruff, A. and Heyman, D., 2009. Colorbrewer 2.0: Color advice for maps. Online Resource, Winter. URL: http://colorbrewer2.org (last accessed July 2012).
[2] Duchowski, A. T., Price, M. M., Meyer, M. and Orero, P. 2012, Aggregate Gaze Visualization with Real-time Heatmaps. In Proceedings of the Symposium on Eye Tracking Research and Applications. ACM, New York, USA, 13-30. DOI=10.1145/2168556.2168558.
[3] Goldberg, J. H. and Kotval, X. P. 1999. Computer interface evaluation using eye movements: methods and constructs. International Journal of Industrial Ergonomics 24 (6), 631-45. DOI=10.1016/S0169-8141(98)00068-7.
[4] Holmqvist, K., Nystrˆm, M., Andersson, R., Dewhurst, R., Jarodzka, H., Van De Weijer, J. 2011. Eye Tracking: A Comprehensive Guide to Methods and Measures. Oxford University Press.
[5] Pondosa, V., Scott, D. and Findlay, J. 1995. A probability vector and transition matrix analysis of eye movements during visual search. Acta Psychologica 88 (2), 167-185. DOI=10.1016/0001-6918(95)94012-Y.
[6] Raschke, M., Chen, X. and Ertl, T. 2012. Parallel scan- path visualization. In Proceedings of the Symposium on Eye Tracking Research and Applications, ACM, New York, USA, 165-68. DOI=10.1145/2168556.2168583.
[7] Schulz, C. M., Schneider, E., Fritz, L., Vockeroth, J., Hapfelmeier, A., Brandt, T., Kochs, E. F. and Schneider, G. 2011. Visual attention of anaethetists during simulated critical incidents. Britisch Journal of Anaesthesia. 106 (6). 807-13. DOI=10.1093/bja/aer087.
[8] Tsang, H. Y., Tory, M. and Swindells, C. 2010. eSeeTrack - visualizing sequential fixation patterns, IEEE Transactions on Visualization and Computer Graphics 16 (6), 953-62. DOI=10.1109/TVCG.2010.149.
Proceedings of Eye Tracking South Africa, Cape Town, 29-31 August 2013


List of papers
This thesis is based on the following papers, which are referred to in the text by their Roman numerals.
I Learning Where to Look: Modeling Eye Movements in Reading
Mattias Nilsson and Joakim Nivre
In Proceedings of the Thirteenth Conference on Computational Natural Lan- guage Learning (CoNLL), Boulder, Colorado, USA, pp. 93–101, June 4–5, 2009.
II Towards a Data-Driven Model of Eye Movement Control in Reading
Mattias Nilsson and Joakim Nivre
In Proceedings of the 2010 Workshop on Cognitive Modeling and Computa- tional Linguistics (ACL 2010), Uppsala, Sweden, pp. 63–71, July 15, 2010.
III Entropy-Driven Evaluation of Models of Eye Movement Control in Reading Mattias Nilsson and Joakim Nivre
In Proceedings of the 8th International NLPCS Workshop, Copenhagen, Den- mark, pp. 201–212, August 20–21, 2011.
IV A Survival Analysis of Fixation Times in Reading
Mattias Nilsson and Joakim Nivre
In Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics (ACL 2011), Portland, Oregon, USA, pp. 107–115, June 23, 2011.
V Time-Varying Effects on Eye Movements during Reading Mattias Nilsson and Joakim Nivre
Submitted
Reprints were made with permission from the publishers.
Acknowledgements
Many people contributed to this thesis. In particular, I want to thank the fol- lowing.
I am very grateful to my supervisor Joakim Nivre for his encouragement and guidance over the last few years and not least for his remarkable patience with which he helped me to structure this thesis into what I hope is a coher- ent form. I am also very thankful to my co-supervisors Ina Bornkessel and Shravan Vasishth for inviting me to join their research groups in Leipzig and Potsdam respectively, and for reading and commenting on an earlier draft of this thesis. Thanks is also due to my colleagues in the computational linguis- tics group at Uppsala University who read and provided useful comments on an earlier draft. I am also grateful to Erik Reichle for encouraging me in the first place, responding so thoughtfully to my questions and for digging up the old models.
This thesis would not have been written without the love and support my family has always given me. Thank you. Finally, I want to express my love to Chiara for her compassion, generosity, and appreciation of what is important in our lives.
Contents
1 Introduction ................................................................................................ 11 1.1 ResearchQuestions.......................................................................... 11 1.2 OutlineoftheThesis ....................................................................... 12
2 Eye Movements in Reading: Data and Models ........................................ 14 2.1 BasicCharacteristics ....................................................................... 14 2.2 WheretheEyesMove ..................................................................... 15 2.3 WhentheEyesMove ....................................................................... 16 2.4 Models of Eye Movement Control in Reading .............................. 18
3 Modeling Eye Movements in Reading ..................................................... 21
3.1 LevelsofDescription ....................................................................... 21 3.1.1 Computational .................................................................... 22 3.1.2 Algorithmic ......................................................................... 23 3.1.3 Implementational ................................................................ 24
3.2 ModelsandPredictions ................................................................... 24
4 WheretheEyesMove ................................................................................ 26 4.1 Background ...................................................................................... 26 4.2 SaccadeTargetSelectionasClassification..................................... 27
4.2.1 TransitionSystem ............................................................... 27 4.2.2 Learning Transitions ........................................................... 29 4.2.3 SearchAlgorithm ............................................................... 29 4.2.4 Experimental Results ......................................................... 30
4.3 Entropy-Based Model Assessment ................................................. 33
5 WhentheEyesMove ................................................................................. 37 5.1 Background ...................................................................................... 37 5.2 Saccade Timing as Time-to-Event Modeling ................................. 39
5.2.1 TheSurvivalFunction ........................................................ 39 5.2.2 Kaplan-Meier Survival Estimate ....................................... 40 5.2.3 TheHazardFunction .......................................................... 41 5.2.4 The Cox Proportional Hazards Model .............................. 41 5.2.5 Extended Cox Model: Time-Varying Effects ................... 42 5.2.6 PredictionError .................................................................. 44 5.2.7 ExperimentalResults ......................................................... 45
6 Conclusion .................................................................................................. 50 6.1 MainContributions .......................................................................... 50
6.2 FutureDirections ............................................................................. 51 7 OverviewofthePapers .............................................................................. 53 References ........................................................................................................ 55
1. Introduction
The eye movements you make, as you read sentences like this one, are one of the few behavioral means by which we can study and make inferences about the nature of the cognitive processes that support language comprehension. Understanding the links between the observed data – eye movement behavior – and the underlying processes is one of the great challenges of cognitive sci- ence. The overall goal of this thesis is to find novel methods and models for understanding these links. In a general sense, our work builds on more than three decades of eye movement research and psycholinguistic studies using eye movement data as a means for studying cognitive processes. The methodological intuition for this research is well captured in the immediacy- of-processing and eye-mind assumptions of Just and Carpenter (1980). These assumptions hold that a reader carries out the processes required to understand each word and its relationship to previous words in a sentence as soon as the word is encountered, and for as long as the eyes remain on the word. That is, it is assumed that eye movements are tightly controlled and coordinated with the cognitive processes of reading. Thus, if eye movement behavior varies as a function of the cognitive processes associated with language comprehension, as suggested by Just and Carpenter, it may be possible to make inferences from the data to the processes involved. Although it is now generally agreed that this relationship is not always as tight as Just and Carpenter assumed, nu- merous studies since have shown that cognitive processes have an essentially immediate influence on eye movements in reading.
1.1 Research Questions
The methods and models presented in this thesis focus on the questions of when and where the eyes move during reading. These questions have been at the center of attention for decades in research on eye movements in read- ing and refer to the two central, generally unconscious, decision processes involved in reading. In asking when the eyes move, we are asking a question about time: what determines the duration of fixations in reading? In asking where the eyes move, we are asking a question about space: what determines the location of fixations in reading, or, from a slightly different perspective, what determines the length and direction of eye movements from one fixation to the next? Much of what is known about these processes is due to exper- imental research carried out since the 1970s using a variety of experimental
11
techniques, such as the moving window paradigm (Reder, 1973; McConkie and Rayner, 1975), the moving mask paradigm (Rayner and Bertera, 1979), and the boundary paradigm (Rayner, 1975; Balota et al., 1985).
Current research addressing these questions, however, is centered on com- putational modeling and simulation of eye movements in reading. A number of models have emerged to date which make explicit assumptions about the time course of cognitive processes, and produce detailed predictions of when and where the eyes move, which can be compared to human reading data. These models generally develop from a set of assumptions about the algorith- mic, or procedural, relationship between the perceptual, cognitive, and motor processes that guide the eyes during reading. Theoretical constraints, in accord with experimental evidence, are then imposed on the model parameters.
In this thesis, we take a different approach based on the use of empirical eye movement data, as recorded in large eye tracking corpora, and data-driven modeling methods. We explore the idea that empirical data carries rich infor- mation about the processes that guide eye movement decisions. The central research question we address is how data-driven methods can be used to model eye movements in reading and to recover characteristics of the underlying pro- cesses. We also emphasize the role of prediction in eye movement modeling, and an additional research question we address, therefore, is how to evaluate whether the models make good predictions about human reading behavior.
A great deal of experimental evidence suggests that the decisions of when and where to move the eyes are made independently from one another and depend on different mechanisms. We maintain this separation between when and where and explore two data-driven modeling methods, each addressing a different aspect of eye movement behavior, and each paired with a differ- ent evaluation method. The spatial question – where the eyes move – is ap- proached using standard machine learning and classification methods. We present a flexible model, with few fixed assumptions, that allows for a number of parameters to be explored empirically, and we show how the predictions of the model can be evaluated on held-out data using the notion of entropy. The temporal question – when the eyes move – is explored using time-to-event modeling (also known as survival analysis). We show how this method can be used to study the timing and strength of processes that influence the decision of when to move the eyes, and, again, we use an entropy-related method to evaluate the predictions of our model on held-out data.
1.2 Outline of the Thesis
The remainder of this thesis is structured as follows.
Chapter 2 provides background information about eye movements and reading research. We review some basic characteristics of eye movements in reading,
12
factors which influence when and where the eyes move, and models of eye movement control in reading.
Chapter 3 broadens the discussion of models of eye movements in reading. We show that different models are driven by different motivations and concerns, which reflects that work is progressing at different descriptive levels. We fur- ther discuss the role of prediction and evaluation.
Chapter 4 demonstrates that the problem of where the eyes move can be ap- proached as a machine learning problem. We present a transition-based model that is guided by a classifier to select saccade targets during reading. We show how such a model can learn to produce eye movements that are similar to hu- man readers. Further, we discuss problems in treating saccade target selection as classification and present an alternative approach based on probabilistic sac- cade models. For these models, we propose a probabilistic evaluation method based on measuring the entropy, relative to a model, with respect to indepen- dently observed eye movement behavior.
Chapter 5 demonstrates that the problem of when the eyes move can be ap- proached as a time-to-event modeling problem. We motivate the use of time- to-event modeling, review the basic concepts and methods, and show how Cox hazards models can be applied to model the influence of covariates on the de- cision to move the eyes. We further propose that Cox hazards modeling with time-varying effects can be used to recover some basic time course character- istics of the short-lived processes that influence eye movements. Some results supporting this hypothesis are presented. In addition, we propose a probabilis- tic evaluation method for hazards models of eye movements based on the Brier score.
Chapter 6 summarizes the main results and contributions of the thesis. We conclude with a discussion of promising directions for future research.
Chapter 7 provides a brief overview of the papers on which this thesis is based.
13
2. Eye Movements in Reading: Data and Models
This chapter provides background information on eye movements during read- ing. We review basic characteristics of eye movements in reading, research on where and when the eyes move, including measures of eye movements, and models of eye movement control in reading. This review does not attempt to cover all relevant issues and is by no means exhaustive. Instead, we focus the discussion primarily on aspects that have some bearing on papers I–V. Com- prehensive summaries of research on eye movements in reading are provided by Rayner (1998, 2009a).
2.1 Basic Characteristics
The two basic components of eye movements in reading are fixations and sac- cades. Fixations are the short periods of time during which the eyes remain fairly still, and saccades are the rapid movements the eyes make between fix- ations. Because vision is suppressed during saccades, information from the visual array is obtained only during fixations. The amount of visual informa- tion that a reader is able to make effective use of during a fixation is referred to as the perceptual span. The size of the perceptual span is asymmetric to the right and left of the fixation, and the size as well as the direction of the asym- metry vary as a function of the writing system used. For readers of left-to-right orthographies, like English, the perceptual span extends 14–15 letter spaces to the right of the fixation but only 3–4 letter spaces to the left (McConkie and Rayner, 1975, 1976; Rayner and Bertera, 1979). The size of the perceptual span is also influenced by reading skill. Beginning readers and readers with dyslexia generally have a smaller span than more skilled readers. The percep- tual span is distinct from the word identification span which is the area from which a word can be identified during a fixation. The word identification span is smaller than the perceptual span and extends usually only about 7–8 letter spaces to the right of the fixation (Rayner et al., 1982).
The average fixation duration in reading is on the order of 225–250 ms, and the average saccade length is 7–9 letter spaces for readers of alphabetic writ- ing systems (Rayner, 2009a). Hence, a skilled reader typically moves the eyes about 7–9 letter spaces every quarter of a second. There is, however, a con- siderable spread around these averages. Fixation durations for an individual
14
reader often range from 50 ms to about 600 ms, and saccade lengths from 1 letter space to about 20 letter spaces.
Three other basic characteristics of reading behavior need to be considered. First, not all words are fixated. About 20–30% of the words in a text are skipped during reading. A word that is skipped does not receive any fixation. There is no reason, however, to assume that the word is not being processed. On the contrary, there is evidence that skipped words do get processed (Fisher and Shebilske, 1985). The majority of skipped words are short function words. Second, readers do not invariably progress in the direction of the text. About 10–15% of all saccades in reading are regressions, that is, saccades that move backwards in the text. Most regressions are to the immediately preceding one or two words but they also often stretch a longer distance. Third and finally, the same word is often fixated more than once in succession, in particular if it is a long word. That is, many words get refixated during reading.
2.2 Where the Eyes Move
The question of where the eyes move during reading may be decomposed into two problems: (a) which word is going to be fixated next, and (b), where in the word the eyes (i.e., the saccade) is going to land. Another distinction, how- ever, is often made between where the eyes intend to move, and where they actually go. Although there is some variability in where the eyes land in a word, they tend to land halfway between the beginning of a word and the mid- dle of a word. This is referred to as the preferred viewing location (Rayner, 1979). It is generally argued, however, that saccades are aimed at the center of words but fall short due to different sources of error arising from the fact that saccades are motor movements and require muscle control. Given that most saccades tend to land at or around the preferred viewing location, the critical question is which word is going to be the target for the next saccade. This problem is referred to as saccade target selection (McConkie et al., 1994). As the intended saccade targets are unknown, we focus the discussion here on the observed saccade targets, in the empirical distribution of eye movements. The majority of saccades in reading lands in one of the three following words, with a probability which decreases with the distance from the fixated word. The main factors influencing whether a word will be skipped are the length, fre- quency, and predictability of the word. Thus, short, frequent and predictable words are often skipped. By predictability we refer generally to the extent to which words are contextually constrained and thus more easily predicted from the preceding context.1 Generally, word length has the greatest influence on skipping, and predictability is more important than frequency (Brysbaert and
1In experimental reading studies, predictability is often estimated using a cloze task (Taylor, 1953). The cloze score for a given word then corresponds to the proportion of subjects that correctly guesses the word when presented with the preceding sentence context.
 15
Vitu, 1998; Rayner, 1998). As noted earlier, there are two other basic types of eye movements in reading, refixations and regressions. The probability of refixating a word increases if the word is longer than average or if the initial fixation on the word lands near the end of a word, rather than near the center (McConkie et al., 1989). In the latter case, it is argued that a refixation is trig- gered to move the eyes to a better viewing location. Concerning regressions, short-range regressions to a previous word might be due to incomplete lexical processing of the word (Vitu et al., 1998). This could happen when a reader moves the eyes from a word too early and then regresses back to the word to continue processing it. Long-range regressions, instead, are generally argued to be triggered by comprehension failures in syntactic parsing or semantic in- terpretation.
The spatial characteristics of eye movement behavior in reading are often summarized in terms of probabilities for different types of saccades calculated over a group of readers. Commonly reported measures include the probability of a word being skipped, fixated once, and fixated more than once. These mea- sures are typically calculated based on the first pass reading through the text. Similar measures, however, are also sometimes reported for the probability of regressing to or from a word. Often, the measures are further averaged over classes of words with different frequency and length to describe the effect of these variables on spatial saccade behavior.
2.3 When the Eyes Move
The amount of time spent fixating a word is influenced by a number of vari- ables related to lexical word identification, syntactic parsing, semantic inter- pretation and discourse representation. A particularly relevant finding which attests to the fact that cognitive processes may influence fixation durations, is that information gets into the processing system very early during a fixation. Experiments have shown that if readers are given just 50–60 ms on each word before the word is withheld from view, then reading proceeds quite normally (Ishida and Ikeda, 1989; Rayner et al., 1981, 2003). If the word is withheld earlier than that, however, reading is disrupted. These results do not imply that a reader completes the processing of a word within 50–60 ms. Rather, the results suggest that this amount of time is sufficient to encode the information needed for reading, thus leaving time for other cognitive processes to develop. Furthermore, in these experiments it is shown that after a word is withheld, the time the eyes remain in place depends on the frequency of the word. The eyes tend to remain longer if the word is a low-frequency word when compared to a high-frequency word, even though the word is no longer in view. More gener- ally, the influence of word frequency on fixation durations is well documented (e.g., Inhoff and Rayner, 1986; Rayner and Duffy, 1986; Schilling et al., 1998; Juhasz and Rayner, 2003).
16
In addition to word frequency, word length also influences fixation dura- tions. Longer words are fixated for a longer time than shorter words. An additional important variable is contextual constraint or word predictability. Words are fixated for less time if they are contextually constrained and eas- ier to predict than if they occur in more neutral contexts (Ehrlich and Rayner, 1981; Rayner and Well, 1996). It is important to note that word predictabil- ity, in general, may in fact incorporate a host of different lexical, syntactic, semantic and discourse variables. Readers presumably draw on all kinds of available linguistic evidence in order to constrain the possible interpreta- tions of an upcoming word. Other variables which have been shown to influ- ence the time spent on a word include: lexical ambiguity (Duffy et al., 1988; Rayner and Frazier, 1989; Sereno et al., 2006), syntactic ambiguity (Frazier and Rayner, 1982; Altman et al., 1992; Clifton et al., 2007), semantic rela- tions between words in a sentence (Carroll and Slowiaczek, 1986; Sereno and Rayner, 1992), anaphora and coreference (Ehrlich and Rayner, 1983; Duffy and Rayner, 1990). More recently, information-theoretical measures of com- plexity, such as surprisal and entropy have been used to approximate the ease or difficulty associated with processing individual words (Hale, 2001, 2003, 2006; Levy, 2008). These measures have also been demonstrated to affect fix- ation times in reading (Boston et al., 2008; Demberg and Keller, 2008; Frank, 2010; Boston et al., 2011).
There are two additional effects that need to be mentioned: spillover effects and parafoveal preview effects. The spillover effect is another type of word frequency effect. It has been demonstrated that there is a tendency for the eyes to remain longer on a word when the previous word is a low frequency word (Rayner and Duffy, 1986). Thus, it appears that the decision of when to move the eyes is modulated not only by the frequency of the fixated word but also, to some extent, by the frequency of the previous word. While the spillover effect relates to how the previous word may influence the fixation time on the current word, the parafoveal preview effect relates to how information about the upcoming word may influence the subsequent duration of that word. More specifically, it has been shown that readers often obtain information from the word to the right of the fixation (the parafoveal word) (Balota et al., 1985; Pollatsek et al., 1992). This subsequently facilitates the processing of that word once it is fixated in the sense that the fixation duration is slightly reduced on the word. Overall, parafoveal preview effects suggest that the processing of a word often begins before the word is actually fixated.
The temporal characteristics of eye movements in reading are commonly summarized in terms of different word-based measures of fixation durations, generally assumed to reflect processing time. It is worth noting that the aver- age fixation duration is not used. While the average fixation duration may be a useful descriptive statistic as such, it is not, however, an appropriate mea- sure of the processing time of words. Consider, for example, a word which is refixated. The mean fixation duration (i.e., the mean of the individual fixa-
17
tions on the word) clearly underestimates the time the eyes actually remain on the word. Moreover, the fact that some words are refixated and others are not further complicates the use of average fixation duration. Instead, three other word-based measures are typically used. These measures are single fixation duration, first fixation duration and gaze duration. Single fixation duration is the duration of the fixation on a word which is fixated exactly once on the first pass through the text. First fixation duration is the duration of the first fixation on a word on the first pass, irrespective of whether the word receives additional fixations. Gaze duration is the sum of all fixation durations on a word on the first pass (i.e., including refixations), prior to moving to another word. These measures are typically averaged over a group of readers to pro- duce mean durations over words. In turn, words are often categorized into different word frequency or word length classes and the average durations of these classes are then calculated. The three measures mentioned here (in particular single- and first-fixation duration) are generally assumed to reflect early cognitive processing activities of a word as they are calculated only for fixations made during the first pass and thus exclude regressions to previous words. However, additional measures which are assumed to reflect later cog- nitive processing activities are also frequently used. One such measure is the total fixation duration, which is the sum of all fixation durations on a word, including regressions to the word.
2.4 Models of Eye Movement Control in Reading
As we noted in the previous chapter, much current research is centered on computational models that simulate eye movements during reading. A number of computational models of eye movement control in reading have emerged in recent years (see Reichle, 2006, for an overview of 5 models). In general, these models attempt to explain the processes which underlie observed read- ing behavior. As noted by Rayner (2009b), a strength of the models is that they produce predictions about actual fixation durations rather than about pro- cessing costs or other more indirect measures. Likewise, they produce precise predictions for saccade lengths and fixation locations. Thus, the simulation data which the models produce can be directly compared to human eye move- ment data. The differences between these models can be understood in terms of the assumptions made about how perceptual, cognitive, and motor control processes guide the eyes through the text. With respect to the decision of when and where the eyes move, the differences between the models tend to be greater in the assumptions about when the eyes move. Two assumptions in particular differ between the models. The first concerns the extent to which language and cognitive processes influence the decision of when to move the eyes, and the second concerns the way in which attention is allocated to words during reading. To exemplify how models may differ in these issues we out-
18
line the basic assumptions of the most influential model to date, E-Z Reader (Reichle et al., 1998, 2003, 2009), and then mention how other models differ. E-Z Reader is built on the assumption that the process of identifying words
is “the engine that drives eye movements” through the text. More specifically, lexical access occurs in two stages in E-Z Reader. The duration of these stages is assumed to be a function of the word frequency and contextual predictability of the word being processed (the effects of these variables, however, differ be- tween the two stages). The completion of the first stage, called the “familiarity check”, is the signal to start planning a new saccade to the next word (the word to the right of the currently processed word). The completion of the second stage, called the “completion of lexical access”, is the signal to shift the covert attention (the mental focus) to the next word and begin the familiarity check on that word via parafoveal preview. At this point, one of two things will happen. If a preliminary stage of saccade planning, called the “labile stage”, finishes before the familiarity check on the word now being processed, a saccade will be executed and the next word will be fixated. If, however, the familiarity check finishes first, the current saccade being planned will be canceled and a new saccade will be planned one word further downstream in the text. In this second case, the word being processed will be skipped. Thus, the main assumptions of E-Z Reader are that lexical access is the trigger to move the eyes (i.e., to start planning a saccade), and that attention is allocated strictly serially, such that the processing of the next word does not begin until the processing of the current word has completed. These assumptions differ from the assumptions of other models, like SWIFT (Engbert et al., 2002, 2005) and Glenmore (Reilly and Radach, 2006). In these models attention is allocated across words in the perceptual span, thus allowing parallel lexical processing of words. Furthermore, in SWIFT it is assumed that saccades are generated based on a random timer which initiates saccade planning at random intervals of time (though based on a preferred mean rate of saccades). Variables like word frequency and word predictability do influence fixation time in SWIFT but only indirectly, by inhibiting the random timer when words are difficult to process. In other words, saccades are initiated autonomously at times which are only occasionally influenced by current cognitive processing. Yet another model, the Competition/Activation model (Yang and McConkie, 2001; Yang, 2006), assumes that cognitive processes do not have any immediate influence on fixation times, but rather only affects very long fixations.
Most models of eye movement control in reading have been demonstrated to account for basic characteristics of eye movements during reading. The models generally predict word frequency and predictability effects on fixa- tion times similarly to what is observed in human readers. They also account for saccade lengths, skipping rates and, to some extent, regressions, although some of these effects may be hard-wired in the models (Rayner, 2009b).
19
In the next chapter we broaden the scope of discussion to include an overview of different approaches to eye movement modeling. This overview then serves as background to introduce some basic characteristics of our approach.
20
3. Modeling Eye Movements in Reading
In this chapter we broaden the discussion on eye movement modeling initiated in the previous chapter. This chapter is divided in two sections. In section 3.1, we discuss different approaches to eye movement modeling and make an at- tempt at separating them using Marr’s notion of different levels of description for cognitive systems (Marr, 1982). In section 3.2, we comment on models and predictions and introduce some recurring themes in later chapters.
3.1 Levels of Description
Models of eye movements in reading can be compared and contrasted to one another in a number of different ways. In the previous chapter, we noted, for example, that models of eye movement control make different assumptions about how language processing affects fixation times and about the nature of attention allocation, that is, whether more than one word may be processed at any given time. At a more general level, it is also possible to distinguish between different approaches to modeling, such as inside-out and outside-in approaches (Feng, 2006), or theory-driven and data-driven modeling (Feng, 2001). These terms are generally meant to distinguish between models that emphasize a particular theory of eye movements (inside-out or theory-driven), as opposed to models that emphasize the use of empirical data to model eye movement behavior (outside-in or data-driven). Although these terms may convey the general idea, they need to be nuanced in specific contexts. The dis- tinction between theory-driven and data-driven modeling, for example, may appear to imply that data-driven models are theory-independent or atheoreti- cal, or perhaps that theory-driven models are non-empirical. Neither of these implications is valid. Models of eye movements in reading are driven by dif- ferent motivations and concerns. To understand these, it may be more fruitful to distinguish between models at different levels of description, in line with David Marr’s celebrated account of explanatory levels of cognitive systems. Marr argues that cognitive systems, or information processing systems in gen- eral, may be described at three independent though complementary levels: the computational, the algorithmic, and the implementational level. Models or descriptions at the computational level focus on what functions the system is computing, and why those computations are required to achieve the goals of the system. Broadly, descriptions at this level are consistent with accounts
21
that help identify conditions and constraints associated with the function be- ing computed, but abstracts away from how the computation is actually carried out. In contrast, the algorithmic and implementational levels address the ques- tion of how computations are performed. The algorithmic level specifies the algorithms and representations involved in the computations, while the imple- mentational level is concerned with how algorithms and representations may be realized physically, for example by neural processes in the brain.
While Marr emphasized the importance of the computational level, the strength of the proposal is that it motivates research at all levels and allows researchers to abstract away from specificities which are extraneous to their primary concern. For example, we may pose theories of which algorithms solve particular higher brain functions without making claims about the neu- ral realization of the algorithms.
While we do not want to argue that all models of eye movements in reading map neatly to one or the other of Marr’s levels, it is clear that work is now carried out at multiple levels of description, largely corresponding to Marr’s three levels. Thinking in terms of Marr’s levels may help to understand how different models relate or may be compared to one another, both across and within levels. In the next section, we briefly exemplify modeling work at different descriptive levels and discuss some basic methodological concerns that characterize models at each level.
3.1.1 Computational
Much work at the computational level draws inspiration, methodologically, from the mathematical theory of human response time (Luce, 1986; Van Zandt, 2002). Studies in this context typically treat eye movements as observable re- sponses, or outcomes, of an underlying stochastic process. Mathematical and statistical methods are applied to model the empirical distribution of eye move- ments on the assumption that essential properties of the underlying stochastic process can be characterized that way. In turn, it is generally assumed that knowledge about the precise distribution functions can inform or place con- straints on theories about mechanisms of eye movements in reading. This line of research thus emphasizes the use of data in deriving mathematical models of eye movement behavior that characterize the basic constraints and conditions the underlying processes must satisfy. Generally, these models have tended to downplay the role of cognition, often assuming that language processing only influences very long fixations when the initially planned saccade is delayed or canceled due to processing difficulty.
Model assessment is typically based on comparing the fitted distributions to the observed distributions using some goodness-of-fit statistic. The assess- ment is generally based on the same set of data that is used to estimate the parameters of the model. Analyses of this kind include models of within-word
22
landing positions (McConkie et al., 1988; Radach and McConkie, 1998), fix- ation durations (McConkie et al., 1994; McConkie and Dyre, 2000), regres- sions (Vitu et al., 1998; Vitu and McConkie, 2000), refixations (McConkie et al., 1989; Radach and McConkie, 1998), and skipping rates (McConkie et al., 1994). More recent work in this tradition is exemplified by Feng (2006, 2009). Other work at the computational level, addressing how eye movements in reading can be explained in terms of more general goals of human behav- ior, is exemplified by the work of Legge et al. (1997) and, more recently, by Bicknell and Levy (2010).
3.1.2 Algorithmic
Models at the algorithmic level largely correspond to the models of eye move- ment control discussed in the previous chapter. Here, we focus on the basic modeling approach, first introduced by the development of E-Z Reader. This approach starts with a set of assumptions concerning the underlying control structure of eye movements in reading. This structure, often implemented as a state machine, defines the set of perceptual, cognitive, and motor processes which support eye movement control, and specifies the relationship between the processes. These processes are internal to the system and not directly ob- servable from empirical data. Theoretical constraints are therefore typically imposed on the parameters involved in executing the processes. Typically, the mean and standard deviation of the distribution for a given process (e.g., lexical processing time) are specified; variability is then built into the model by random sampling from the assumed distribution whenever a process is ex- ecuted. As noted by Reichle et al. (1998), an important concern is that the parameter values defining the distributions should be based on plausible esti- mates given experimental evidence.
In assessing the models, a parameter search is performed to determine the best fitting values for the model parameters given the observed data. The as- sessment is based on the same data that is used to tune the parameters. In other words, one attempts to identify the estimates which minimize the error of the model on the fitted data. Typically, the best fitting parameter values are assessed qualitatively so that they are in reasonable agreement with other evidence. The error that is minimized is usually the root mean squared error between observed and predicted mean values for different eye movement mea- sures such as gaze duration and skipping probability. The means are typically calculated over a small set of word frequency classes in order to assess the models’ capacity to reproduce frequency effects on eye movements in read- ing. Additionally, more subtle effects (e.g., spillover, preview and skipping effects) are often also assessed based on the simulation data.
23
3.1.3 Implementational
Given that little is known about the underlying neural mechanisms of eye movements in reading, it is not surprising that there has been less effort at the implementational level. While some ideas for a possible layout of the neu- ral implementation of E-Z Reader have been discussed (Reichle et al., 2003), the first biologically inspired neural network model of eye movement control in reading was presented only recently (Heinzle et al., 2010). The basic goal of this model is to shed some light on how the brain may control eye move- ments in reading. The model is implemented as a layered network of spiking neurons that can control sequences of eye movements in reading. Interaction with a cortical word processing module allows the model to read sequences of x-strings (as a simple representation of normal words).
The performance of the model is assessed rather qualitatively by examin- ing how a set of simulation-based descriptive statistics compares with those observed in human readers in general. The evaluation focuses primarily on demonstrating the model’s capacity to account for some basic aspects of read- ing behavior, such as total fixation times and skipping rates as a function of word length.
3.2 Models and Predictions
Under the usual view of eye movements in reading, the two primary behav- iors of the underlying perceptual, cognitive and motor processes we observe are fixation durations and fixation locations. The problem, as approached in our work, is to generate good predictions of eye movement behavior, that is, fixation durations and fixation locations that are in good agreement with the observed performance. In this sense, our primary concern is to characterize eye movement performance, rather than eye movement control. Thus, overall, the methods employed and models presented in this thesis are best understood at the computational level of description in Marr’s taxonomy.
Assuming the central problem is to generate good predictions of eye move- ment behavior, an important issue arises – how do we know if a model makes good predictions? As is clear from the review in the previous section, the eval- uation of models, across levels, is generally based on comparing model predic- tions to empirical observations. The accuracy of the predictions, however, is usually only assessed on the same set of data that is used to fit the parameters of the model. In other words, models are rarely evaluated on held-out data as is standard in other modeling fields (e.g., artificial intelligence, natural language processing and machine learning). The potential problem of not separating data for parameter estimation (training data) and model evaluation (test data) is that the model being evaluated is then selected purposively to lie near the observed data points, which is what fitting means but generally not what pre- diction means. By prediction we rather refer to a statement about the outcome
24
of observations we have not seen previously. In principle it is of course pos- sible to use the results of our best parameter estimates on the fitted data also as an estimate of how accurate the model is for predicting novel outcomes. Only, we risk that this estimate is overly optimistic, because the error on the training data tends to underestimate the true error of the model. If our goal is prediction, we should instead consider how well the model generalizes to previously unseen behavior, as measured by the prediction error of the model. The prediction error is the observed inaccuracy of the fitted model applied to a new, representative, set of data points not used during model development.
However, saying that models are assessed with respect to their prediction error is only half an answer to the general question, if it is not also specified how predictions are actually compared to the data. In some contrast to current work, we propose to assess the precision of probability predictions, rather than the average of count predictions. The idea follows from that the models we consider yield a probability distribution over space and time. The basic intu- ition is that good models of eye movement behavior assign high probability to empirically observed data. Measuring the probability of human behavior, relative to a model, is one way of assessing the similarity between predictions and the observed data. This measurement allows for a finer assessment of a model’s ability to generate accurate predictions than mere averages of count predictions. Using it in practice also presumes that the model is evaluated against a test sample which is drawn independently of the data the model is fitted to. In chapter 4 we elaborate further on the idea and show how entropy, a measure of probability, can be used to assess probabilistic models of where the eyes move during reading. In chapter 5, we relate the same idea to the Brier score (Brier, 1950), which is employed in order to assess the accuracy of probabilistic temporal predictions of when the eyes move.
25
4. Where the Eyes Move
In this chapter we demonstrate that the problem of determining where the eyes move during reading may be approached as a machine learning problem.1 The models we introduce learn, from empirical data, where to move the eyes under different conditions associated with the words being read.
This chapter is divided into three parts. In the first part, section 4.1, we give some background to the models discussed in this chapter. In the second part, section 4.2, we present a transition based model of saccade target selection as a classification task. We show how this model can be used to predict some basic characteristics of human saccade behavior. This section is based on the content of paper I. In the third part, section 4.3, we discuss some problems in treating saccade target selection as a classification task. In particular, given the large inherent variability in saccade behavior, exact categorical predictions for where the eyes move may be of limited value. Therefore, we propose that target selection models be defined as probabilistic models yielding a probabil- ity distribution over the outcome variable. This allows for a more fine-grained evaluation method, measuring the entropy, relative to a model, on unseen eye movement data. This section is based on the content of paper III.
4.1 Background
It has been argued that target selection during reading is based on strategies which have developed and become automated with years of reading experi- ence. The model by Reilly and O’Regan (1998), for example, suggests that the eyes are guided by a simple strategy based on targeting the longest word in a right parafoveal window extending 20 characters to the right of the fix- ated word. This simple strategy, they show, gives better fit to empirical data than a more linguistically oriented strategy based on skipping high-frequency words. In the model by Brysbaert and Vitu (1998), the decision to skip a word is based on the length of the word as well as on how often a word of a certain length and at a certain distance can be skipped without inhibiting comprehension. This strategy, they assume, is learned from experience. The SERIF model (McDonald et al., 2005) extends this proposal beyond skipping
1It should be pointed out that McDonald (2003) provides the first attempt, to our knowledge, at using classification methods for target selection. The work we present is a considerable extension to this work and classification is used here as one component in a larger and more flexible model that allows for the generation of complete fixation sequences over text.
 26
to the selection of one of three candidate words in the forward perceptual span. A target is selected based on random sampling from a cumulative probability distribution, whose parameters are estimated empirically based on a logistic regression model. The classifiers and probabilistic models we present in this chapter can, similarly, be viewed as simple approximations to the accumu- lated experience a reader’s eye guidance system has built up over the years. The basic model, however, does not involve any particular assumption about the strategies or the variables that influence saccade target selection. These are instead parameters that should be explored experimentally.
4.2 Saccade Target Selection as Classification
We review the basic task before we consider the model. We use the following simple representations of a text and a fixation sequence. Let a text T be rep- resented as a sequence of word tokens w1,w2,...,wn, and a fixation sequence F for T be represented as a sequence of token positions in T, i1,i2,...,im. For example, the short text John gave Mary the book is represented by T = John, gave, Mary, the, book; and a fixation sequence over this text corresponding to John – gave – John – Mary – Mary – book is represented by F = 1,2,1,3,3,5. The task we now consider is to predict the fixation sequence F for a given reader R on some text T. Generally, we also assume that this prediction is over a particular reading of the text T , as different readings of T by the same reader R would generally yield different fixation sequences. In the following, we outline a model for this task involving three basic components: a transition system for saccades in reading; a classifier that predicts the next transition and hence the saccade target; and a search algorithm that generates saccades over the reading of text. We discuss these components in turn.
4.2.1 Transition System
A transition system is an abstract machine consisting of a set of configurations (or states) and transitions between configurations. In the transition system here, a configuration represents a fixation state while a transition represents a saccade (fixation state update). A fixation state in this system is essentially a representation of the text relative to the fixated word, which may or may not be the word currently attended to. Given that a fixated word is generally also attended to at some point during the fixation, we may however assume that this is usually the case. A fixation configuration in the transition system is a triple C = L,R,F, where
1. L is a list of tokens representing the left context, including the currently fixated token and all preceding tokens in the text.
27
2. R is a list of tokens representing the right context, including all tokens following the currently fixated token in the text.
3. F is a list of token positions, representing the fixation sequence so far, including the currently fixated token.
For example, assuming the text to be read is John gave Mary the book, then the configuration
([John, gave, Mary], [the, book], [1, 2, 1, 3])
represents the state where Mary is currently fixated, and John, gave, John were previously fixated, in that order.
For any text T = w1 . . . wn , we define initial and terminal configurations as follows:
1. Initial: C = ([],[w1,...,wn],[])
2. Terminal: C = ([w1,...,wn],[],F) (for any F)
In the initial configuration, all tokens remain to be read (i.e., L and F are empty). In the final configuration, all tokens have been read (i.e., R is empty), and at this point F may correspond to any fixation sequence.
We then define the following transitions:2
1. Progress(n):
([l|wi],[wi+1,...,wi+n|r],[f|i]) ) ([l|wi,wi+1,...,wi+n],r,[f|i,i+n])
2. Regress(n):
([l|wi n,...,wi 1,wi],r,[f|i]) ) ([l|wi n],[wi n+1,...,wi|r],[f|i,i n])
3. Refixate:
([l|wi],r,[f|i]) ) ([l|wi],r,[f|i,i])
This transition system accounts for the possible interword and intraword (re- fixation) saccades in reading, while also keeping track of the fixation sequence. The transition Progress(n) generates a progressive saccade of length n, which means that the next fixated word is n positions forward with respect to the currently fixated word (i.e., n 1 words are skipped). Similarly, the transi- tion Regress(n) generates a regressive saccade of length n. If the parameter n of either Progress(n) or Regress(n) is greater than the number of words re- maining in the relevant direction, then the longest possible movement is made instead, in which case Regress(n) leads to a configuration that is similar to the initial configuration in that it has an empty L list, while Progress(n) leads to a terminal configuration. The transition Refixate, finally, generates a refixation, which means that the next word fixated is the same as the current.
2We use the variables l, r and f for arbitrary sublists of L, R and F, respectively, and we write the L and F lists with their tails to the left, to maintain the natural order of words.
 28
To give a simple illustration of how this system works, we may consider the transition sequence corresponding to the reading of the text John gave Mary the book used as an example in Section 4.2:
Initial ) ([],[John,gave,Mary,the,book],[]) Progress(1) ) ([John], [gave, Mary, the, book], [1]) Progress(1) ) ([John, gave], [Mary, the, book], [1,2])
Regress(1) ) ([John], [gave, Mary, the, book], [1,2,1]) Progress(2) ) ([John, gave, Mary], [the, book], [1,2,1,3])
Refixate ) ([John, gave, Mary], [the, book], [1,2,1,3,3]) Progress(2) ) ([John,gave,Mary,the,book],[],[1,2,1,3,3,5])
4.2.2 Learning Transitions
The transition system we have described defines the set of possible saccade transitions in reading but involves no mechanism for deciding which transi- tion to make in a given configuration. One way to introduce such a mecha- nism, however, is to use empirical data to train a classifier to predict the next transition, given any configuration. In order to train such a classifier we need to define a set of features to represent the data. In other words, we must specify the kind of information we assume to be relevant for the task. A useful starting point, then, is the evidence reviewed in chapter 2 about variables influencing where the eyes move during reading. Thus, we might want to include features, or predictors, for the length, frequency and predictability of a few words to the right of fixation, and perhaps also of the currently fixated word (we noted, for example, that the likelihood of refixating a word depends on the length of the word). Assuming that the empirical eye movement data is overlaid with this kind of information, we can create training data for classifiers. This is done by reconstructing the transition sequence for the reading of a text and by ex- tracting the feature information associated with each resulting configuration. A training instance for the classifier then consists of a feature vector repre- sentation of a fixation state, and the observed saccade transition out of that state. At this stage, the problem has been reduced to a standard supervised machine learning problem and it is then straightforward to train a classifier using some learning algorithm to predict the most probable transition out of any configuration. There is a wide variety of learning algorithms that could be used for this purpose, including logistic regression, support vector machines, neural networks and decision trees.
4.2.3 Search Algorithm
Once we have trained a classifier f that predicts the next transition t out of
any configuration C, we can use it as an oracle for modeling target selection 29
through the reading of a text T = (w1,...,wn). Let Rc be the list R of con- figuration c and Fc be the list F of configuration c. The following algorithm achieves this:
Read(w1,...,wn)
1: c ([],[w1,...,wn],[]) 2: while Rc 6= []
3: t f(c)
4: c t(c)
5: return Fc
We start in the initial configuration and as long as there are more words to the right of fixation we apply f (c) to get t , and then apply t (c) to update the current configuration. When the search terminates, we return the fixation sequence F of the current configuration c.
4.2.4 Experimental Results
The model we have outlined so far is flexible enough to allow exploration of a range of different models of saccade target selection. As it stands, we have made no specific assumptions about what feature information to bring to the learning task, or which learning algorithm to use for guiding the deci- sions of the classifier. Rather, these are parameters to be explored empirically. We finish this section with an overview of some experimental results using a particular instantiation of the general model. In these experiments, we train separate classifiers for different readers and then compare the predicted and the observed fixation sequences on held-out test data.
The results we report are based on data from the English section of the Dundee eye tracking corpus (Kennedy and Pynte, 2005). This data set con- tains the eye tracking record of 10 native English-speaking adults reading 20 newspaper articles collected from The Independent newspaper. The newspaper articles consist of roughly 2500 words each, giving about 50000 words total. The eye movements were recorded using a Dr. Bouis eye-tracker, sampling the position of the right eye at a rate of 1000 Hz (once per millisecond).
The parameters of the classifiers are estimated on the first 16 texts in the Dundee corpus while the remaining four texts (17-20) are held out for vali- dation and evaluation purposes. The results we report are based on the blind test data comprising the last two Dundee texts (19-20). The classifiers are trained using an off-the-shelf implementation of logistic regression available in Weka, a publicly available collection of machine learning software written in Java (Witten and Eibe, 2005). We limit the task in these experiments to that of predicting saccades falling roughly within the perceptual span of the fixated word (98.3% of all saccades in training data).
The classifiers are trained using only a few features known to influence sac- cade decisions, such as the word length and word frequency (broken down into
30
    Reader
     # sentences
  Fixation Accuracy Baseline Model
Fixations Prec Rec F1
    Skips Prec Rec F1
      a b c d e f g h i j
                              136
156
151
162
182
157
129
143
196
166
                    53.3 70.0 55.7 66.5 59.9 70.9 69.0 78.9 51.7 71.8 63.5 67.9 43.3 56.6 57.6 66.9 56.4 69.1 66.1 76.3
69.9 73.8 71.8 65.2 85.8 74.1 72.5 82.8 77.3 84.7 84.8 84.7 69.1 78.4 73.5 70.9 83.7 76.8 49.9 80.8 61.7 69.4 76.3 72.7 69.6 80.3 74.6 82.2 81.9 82.0
                    69.0 65.8 67.4 70.3 80.4 75.0 67.4 53.1 59.4 66.0 65.8 65.9 75.3 65.2 69.9 58.7 40.2 47.7 72.2 38.1 49.9 62.8 54.3 58.2 68.2 54.7 60.7 65.0 65.4 65.2
                              Average
   157.8
  57.7 69.5
70.3 80.9 75.2
  67.5 58.3 62.6
      Table 4.1. Fixation and skipping accuracy on test data; Prec = precision, Rec = recall, F1 = balanced F measure.
five frequency classes) of the currently fixated word and of words to the right of fixation. The distance between the current fixation and recent fixations is also included as it influences regression probability. Regressions occur more often after skipping, for example. The accuracy of the models is measured as follows. First, we compute the fixation accuracy, that is, the proportion of words that are correctly fixated or skipped by the model, which we also broke down into precision and recall for fixations and skips separately.3 Secondly, we compare the predicted fixation distributions to the observed fixation distri- butions, both over all words and broken down into five frequency classes of words.
Table 4.1 shows the fixation accuracy, and precision, recall and F1 (un- weighted harmonic mean of precision and recall) for fixations and skips, for each of the ten different models and the average across all models. The fixa- tion accuracy is compared to baseline models which always predict the most common type of saccade for a given reader (Progress(2) for readers a and e, and Progress(1) for the others).
If we consider the fixation accuracy, we see that all models improve sub- stantially on the baseline models. The variation in the relative improvement between models is however quite large, ranging from 4.4 percentage points for the model of reader f to 20.1 percentage points for the model of reader e. Averaged over all models, the results improve over the baseline by 11.8%. Comparing the precision and recall for fixation and skips, we see that while precision tends to be about the same for both categories (with a few notable exceptions), recall is consistently higher for fixations than for skips. This is
3Fixation/skip precision is the proportion of tokens fixated/skipped by the model that were also fixated/skipped by the reader; fixation/skip recall is the proportion of tokens fixated/skipped by the reader that were also fixated/skipped by the model.
 31
                    abcdefghij
Figure 4.1. Proportion of fixated tokens grouped by reader and model
likely due to a tendency of the model to overpredict fixations, especially for low-frequency words. This has a great impact on the F1 measure, which is considerably higher for fixations than for skips.
Figure 4.1 shows the distributions of fixations grouped by reader and model. The models are reasonably good at modeling the empirical fixation distribu- tions of the readers. However, the models tend to overestimate the fixation rate, looking at more words than the readers, as noted above. This suggests that the models lack sufficient information to learn to skip words more often. As noted in chapter 2, an additional important determinant of the decision to skip a word, which we have not included in these models, is word predictabil- ity. It is possible that the inclusion of some estimate of word predictability, such as the predictions of an n-gram language model, would increase the skip- ping rate and improve the results.
Figure 4.2, finally, shows the mean observed and predicted fixation and skipping probability as a function of word frequency class, averaged over all models. The effect of the frequency of the word on performance is comparable to the readers, although the models typically tend to exaggerate the observed effect. Thus, too few words are skipped in the lower to medium frequency classes (1–3), while too many of the most frequent words (5) are skipped. The skipping rate (and thus fixation rate) is, however, very well-predicted for words in frequency class (4), that is for words with an estimated 1001–10000 occurrences per million words.
32
Reader Model
  Proportion
0.0 0.2 0.4 0.6 0.8
   F
F
S
S
F
F
S
S
F
F Fixation ! Observed F Fixation ! Predicted S Skipping ! Observed S Skipping ! Predicted
S
 FS
S S
F F
SF F
S
            12345 Frequency class
Figure 4.2. Mean observed and predicted fixation and skipping probability for five frequency classes of words
4.3 Entropy-Based Model Assessment
In this section we discuss some problems in treating saccade target selection as a classification task and propose an alternative view of the problem which leads to a more refined model assessment based on entropy.
The classification-based approach to saccade modeling we outlined in the previous section relies on hard classification decisions. In other words, the model always gives a single discrete categorical prediction, which defines the most likely saccade target given the current fixation state. We showed that these predictions can be used to assess the accuracy of a model by compar- ing them to empirical observations using a 0-1 loss function which counts the number of correct (or incorrect) predictions. One limitation to this approach, however, is that some information is lost, such as the partitioning of the pre- dictions between the possible classes. When an instance is misclassified, for example, we do not know how “close” the prediction may be to being cor- rect. This is, however, relevant information since the large variability inherent in human saccade behavior renders hard classifications and exact predictions highly uncertain. The classification error, therefore, appears to be a crude measure of model performance.
If instead of stating the problem as a classification task, we consider it to be one of assigning probabilities to fixation sequences, we may assess models differently. In principle, we may then measure the distance between the pre- dicted distribution of saccade targets given the model and the actual, or true, distribution. In practice, this may be achieved by measuring the entropy, or
33
Fixation probability
0.0 0.2 0.4 0.6 0.8 1.0
more precisely, by measuring an approximation to a quantity known as the cross entropy between probability distributions. Rather than asking how often the model agrees (exactly) with the observed human behavior, we may then ask how probable the observed behavior is from the model’s point of view. Intuitively, the entropy of a test sample relative to a model measures the uncer- tainty or average surprise associated with the observations in the test sample, as perceived by the model. A “good” model of human saccade behavior, then, is a model that assigns high probability and low entropy to representative test data. The lower the uncertainty, the better the model, being less surprised on average by the observed behavior. Or, from a slightly different view, by mea- suring the entropy of a model, we assess how similar the behavior that we can expect from the model is to the observed human behavior.4
The use of entropy presupposes a probabilistic model which defines a prob- ability distribution over the outcome, that is, over the possible saccade transi- tions. Under this view, there are a number of candidate words at each fixation, where each candidate has a certain probability of being selected as the target for the subsequent saccade. Thus, the basic constraint we impose is that, given a text T , a saccade model can assign a probability to any arbitrary sequence of fixations F over the text:
P(F|T) = P(i1,i2,...,im|T)
Given any such model, we may assess the entropy of the model using essen- tially the same transition-based model as we outlined in the previous section, even though we are not doing classification, but only estimating probabilities. We mention how this is done below but first we outline the notion of entropy and how we may apply it. For a random variable X with n outcomes, the entropy H(X) is
n
H(X) =   Â p(xi)log2 p(xi)
i=1
where p(xi) is the probability of outcome xi. The quantity H(X) is a measure of the uncertainty associated with the variable X, quantifying the expected or average surprise over all possible outcomes. Lower values of entropy indicate less average surprise, or likewise, lower uncertainty. Essentially, uncertainty is greatest when the chances for one outcome is no different from the chances of another. In other words, entropy is maximal when the probability mass is evenly distributed so that all outcomes are equally likely.
An often used variant of entropy, known as the cross entropy, allows us to compare probability distributions of a random variable X. By replacing the surprise value in the definition of entropy with an estimate derived from a
4In this context, it is worth noting that entropy-based measures of probabilistic models are widely used in many modeling fields, for example, in artificial intelligence (AI) and natural language processing (NLP).
 34
model m, we get the cross entropy of the model with respect to the true model
p:
n
Hc(p,m) =   Â p(xi)log2 p(xi|m)
i=1
The cross entropy Hc(p,m) is an upper bound on the true entropy H(p), which
means that the cross entropy of a model m(x) on some distribution p(x) is greater than or equal to the actual entropy of the true distribution H(p):
H(p)Hc(p,m)
In principle, then, the cross entropy can be used as a model evaluator on the as- sumption that the lower the cross entropy, the better the model (i.e., the closer it is to the true entropy). This presupposes however that the true probability distribution p is known, which is not the case in practice. The solution is to estimate how well the model predicts a separate test sample drawn from the same distribution p. Better models of p will tend to assign higher probabilities, and thus lower surprisal, to the observed events in the test sample.
The cross entropy of the test data, given the model, is known as the log- probability (log-prob or LP). Given a saccade model M and a test sample F (for some text T ), the log-prob is:
LP(M) =  1 log2 p(F|M) n
This formula gives the average surprise associated with an observed saccade behavior over some text, as perceived by the model. Intuitively, a better sac- cade model assigns lower log-prob to the test sample, being less surprised on average. Importantly, in order for the log-prob to be a reliable performance measure, we must use a training sample to estimate the parameters of the model and then a different but representative sample to test or evaluate the model.
Once we have trained a probabilistic model, or probabilistic classifier, we can derive the necessary probabilities for assessing the entropy of a test sample relative to the model using the same model as before. The important differ- ence is that we do not use the classifier to return the most probable saccade transition given the model, but, instead, to return the probability assigned by the model to the observed saccade transition in the test data. We sum the surprise for each observed saccade target in the data (the negative log of the probability) and the log-prob is then given by the average.
Figure 4.3, shows the result of one experiment using entropy-based model assessment. In this experiment, saccades of different lengths are grouped into five saccade types5 and the model is assessed with respect to the entropy as- signed to each of these types in the test data. The entropy is in bits per fixation,
5The types are forward (move forward to next token); regress (regress to any previous token); refixate (fixate current token); skip (move forward to the second next token); and other (move forward to any other token).
  35
                      36
4
3
2
Saccade Type
Figure 4.3. Entropy per fixation grouped by saccade type averaged over all models of
readers a-j.
averaged over all ten models (of the ten readers in the Dundee corpus). The results identify the relative difficulty in modeling different types of saccades. We see that the model is more surprised, on average, when readers refixate a word or regress to a previous word than when they move their eyes to the next word or skip over a word. We also see, as we would expect, that regressive saccades are much more surprising than any other saccade type.
More generally, this is an example of a particular setup of the model that provides a way to test different learning strategies and predictors with respect to, for example, how well they reduce the entropy of regressive eye move- ments in reading. Analyzing the conditions under which regressions occur is considerably more difficult in the classification-based version of the model since only very few of the classifications result in regressive eye movements.
Regress Refixate Forward Skip Other
Entropy per Fixation
5. When the Eyes Move
In this chapter we demonstrate that the problem of determining when the eyes move during reading may be approached as a time-to-event modeling prob- lem.1 More specifically, given their wide use in studying events that occur over time, we introduce Cox hazards models for eye movement modeling and show how these models can be applied to address questions about the strength, as well as the timing, of processes that influence the decision to move the eyes. We further present an evaluation metric, commonly applied to time-to-event models, for assessing the predictive accuracy of the models. This metric is based on precisely the same intuition as the measure of entropy we developed in chapter 4: good models assign high probability to empirically observed behavior. Here, this intuition is expressed differently in order to assess proba- bilistic predictions about temporal events.
The chapter is divided into two parts. In the first part, section 5.1, we pro- vide background that serves both as an introduction to the methods and models later presented and as a concise summary of the main results of this chapter. In the second part, section 5.2, we first review the basic concepts and methods we use, and then summarize some of the experimental results from applying these methods to eye tracking corpus data.
This chapter is based on the contents of paper IV and V.
5.1 Background
Time-to-event modeling is concerned with studying the length of time before an event of interest occurs and, more generally, addresses the challenges in- volved in modeling duration data. Although predominantly used in biomedical sciences, where the approach is referred to as survival analysis,2 the notion of time as involving a past, a present, and a future makes time-to-event modeling generally useful for analyzing events occurring in time (Aalen et al., 2008). Essentially, the methods can be applied to any data involving temporal obser- vations with a well-defined starting point (point when the “clock” starts) and
1We note that paper II also addresses the problem of determining when the eyes move during reading (as well as where). Although introducing some novel ideas, the model presented in this paper is simpler but similar in style to more sophisticated models of eye movement control like E-Z Reader. Because paper IV and V offer more novelty to the field, we focus the discussion in this chapter exclusively on the material presented in these papers.
2In survival analysis, the methods are used to study lifetime data (hence the name) and thus often, literally, the time until death occurs.
 37
end point (point when the “clock” stops). The distance on the time scale be- tween these points is generally referred to as survival time (or time-to-event), regardless of the nature of the event (Hosmer et al., 1999).
Central notions of time-to-event modeling, such as survival and hazard to describe the temporal nature of events, are frequently used also in experimen- tal and mathematical psychology, in particular in the study of human response time (see, for example, Luce, 1986; Van Zandt, 2002), and, to a lesser ex- tent, in eye movement and reading research since the work by McConkie and colleagues (McConkie et al., 1994; McConkie and Dyre, 2000; Yang and Mc- Conkie, 2001; Feng, 2009). Here, we extend this latter line of work by the use of Cox hazards modeling,3 which is, by a wide margin, the most common method in time-to-event analysis for analyzing the relationship of explanatory variables to survival time.
In applying Cox modeling to reading time data, it makes sense to focus on the survival function distribution of the data, as Cox models are typically used to estimate the chances of survival over time under different conditions. Hence, we set ourselves the overall goal to model the survival function of fixation durations in reading. In other words, we aim to model, as accurately as possible, the probability that a fixation lasts beyond a given length of time. The Cox proportional hazards model is introduced for this purpose (Cox, 1972) and we show how this model can be applied to account for covariate effects, such as the length and frequency of the word fixated, on the chances of survival. It is shown that this model improves in predicting survival over a simpler model which does not take such effects into account.
However, the Cox proportional hazards model is based on one particular as- sumption, the assumption of proportional hazards. This means that covariate effects on the hazard – or risk – of making a saccade at any time are con- stant over time. In many applied settings of survival analysis this assumption does not hold.4 We evaluate this assumption of the Cox proportional hazards model and the results suggest that the hazards are not proportional. Instead, the influence of covariates typically changes over time and decays in the long run. Moreover, in a second model which relaxes the proportional hazards as- sumption, we demonstrate that by partitioning the time-axis and allowing the strength of covariates to vary over time, we reduce the prediction error no- tably, when compared both to the previous simpler estimate and the covariate- adjusted model of the survival function based on the Cox proportional hazards model. This supports the initial result that the effect on the hazard is different at different points in time. While we do not provide a concise explanation why such time-varying effects should occur, these findings resonate, from a psycholinguistic point of view, with the idea that different kinds of informa-
3The proposal to use Cox models for eye movement modeling is due to Feng (2009).
4For example, a new drug may work well initially but then gradually lose its efficacy over time. Or conversely, a drug may be effective in the long run but perhaps have some small adverse effect early after administration.
38

tion are made available to the processing system at different times. If certain processes (e.g., lexical processing) act on the output of others (e.g., visual processing) during the course of a fixation, we would, intuitively, expect the strength of covariates to vary with time. Taken together, our results suggest that Cox hazards modeling with time-varying effects can recover some ba- sic time course characteristics of the short-lived processes that influence the decision to move the eyes. Further research, however, is required to better understand the nature of these results.
5.2 Saccade Timing as Time-to-Event Modeling
In this section, we first review the basic concepts and methods we make use of in applying time-to-event modeling to the timing of saccades during read- ing. This includes an outline of the survival and the hazard functions, the Kaplan-Meier estimate of the survival function, the Cox proportional hazards model, the extended Cox model with time-varying effects, and the Brier score for model evaluation. Subsequently, we summarize experimental results from applying these methods to data.
5.2.1 The Survival Function
We denote by T the random variable for the survival time, or duration, of a reading fixation.5 The saccade terminating a fixation then defines the event of interest for a given observation (fixation). Since T denotes time, its possible values include all non-negative numbers, that is, T   0. Next, we denote by t, any specific value of interest for the random variable T . The survival function, denoted S(t), gives the probability that the survival time is longer than some specified time t. In other words, S(t) is the probability that the random variable T exceeds the specified time t:
S(t) = P(T > t) The survival function has the following property:
S(t)  S(t0) if t   t0
where t and t0 denote two specified values of time. Thus, S(t) is a non- increasing function of time, heading downward as t increases. Furthermore, the following properties are usually assumed:
S(t)=(1 fort=0 0 fort=•
5In the previous chapter, T denotes a text. Throughout this chapter, T denotes survival time. 39

Given these properties the survival function can, theoretically, be graphed against time as a decreasing smooth curve that starts at one and approaches zero as time tends to infinity. In practice, however, the value of an estimated survivalfunctionisconstantbetweenobservedsurvivaltimest1,t2,...,tn,yield- ing a step function, rather than a smooth curve.
5.2.2 Kaplan-Meier Survival Estimate
The Kaplan-Meier estimate (Kaplan and Meier, 1958) is the most frequently used method for estimating the survival function from time-to-event data in thepresenceofcensoredobservations.6 TheKaplan-Meierestimateisbased on the simple intuition that in order to be alive at some point in time, it is necessary to survive all previous time points. Let the observed survival times in a sample of n observations be ordered such that:
t1 t2 ...tn
The Kaplan-Meier estimate of surviving longer than to time t is:
Sˆ ( t ) = ’ n i   d i tit ni
where ni is the number of observations at risk (alive and not censored) just prior to time ti, and di, the number of deaths at ti (i.e., the number of observa- tions with survival time ti).
When there are no censored observations, the Kaplan-Meier estimate re- duces to the proportion of survival times greater than t. In other words, the survival function at time t is given by the number of fixations still at risk of terminating at time t, divided by the total number of fixations:
Sˆ ( t ) = n i n
The Kaplan-Meier estimate does not take account of the additional influence that covariates may have on survival time. To describe how other factors may influence survival, the Cox proportional hazards model is typically used in- stead. Mathematically, this model is specified in terms of the hazard function rather than the survival function. There is, however, a defined relationship between the two functions such that it is always possible to derive one from the other. We introduce the hazard function next, before turning to the Cox proportional hazards model.
6Censoring in time-to-event analysis occurs when the precise survival time for an observation is not known, only that the observation remained alive for as long as it was observed.
   40
5.2.3 The Hazard Function
Intuitively, the hazard function, denoted h(t), gives the risk for the event to occur in the next instant given that it has not yet happened. It is defined as:
h(t)= lim P(tT<t+Dt|T t) Dt!0 Dt
The conditional probability in the numerator of the hazard function formula gives the probability that the survival time, T , will lie in the interval between t and t + Dt where Dt denotes an infinitesimally small interval of time, given that the survival time is greater than or equal to t. More intuitively, it is described by:
h(t)= f(t) S(t)
where f(t) denotes the probability density function (pdf) of T, and S(t) de- notes the survival function of T . In other words, it is equal to the unconditional probability that a saccade occurs at time t , f (t ), divided by the probability that a saccade does not occur before t, S(t). In eye movement research, the hazard has been referred to as the momentary, or instantaneous saccade likelihood, or as the conditional saccade rate (Yang and McConkie, 2005).
5.2.4 The Cox Proportional Hazards Model
The Cox proportional hazards model regresses the hazard function, h(t), on a set of covariates. Let x1,x2,...,xp be the values of p covariates X1,X2,...,Xp, and let b1,b2,...,bp be the corresponding regression parameters. Accord- ing to the Cox proportional hazards model, the hazard function, h(t), for an observation with covariate values x1,x2,...,xp, is given as:
p
h(t) = h0(t)exp(Âbixi)
i=1
The model states that the hazard function at any time t is the product of two quantities: the baseline hazard function, denoted h0(t), and the exponentiated linear sum of b1x1 +b2x2 +...+bpxp. The baseline hazard, h0(t), defines the hazard at time t for an observation whose covariate values are equal to zero, i.e., if x1 = x2 = ... = xp = 0, then h(t) = h0(t)exp(0) = h0(t). That is, when there are no covariates in the model, the hazard function reduces to the baseline hazard. While the baseline hazard involves t and varies with time, the exponential expression does not. This is the assumption of proportional hazards. Thus, although the hazard rate may vary over time, the assumption is that covariate effects multiply the baseline hazard by a constant factor at all times.
  41
The outcome of fitting a Cox proportional hazards model yields an equation for the hazard at time t as a function of one or more explanatory variables. The size of the effects of the explanatory variables is usually interpreted in terms of their hazard ratios, which are obtained by exponentiating the values of the estimated regression coefficients. For covariates with hazard ratios less than 1, increasing values of the covariates are associated with lower hazard and longer survival times. Conversely, when hazard ratios are greater than 1, increasing values of the covariates are associated with higher hazard and shorter survival times. More specifically, the hazard ratio measures the change in the risk for the event to happen relative to a unit increase in the value of the covariate, assuming the values of the other covariates in the model are held constant. A hazard ratio of 1 means no effect, a hazard ratio below 1 means a reduced relative risk and a hazard ratio above 1 means an increased relative risk for the event. Under the assumption of proportional hazards, the hazard ratio is constant and independent of time.
The primary diagnostic test for assessing whether the hazard is proportional in covariates is based on the Schoenfeld residuals (Schoenfeld, 1982) from the fitted Cox proportional hazards model. The Schoenfeld residual for an obser- vation with survival time ti for a given covariate is the value of the covariate, minus a weighted average of the values of the covariate for the remaining observations that are still at risk of being terminated at the same time ti. If the proportional hazards assumption holds for the covariate, the Schoenfeld residuals for that covariate will be independent of time. This can be tested by correlating the residuals with the survival times. A p-value less than .05 then indicates a departure from proportionality and hence that the effect on the hazard changes over time. In this case, we may still use the Cox proportional hazards model but risk to have biased estimates, or, we can use an extended version of the Cox proportional hazards model to try and account for the time- varying effects.
5.2.5 Extended Cox Model: Time-Varying Effects
The extended Cox model is based on the intuition that, even though the pro- portional hazards assumption does not hold over the whole time period, it may still hold over shorter time periods. In other words, the idea is to partition the time axis into shorter time intervals, over which the effect on the hazard re- mains constant. Separate effects can then be estimated for each time interval.
The extended Cox model can be expressed as:
pp
h(t) = h0(t)exp(Âbixi + Âgixigi(t))
i=1 i=1
where bi and gi are unknown regression parameters and gi(t) is some specified
function of time for xi. The extended Cox model, like the Cox proportional 42
hazards model, gives the hazard at time t for an observation with a given spec- ification of covariate values. Note that gi may be equal to zero, in which case there is no defined interaction with time for covariate xi. Thus, the extended Cox model allows the inclusion of covariates with as well as without time- varying effects.
A common form for gi(t) is to let it be a “heaviside function” of time, where gi(t) = 1 when t is greater than some specified time, t0, and gi(t) = 0 when t is less than or equal to t0:
gi(t)=(1 ift>t0 0 iftt0
Based on the idea that the proportional hazards assumption holds at least over shorter time periods, separate effects can then be estimated for each period t  t0 and t > t0. This use of heaviside functions may be extended to give separate estimated effects over several time intervals. In other words, each covariate can then relate differently to the hazard at several different points in time. The choice of the cut-off value t0 is typically based on either measures of central tendency or on change-points in the shape of the hazard or survival curve.
The hazard ratios from the model are then interpreted as functions of time. We consider a simple model involving only one explanatory variable X and a single heaviside function g(t). The extended Cox model is then given as follows:
where
h(t) = h0(t)exp(bx+gxg(t)) g(t)=(1 ift>t0
0 iftt0
We then obtain two different hazard ratios for the effect of X, depending on the value of t. One value for the effect when t is greater than t0 and another value when t is less than or equal to t0:
tt0: HR = exp(b) t>t0: HR = exp(b+g)
There is a mathematically equivalent way to write this model that uses two heaviside functions instead of one and no main effect for the explanatory vari- able X. Although less intuitive in its specification, this version is sometimes preferred in practice as it eliminates the need to add up coefficients in the final model.
43
5.2.6 Prediction Error
We have described three different models that can be used to derive models of the survival function for a fixation duration distribution: the Kaplan-Meier estimate, The Cox proportional hazards model, and the extended Cox model with time-varying effects. Given such models we want to assess how accurate they are in predicting the chances of survival over time. For this purpose we use the Brier score, a commonly used evaluation metric for survival models.
The basic intuition behind the Brier score is as follows. If an observation is still alive at some specified time t, the predicted survival probability for that observation should be high at that time, and conversely, if the observation is not alive (i.e., the saccade has occurred by time t), the survival probability should be low. It is worth noting that in using the Brier score, we once again apply the idea of using entropy as a measure of how similar a model is to ob- served data. Specifically, we expect that a good model estimates high chances (low entropy) of being alive for those observations that are alive at some stated value t, and low chances (high entropy) for those observations which are not.
Let Yi(t) denote the survival status of observation i at some specified time t, which is equal to 1 if the observation is still alive at time t and 0 otherwise. Let further Sˆi(t) denote the predicted survival probability, given a model, for observation i at the same time t. The Brier score at time t for observation i is defined as the squared difference between the observed survival status (0 or 1) and the predicted survival probability at time t. Calculated over all observa- tions i1,i2,...in in a test sample, the Brier score gives the mean squared error between the observed survival status and the predicted survival probability at some specified point in time t:
1n
BS(t)=n (Y(t) S(t))
ˆ Âi ˆi 2 i=1
 Note that the Brier score is a function of time and may be used to evaluate the prediction error of a model of S(t) at any stated value of time t. At each value of t, the Brier score is based on an average computed over all observations in the sample. The lower the Brier score, the lower the prediction error. Pos- sible values for the Brier score range from 0 to 1. A non-informative model with a 50% incidence of the outcome gives a 0.25 Brier score at time t. Of- ten, however, the Kaplan-Meier estimate, which does not depend on covariate information, is used as the basis for comparison. The Brier score is usually fol- lowed over time, from the shortest to the longest survival time in the test data. In this way we obtain a prediction error curve that visualizes how accurate a model is in predicting survival over the whole distribution.
44
 Variable
word length
log word frequency log bigram probability syntactic surprisal saccade distance eccentricity
Table 5.1. Covariates in the Cox proportional hazards model (time-constant effects model).
5.2.7 Experimental Results
We now summarize some experimental results reported in paper V. The over- all purpose of these experiments is to compare the predictive performance of models with time-constant effects to models with time-varying effects us- ing the methods we have described. Overall, these experiments show that (i) the use of Cox proportional hazards model reduces prediction error over the Kaplan-Meier survival estimate slightly; (ii) some covariates have time- varying effects on the hazard; and (iii) a model with time-varying effects re- duces prediction error notably over the previous two models. In this summary, we focus exclusively on the prediction error of the models. However, a de- tailed summary of the regression coefficients, hazard ratios, p-values, 95% confidence intervals, along with Schoenfeld plots of time-varying effects is contained in paper V.
The results are based on first fixation duration data from the Dundee Cor- pus. The covariates we use in fitting the Cox proportional hazards model are shown in table 5.1.7 Word length is in number of character spaces; log word frequency is the natural logarithm of the word’s estimated frequency in lan- guage use; bigram probability refers to the conditional probability of the word given the preceding word. Syntactic, or structural surprisal (Originally due to Hale, 2001) is a measure of the uncertainty, in the information-theoretic sense, of a word in its syntactic context. Intuitively, the surprisal increases when highly expected structural expectations turn out to be incorrect at a word. Saccade distance refers to the distance between the previous and the current within-word fixation position, and eccentricity to the number of characters the current fixation position deviates from the center of the word. This collection represents a fairly standard set of covariates often used in regression analysis of reading time data.
7Since the goal of this study is not to motivate or provide evidence for novel covariate influences on eye movements, we keep our review of covariates short here. A longer discussion of the covariates, in the context of time-varying effects, is found in paper V.
   45
Word frequency and bigram probability estimates are based on occurrences in the Google 5-gram corpus (Brants and Franz, 2006) while surprisal esti- mates are based on a probabilistic parser (Roark et al., 2009) trained on the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993).
The model parameters are estimated on the first 16 texts in the corpus. The remaining four texts (17-20) are held out for evaluation, giving roughly a 80/20 percent training/test split of the data. The data consists in total of 175884 first fixations, 141692 for training and 34192 for evaluation.
Models
We introduce the following shorthand notation for the covariates in the model. Let X1 = word length, X2 = log word frequency, X3 = log bigram probability, X4 = syntactic surprisal, X5 = saccade distance, and X6 = eccentricity. The Cox proportional hazards model for the hazard function then takes the form:
6
h(t) = h0(t)⇥exp(Âbixi)
i=1
We assess the assumption of proportional hazards for this model by correlat- ing the scaled Schoenfeld residuals for each individual covariate against rank ordered survival time. Following standard use of this test, a rejection of the null hypothesis at the 5% level indicates that the hazard ratio for a given co- variate is non-proportional and thus that the covariate’s effect on the hazard is varying over time. The results indicate non-proportional hazards in four co- variates, namely word length, log word frequency, saccade distance and eccen- tricity. No evidence of non-proportionality is found for log bigram probability or syntactic surprisal. While we do not discuss the time-varying effects in any detail in this summary, we note that the main result based on our analysis of the Schoenfeld test is that all time-varying covariates show similar decaying effects over time. For example, the (positive) effect of word frequency on the hazard appears to be strong between 180 and 210 ms. During this time, the frequency of the word has a relatively strong impact on the hazard. For fixa- tions still surviving at around 300 ms, however, the risk of making a saccade in the next instant does not appear to depend much on the frequency of the word.
To account for these time-varying effects we consider an extended Cox model with heaviside functions of time for covariates with non-proportional hazards. In other words, we define g(t) as heaviside functions of time, taking the value 1 if t is greater than some specified value, t0, and the value 0 when t is less than or equal to t0. Our choice of cut-off values for t0 is based on the shape of the empirical hazard function for reading fixations. As described in Feng (2009), the hazard function for the distribution of reading fixations is characterized by four periods: an early slow rising period until about 130 ms, a second rapidly rising period until about 180 ms, a third less rapidly ris- ing period until about 250 ms, and then a slow decline with a long right tail.
46
We extend the Cox proportional hazards model so that we obtain four hazard ratios for each of the time-varying covariates, corresponding to four different time intervals identified by three change-points at 130, 180, and 250 ms. As in the original model, a single hazard ratio is obtained for each of the covariates bigram probability and syntactic surprisal. The time varying hazards model is specified as follows:8
h(t) =
where
and
44
h0(t)⇥exp([Âb1ix1 ⇥gi(t)]+[Âb2i ⇥gi(t)]+b3x3 +b4x4 +
i=1 i=1 44
[Âb5ix5 ⇥gi(t)]+[Âb6ix6 ⇥gi(t)])
i=1
g1(t) = (1 0
g3(t) = (1 0
i=1
if0<tt1 g2(t)=(1 if otherwise 0
ift1 <tt2 if otherwise
ift2<tt3 g4(t)=(1 ift>t3
if otherwise 0 if otherwise
t1 =130 t2 =180 t3 =250
The hazard ratio for the variables word length, word frequency, saccade length and eccentricity now varies with time. For each of these variables it assumes four distinct values depending on the value of t. We obtain the hazard ra- tios from the fitted model by separately exponentiating each of the estimated coefficients in each time interval:
0<t130: HR=exp(bˆi1)
130<t 180: 180<t 250: t > 250 :
HR=exp(bˆi2) HR=exp(bˆi3) HR = exp(bˆi4)
The hazard ratios for bigram probability and syntactic surprisal do not vary with time. These covariates occur in the model only as main effects and their hazard ratios are given by:
HR = exp(bˆi)
8Note that we use the alternative formalization of the extended Cox model mentioned in section 5.2.5.
 47
 Figure 5.1. Prediction error curves (Brier score) on test data between the observed survival status and the predicted survival probability, for Kaplan-Meier estimate, Cox proportional hazards model (time-constant effects) and the extended Cox model (time- varying effects).
Once again, we assess the proportional hazards assumption, now for the re- fitted time-varying effects model. The evidence for non-proportional hazards is now much weaker in comparison to the original Cox model. In particu- lar, there is no longer evidence for non-proportional hazards in the covariates word length, word frequency and eccentricity, in any of the time intervals. This suggests that the effect of these covariates now varies over time but re- mains constant prior to 130 ms, between 130 and 180 ms, between 180 and 250 ms, and after 250 ms. We still find non-proportional hazards in the covari- ate saccade distance, in the two intervals between 130 and 250 ms. Overall, however, we conclude that the time-varying hazards model gives piecewise constant effects over all time.
Results
Figure 5.1 shows the prediction error curves for the survival functions based on the Cox proportional hazards model, the extended Cox model with time- varying effects, and, as a basis for comparison, the Kaplan-Meier estimate of the survival function. The prediction error at any time t is measured by the Brier score, which gives the mean squared error over all fixations between the
48
observed survival status and the predicted survival probability at time t. The survival status refers in this case to whether a fixation is still alive (i.e., word is still fixated) at the given time t or if it is not (i.e., saccade has been launched) The Kaplan-Meier estimate at time t is equal to the proportion of surviving fixations in the training data at time t.
The prediction error curves resemble positively skewed gaussian distribu- tions, with lower prediction error towards the origin and the end point of the time axis, where there is also less variation in the data. For example, it is easier to predict whether a fixation is still alive at 100 ms and at 300 ms than by 200 ms, because most fixations are usually longer than 100 ms and shorter than 300 ms. The variation is much greater around 200 ms.
When we first compare the time-constant effects model to the Kaplan-Meier curve, we see that this model makes slightly better predictions when there is greater variation in the data, roughly between 175 and 225 ms. Overall, how- ever, it seems that the improvement resulting from taking covariates into ac- count in predicting the chances of survival is relatively marginal. For fixations shorter than 175 ms and longer than 225 ms, covariate information does not seem to be very useful for predicting survival. The time-varying effects model, however, does much better, in comparison to both previous models. Just as for the time-constant model, there is not much improvement before 175 ms, but around this point in time the prediction error decreases for the time-varying ef- fects model, while it continues to increase for the other models. The prediction error remains substantially lower until about 400 ms. In sum, the improvement offered by the time-constant effects model appears only marginal and limited to a short time window around 200 ms. By contrast, the improvement of the time-varying hazards model seems fairly substantial and extends much further out in the right tail of the fixation distribution.
To conclude, we have demonstrated that eye movement modeling can be addressed as a time-to-event, or survival analysis problem. The approach we have outlined is based on three components: survival function estimation (goal of analysis), Cox hazards modeling (method of analysis), and Brier score eval- uation (assessment of analysis). We have further demonstrated results which indicate that the assumption of proportional hazards, or time-stationary covari- ate effects, is violated for first fixation reading time data. These results imply that time itself is an important predictor of survival time, since covariates may relate differently to the hazard at different points in time. More generally, the results also suggest that Cox hazards modeling can recover some basic time course characteristics of the short-lived processes that influence the saccade timing during reading. Further research into this matter is required, however, before any safe conclusions can be drawn.
49
6. Conclusion
In this chapter, we summarize what we take to be the main contributions of the thesis and point to promising directions for future research.
6.1 Main Contributions
This thesis presents new methods and models for understanding eye move- ments in reading based on the use of eye tracking corpora and data-driven modeling. The methods and models proposed address the central decisions that underlie eye movement behavior: when and where to move the eyes. We explore the idea that empirical eye movement data carries rich information about the processes that guide these decisions.
Throughout the thesis we emphasize the role of prediction in eye move- ment modeling and propose that models should be evaluated with respect to how well they account for previously unseen data. By assessing the predic- tion error, rather than the training error, we reduce the risk of making overly optimistic assumptions about the ability of models to generate accurate pre- dictions. In close connection with this proposal, we further propose a new intuition on which the assessment of eye movement models can be based: bet- ter models assign higher probability to representative, but independently ob- served, behavior. This intuition is made explicit in the form of two evaluation metrics for spatial and temporal predictions, respectively.
The decision of where to move the eyes is approached using standard ma- chine learning methods. The model proposed learns where to move the eyes under different conditions associated with the words being read. Applied to new text, the model moves the eyes in ways it has learnt, showing similar char- acteristics to human readers. We demonstrate, for example, that this model can be trained on individual readers to predict their individual eye movement be- havior reasonably well on new data. The model is flexible, contains few fixed parameters, and can be used to explore a range of different learning strate- gies and factors influencing eye movement decisions. Further details on these methods and models are found in paper I, to some extent in paper II, and in paper III.
The decision of when to move the eyes is approached using time-to-event modeling. The modeling strategy we present is based on three components: survival function estimation, Cox hazards modeling, and Brier score evalua- tion. The models proposed learn the timing of eye movements under different
50
conditions associated with the words being read, and, applied to new text, es- timate the probability that a fixation falling on a word survives for a given length of time. We demonstrate that the Cox proportional hazards model is better in estimating survival than a simpler model which does not take covari- ate effects into account. Furthermore, we also show that covariate effects on the hazard of making a saccade change over the time course. By partitioning the time-axis and allowing the strength of covariates to vary over time, we reduce the prediction error notably over the Cox proportional hazards model. This result attests to the influence of time-varying effects on the decision of when to move the eyes. More generally, these results suggest that Cox hazards modeling can recover some basic time course characteristics of the short-lived processes that influence saccade timing during reading. If this is the case, Cox hazards modeling may open up novel ways for studying cognitive processes from corpus data and for deriving constraints on computational models of eye movement control in reading. Further details about our work on when the eyes move in reading are found in paper III, IV and V.
6.2 Future Directions
The methods and models presented in this thesis open up a number of di- rections for future investigation. With respect to the methods and models of where the eyes move in chapter 4, we have already pointed out that they allow for further exploration. Most obviously perhaps, the basic model allows for different learning algorithms and feature models to be tested experimentally. Our own experiments involved a logistic regression model and a small set of features, or predictors, motivated by experimental findings on saccade target selection during reading. However, this is only one possible setup and not a restriction of the model. Different learning algorithms may, for example, be compared in order to better understand what types of learning methods per- form well in modeling the eye movement decisions. Perhaps less obviously, the transition system of the model is also a parameter that can be varied, and alternative formulations can be explored which may, in turn, affect the learning process and the results.
With respect to the methods and models of when the eyes move in chapter 5, we made an interesting finding that covariate effects change over the time course and that by modeling these changes we improve our predictions of sur- vival time. We consider it an important task for the future to learn the cause for these time-varying effects on the hazard and how to interpret them. While we have suggested that they may relate to different stages of cognitive processing during a fixation, further control experiments, as well as theoretical investiga- tions, are necessary to evaluate the validity of this proposal. In addition to the models we have explored, the time-to-event modeling approach itself allows for further exploration. The analysis of “competing risks” for different types
51
of saccades (e.g., regressions, refixations, and progressive saccades) is an in- teresting example. Competing risk analysis is an extension to time-to-event analysis which allows for more than one type of terminating event (but only one event can occur for each observation). This method provides dedicated techniques to explore how the hazard for competing saccade events evolves over time under different covariate conditions. Finally, it is worth noting that we have not addressed individual differences in temporal reading behavior, which are known to be substantial. We plan to address this issue by intro- ducing frailties in the models. Frailties are introduced in hazard models to account for unobserved heterogeneity, or random effects, between individuals or subgroups of individuals, and allow for individual differences in the hazard functions.
The models explored in this thesis are best described at Marr’s computa- tional level and do not address the specific mechanisms involved in eye move- ment control during reading. We hope, however, that some of our results may prove useful for identifying basic constraints on lower level models, such as the time-varying effects of different covariates on fixation durations. In con- tinuing our investigations along these lines, we hope to match some of the major progress in eye movement modeling seen at other levels of description, and more generally to contribute to the wider understanding of eye movements and cognitive processes.
52
7. Overview of the Papers
I. This paper introduces the use of supervised machine learning methods to predict the saccade behavior of individual readers. The model we present is based on three components: a simple transition system for saccades in reading; a log-linear classifier that predicts the next transition, and hence the saccade target; and a simple search algorithm that derives a fixation sequence over any text, guided by the classifier. We train separate models for different readers and assess each model with respect to its capacity to predict the saccade be- havior of the same reader the model is trained on but on other texts. We show that the models do fairly well on this task, largely reproducing the fixation dis- tributions and fixating the same words as the individual readers. The models respond to word frequency in ways similar to human readers, often skipping over more frequent words.
II. This paper builds on paper I and presents a model that predicts the time course of eye movements, in addition to where the eyes move. This model introduces a set of processes assumed to control the timing of eye movements and imposes theoretical constraints on their durations based on empirical and experimental estimates. The decision to move the eyes is delayed as a function of the ease or difficulty in processing the fixated word. The decision of where to move the eyes is construed as an automated low-level process, approxi- mated here using an induced classifier based on the model presented in paper I. Regressions occur in the model as a result of occasional desynchronization between the processes of when and where to move the eyes. In evaluating the model against held-out data, we show that it predicts observed mean gaze du- rations and mean skipping probabilities over different word frequency classes with good accuracy.
III. This paper presents a method for the evaluation of probabilistic saccade models based on computing the test sample entropy, relative to a model, in- stead of the accuracy of argmax-prediction. The basic intuition of the pro- posal is that a model that approximates human behavior well is a model that assigns high probability, and thus low entropy, to some representative and in- dependently observed behavior. Given the large variation that exists in eye movement behavior, both between and within readers, this kind of probabilis- tic assessment of the prediction capacity of a model seems appropriate. An
53
example of how it may apply is demonstrated on essentially the same sac- cade model as presented in paper I but with a different interpretation. The entropy associated with the observed eye movement behavior is measured and reported.
IV. This paper construes fixation times in reading as time-to-event data and uses methods from survival analysis to model the time course of eye move- ments. We discuss the motivation for this approach and focus on modeling the survival function of fixation time. We derive estimates of the survival function using Cox proportional hazards model to adjust for the influence of linguistic effects on the empirical estimate. The survival models are assessed using the time-dependent Brier score, which can be used to evaluate the prediction error of a model at any stated value of time. The adjusted model, averaged over all readers, is shown to reduce prediction error within a limited time window, roughly between 150 and 250 ms. following the onset of a fixation.
V. This paper extends paper IV. We further motivate the use of survival analyt- ical methods for modeling the time course of eye movements in reading and introduce models with time-varying effects of the covariates. Such models, we show, afford a detailed analysis of the time-course. We model the survival function of fixation time and demonstrate, on the one hand, that a time-fixed model that adjusts for the influence of cognitive effects reduces prediction er- ror over the empirical estimate which disregards such effects and, on the other, that a model with time-varying effects, employing heaviside functions of time, improves considerably over the time-fixed model.
54
References
Aalen, O. O., Borgan, O., and Gjessing, H. K. (2008). Survival and Event History Analysis: A Process Point of View. Springer.
Altman, G. T., Garnham, A., and Dennis, Y. I. L. (1992). Avoiding the garden path: Eye movements in context. Journal of Memory and Language, 31:685–712.
Balota, D. A., Pollatsek, A., and Rayner, K. (1985). The interaction of contextual constraints and parafoveal visual information in reading. Cognitive Psychology, 17:364–390.
Bicknell, K. and Levy, R. (2010). A rational model of eye movement control in reading. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1168–1178.
Boston, M. F., Hale, J., Kliegl, R., Patil, U., and Vasishth, S. (2008). Parsing costs as predictors of reading difficulty: An evaluation using the Potsdam Sentence Corpus. Journal of Eye Movement Reasearch, 2:1–12.
Boston, M. F., Hale, J. T., Vasishth, S., and Kliegl, R. (2011). Parallel processing and sentence comprehension difficulty. Language and Cognitive Processes, 26:301–349.
Brants, T. and Franz, A. (2006). Web 1T 5-gram Version 1. Linguistic Data Consortium.
Brier, G. W. (1950). Verification of forecasts expressed in terms of probability. Monthly Weather Review, 78:1–3.
Brysbaert, M. and Vitu, F. (1998). Word skipping: Implications for theories of eye movement control in reading. In Underwood, G., editor, Eye guidance in Reading and Scene Perception, pages 124–147. Elsevier science Ltd.
Carroll, P. and Slowiaczek (1986). Constraints on semantic priming in reading: A fixation time analysis. Memory and Cognition, 14:509–522.
Clifton, C., Staub, A., and Rayner, K. (2007). Eye movements in reading words and sentences. In van Gompel, R., editor, Eye movements: A window on mind and brain, pages 341–372. Amsterdam: Elsevier.
Cox, D. R. (1972). Regression models and life-tables. Journal of the Royal Statistical Society. Series B (Methodological), 34:187–220.
Demberg, V. and Keller, F. (2008). Data from eye-tracking corpora as evidence for theories of syntactic processing complexity. Cognition, 109:193–210.
Duffy, S. A., Morris, R. K., and Rayner, K. (1988). Lexical ambiguity and fixation times in reading. Journal of Memory and Language, 27:429–446.
Duffy, S. A. and Rayner, K. (1990). Eye movements and anaphor resolution: Effects of antedecent typicality and distance. Language and Speech, 33:103–119.
Ehrlich, S. F. and Rayner, K. (1981). Contextual effects on word perception and eye movements during reading. Journal of Verbal Learning and Verbal Behavior, 20:641–655.
55
Ehrlich, S. F. and Rayner, K. (1983). Pronoun assignment and semantic integration during reading: Eye movements and immediacy of processing. Journal of Verbal Learning and Verbal behavior, 22:75–87.
Engbert, R., Longtin, A., and Kliegl, R. (2002). A dynamical model of saccade generation in reading based on spatially distributed lexical processing. Vision Research, 42:621–636.
Engbert, R., Nuthmann, A., Richter, E., and Kliegl, R. (2005). SWIFT: A dynamical model of saccade generation during reading. Psychological Review, 112:777–813.
Feng, G. (2001). SHARE: A stochastic, hierarchical arcitecture for reading eye-movement. PhD thesis, University of Illinois at Urbana-Champaign.
Feng, G. (2006). Eye movements as time-series random variables: A stochastic model of eye movement control in reading. Cognitive Systems Research, 7:70–95.
Feng, G. (2009). Time course and hazard function: A distributional analysis of fixation duration in reading. Journal of Eye Movement Research, 3:1–23.
Fisher, D. F. and Shebilske, W. L. (1985). There is more that meets the eye than the eye mind assumption. In Groner, R., McConkie, G. W., and Menz, C., editors, Eye movements and human information processing. Amsterdam: Elsevier. Amsterdam: Elsevier.
Frank, S. L. (2010). Uncertainty reduction as a measure of cognitive processing effort. In Proceedings of the ACL Workshop on Cognitive Modeling and Computational Linguistics.
Frazier, L. and Rayner, K. (1982). Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences. Cognitive Psychology, 14:178–210.
Hale, J. (2001). A probabilistic early parser as a psycholinguistic model. In
Proceedings of the second conference of the North American chapter of the
Association for Computational Linguistics, volume 2, pages 159–166.
Hale, J. (2003). The information conveyed by words. Journal of Psycholinguistic
Research, 32:101–123.
Hale, J. (2006). Uncertainty about the rest of the sentence. Cognitive Science,
30:643–672.
Heinzle, J., Hepp, K., and Martin, K. A. C. (2010). A biologically realistic cortical
model of eye movement control in reading. Psychological Review, 117:808–830. Hosmer, W. D., Lemeshow, S., and May, S. (1999). Applied Survival Analysis:
Regression Modeling of Time-To-Event Data. New York: Wiley.
Inhoff, A. W. and Rayner, K. (1986). Parafoveal word processing during eye fixations
in reading: Effects of word frequency. Perception & Psychophsics, 40:431–439. Ishida, T. and Ikeda, M. (1989). Temporal properties of information extraction in
reading by a text-mask replacement technique. Journal of the Optical Society of
America A, 6:1624–1632.
Juhasz, B. J. and Rayner, K. (2003). Investigating the effects of a set of
intercorrelated variables on eye fixation durations in reading. Journal of
Experimental Psychology: Learning, Memory & Cognition, 29:1312–1318. Just, M. A. and Carpenter, P. A. (1980). A theory of reading: From eye fixations to
comprehension. Psychological Review, 87:329–354.
Kaplan, E. L. and Meier, P. (1958). Nonparametric estimation from incomplete
observations. Journal of the American Statistical Association, 53:457–481. 56
Kennedy, A. and Pynte, J. (2005). Parafoveal-on-foveal effects in normal reading. Vision Research, 45:153–168.
Legge, G. E., Klitz, T. S., and Tjan, B. S. (1997). Mr. Chips: An ideal-observer model of reading. Psychological Review, 104:524–553.
Levy, R. (2008). Expectation-based syntactic comprehension. Cognition, 29:375–419.
Luce, R. D. (1986). Response Times: Their role in inferring elementary mental organization. New York: Oxford University Press.
Marcus, M. P., Santorini, B., and Marcinkiewicz, M. A. (1993). Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330.
Marr, D. (1982). Vision: A Computational Investigation into the Human Representation and Processing of Visual Information. New York: Freeman.
McConkie, G., Kerr, P., Reddix, M., and Zola, D. (1988). Eye movement control during reading: I. The location of initial eye fixations on words. Vision Research, 28:1107–1118.
McConkie, G., Kerr, P., Reddix, M., Zola, D., and Jacobs, A. (1989). Eye movement control during reading: II. Frequency of refixating a word. Perception & Psychophysics, 46:245–253.
McConkie, G. W. and Dyre, B. P. (2000). Eye fixation durations in reading: Models of frequency distributions. In Kennedy, A., Heller, D., and Pynte, J., editors, Reading as a perceptual process, pages 683–700. Oxford: Elsevier.
McConkie, G. W., Kerr, P. W., and Dyre, B. P. (1994). What are normal eye movements during reading: Toward a mathematical description. In Ygge, J. and Lennerstrand, G., editors, Eye movements in reading: Perceptual and language processes, pages 315–327. Oxford: Elsevier.
McConkie, G. W. and Rayner, K. (1975). The span of the effective stimulus during a fixation in reading. Perception & Psychophysics, 17:578–586.
McConkie, G. W. and Rayner, K. (1976). Asymmetry of the perceptual span in reading. Bulletin of the Psychonomic Society, 8:365–368.
McDonald, S. A. (2003). Saccade target selection as a classification problem. Poster at the XIII Conference of the European Society for Cognitive Pyschology (ESCOP), Granada, Spain. September 17-20.
McDonald, S. A., Carpenter, R., and Schillcock, R. C. (2005). An anatomically-constrained, stochastic model of eye movement control in reading. Psychological Review, 112:814–840.
Pollatsek, A., Lesch, M., Morris, R. K., and Rayner, K. (1992). Phonological codes are used in integrating information across saccades in word identification and reading. Experimental Psychology: Human Perception and Performance, 18:148–162.
Radach, R. and McConkie, G. (1998). Determinants of fixation positions in reading. In Underwood, G., editor, Eye guidance in reading and scene perception, pages 77–100. Oxford, England: Elsevier.
Rayner, K. (1975). The perceptual span and peripheral cues in reading. Cognitive Psychology, 7:65–81.
Rayner, K. (1979). Eye guidance in reading: Fixation location in words. Perception, 8:21–30.
57
Rayner, K. (1998). Eye movements in reading and information processing: 20 years of research. Psychological Bulletin, 124:372–422.
Rayner, K. (2009a). Eye movements and attention in reading, scene perception, and visual search. The Quarterly Journal of Experimental Psychology, 62:1457–1506.
Rayner, K. (2009b). Eye movements in reading: Models and data. Journal of Eye Movement Research, 2:1–10.
Rayner, K. and Bertera, J. H. (1979). Reading without a fovea. Science, 206:468–469.
Rayner, K. and Duffy, S. A. (1986). Lexical complexity and fixation times in reading: Effects of word frequency, verb complexity, and lexical ambiguity. Memory & Cognition, 14:191–201.
Rayner, K. and Frazier, L. (1989). Selection mechanisms in reading lexically ambiguous words. Journal of experimental psychology: Learning, Memory, and Cognition, 15:779–790.
Rayner, K., Inhoff, A. W., Morrison, R. E., Slowiaczek, M. L., and Bertera, J. H. (1981). Masking of foveal and parafoveal vision during eye fixations in reading. Journal of Experimental Psychology: Human perception and performance, 7:167–179.
Rayner, K., Liversedge, S. P., White, S. J., and Vergilino-Perez, D. (2003). Reading disappearing text: Cognitive control of eye movements. Psychological Science, 14:385–388.
Rayner, K. and Well, A. D. (1996). Effects of contextual constraint on eye movements in reading: A further examination. Psychonomic Bulletin & Review, 3:504–509.
Rayner, K., Well, A. D., Pollatsek, A., and Bertera, J. H. (1982). The availability of useful information to the right of fixation in reading. Perception & Psychophysics, 31:537–550.
Reder, S. M. (1973). On-line monitoring of eye position signals in contingent and noncontingent paradigms. Behaviour Research Methods & Instrumentation, 5:218–228.
Reichle, E., editor (2006). Cognitive Systems Research. 7:1–96. Special issue on models of eye-movement control in reading.
Reichle, E., Pollatsek, A., Fisher, D., and Rayner, K. (1998). Toward a model of eye movement control in reading. Psychological Review, 105:125–157.
Reichle, E., Rayner, K., and Pollatsek, A. (2003). The E-Z Reader model of eye-movement control in reading: Comparisons to other models. Behavioral and Brain Sciences, 26:445–476.
Reichle, E., Warren, T., and McConnell, K. (2009). Using E-Z Reader to model the effects of higher-level language processing on eye movements during reading. Psychonomic Bulletin & Review, 16:1–21.
Reilly, R. G. and O’Regan, J. K. (1998). Eye movement control during reading: a simulation of some word-targeting strategies. Vision Research, 38:303–317.
Reilly, R. G. and Radach, R. (2006). Some empirical tests of an interactive activation model of eye movement control in reading. Cognitive Systems Research, 7:34–55.
Roark, B., Bachrach, A., Cardenas, C., and Pallier, C. (2009). Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing. In Proceedings of the Conference on Empirical
58
Methods in Natural Language Processing (EMNLP), pages 324–333. Schilling, H. E. H., Rayner, K., and Chumbley, J. I. (1998). Comparing naming,
lexical decision, and eye fixation times: Word frequency effects and individual
differences. Memory & Cognition, 26:1270–1281.
Schoenfeld, D. (1982). Partial residuals for the proportional hazards model.
Biometrika, 69:51–55.
Sereno, S. C., O’Donnell, P. J., and Rayner, K. (2006). Eye movements and lexical
ambiguity resolution: Investigating the subordinate bias effect. Journal of
Experimental Psychology: Human Perception and Performance, 32:335–350. Sereno, S. C. and Rayner, K. (1992). Fast priming during eye fixations in reading.
Journal of Experimental Psychology: Human Perception and Performance,
18:173–184.
Taylor, W. (1953). Cloze procedure: a new tool for measuring readability.
Journalism Quarterly, 30:415–433.
Van Zandt, T. (2002). Analysis of response time distributions. In Wixted., J. T. and
Pashler, H., editors, Stevens’ Handbook of Experimental Psychology (3rd Edition), Volume 4: Methodology in Experimental Psychology, pages 461–516. New York: Wiley.
Vitu, F. and McConkie, G. W. (2000). Regressive saccades and word perception in adult reading. In Kennedy, A., Radach, R., Heller, D., and Pynte, J., editors, Reading as a perceptual process, pages 301–326. Oxford: Elsevier.
Vitu, F., McConkie, G. W., and Zola, D. (1998). About regressive saccades in reading and their relation to word identification. In Underwood, G., editor, Eye Guidance in Reading and Scene Perception. Oxford: Elsevier.
Witten, I. H. and Eibe, F. (2005). Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann.
Yang, S. N. (2006). An oculomotor-based model of eye movements in reading: The competition/interaction model. Cognitive Systems Research, 7:56–69.
Yang, S. N. and McConkie, G. W. (2001). Eye movements during reading: A theory of saccade initiation times. Vision Research, 41:3567–3585.
Yang, S. N. and McConkie, G. W. (2005). New directions in theories of eye-movement control during reading. In Underwood, G., editor, Cognitive processes in eye guidance, pages 105–130. Oxford University Press.
                                                                            Designing Interaction and Visualization for Exploration of System Monitoring Data
A design-oriented research study on exploring new ways of designing useful visualizations and interaction for system monitoring data using web technologies
JONATAN DAHL
Master’s Thesis at CSC Supervisor: Anders Lundström Examiner: Kristina Höök
Company: Spotify AB Supervisor: Niklas Ek
2014-06-23
Abstract
System monitoring is a practice that is frequent within companies providing digital products to consumers and is a common way to help developers contribute to a good end- user experience by ensuring a high availability and good performance of the product.
This thesis is a design-driven exploratory study on de- signing interaction and visualization for system monitoring data, using web technologies. The design space spans over interaction design and technical domains, exploring system monitoring data interaction and visualization from an HCI perspective as well as technical possibilities and limitations of the web platform.
An artifact embodying new ideas and design visions re- garding the topic is created in close collaboration with the target users. The artifact expresses possible and poten- tially valuable inventions regarding exploration of system monitoring data. It also emphasizes the close relationship between system monitoring and physical space and how the interaction with it can provide a useful sense of place to the data.
Technical insights and good practices regarding devel- oping performant data visualization user interfaces is also presented and motivated, where two methods providing dif- ferent strengths and weaknesses are described.
Contents
1 Introduction 1
1.1 Purposeandgoal............................. 2 1.2 Problemdefinition ............................ 2 1.3 Scopeanddelimitations ......................... 2 1.4 Definitions................................. 3
2 Background 5
2.1 SystemMonitoring............................ 5 2.2 SoftwareReliabilityEngineering-SRE. . . . . . . . . . . . . . . . . 6 2.3 AboutSpotify............................... 6
2.3.1 SystemMonitoring........................ 6
3 Theory 9
3.1 DataandInformationVisualization................... 9
3.2 ExploringData.............................. 10 3.2.1 Dataexplorationtechniques................... 12
3.3 Researchanddesignmethods ...................... 15 3.3.1 Researchthroughdesign..................... 15 3.3.2 Rapidprototyping ........................ 17
3.4 SpaceandplaceinHCI ......................... 17
4 Method 19
4.1 Researchanddesignprocess....................... 19 4.1.1 Currentandpreferredstate ................... 19 4.2 Targetusers................................ 20
5 Pre-study 21
5.1 Introduction................................ 21
5.2 Method .................................. 21 5.2.1 Brainstormingsessions...................... 22
5.3 Results................................... 22
5.4 DiscussionandConclusions ....................... 23
6 Design phase 25
6.1 Introduction................................ 25 6.2 Method .................................. 25 6.3 Results................................... 27
6.3.1 DescriptionoftheFinalPrototype . . . . . . . . . . . . . . . 28 6.3.2 Technologiesused ........................ 29 6.3.3 Valueandusefulness....................... 33
6.4 Discussionandconclusions........................ 33 6.4.1 Process .............................. 33 6.4.2 Asenseofplace ......................... 34
7 Discussion 35
7.1 Researchanddesignprocess....................... 35 7.2 Dataandplaceinsystemmonitoring.................. 36 7.3 Technicalevaluation ........................... 36
7.3.1 Processingdatainthebrowser ................. 37 7.3.2 Renderinginthebrowser .................... 37 7.3.3 Anaccessibleplatform...................... 38
7.4 Extensions................................. 38
8 Conclusions 41 References 43
Chapter 1 Introduction
There are many important factors that a ect the success of a digital product and one of them, that has gotten more and more important over the last years, is the user experience. Furthermore, there are a great deal of factors that influence the user experience, and one of those factors is the performance of the product. For example, an unusually long delay of the response to an action performed by a user is likely to a ect the user experience negatively.
One way to help ensure a good performance of a digital product is to monitor it in di erent ways. By monitoring it, developers can obtain important information that helps them keep the product performing well. This can be done using tools that collect and visualize di erent metrics in the product, usually related to software or hardware performance. Alerting developers in times of a severe software or hardware failure is also common.
Spotify has a large amount of services (software) and servers (hardware) that make up the internal infrastructure of their music streaming service. A service is a piece of software that usually handles the logic behind a specific feature, such as playing songs, searching for a song or handling playlists. A server is a piece of hardware on which these services run. One service can be run on multiple servers for increased performance and capacity.
The concept of monitoring services and servers like this is referred to as System Monitoring (IBM, 2013). At Spotify, there is an elaborate internal infrastructure built for this. There is software, or tools, used by certain developers to collect, store and visualize di erent sorts of data from these services and servers to help them gain valuable information about how they are performing, which enables them to find and solve potential or existing problems. This, in turn, helps them ensure a high availability and good performance of the Spotify services, thus contributing positively to the resulting end-user experience of the Spotify product.
System monitoring can hence be summarized as being a toolbox to help devel- opers ensure a high availability and good performance of a digital product, and this design-driven thesis investigates new ways for developers at Spotify AB to explore system monitoring data.
1
1.1 Purpose and goal
Within the context described in the previous section, the purpose of this thesis is to explore visualization and interaction with large volumes of system monitoring data, having the developers at Spotify who are using monitoring tools as the target audience.
The goal is to produce a design artifact (Zimmerman, Forlizzi, & Evenson, 2007), to create new knowledge about what is possible to do regarding data explo- ration within system monitoring at Spotify AB. A second aim is to gain insights about technical limitations and opportunities given the choice of technologies and platform.
1.2 Problem definition
It can be di cult to find the right information quickly in large volumes of data, especially if the type of information sought for is unknown. If it is known, tools can be specifically designed to help the user extract that type of information from the data. However, if it’s unknown, a better approach might be to design a tool in a way that lets the user explore the data more freely and thus have a better chance of finding useful information within it (Keim, 2002). How a tool like this should be designed is not completely obvious though. The problem is under-constrained, and di erent designs needs to be explored and tested to come up with useful solutions to it.
1.3 Scope and delimitations
The produced design artifact is a functional web application prototype that aims to lead to discoveries of new information regarding the subject of this thesis, and not to be released as a polished product for widespread use at Spotify AB.
The application will be using modern web technologies and run in modern web browsers but there will be no evaluation of cross-browser compatibilities or impli- cations using legacy web browsers or technology.
Huge volumes of data being tracked and stored within companies today is often referred to as big data, being defined as "Big data is a term describing the storage and analysis of large and or complex data sets using a series of techniques including, but not limited to: NoSQL, MapReduce and machine learning." (Ward & Barker, 2001). In my personal experience, big data is often associated with business intelligence analytics with the purpose of creating valuable insights for strategic decision-making processes. Though technologies like MapReduce and machine learning occur within system monitoring practices, it’s not within the scope of this thesis and will not be a part of the project. This thesis focuses on the HCI aspects of exploring data and not the computational side.
2
1.4 Definitions
Spotify service
The Spotify service refers to the product that is exposed to the end-users; the desktop or mobile client where private individuals may log in using their Spotify account and play music.
Spotify services
The Spotify services are the internal software that together make up the Spo- tify service; the software that runs on one or multiple servers in the Spotify back-end, that handles logic such as authenticating users or delivering songs to the clients.
Spotify
Spotify or Spotify AB refers to the company and not the product.
3
Chapter 2 Background
This chapter provides explanations of central concepts and definitions required to fully understand all parts of the report. Terms like System Monitoring and Software Reliability are explained, both in general and in the context of Spotify.
2.1 System Monitoring
Digital products with millions of active users, like Spotifys music streaming service, can have a large and complex back-end1 consisting of a huge amount of servers with numerous pieces of software running on them that communicate with end-users and each other. Monitoring all of those servers and systems closely helps developers to maintain them e ciently and have the product perform well, contributing positively to the end-user experience.
“ System Monitoring refers to collecting key system performance metrics at periodic intervals over time ” (IBM, 2013)
Usually, monitored metrics range from low-level metrics like CPU2 performance, memory management or read-write operations on the disk to high level metrics such as the amount of connected users, number of failed user logins or number of searches performed by a user, etc. Metrics like these can be visualized as time series, to show their change in value over time, or trigger alerts if their value falls below or exceeds a certain threshold. Alerts are meant to instantly notify developers when anything that could critically a ect the products performance or availability occurs, for ex- ample issues that could have economical consequences or have seriously negative impact on the end-user experience. The alerted developers are then responsible for resolving the issue as quickly as possible.
1The internal infrastructure of hardware and software that a digital product consists of 2Central Processing Unit: handles most of the calculations in a computer
5

2.2 Software Reliability Engineering - SRE
Within technology departments there are often certain developers whose main re- sponsibility is to ensure that the service that the company provides is available to the end-users. These developers are often frequent users of system monitoring tools. It’s important that they have access to good tools that help them find the right information quickly so that they can find and fix problems that might a ect the availability of the service. They’re usually referred to as Software Reliability Engineers (Geraci, 1991).
"On-call" A developer who is on-call will be instantly notified when an alert is triggered. They are usually on-call for a specified period of time. Being on-call means that they might be awakened at any time of the day, including in the middle of the night. This type of alerts should only be triggered when something really severe occurs.
2.3 About Spotify
Spotify is a Swedish company providing a music streaming service to private indi- viduals. Spotify was founded in 2006 and launched in 2008 and has since grown to a global service having over 10 million subscribers, over 40 million active users and acting in over 56 markets in the world (Spotify, 2014).
The mission Spotify has is to change the way people listen to and access music, by changing the way the music industry and its licensing has worked in the past. What used to be stored on physical media and could only be acquired in record stores can now be accessed instantly from anywhere in the world over the Internet, using the Spotify service.
The company has grown a lot since 2006 and so has the complexity of its product, which increases the need for e ective system monitoring solutions.
2.3.1 System Monitoring
At Spotify, there is a team of developers responsible for providing an infrastructure of system monitoring tools and services. They have developed a monitoring pipeline that collects metrics, stores them and allows for retrieval of them in visualization interfaces. The target users for the monitoring team are other developers at Spotify, especially Software Reliability Engineers.
Exactly what metrics are being collected by these monitoring tools are decided by their users. They track the metrics they want - on the servers and software they work with - and send them into the monitoring pipeline for storage and then retrieval in the visualization interfaces, or monitoring interfaces, provided by the monitoring team.
In other words, the monitoring team maintains an infrastructure that enables other developers to easily track metrics that are important to them, and visualize
6
them in web based user interfaces. These interfaces often consist of line graphs displaying time series, and o er interaction to allow for some data exploration. The monitoring team constantly works on improving the monitoring infrastructure and the visualization tools. This thesis explores new ways of interacting with and visual- izing data in tools like these. The designs being explored are independent from the existing visualization tools but depends on the back-end monitoring infrastructure, i.e. the existing data collection and storage solutions.
7
Chapter 3 Theory
This chapter describes theory behind data visualization, data exploration and interaction techniques that have been used in the development of the pro- totype. The ideas behind the design-oriented research method used for this thesis is also explained.
3.1 Data and Information Visualization
The purpose of visualizing data is to help us gain insight, to acquire new information and knowledge that we wouldn’t be able to acquire by observing raw data. By looking at diagrams or charts we create mental interpretations and draw conclusions from the visualized data. The visualized data in itself is not telling us anything directly, but with the help of how it is represented visually, we are able to gain insights from it (Spence, 2007). In this way, how data is visualized will probably a ect our ability to draw conclusions from it, and that’s one of the areas to be explored by this thesis.
Spence (2007) further proves the point that the result of observing charts and diagrams is created entirely in the mind of the viewer by demonstrating a few examples of data visualizations. Today, it might be easy to assume that data visualizations are results of calculations done by computers. However, computers are rather a tool that makes it easier to create the visualizations. Figure 3.1 is a classic example within data and information visualization history. It’s the french engineer Charles Joseph Minards visualization of Napoleons march to, and retreat from, Moscow. The visualization is quite self-explanatory and gives the viewer a lot of information about the expedition without having to spend a considerably higher amount of time to read about it in a book, or similar. And it’s easy to remember.
Another interesting example is one where a cholera epidemic in 1800’s London was e ectively stopped thanks to insight gained from correlating data, which was made possible as a result of the way the data was visualized. Figure 3.2 shows a map over the Soho district in London. By establishing that the deaths from cholera were clustered around one of the water pumps, which was made possible with the
9
 Figure 3.1. Minard’s map of Napoleons march to, and retreat from, Moscow. (Tufte, 1983)
visualization, the epidemic could be stopped.
The process of visualizing data can be split into 4 phases. (1) capture the data,
(2) prepare the data, (3) process the data and (4) visualize the data (Schroeder & Noy, 2001). The first step happens in the start of the monitoring pipeline at Spotify, where developers set up what metrics to track and send them into the monitoring pipeline for storage. Step 2 and 3 is partially handled by the monitoring infrastructure when retrieving the data from a monitoring interface. This project focuses on the visualization step and to some extent on the processing step.
Thanks to the advancement of digital technology we are now able to explore data and gain information using methods that weren’t available before the information technology era. It’s nowadays possible to interact with data in real time to increase our ability to gain insights from it, using exploration techniques such as filtering, linking and brushing that I will explain in the following sections.
3.2 Exploring Data
The amount of data tracked and stored at companies today, including Spotify, can be vastly large. The bigger the volume of data, the more complex it may be to interpret; to find valuable information within it. If it’s hard to understand the data, calculating complex algorithms - for generating insights and valuable information - might not be such a good approach. Instead, it’s could be more valuable to design solutions that allows humans to explore the data and use their perceptive skills to mine information from it (Keim, 2002).
The process of exploring data is, according to Keim (2002), also a hypothesis generating process. The exploration leads to insights that change the hypothesis
10
 Figure 3.2. Map of Londons Soho district in 1845 showing locations of cholera deaths and water pumps. (Tufte, 1983)
11
iteratively, or create new hypotheses. He also states that human exploration of visualized data often leads to a higher confidence in the validity of the findings.
Shneiderman (1996) phrased the "Information Seeking Mantra" as: Overview first, zoom and filter, and then details-on-demand. When exploring data, the user should be first presented with an overview of all the data. The user can then, using interactive tools, zoom the visualizations and filter out irrelevant data to find interesting patterns. When the user has succeeded in filtering out a subset of the data that is the most interesting to their current hypothesis or exploration goal, the user can request additional details about this data.
3.2.1 Data exploration techniques
Simple and low dimensional data can often be easily visualized and explored using basic line graphs, x-y plots and histograms. However, more complex data with many dimensions may need di erent approaches to be able to be e ciently explored by a human user. Keim (2002) classifies data exploration techniques for complex and large data sets based on three criteria:
(1) The data to be visualized
The dimensionality in data is decided by the number of variables to be found. For example, comparing di erent car models could mean that each car has variables like mileage, horse powers, weight, construction year and more. This data is con- sidered multidimensional and requires other types of visualization and interaction techniques to be able to be e ectively explored.
Some data may be of the relational type where di erent pieces of the data, called "nodes", have a relationship to one or more other nodes. Examples are e-mail conversations among people or the file structure of a hard disk.
(2) The visualization technique
In addition to 2D and 3D visualizations such as line graphs, bar charts and x-y-z plots, there are some other visualization techniques that can be used to make sense of complex data that is of higher dimensionality or relational/hierarchical.
When visualizing and exploring high dimensional data, parallel coordinates is a good solution (Figure 3.4) for better understanding the data or to find interesting variable correlations or patterns. The principle of this technique is to display all the dimensions (variables) parallel to each other and have lines connect all variables that belongs to the same entity. As in the car example mentioned earlier, each line would be a car.
For hierarchical data, the treemap is a useful visualization solution to allow for exploration of the data. The treemap is a rectangular area subdivided by every level of the hierarchy, where each child node are grouped inside their parent node. The treemap was created by Ben Shneiderman (Shneiderman, 1992).
12
 Figure 3.3. Treemap for hierarchical data. (JuiceAnalytics, 2009)
To give further meaning to the visualized data and communicate a certain di- mension, di erent encodings such as color, position or size can be applied (Iliinsky & Steele, 2011).
(3) The interaction and distortion technique
Interaction is a crucial component of data exploration. Not only does it provide the steps necessary for the "Information Seeking Mantra" mentioned in the previous section, but it fills the gap between them (Keim, 2002). Interaction allows the users to actively explore the data by manipulating the visualizations in real time, allowing for finding relations and correlations within the data. There are a couple of commonly used interaction techniques for exploring data. Among other, there are brushing, linking, zooming and filtering:
Brushing
Brushing is to change encoding in one or more items as response to a user interaction with another element (Spence, 2007). In figure 3.5 it’s shown by changing the non-selected items into a gray color, to make them appear dimmed and help the user focus on the chosen set of elements.
13
 Figure 3.4. Parallel coordinates for multidimensional data. (ggobi.org, 2013)
Linking
A useful way of making sense of large multidimensional data sets is to split them up into multiple visualizations, that each represents fewer dimensions, and link those together. The linking is an interaction method where a users action in one component is reflected in another, in a way that is meaningful for the user. An example is highlighting a subset of the data in one component that will highlight the same data in the other components (Spence, 2007). Figure 3.5 demonstrates this.
Zooming
When dealing with large amounts of data, zooming is a good way of being able to both get an overview of the data and at the same time being able to drill down do explore the details on a low level.
Filtering
Similar to above, when exploring large volumes of data it’s important to be able to partition it and investigate interesting subsets of the data. One way of doing this is by using the brushing technique to allow the user to filter the data in real time.
14
 Figure 3.5. Brushing and linking 3.3 Research and design methods
Given the exploratory and design-oriented topic of this thesis, a methodology using a model for design research is applied. The execution relies on a combination of the design-oriented research model and rapid prototyping.
3.3.1 Research through design
The term design has previously been more common in HCI practice than in HCI research, and has often been associated with usability engineering; the process of modeling requirements and specifications for user needs and shaping the product accordingly (Zimmerman et al., 2007). In recent years, design has found its way into
15
HCI research and Zimmerman et al. (2007) has developed a model describing and motivating the role of design within HCI research and how to evaluate its outcomes. In this model, they highlight some of the strengths of interaction designers way of working and how that can contribute to both HCI research and HCI practice. One of the biggest strengths of designers is their ability to tackle under-constrained, or "wicked", problems - complex problems with conflicting ideal outcomes, opposing stakeholder goals and many other unknown or complicated variables that makes it hard to define one optimal solution (Rittel & Webber, 1973). The way designers do this is by continuously re-framing the problem by iteratively creating and cri- tiquing design artifacts that aims to transform the world from its current state to a preferred state (Zimmerman et al., 2007), eventually proposing a solution to the
under-constrained problem.
They describe 5 ways interaction designers contribute to HCI research and prac-
tice: First, in their process of finding a preferred state they create opportunities and motivation for research engineers to develop new technologies that hasn’t pre- viously existed. Second, the created artifacts embodies their ideas, which facilitates the communication to the HCI practice community that can bring them to life in commercial products. Third, they facilitate for the HCI research community to engage in under-constrained, "wicked", problems that are di cult to tackle using traditional methods. Fourth, interaction designers make research contributions us- ing their biggest strength; re-framing problems by iteratively trying to design the right thing. Fifth, they motivate the HCI community to discuss the impacts the design artifacts and ideas might have on the world.
Evaluation criteria
Since the design oriented way of performing research di ers from traditional ways within HCI, so does the evaluation of the outcomes. Zimmerman et al. (2007) outlines the evaluation criteria as follows:
Process
In interaction design research, there is no expectation that repeating the pro- cess will yield the exact same results. Instead, the process will be judged as a part of the whole research contribution and needs to be thoroughly described and motivate all rationales and decisions.
Invention
The result of an interaction design research project must be something new, that contributes something novel back to the HCI research community. To demonstrate this, a literature review must be made to situate the result and prove that it’s an advancement of existing knowledge. By elaborately articu- lating the invention, the details about the invention is communicated to the HCI community, guiding them on what to build. This directly relates to the second type of contribution that interaction design researchers can provide to the research community mentioned earlier.
16
Relevance
In traditional research methods, validity is one form of evaluation of the re- sults. This is typically done by either benchmarking the performance increase of the new solution or by disproving the null hypothesis.
In the research through design approach, this is not always applicable. There is no way of proving that the solution is the right one. Two designers following the exact same method are highly unlikely to come up with the same results.
Instead, the relevance is evaluated. Relevance is positioning the contribution in the real world and evaluating its impact. For example, is this useful? Does it transform the world to a better state? The "better", or "preferred", state must also be elaborately articulated and there needs to be motivations on why it is better.
Extensibility
3.3.2
The final criterion for evaluating an interaction design research contribution requires that the community are able to build upon the outcomes. Either by applying the methods used on future research or by leveraging the knowl- edge produced by the resulting artifacts. The research needs to have been documented well so that this is possible.
Rapid prototyping
Research through design might be considered to implicitly include rapid prototyping methodologies, but to further clarify the principles and benefits of the method, it’s here described in more detail.
When rapidly prototyping a software, the development process is done iteratively and in close collaboration with the target users. Features are delivered incrementally and the requirements change over the course of the development process according to the continuous feedback gathered from the users (Luqi, 1989).
The motivation for the use of this method is the lack of exact requirements and knowledge about the artifact that was going to be produced. Rapid prototyping allows for exploration of requirements and features iteratively and minimizes the risk of errors or bad design decisions thanks to the close involvement of the users.
3.4 Space and place in HCI
The later stages of the project lead to the discovery of a connection to some concepts that exists within interaction design, the notion of space and place. The metaphor of space has been used a long time by interaction designers to design user friendly systems (Harrison & Dourish, 1996). A typical example is the desktop metaphor when designing user interface for computer operating systems (such as Windows, Linux and OS X).
The concept of place shares properties with space but is also di erent from it in a couple of ways. While a space is a three dimensional physical room, a place is all
17
that but with some additional properties. Place is defined by how it relates to its context and surroundings and by how we adapt and appropriate the space it exists in. A house is a space but my house is a place.
The point that Harrison and Dourish (1997) make is that it is actually the notion of place that frames our behavior when interacting with systems, especially the ones providing collaborative features, and not space, as previously believed. While the application created in this project is not collaborative in the same sense as the systems they are describing, it’s however interesting to view how system monitoring is closely tied to physical space and how interacting with this application gives these physical spaces a sense of place.
18
Chapter 4 Method
This thesis follows a design-oriented research methodology and is initiated with a theroetical enquiry on relevant theory for this project that is combined with a contextual pre-study followed by a design phase, which are described in detail in their own separate chapters following this chapter. The design driven approach of the thesis follows the model of Zimmerman et al. (2007) of how interaction design contributes to human-computer interaction research, as described in their concept of Research Through Design.
4.1 Research and design process
The project starts with a pre-study to map out the existing system monitoring tools at Spotify to understand what their purpose are and how they are being used by the developers. A study is also made on the practice of system monitoring in general and why it’s important for companies with digital products of this size. This is combined with an inquiry about data visualization with an emphasize on data exploration. Guidelines and good practice for how to design interaction for data exploration and how to visualize data are investigated. The pre-study also results in possible starting points for the development av a functional prototype.
This is followed by a design phase where a design artifact is produced using ideas and knowledge generated from the pre-study. The artifact is rapidly prototyped and iteratively tested on the target users. The prototyping process does not include lo-fi and hi-fi sketches but instead real development of a web application directly from the start, the motivation for this being the requirement of using live monitoring data for the purpose of design exploration.
4.1.1 Current and preferred state
In accordance with the Research Through Design method this thesis applies, the current and preferred state of the world must be articulated and motivated so that
19
it is possible to determine the relevance of the design artifact, the purpose of the artifact being to shift the state from current towards the preferred.
Current state
At Spotify AB, there are some ways of visualizing and interacting with system monitoring data.
Preferred state
At Spotify AB, there are new and useful ways to visually explore and interact with system monitoring data.
4.2 Target users
The main users of the monitoring tools at Spotify are the so called Software Re- liability Engineers, which makes them natural to have as target audience for this project as well.
The fact that this audience possesses deep knowledge about the system moni- toring and back-end architecture of Spotify influences the design of the prototype as certain assumptions can be made regarding how they will perceive elements of the interface and how they will interact with it. There is no need for extensive ex- planations on what data is available and how to interpret, for example, time series that are generated from it, as they are already used to working with tools providing that type of presentation.
20
Chapter 5 Pre-study
5.1 Introduction
Given the open ended nature of this thesis, the project started with brainstorming sessions to explore interesting challenges and opportunities regarding designing for system monitoring. 1
The mission of the monitoring team is to enable developers to quickly identify and fix technical issues, which is why that was made the goal of this project as well. This is especially important in times of serious technical issues that might have a severe impact on the user experience of the Spotify service, such as user being completely unable to play music, that could lead to economical consequences for the company.
The aim of the pre-study was to explore and define the current situation regard- ing system monitoring at Spotify AB; to dive and look into how things work, what tools exist, how people work and what the monitoring infrastructure looked like, and eventually find good starting approaches to the project.
5.2 Method
Three meetings were held during the first few weeks, where I and three developers brainstormed interesting problems and challenges related to system monitoring. A goal with these meetings was to understand was system monitoring really meant and what possible opportunities there where for proposing new and useful design ideas.
I also reviewed and tested the existing system monitoring tools being used within Spotify AB to help me understand basic concepts about system monitoring and how the developers use these tools.
 1Service Reliability Engineers
21
5.2.1 Brainstorming sessions
The participants in the brainstorming sessions were three developers at Spotify AB:
• Developer 1: Front-end web developer in the monitoring team. Designs and builds interfaces for the monitoring tools. Has been at Spotify for 1 year
• Developer 2: Back-end developer and team leader for the monitoring team. Has been at Spotify for 3 years.
• Developer 3: Web developer and team leader for all front-end web develop- ers. Has been at Spotify for 1 year.
5.3 Results
By studying the existing monitoring applications I learned how developers used them and what the objective of using those tools were. Typical use cases are: investigating ongoing technical issues, post-mortems and capacity planning. Post- mortem means to investigate a technical issue after it’s been resolved, to determine what happened, why, and what knowledge can be gained from it to know how to prevent it from happening again in the future. Capacity planning is to ensure the systems have su cient memory and processing power, etc, and plan for upgrades.
The brainstorming sessions generated three major ideas on possible design ex- plorations, all of which would provide features missing from the current monitoring solutions:
(1) Sharable graph dashboards
The first idea that emerged from the brainstorming sessions was to design an application where developers could create their own dashboard of monitoring graphs2. These dashboards would then be able to be shared among other de- velopers. The value of this would be that developers would create dashboards for certain troubleshooting cases, i.e. for a specific type of problem that might occur not only once. Then other developers would be able to more quickly troubleshoot a problem by reusing an existing dashboard that someone else created for a similar problem, and thus be able to e ciently solve the problem.
(2) Graph annotations
A second idea was to translate real life interaction into digital by observing how developers interact with and discuss monitoring graphs in person and then design a solution that would allow the same type of interaction digitally. This can be described as a distributed digital model of real life interaction that would not only potentially enable larger scale collaboration between de- velopers but also persistent storage of the communication.
2A collection of visualization were each visualization would show relevant system monitoring data
22

(3) Health overview of services
The third idea was to design a solution that would enable developers to more e ortlessly get an overview of the overall status of all the services in the Spotify infrastructure. This solution would rely on defining one or a few specific metrics that would define the "health" of a service; whether it’s functioning normally or not. This would provide developers with insights regarding the overall "health status" of the entire infrastructure and could possible lead them to earlier discoveries of potential system failures.
The last idea gained most interest among the developers and was decided to be the starting point for the project.
5.4 Discussion and Conclusions
The brainstorming sessions and the review of existing tools provided the knowledge required to be able to start exploring useful design solutions for data visualization and interaction. By knowing how the tools worked and how the developers used them, along with what the developers considered was missing from the current solutions, the project was able to start transitioning into the design phase and the actual creation of a design artifact.
Commonly across all the ideas were the challenges of providing developers with the right information at the right time, and enabling that to happen during a short amount of time. This is especially challenging when the type of information sought for is unknown, which supports the idea of designing a solution that allows devel- opers to explore through a "top-down" approach, as suggested in design proposition 3.
23
Chapter 6 Design phase
6.1 Introduction
Following the pre-study, the development of the prototype started with the initial idea to provide a service health overview that would help developers to more quickly identify potential system failures, or knowing where they had occurred. However, this eventually proved to have some di culties tied to it and the prototype design changed direction. The new form was a data exploration tool where information could be correlated to the physical location of their sources. This also opened up for the exploration of the HCI concepts space and place and their representation in the prototype.
6.2 Method
The prototype creation required quick development of a functional application that could be directly tested on the target users, as well as decisions about what the data visualization process would look like from a technical point of view. No analogue lo-fi or hi-fi sketch prototypes were made, the digital application logic was developed directly from the start. The reason for this was the benefit of being able to use the system monitoring data instantly and have the interaction and visualization with it tested on developers from the beginning.
As part of the rapid prototyping, informal and spontaneous user testing was performed continuously to gather feedback to help making new design decisions. These user sessions consisted of demonstrating for, in total, 5 service reliability engineers, 2 monitoring engineers and 5 web developers the latest progress of the prototype and gather their feedback on what features that could potentially improve its usefulness.
25
 Figure 6.1. Server and services overview (treemaps). Visualizing size hierarchy between di erent grouping of servers and services.
26
6.3 Results
The first version is the visualization of the back-end infrastructure in a summarized overview, the interface of this version can be seen in figure 6.1. It comprises three treemaps showing di erent hierarchical groupings of the servers and services of the Spotify back-end, where the size is measured in number of servers they consist of. The topmost treemap shows the size relationship of the data centers1 across the world. The middle treemap shows the size relationship of the pods, which are sub groups in each data center, and the treemap at the bottom shows the size of the internal services.
The method behind defining service health was to pick one or a few key metrics that could be considered to hold all information about how any service is performing. Since all the di erent services function very di erently, its health is also defined di erently. There was, however, one metric that was believed to be enough for defining the health of any service. That was latency2. If the latency would heavily diverge from a "normal" value, it would signal an anomaly that would represent "bad health" for the service.
However, I and the target users soon came to the conclusion that it could still vary heavily from service to service, which would make it more di cult to create one definition of service health for all services. The same amount of latency divergence could have di erent amount of impact on di erent services. We also realized that the action of designing a tool that, by itself, judges the data - i.e. decides whether the data is represented something "good" or "bad" - puts an immense responsibility on the designer of the product. There were also speculations about the possibility to involve machine learning in this design to programmatically compute and tweak di erent threshold values for this metric, or for the choice of metric, but it was soon discarded as being too out of scope for this project. This lead to the decision of going for a more objective design that instead would allow for the user to judge the data and for the application to present the data unbiased.
The first manifestation of this new direction of the prototype is shown in figure 6.2. It consists of a line graph (top) that is able to show a system performance metric over the period of one week, starting from now on the right hand side and spanning to one week ago on the left hand side. Below it is a treemap showing all the servers, their geographical placement and their internal groupings at each location. The interaction consists of users being able to query any metric stored in the monitoring pipeline3 from a text input field and then interact with it by using the mouse to draw a rectangular selection in the line graph that is reflected in the treemap.
During one of the informal user testing sessions, facts about a previously, for me, unknown data source in the Spotify back-end that held more information about the servers appeared. This new data source made it possible to add another grouping
1A collection of servers in a physical location
2The time interval between a request and response of an action in a service 3See section 2.3.1
27

 Figure 6.2. Final prototype, early version. A line graph (above) and a treemap (below) displaying a system performance metric (a time series spanning one week) and the physical grouping and geographical placement of the data sources.
dimension to the treemap. The new treemap also shows how the servers are grouped in racks which heightens the resolution of the physical placement information (see figure 6.3).
6.3.1 Description of the Final Prototype
The final version can be summarized as an objective and exploratory tool where the users can explore the system performance data of the Spotify back-end as time series, and correlate that data to the physical location of their sources. The final user interface is shown in figure 6.3, it consists of two visualizations that are interactively linked together by the input of the user: a line graph and a treemap. Details about the final prototype are explained in the next section.
Line graph
The line graphs retrieves temporal system monitoring performance data from a data source that exists within the monitoring back-end at Spotify. The data it retrieves is processed by the application and then rendered in the web browser onto the screen. The processing consists of transforming the incoming
28
data structure to a form that is compliant with the functions that calculates the screen coordinates of the data points that make up each line.
After some experimenting with di erent methods of rendering large amounts of lines in the browser the canvas-method was chosen, which is more perfor- mant than the svg-method for a large volume of data points (see section 7.3.2 for an explanation of the two methods).
The line graph shown in figure 6.3 is showing user activity over one week. The peaks and dips signifies day and night cycles. Di erent groupings of lines can be observed in the line graph, and some of them are o  phase with others. This signifies their ties to di erent physical locations, which connects them with di erent time zones.
Treemap
The treemap visualizes all the servers in the Spotify back-end, the way they are grouped, the relationships of the groups and their geographical and physical placement. It retrieves its data from a source within the Spotify back-end, processes it and renders it in the browser. The processing of the data includes transforming the structure of the incoming data to a hierarchical structure that is appropriate for the functions that calculates the sizes and positions of the rectangles that make up the visualization.
Interaction
As previously established, interactivity is a fundamental criteria when designing for data exploration. The interactivity in the final prototype consists of a real time linking of selections in both visualizations. The user can brush (select) a subset of the data in the line graph and the sources of the selected data will be highlighted in the treemap (figure 6.4 and 6.5). This allows for exploration of interesting patterns in the line graphs and their correlation to physical grouping and geographical placement of their data sources. For example, selecting a peak in one group of lines and then selecting a peak in another group of lines that is o  phase with the first one would inform the user that the first group of lines belong to servers and services located in Europe while the other are located in North or South America.
Following the "Information Seeking Mantra", the treemap o ers details-on-demand when the user hovers the mouse cursor over a specific server (figure 6.6). That ac- tion provides the user with information about the name of the server and groups it belongs to, which the user can use in other monitoring tools to gain more knowledge about that server and the situation being explored.
6.3.2 Technologies used
This prototype was built using web technologies, these included HTML5 (W3C, 2014a), CSS3 (W3C, 2014b) and JavaScript (ECMA International, 2011). To sup-
29
 Figure 6.3. Final prototype. A line graph (above) and a treemap (below) displaying a system performance metric (a time series spanning one week) and the physical grouping and geographical placement of the data sources.
 Figure 6.4. Final prototype. The selection made in the line graph is linked and mirrored in the treemap
30
 Figure 6.5. Final prototype. Selection in the line graph.
port the creation of this static web application, two JavaScript frameworks were
used: d3.js (d3.js, 2013) and Angular.js (angular.js, 2014). Application and Platform
The prototype is developed as a static web application hosted internally at Spotify. A static web application is a website that is not using a server for computations. The application is downloaded in its entirety and run solely in the web browser. The reason for this architecture is that all functionality needed for the server-side (back-end) already exists internally at Spotify. That includes di erent web servers providing APIs from which it is possible to retrieve di erent kinds of monitoring data. The prototype uses those APIs to retrieve its data for the visualizations.
Angular.js Angular.js is a popular JavaScript framework maintained by Google to create application logic, such as navigation between di erent views and handling data transfer between back-end APIs and the visualizations.
The reason for the choice of this framework was it’s solid and structured ap- plication logic functionality and that it’s well established within the static web application community.
31
 Figure 6.6. Final prototype. Tooltip on mouseover displaying detailed information about a server. "Details-on-demand"
Visualizations
The visualizations are built using a combination of JavaScript, HTML5 and CSS3. JavaScript for computations and processing of the data retrieved from the monitor- ing back-end (See 2.3.1), and the overall application logic regarding visualizations. HTML5 for structure and CSS3 for presentational aesthetics. The use of these tech- nologies allows for rich interactivity of the visualizations, which is the fundamental base for exploring data.
D3.js D3.js is a powerful JavaScript framework to craft custom data visualiza- tions. There are other frameworks that come shipped with finished components such as line graphs, bar charts and other standard types of diagrams, but d3 is a set of tools and functions to allow developers to design their own custom charts and diagrams, which can range from simple line graphs to more advanced multi- dimensional visualizations.
This was chosen as a visualization framework because of its flexibility in allow- ing developers to create custom visualizations. A framework like this is ideal for exploratory design-oriented projects.
32
6.3.3 Value and usefulness
The goal throughout the process has been to create something useful for the target users, and especially exploring what could be useful, given the current state of the design artifact but also what it could become, which is one important factor to consider when evaluating design outcomes from interaction design research. Based on the feedback from the users, there are some value propositions for this prototype in its current state:
Capacity planning
All servers, or nodes, in a large back-end like this carries di erent amount of net- work tra c, or load. This needs to be evenly distributed among them to function as e cient as possible, and support a good user experience without latency and delays. By visually seeing the geographical grouping of servers and what services they are connected to, and a specific metric value at a certain time, it would be possible to determine whether some places are over- or under-provisioned regarding computational power and network capacity, etc. For example, a graph showing a group of servers having all their bandwidth used up - and they’re all in the same location - could mean that a certain group of users will experience latency problems in the near future, which in turn could lead to them being unable to log in or play songs.
Post-mortems
When a serious issue has occurred, developers investigate what happened, and why, to learn from it and prevent it from happening again. This is referred to as post- mortem. By correlating a certain metric with the geographic placement of their sources (servers) it would be possible to gain insight about the cause of the problem, if its somehow related to the organization, grouping or interrelationships between the servers.
6.4 Discussion and conclusions 6.4.1 Process
The form of the prototype changed continuously throughout the design process. Ideas that initially seemed useful and interesting later proved to be either too dif- ficult to implement or not as interesting anymore. This is the strength of rapidly prototyping a design artifact to create knowledge within a given context; the prob- lem is constantly re-framed and better and more interesting results are yielded.
There were no specifically planned milestones or organized user testing sessions throughout the design phase. The designer (me) being constantly physically present next to the target users allowed for spontaneous but e ective user testing of the
33
prototype and ideas. I believe that this ease of interacting with the target users contributed positively to the evolution of the prototype.
6.4.2 A sense of place
The final application prototype allows for looking into closed rooms, into physical spaces located far from where the observer is. It’s a digital, simplified representation of physical spaces; servers organized in racks that are placed in large server halls in di erent locations around the world.
Bringing back Harrison and Dourish (1996) ideas regarding space and place in interaction design, I would not argue that there is a sense of placeness that frames the interaction and user behavior in the prototype in its current state, according to their definition. However, if one would envision a state where users are able to appropriate and adapt this virtual space together through their interaction by contributing persistent data and content of their own, generated from their behavior and usage of the prototype, a placeness that more closely resembles their definition could be created. One usefulness of this could be found in how this could influence future behavior of users by potentially helping them solve problems more quickly by relying on relevant content created by previous users.
The design of the prototype has been influenced by spatial properties of the real world, such as grouping and hierarchy of the servers. Additionally, another type of placeness can be observed in the prototype through the interactive linking of the visualization components. The relation between the physical locations and the temporal data is what di erentiates these virtually represented physical spaces from each other and adds a dimension of place to it. This place information can be useful for developers when searching for information in the data in the way that di erent metric values have di erent meaning depending on the relative time and location of their sources. The significance of place contextuality of the data can be observed by envisioning a scenario where all servers that handles user logins are located in the same geographical location, and a power outage occurs, then no users are able to log in. However, if the servers are instead distributed over several locations, the tra c from users a ected by an outage in one location could be transferred to another location that isn’t a ected.
Paradoxically, these spaces are still represented spacelessly, there is no notion of distance between di erent servers or server groups nor between data centers across the world. Distance has meaning when sending digital information between physical locations as it a ects the time it takes to reach its destination, digital messages are transferred very fast but not instantaneous. However, in this visual representation, the distance has been completely abstracted away. This might not necessarily be preferable, but its likely not vitally a ecting the quality of the exploration experience of the user. This topic could be further investigated in another research project though.
34
Chapter 7 Discussion
Throughout the process of this thesis, insights were gained regarding the de- sign process and the final design artifact, as well as the role of space and place within system monitoring. Those findings are discussed and evaluated in this chapter
7.1 Research and design process
A strength I observed with the research through design methodology is how the design is shaped together with the target users; how it defines the feedback and design decisions loop. Instead of having discussions about possible or impossible features and trying to envision what might be and demand constructive and creative feedback from the users based on something that doesn’t yet physically exist - that is just an idea - it’s instead possible to present the physical creation and they can see it for what it is, and understand the possible opportunities and concepts of it. This way it’s easier for the users to give feedback on the product. To communicate an idea to someone is much easier and more e ective when showing it, embodied in a physical object, than using verbal language explaining its features and properties and expecting them to fully understand the scope of the vision.
Zimmerman et al. (2007) points out that it’s also important to not only consider the final result as something "final", but rather as something that has created insight and knowledge in the process and might become much more, given further iterations and tests. This is very accurate regarding the prototype created in this thesis. Knowledge regarding technical implementation solutions and interaction with and exploration of system monitoring data has been gained. Additionally, many ideas and suggestions about features have been expressed throughout the process but has not been manifested in the prototype. But can, however, potentially be implemented in the future.
My interpretation of the preferred state, that Zimmerman et al. (2007) stresses as an important part of design research in their model, is that it is something to aim for but not necessarily achieve by all means. Both for the reason that the knowledge
35
generated in the process is that which is of value and because I believe that the preferred state is impossible to achieve. The reason being that it’s not possible to define it in a complete and absolute manner to be able to actually validate whether it’s been achieved, nor the design artifact. In the case of this project, the design artifact has definitely shifted the current state towards the preferred state but it hasn’t completely fulfilled it. Not even including all the possible extensions and visions of what it could be. However, I think the goal has been achieved through the generation of new ideas and opportunities for what is possible to do regarding exploration of system monitoring data at Spotify AB.
7.2 Data and place in system monitoring
Another interesting discovery generated from the final prototype is how it empha- sizes that system monitoring is tightly linked to the physical world. The final prototype presents an interface for visually correlating data to the geographical and physical placement of the sources. What has been created is a virtual space con- necting physical places. The prototype enables users to look into closed rooms in di erent places of the world, concurrently. The feat to link temporal system per- formance data to the physical placement of their source in this way is new within the scope and context of this thesis; system monitoring at Spotify AB. Based on Zimmermans et al. (2007) criteria for evaluating design artifacts in design research, this is the invention criterion of the design.
This feature is also what adds a dimension of place to this visual representation. The linking of selections in the performance metric data to the server data gives the representations context in the form of geographical and physical location as well as time and grouping which adds more meaning to the data. For example, certain patterns in the data have di erent meaning whether it’s day or night, whether their sources are located in Europe or America or whether the servers are grouped in the same rack. All of this is likely to have meaning when correlated to other patterns in the system monitoring data. Examples of this is, as mentioned earlier, that user activity patterns are heavily tied to their geographical location as well as time of day. Or when a piece of software running on multiple servers, the servers should be as physically separated and distributed as possible to minimize the risk of software failure in case of technical hardware issues that could a ect multiple servers if they are physically grouped tightly together.
7.3 Technical evaluation
The web as platform imposes a couple of technical restrictions. Most notably, computational power, both for processing data and visualizing data. There are many di erent choices that can be made to a ect the performance of the prototype. The implications of the di erent options are explained as well as which ones are best suited for what situation:
36
7.3.1 Processing data in the browser
Processing large volumes of data requires strong computational power. A web browser on a single agent running a JavaScript application is limited in resources, compared to the large cluster of servers collecting and storing the data in the Spotify back-end. Therefore, the data received to the web browser should be pre-processed, preferably down-sampled using a sensible algorithm, so that the volume received can be processed in a reasonable amount of time by the browser. There is no point in having more data points than the amount of pixels the screen is able to render. This time variable is very likely an important factor a ecting the user experience of the application.
7.3.2 Rendering in the browser
Visualizing, i.e. rendering, large volumes of data in a web browser will also require significant amount of computational power. A sensible thing to do in the processing stage, in the browser, is to further down-sample the data so that the amount of data points available in one axis is not higher than the amount of pixels in the pixel range required to display it on the screen.
Two techniques were used for the rendering of data. The first one uses multiple svg elements in the web browser and the second one uses one canvas element in the browser. Both of which are o cial html5 elements. These two techniques pose di erent strengths and weaknesses:
SVG: The technique of rendering visualizations using svg elements means produc- ing a very high amount of html elements on the page. Each html element rendered on a web page takes up memory and processing power, and a large amount of them will eventually lead to very bad performance, which in turn creates a bad user ex- perience. So, using svg elements for a high volume of data has a high risk of leading to bad performance of the web browser.
However, the benefits of using svg is that they are easy to implement interaction for since web browsers allow for very easy attachment of di erent input listeners to every html element, or node, it contains. Therefore, adding interactivity - such as listening to click or drag events on the mouse - to an svg visualization requires relatively little work.
CANVAS: Visualizing using canvas di ers from svg in the sense that there is only one element being rendered in the browser. There’s only one canvas element, as opposed to the svg method where all visualization elements are comprised of many di erent svg elements.
The canvas is one element only containing pixels in di erent colors, which is much cheaper performance-wise for the browser to render. So in the processing phase, all screen coordinates of the visualizations are calculated on the canvas and painted as a pixel with a certain color.
37
The disadvantage of canvas though, is that it’s much harder to implement in- teractivity to the visualizations. Because the canvas is just a 2-dimensional area containing painted pixels, instead of an abstract tree of html nodes. There is no way to attach any input event listeners to it, as it would be when having many html nodes as in the svg method.
It’s still possible to implement interaction with the visualization, though it needs a lot more work.
7.3.3 An accessible platform
A big benefit of using the web as platform for developing an application such as this is the high level of accessibility it provides for the users. A web application does not require any cumbersome acquirement procedures, such as downloading and installing a software on the system, which is a time consuming task, and requires a higher gain expectation from the user to be willing to go through with it.
Since a web application is instantly accessible by a the user through a web browser, I think the conversion rate from non-users to users are much higher than if they would have to download and install a software. I believe it’s easier to let a user use an application and discover its value by themselves rather than having to explain it, or even trying to convince, them about it.
7.4 Extensions
There are many ideas that are not incorporated in the final prototype, but are useful and could provide great value if implemented.
More details
There exists a lot more information about the servers visualized in the treemap than what is shown. Not all servers are actually active, some are being initi- ated and some are not being used at all. This information would be useful to visualize, and allow for the users to filter and correlate the status of the data sources (servers) with the data.
Correlate di erent data types
Currently, it’s only possible to correlate one data type, one metric, with its data source. By overlaying multiple metrics in the line graph and linking them to their sources in the treemap with color encoding, further insights could be gained about the state of the back-end servers and software. For example, bandwidth usage of incoming tra c is likely to correlate with the number of connected users.
Modes of exploration
Further opportunities of exploring the data using human perception would possibly be created by allowing for changing the scales of the line graph. Examples could be to have logarithmic scales, normalized scales or integrals.
38
It’s hard to say what exact purpose this would serve but since the whole meaning of this prototype is to explore the data and not just display it for a pre-defined reason, it might be useful features to have.
It could prove useful to be able to add, divide or subtract values between di erent metrics. In the example from the previous section, this could be ex- emplified as dividing the number of bits of the incoming tra c by the number of currently connected users; incoming bits per user. Again, it’s hard to pre- dict exactly what insights this information would create but it’s aligned with the basic concepts of data exploration; to let human perception do the work.
39
Chapter 8 Conclusions
The purpose and goal of this project was to explore new ways of visualizing and interacting with system monitoring data at Spotify AB. The work resulted in a de- sign artifact that embodies knowledge about some of the things that are possible to achieve regarding data exploration within this context and an additional emphasize is also given to the relation between system monitoring data and physical space, and its meaning within system monitoring at Spotify AB.
This research contribution shows that by geographically and physically contex- tualizing system monitoring data, a sense of place is added that helps developers correlate system performance with physical placement of software and hardware, to predict future performance related bottlenecks or investigate whether past incidents were related to the physical placement of the systems. It also exposes possibilities for providing general assistance in the developers work on investigating technical issues and ensuring a high availability of the Spotify service by potentially allowing for new types of interaction with the data, such as overlaying metric visualizations or performing math operations on di erent sets of metrics as means of exploring possible correlations. The design artifact produced materializes these design visions and acts as a conduit to communicate these ideas to the developers at Spotify AB, increasing the likelihood that they will result in products for widespread use within the company.
Technical insights regarding good practices and e ective solutions to ensure performant and useful user interfaces for data visualization when using web-based technologies have been gained as well, where the recommendation is to use the pixel graphic based canvas method for very large volumes of data, and the node tree based svg method when the amount of data is smaller and the requirement for interactivity is higher. This further emphasizes the strength in creating design artifacts for research contribution in the way this feeds back technological knowledge to the human-computer interaction and the computer science community.
Hopefully the results and conclusions of this thesis will serve as inspiration for further development of useful monitoring tools within Spotify AB as well as a resource for continued exploration and research on this topic.
41
References
angular.js. (2014). Retrieved 2014, from https://angularjs.org/
d3.js. (2013). Retrieved 2014, from http://d3js.org/
ECMA International. (2011, June). Ecmascript language specification. 5.1. ECMA
International.
Geraci, A. (1991). Ieee standard computer dictionary: compilation of ieee standard
computer glossaries (F. Katki, L. McMonegal, B. Meyer, J. Lane, P. Wilson,
J. Radatz, . . . F. Springsteel, Eds.). Piscataway, NJ, USA: IEEE Press. ggobi.org. (2013). Retrieved 2014, from http://www.ggobi.org/
Harrison, S. & Dourish, P. (1996). Re-place-ing space: the roles of place and space
in collaborative systems. Citeseer, 7, 67–76.
IBM. (2013). Operational monitoring of system performance. Retrieved October
15, 2013, from http://pic.dhe.ibm.com/infocenter/db2luw/v9r7/index.jsp?
topic=/com.ibm.db2.luw.admin.perf.doc/doc/c0054690.html
Iliinsky, N. & Steele, J. (2011). Designing Data Visualization. O’Reilly Media. JuiceAnalytics. (2009). Retrieved 2014, from http://www.juiceanalytics.com/ Keim, D. A. (2002). Information visualization and visual data mining. IEEE Trans-
actions on Visualization and Computer Graphics, 8. doi:10.1109/2945.981847 Luqi. (1989, May). Software evolution through rapid prototyping. Computer, 22 (5),
13–25. doi:10.1109/2.27953
Rittel, H. W. J. & Webber, M. M. (1973). Dilemmas in a general theory of planning.
In Policy sciences 4 (pp. 155–166).
Schroeder, M. & Noy, P. (2001). Multi- Agent Visualisation Based on Multivariate
Data, 85–91.
Shneiderman, B. (1992, January). Tree visualization with tree-maps: 2-d space-
filling approach. ACM Trans. Graph. 11 (1), 92–99. doi:10.1145/102377.115768 Spence, R. (2007). Information visualization: design for interaction (2nd edition).
Upper Saddle River, NJ, USA: Prentice-Hall, Inc.
Spotify. (2014). Retrieved 2014, from http://press.spotify.com/se/information/ Tufte, E. R. (1983). The visual display of quantitative information. Cheshire: CT,
Graphic Press.
W3C. (2014a). Retrieved 2014, from http://www.w3.org/html/wg/drafts/html/
master/
W3C. (2014b). Retrieved 2014, from http://www.w3.org/Style/CSS/specs.en.html
43
Ward, J. S. & Barker, A. (2001). Undefined By Data : A Survey of Big Data Defi- nitions. arXiv: arXiv:1309.5821v1
Zimmerman, J., Forlizzi, J., & Evenson, S. (2007). Research through design as a method for interaction design research in HCI. Proceedings of the SIGCHI conference on Human factors in computing systems - CHI ’07, 493. Retrieved from http://portal.acm.org/citation.cfm?doid=1240624.1240704
   Eye Movements, Strabismus, Amblyopia, and Neuro-Ophthalmology
Eye Movement Alterations During Reading in Patients With Early Alzheimer Disease
Gerardo Fern ́andez,1,2 Pablo Mandolesi,1,2 Nora P. Rotstein,1,3 Oscar Colombo,4 Osvaldo Agamennoni,1,2 and Luis E. Politi1,3
1Universidad Nacional del Sur, Bah ́ıa Blanca, Argentina
2Instituto de Investigaciones en Ingenier ́ıa Ele ́ctrica (UNS CONICET), Bah ́ıa Blanca, Buenos Aires, Argentina 3Instituto de Investigaciones Bioqu ́ımicas de Bah ́ıa Blanca (UNS CONICET), Bah ́ıa Blanca, Buenos Aires, Argentina 4Hospital Municipal de Agudos, Bah ́ıa Blanca, Buenos Aires, Argentina
Correspondence: Luis E. Politi, IN- IBIBB, UNS-CONICET, Camino La Carrindanga Km 7, 8000 Bah ́ıa Blan- ca, Buenos Aires, Argentina; inpoliti@criba.edu.ar.
Pablo Mandolesi, Departamento de Ingenier ́ıa Ele ́ctrica y de Computa- doras (UNS) and Comisio ́n de Inves- tigaciones Cient ́ıficas de la Provincia de Buenos Aires, Argentina; pmandolesi@uns.edu.ar.
Osvaldo Agamennoni, Departamento de Ingenier ́ıa Ele ́ctrica y de Compu- tadoras (UNS) and Comisio ́n de InvestigacionesCient ́ıficasdelaPro- vincia de Buenos Aires, Argentina; oagamen@uns.edu.ar.
Submitted: July 20, 2013 Accepted: November 14, 2013
Citation: Fern ́andez G, Mandolesi P, Rotstein NP, Colombo O, Agamennoni O, Politi LE. Eye movement alterations during reading in patients with early Alzheimer disease. Invest Ophthalmol Vis Sci. 2013;54:8345–8352. DOI: 10.1167/iovs.13-12877
PURPOSE. Eye movements follow a reproducible pattern during normal reading. Each eye movement ends up in a fixation point, which allows the brain to process the incoming information and to program the following saccade. Alzheimer disease (AD) produces eye movement abnormalities and disturbances in reading. In this work, we investigated whether eye movement alterations during reading might be already present at very early stages of the disease.
METHODS. Twenty female and male adult patients with the diagnosis of probable AD and 20 age-matched individuals with no evidence of cognitive decline participated in the study. Participants were seated in front of a 20-inch LCD monitor and single sentences were presented on it. Eye movements were recorded with an eye tracker, with a sampling rate of 1000 Hz and an eye position resolution of 20 arc seconds.
RESULTS. Analysis of eye movements during reading revealed that patients with early AD decreased the amount of words with only one fixation, increased their total number of first- and second-pass fixations, the amount of saccade regressions and the number of words skipped, compared with healthy individuals (controls). They also reduced the size of outgoing saccades, simultaneously increasing fixation duration.
CONCLUSIONS. The present study shows that patients with mild AD evidenced marked alterations in eye movement behavior during reading, even at early stages of the disease. Hence, evaluation of eye movement behavior during reading might provide a useful tool for a more precise early diagnosis of AD and for dynamical monitoring of the pathology.
Keywords: saccadic movements, Alzheimer disease, eye fixation, eye movements
  Healthy human subjects move their eyes during reading every quarter of a second on the average, sending new information to the brain each time the eyes remain fixated. In normal subjects, fixation duration is usually between 150 and 250 ms, with values stretching from 100 to over 700 ms. The distance the eyes move in each saccade ranges between 1 and 20 characters, usually moving between 7 to 9 characters, and execution of the saccade takes approximately 20 to 50 ms.1 Saccade primary function is to bring a new region of text into the foveal vision. Some properties of the not yet fixated incoming words must become available during the ongoing word fixation. Research on the perceptual span established that parafoveal visual information extending approximately 10 characters in reading direction can influence the word processing in progress. In healthy readers, this information is used for selecting the next saccade target and for determining the size of the next saccade. The distance between the last fixation on a word and the next fixation to the right is defined as an outgoing saccade.2–4 Several authors propose that once a fixation is made, processing of information is critical in programming the next saccadic movement. Hence, brain processing at fixations points appears to be relevant to execute
Copyright 2013 The Association for Research in Vision and Ophthalmology, Inc. www.iovs.org j ISSN: 1552-5783
saccades.5,6 Recent work shows that the number of characters that healthy readers will move their eyes to the right depends, in part, on the difficulty of processing the previously fixated word; in general, the easier the processing, the longer the outgoing saccade.7–9
In Alzheimer disease (AD), progressive neuropathological changes within the neocortex10 make AD patients prone to visual and attentional disturbances.11 Mendez et al.11 reported visual field deficits, prolonged visual evoked potentials, abnormal eye movement recordings, and even visual hallucina- tions, among other disturbances in AD patients. In addition, disturbances and abnormal eye movements during reading have been observed in these patients.12–15 During the progression of AD, affected individuals evolve from an initial mild cognitive impairment to severe loss of mental function. Patients with early to moderate AD usually show impairment in learning and a deterioration of episodic memory, symptoms that are typically used for diagnosis of the pathology. However, subtle alterations in movement coordination and planning that may also be present while performing fine motor tasks such as writing or reading are harder to detect and go commonly unnoticed.16,17 Evaluation of eye movements might provide considerable
8345
 Downloaded From: http://iovs.arvojournals.org/pdfaccess.ashx?url=/data/journals/iovs/932983/ on 02/17/2017
Eye Movement Behavior in Alzheimer Disease Patients
IOVS j December 2013 j Vol. 54 j No. 13 j 8346
 insight into the integrity of control circuits in AD. The cognitive control of eye movements is a promising area of research, primarily because of the thorough understanding of the oculomotor system and the ease with which eye movements can be measured.1 Networks and structures involved in a range of eye movement behaviors are well defined, including those that measure working memory and saccadic execution. The existing knowledge on eye movement control could be extended to improve our understanding of more complex behaviors such as attention, inhibitory control, working memory, and decision-making processes.18–23
All the above processes are altered in AD, which leads to early modifications in neurological connectivity that disrupts the processing of incoming information.24,25 Work from Lueck et al.13 showed that patients with moderate AD had abnormalities in eye movements during reading of a text and that reading difficulty correlated with dementia severity. Given that the broad spectrum of events involved in reading require processing of information, coordination, and planning, we investigated whether subtle changes in that processing present at early stages of AD might lead to eye movement alterations detectable by the eye tracker. In this work, we evaluated the eye movement behavior in healthy individuals and AD patients at early stages of the disease during reading of sentences, and whether the decision for when and where to execute a saccade (i.e., to refixate a word or saccade to the next word) were influenced by the frequency, predictability, and length of the words used. Our results provide evidence that even at these very early stages of their disease, AD patients showed marked changes in the pattern of eye movements during word processing, compared to control individuals. This suggests that analysis of eye movements might provide new insights into the pathogenesis of AD and contribute an additional, useful tool for an early clinical diagnosis,26 still a pending problem in AD.
METHODS
Ethics Statement
The investigation adhered to the principles of the Declaration of Helsinki, and was approved by the Institutional Bioethics Committee of the Hospital Municipal de Agudos (Bah ́ıa Blanca, Buenos Aires, Argentina). All patients and their caregivers, and all control subjects signed an informed consent prior to inclusion into the study.
Participants
Twenty patients (12 females and 8 males; mean age 69 years, SD 1⁄4 7.2 years) with the diagnosis of probable AD were recruited at the Hospital Municipal of Bah ́ıa Blanca (Buenos Aires, Argentina). The clinical criteria to diagnose AD at their early stages remain under debate.27 In the present work, diagnosis was based on the criteria for dementia outlined in the Diagnostic and Statistical Manual of Mental Disorders (DSM- IV).28 All AD patients underwent a detailed clinical history, physical/neurological examination, and thyroid function test. Magnetic resonance images were obtained from 12 patients and computerized tomography scans from the other eight patients. Patients also underwent biochemical analysis to discard other common pathologies (hemoglobin, full blood count, erythrocyte sedimentation rate, urea and electrolytes, blood glucose). As a whole, these data contributed to a more precise diagnosis of AD. Patients were excluded if they: suffered any medical conditions that could account for, or interfere with, their cognitive decline; had evidence of vascular lesions in computed tomography or a functional magnetic
resonance imaging; or had evidence of an Axis I diagnosis (e.g., major depression or drug abuse) as defined by DSM-IV. To be eligible for the study, patients had to have at least one caregiver providing regular care and support. Patients taking cholines- terase inhibitors (ChE-I) were not included. None of the subjects was taking hypnotics, sedative drugs, or major tranquilizers. The control group consisted of 20 elderly adults (12 females and 8 males; mean age 71 years; SD 1⁄4 6.1 years), with no known neurological and psychiatric disease according to their medical records, and no evidence of cognitive decline or impairment in activities of daily living. A one-way ANOVA showed no significant differences between the ages of AD and control individuals.
The mean scores of controls and AD patients in the Mini- Mental State Examination (MMSE)29 were 27.8 (SD 1⁄4 1.0) and 23.2 (SD 1⁄4 0.7), respectively, the latter suggesting early dementia. A one-way ANOVA evidenced significant differences between MMSE in AD patients and controls (P < 0.001). The mean score of AD patients in the Adenbrook’s Cognitive Examination (ACE-R) was 82.4, (SD 1⁄4 2.1),30 the cutoff being 84. The mean school education trajectories in AD patients and controls were 15.2 (SD 1⁄4 1.3) and 15.1 years (SD 1⁄4 1.0), respectively. A one-way ANOVA showed no significant differ- ences between the education of AD and control individuals.
Sentence Corpus
The sentence corpus was composed of 75 sentences (620 words), and was constructed with the goal of representing a large variety of grammatical structures in Spanish.
Word and Sentence Lengths. Sentences ranged from a minimum of 5 words to a maximum of 14 words. Mean sentence length was 8.3 words (SD 1⁄4 1.3). Words ranged from 1 to 14 letters. Mean word length was 4.6 letters (SD 1⁄4 2.5).
Word Frequencies. We used the Spanish Lexical L ́exesp corpus31 for assigning a frequency to each word of the sentence corpus. Word frequency ranged from 1 to 264,721 per million. We transformed frequency to log10 (frequency). Mean log10 (frequency) was 3.4 (SD 1⁄4 1.3).
Word Predictability. Word predictability was determined by performing an independent experiment with 18 researchers of the Electrical Engineering and Computer Science Depart- ment, Universidad Nacional del Sur. We used an incremental cloze task procedure in which participants had to guess the next word given only the prior words of the sentence. Participants were between 31 and 62 years old, and did not participate in the reading experiment. Academic background of the reading experiment group and the cloze task group was similar. The average predictability measured from the incremental cloze task was transformed using a logit function. Logits are defined as 0.5 3 ln(pred/[1   pred]); predictabilities of zero were replaced with 1/(2 3 18) 1⁄4  2.55 and those among the five perfectly predicted words with (2 3 18   1)/(2 3 18) 1⁄4 þ2.55, where 18 represents the number of complete predictability protocols.32 Mean logit predictability was  0.9 (SD 1⁄4 0.9).
Apparatus and Eye Movement Data
Single sentences were presented on the center line of a 20-inch liquid-crystal display (LCD) monitor (1024 3 768 pixels resolution; font: regular; New Courier; 12 point, 0.28 in height). Participants were seated in front of the monitor at a distance of 60 cm from the monitor. Head movements were minimized using a chin rest. Correction for the 60-cm viewing distance was performed by using an eye tracker (EyeLink 1000 Desktop Mount; SR Research Ltd., Ontario, Canada), which assessed changes in gaze position by measuring both the reflection of an infrared illuminator on the cornea and the
  Downloaded From: http://iovs.arvojournals.org/pdfaccess.ashx?url=/data/journals/iovs/932983/ on 02/17/2017
Eye Movement Behavior in Alzheimer Disease Patients IOVS j December 2013 j Vol. 54 j No. 13 j 8347
   FIGURE 1. Recording of eye movements during reading of a sentence by a control subject (a) and an AD patient (b). Fixation points for right (red) and left (blue) eyes are included in the graphs. The down and right movements signaled the end of reading; numbering linked to points indicates fixation sequences; fixation durations of each eye are listed with their corresponding colors. The number following fixation duration (after the comma), index the word number in the sentence. Note that in AD patients, the number and duration of fixations per word increased compared with controls. In addition, forward saccades were shorted in AD. The Spanish sentence: ‘‘Pedro, es por el momento, el u ́nico epileptico del hospital,’’ corresponds in English to: ‘‘Peter, is at the moment, the only epileptic at the hospital.’’
pupil size, by means of a video camera sensitive to light in the infrared spectrum.
Eye movements were recorded with an eye tracker (SR Research Ltd.), with a sampling rate of 1000 Hz and an eye
position resolution of 20 arc seconds. All recordings and calibration were binocular. Only right eye data was used for the analyses. Eye movement data from 40 participants reading 75 sentences were cleaned from blinks and track losses.
 Downloaded From: http://iovs.arvojournals.org/pdfaccess.ashx?url=/data/journals/iovs/932983/ on 02/17/2017
Eye Movement Behavior in Alzheimer Disease Patients
IOVS j December 2013 j Vol. 54 j No. 13 j 8348
 TABLE. Eye Movement Behavior Control,
AD, T
n (SD) Value
1617 (1098)* 4.79 434 (224)† 4.34 745 (746)* 3.98 132 (128)‡ 3.76 314 (262)† 2.04 114 (36)*  3.04
first letter of the sentence was to be presented. As soon as both eyes were detected within a 18 radius from the fixation spot, the sentence was presented. After reading it, participants looked at a dot in the lower-right corner of the screen; when the gaze was detected on the final spot, the trial ended. Occasionally, external factors such as minor movements and slippages of the head gear could cause small drifts. To avoid them, we performed a drift correction before each spot presentation.
To assess whether subjects comprehended the texts, they were presented with three alternative multiple-choice ques- tions about the sentence in progress on 20% of the sentence trials. Participants answered the questions by moving a mouse and choosing the response with a mouse click. Overall mean accuracy was 95% (SD 1⁄4 3.2%) in control and 91% (SD 1⁄4 5.4%) in AD. A one-way ANOVA showed no significant differences between comprehension of the answers in controls and in AD patients. The latter were only marginally less accurate than control subjects, probably because they were in an early stage of the pathology, as indicated by MMSE and ACE-R values. Once the comprehension test ended, the next trial started with the presentation of the fixation spot. The experimenter did an extra calibration after 15 sentences were read or if the eye tracker (SR Research Ltd.) did not detect the eye at the initial fixation point within 2 seconds. An example of the eye movements recorded during reading of a sentence, showing eye fixations of controls and AD, is shown in Figure 1.
First-pass fixations (i.e., the initial reading consisting of all forward fixations on a word) were generally used as the
  Total fixations
Total first-pass fixations Total second-pass fixations Regressions
Word skipping
Single fixations
n (SD)
617 (209) 284 (110) 112 (94)
43 (35) 186 (46) 164 (37)
 The eye movements of AD patients and age-matched individuals (control) while reading 75 sentences were compared. Values between brackets represent SD, calculated using a one-way ANOVA. Linear mixed models were also computed for counterchecking ANOVA’s significant effects. Our criterion for referring to an effect as significant was t 1⁄4 jb/SEj > 62.0.
* P   0.001. † P   0.05. ‡ P   0.01.
Fixations shorter than 51 ms and longer than 750 ms and fixations on the first and last word of each sentence were removed for the analysis.
Procedures
Participants’ gaze was calibrated with a standard 13-point grid for both eyes. After validation of calibration, a trial began with the appearance of a fixation point on the position where the
                         FIGURE 2. Effect of word frequencies on the size of the outgoing saccades in control and in AD patients. The graph represents the size (in characters) of outgoing saccades at different word frequencies as indicated in the Materials and Methods section. Note that the size of outgoing saccades was relatively constant at the different word frequencies analyzed for each group and that the values in AD patients were consistently smaller than in controls: the average decrease in the number of characters of outgoing saccades in AD patients with respect to controls was 30%. P < 0.001. Bars represent means 6 SD, calculated using a one-way ANOVA.
 Downloaded From: http://iovs.arvojournals.org/pdfaccess.ashx?url=/data/journals/iovs/932983/ on 02/17/2017
Eye Movement Behavior in Alzheimer Disease Patients IOVS j December 2013 j Vol. 54 j No. 13 j 8349
                                                        FIGURE 3. Effect of word length on the size of the outgoing saccades. The graph represents the size (in characters) of outgoing saccades at different word length in control and AD patients. Note that in AD patients, the size of outgoing saccades was consistently shorter than in controls; the average decrease in the number of characters of outgoing saccades in AD patients with respect to controls was 36%. P < 0.001. Bars represent means 6 SD, calculated using a one-way ANOVA.
primary measure of interest. Second-pass fixations (i.e., rereading) and single fixations (whenever the eyes fix exactly once, on a given word)1 were also evaluated.
Statistical Analysis
We performed a one way-ANOVA test for determining whether mean and standard deviations between groups were signifi- cant. To further evaluate the source of eye movement behavior during reading, we used the lmer program of the lme4 package (version 0.99975-14) for estimating fixed and random coeffi- cients of the variables analyzed in the Table. The lme4 package is supplied in the R system for statistical computing (version 2.15.2; R Development Core Team, 2008) under the GNU General Public License (Version 2, June 1991). Our criterion for referring to an effect as significant was t 1⁄4 jb/SEj > 62.0.
RESULTS
Eye Movement Behavior
Word information is acquired and processed in the brain during fixations. Since patients at early stages of AD already show minor difficulties in processing and interpreting acquired data, we hypothesized that they would increase the number of fixations to compensate for these deficits. We first compared the total number of fixations of AD patients and healthy (control) individuals while reading. As shown in the Table, the mean of the total number of fixations significantly increased
from 617 in controls to 1617 in AD patients. The mean number of first-pass fixations doubled between controls and AD patients, significantly increasing from 284 to 434 fixations, respectively (Table). The amount of second-pass fixations increased even more strikingly in AD patients compared with controls (Table), from 112 in the control group, to 745 in AD patients.
Though most saccades carry the eyes forward through the text, a number of eye movements, regressions, are made backward to words that have already been fixated. Notewor- thy, the number of regressions increased from 43 in controls to 132 in mild AD patients (Table). We then evaluated the number of single fixations, which would be expected to decrease with a higher level of difficulty in interpreting the acquired data. As predicted, the number of single fixations significantly de- creased from 164 in controls to 114 in AD patients (Table). The number of words skipped while reading increased from 186 in controls up to 314 in AD patients (Table). AD progression seemed to markedly affect reading ability; patients with moderate AD not only evidenced greater difficulty to concen- trate in reading tasks, but also increased the number of fixations, regressions, and fixations out of the text compared with patients with mild AD (data not shown).
Word-Based Effect on Outgoing Saccade Sizes
We then analyzed the effect on the size (in characters) of the outgoing saccade of word-based effects, such as word frequency, predictability, or length. The size of outgoing saccades ranged from 6.4 to 6.6 characters in controls (Fig.
   Downloaded From: http://iovs.arvojournals.org/pdfaccess.ashx?url=/data/journals/iovs/932983/ on 02/17/2017
Eye Movement Behavior in Alzheimer Disease Patients IOVS j December 2013 j Vol. 54 j No. 13 j 8350
                                         FIGURE 4. Effect of word predictability on the size of the outgoing saccades. The graph represents the size (in characters) of outgoing saccades at different word predictabilities in control and AD patients. Note that in AD patients the size of outgoing saccades was in every case smaller than in controls; the average decrease in the number of characters of outgoing saccades in AD patients with respect to controls was 32%. P < 0.001. Bars represent mean 6 SD, calculated using a one-way ANOVA.
2) and significantly decreased in AD patients, ranging between 4.2 and 5.3 characters (Fig. 2). The amplitude of the outgoing saccade was independent of the frequency of a given word in both groups.
We then evaluated the effect of word length on the amplitude of outgoing saccades. In controls, outgoing saccades were significantly longer, approximately eight letters, for words having one character, with values of around 6.5 characters for words two or more letters long (Fig. 3). In AD patients, the sizes of outgoing saccades were significantly reduced compared with controls at every word length, ranging between 2.2 and 5.6 characters. The highest reduction was observed for words having one character length (Fig. 3).
We also investigated whether the predictability of a word in a sentence might differentially modify the amplitude of the outgoing saccades. As stated above, the size of outgoing saccades was approximately 6.5 characters in controls, while in AD patients these values were significantly reduced, regardless of the predictability of the word (Fig. 4).
Finally, we analyzed fixation duration as a function of the size of outgoing saccades. In control subjects, fixation durations were 195 and 190 ms for outgoing saccades of one and two characters, respectively. These values significantly increased with the size of the outgoing saccade, up to 215 ms for outgoing saccades of 13 characters. In general, fixation duration in controls increased slightly at longer outgoing saccades. Noteworthy, fixation durations were strikingly longer in AD patients than in controls (Fig. 5) at every saccade size and had a random distribution, evidencing no correlation with the size of outgoing saccades (Fig. 5).
DISCUSSION
The present results reveal that AD patients with mild cognitive impairment already evidenced significant alterations in their eye movements during reading compared with healthy individuals. Patients with mild AD remarkably increased the amount of total fixations and saccade regressions. They also showed longer fixation durations in spite of a reduction in the size of outgoing saccades.
The basic processes involved in eye movements during reading have been thoroughly described in the literature.1,33 There is now a growing consensus that eye movement behavior could be used to evaluate cognitive processing during reading,11,34 since several cognitive processes, including working memory, have been shown to influence saccade parameters.1,35 Changes in eye movements have been linked to neurological conditions such as Parkinson disease and prefron- tal cortex damage.36,37 Mild cognitive impairment and a deficit in working memory are early symptoms of AD, and AD patients have been reported to evidence saccade dysfunctions.13,38 Our results show that eye movements during reading were affected in AD patients at very early stages of the disease. Patients participating in this study had MMSE and ACE-R score values of 23.2 and 82.4, respectively, which reflect slight cognitive impairment. These patients evidenced a significant increase in their total number of fixations, with both the amount of first- and second-pass fixations being increased, compared with controls. In healthy individuals, as is observed in the Table, second-pass fixations are less frequent than first-pass fixations; noteworthy, the former showed the highest increase in AD
  Downloaded From: http://iovs.arvojournals.org/pdfaccess.ashx?url=/data/journals/iovs/932983/ on 02/17/2017
Eye Movement Behavior in Alzheimer Disease Patients
IOVS j December 2013 j Vol. 54 j No. 13 j 8351
                               FIGURE 5. Effect of the size of the outgoing saccade on fixation duration [ms]. The graph represents the fixation duration at different sizes of outgoing saccades in controls and AD patients. Note that fixation durations were longer in AD patients than in controls; the average decrease in the number of characters of outgoing saccades in AD patients with respect to controls was 23%. P < 0.001. Bars represent mean 6 SD, calculated using a one-way ANOVA.
patients, and almost doubled the amount of first-pass fixations observed for this group. Consistently, AD patients exhibited a simultaneous decrease in the amount of single fixations. A number of hypotheses have been proposed to explain the origin of second-pass fixations, including the necessity for comparison with the past image and interpretation of a given word in the context of a sentence.39 The increase in these parameters might reflect the deficit in processing the current and incoming information in AD patients.
Backward eye movements, saccade regressions, which also occur during reading, are likely aimed to integrate previously acquired data with the new incoming information.1 The higher number of regressions observed in AD patients compared with controls is consistent with the impairment in perceptual and cognitive processes associated to word processing in these patients and might be an attempt to compensate for this disability. Remarkably, AD patients also skipped a higher number of words than controls, an observation that might be related to mislocated fixations or a consequence of involuntary triggered saccades.4
Our work also revealed that fixation duration was signifi- cantly longer in AD patients than in controls. Consistent with these data, longer fixation durations have been reported in AD patients during visual search.40 Information uptake is largely restricted to fixations1,4; hence, the longer fixations registered in AD patients might reflect the difficulties in acquiring and processing the visual inputs in these patients. This finding, together with the increase in the amount of fixations, agree with previous observations that readers make more fixations and fixate for longer time when they experience processing difficulty.41
In a recent study with subjects reading Chinese,42 the size of outgoing saccades was proposed to depend on the complexity of the region in which vision is fixated; in general
terms: The easier the processing of the fixated region is, the longer the outgoing saccade is. Interestingly, the size of the outgoing saccades was consistently shorter in AD patients than in controls. This reduction might reflect the impairment in memory and interpretation of words in AD patients compared with healthy individuals. Our results also show that the size of the outgoing saccade was more uniform in controls than in AD during processing of word length, word frequency, or word predictability, probably because controls use reading strategies such as word-based strategies or processing-based strategies. These reading strategies seem to be the rule for words of two or more letters. Outgoing saccades were remarkably longer for one-letter words in controls, which might be due to an enhanced parafoveal processing for one-letter words. Alterna- tively, one-letter words might be processed differently since they are usually connectors and recognized as such by healthy readers. Their use of them for joining previous and incoming words might thus lead to longer saccades. This capacity was absent in AD patients, which showed the shortest outgoing saccades for one-letter words.
The striking modifications observed in the saccadic movements in AD patients at their early stages of development reveal a considerable deterioration of one or more brain structures involved in controlling these behaviors. Visual fixation and saccadic movements are sophisticated responses in which several brain regions participate, including parietal and frontal cortex, basal ganglia, substantia nigra, and subthalamic nuclei, the caudate nuclei and the reticular formation in the brain stem, along with the cerebellum and the superior colliculus in the mesencephalon43–48; all of them must act in a coordinate way. The precise identification of which of the areas affected in AD are responsible for each or all of the alterations observed in eye movements requires further study.
The main conclusion of the present work is that AD patients at early stages of their disease, as those participating in this study, evidenced markedly altered eye movements during reading. This suggests that evaluation of these movements might provide an additional and sensitive tool for the much needed early diagnosis of AD disease.
Acknowledgments
Supported by grants from FONCyT (PICT 2010 1421; OA), Agencia Nacional de Promocio ́n Cient ́ıfica y Tecnolo ́gica, by the Universi- dad Nacional del Sur under Grant PGI 2010 24/K038 (OA), Bah ́ıa Blanca, Argentina; and by DAAD Grant A/11/77105 (GF).
Disclosure: G. Ferna ́ndez, None; P. Mandolesi, None; N.P. Rotstein, None; O. Colombo, None; O. Agamennoni, None; L.E. Politi, None
References
1. Rayner K. Eye movements in reading and information processing: 20 years of research. Psychol Bull. 1998;124: 372–422.
2. Kennedy A, Pynte J, Murray W, Paul S. Frequency and predictability effects in the Dundee corpus. Q J Exp Psychol. 2013;66:601–618.
3. Kliegl R. Toward a perceptual-span theory of distributed processing in reading: A reply to Rayner, Pollatsek, Drieghe, Slattery, and Reichle. J Exp Psychol Gen. 2007;136:530–537.
4. Kliegl R, Nuthmann A, Engbert R. Tracking the mind during reading: the influence of past, present, and future words on fixation durations. J Exp Psychol Gen. 2006;135:12–35.
5. Vitu F, Brysbaert M, Lancelin D. A test of parafoveal on-foveal effects with pairs of orthographically related words. J Cogn Psychol. 2004;16:154–177.
  Downloaded From: http://iovs.arvojournals.org/pdfaccess.ashx?url=/data/journals/iovs/932983/ on 02/17/2017
Eye Movement Behavior in Alzheimer Disease Patients
IOVS j December 2013 j Vol. 54 j No. 13 j 8352
 6. Rayner K, Ashby J, Pollatsek A, Reichle ED. The effects of frequency and predictability on eye fixations in reading: implications for the E-Z Reader model. J Exp Psychol Hum Percept Perform. 2004;30:720–732.
7. Yan M, Kliegl R, Richter E, Nuthmann A, Shu H. Flexible saccade target selection in Chinese reading. Q J Exp Psychol. 2010;63:705–725.
8. Li X, Liu P, Rayner K. Eye movement guidance in Chinese reading: Is there a preferred viewing location? Vision Res. 2011;51:1146–1156.
9. Wei W, Li X, Pollastek A. Word properties of a fixated region affect outgoing saccade length in Chinese reading. Vision Res. 2013;80:1–6.
10. Mosimann U, Felblinger J, Ballinari P, Hess C, Mu ̈ri R. Visual exploration behavior during clock reading in Alzheimer’s disease. Brain. 2004;127:431–438.
11. Mendez MF, Mendez MA, Martin R, Smyth KA, Whitehouse PJ. Complex visual disturbances in Alzheimer’s disease. Neurol- ogy. 1990;40:439–443.
12. Cummings J, Houlihan J, Hill M. The pattern or reading deterioration in dementia of the Alzheimer’s type: observa- tions and implications. Brain Lang. 1986;29:315–323.
13. Lueck K, Mendez M, Perryman M. Eye movement abnormal- ities during reading in patients with Alzheimer disease. Neuropsychiatry Neuropsychol Behav Neurol. 2000;13:77– 82.
14. Daffne K, Scinto L, Weintraub S, Guinessey J, Mesulam M. Diminished curiosity in patients with Alzheimer’s disease as measured by exploratory eye movements. Neurology. 1992; 42:320–328.
15. Fo ̈rstl H, Kurz A. Clinical features of Alzheimer’s disease. Eur Arch Psychiatry Clin Neurosci. 1999;249:288–290.
16. Frank E. Effect of Alzheimer’s disease on communication function. J S C Med Assoc. 1994;90:417–423.
17. Taler V, Phillips N. Language performance in Alzheimer’s disease and mild cognitive impairment: a comparative review. J Clin Exp Neuropsychol. 2008;30:501–511.
18. Hayhoe M, Ballard D. Eye movements in natural behavior. Trends Cogn Sci. 2005;9:188–194.
19. Hoffman J. Visual attention and eye movements. In Pashler H, ed. [Attention]. Hove, UK: Psychology Press; 1998:119–154.
20. Itoh N, Fukuda T. Comparative study of eye movement in extent of central and peripheral vision and use by young and elderly walkers. Percept Mot Skills. 2002;94:1283–1291.
21. Posner M. Orienting of attention. Q J Exp Psychol. 1980;32:3– 25.
22. Yarbus A. Eye Movements and Vision. New York, NY: Plenum Publishing; 1967.
23. Fern ́andez G, Shalom D, Kliegl R, Sigman M. Eye movements during reading proverbs and regular sentences: the incoming word predictability effect [published online ahead of print January 8, 2013]. Lang Cogn Process. doi:10.1080/01690065. 2012.760745.
24. Arn ́aiz E, Almkvist O. Neuropsychological features of mild cognitive impairment and preclinical Alzheimer’s disease. Acta Neurol Scand Suppl. 2003;179:34–41.
25. B ̈ackman L, Jones S, Berger A, Laukka E, Small B. Multiple cognitive deficits during the transition to Alzheimer’s disease. J Intern Med. 2005;256:195–204.
26. Landes A, Sperry S, Strauss M, Geldmacher D. Apathy in Alzheimer’s disease. J Am Geriatr Soc. 2001;49:1700–1707.
27. McKhann G, Drachman D, Folstein M, Katzman R, Price D, Stadlan EM. Clinical diagnosis of Alzheimer’s disease: report of the NINCDS-ADRDA Work Group under the auspices of Department of Health and Human Services Task Force on Alzheimer’s Disease. Neurology. 1984;34:939–944.
28. American Psychiatric Association. Diagnostic and Statistical Manual of Mental Disorders: DSM-IV. 4th ed. Washington, DC: American Psychiatric Association; 1994.
29. Folstein M, Folstein S, McHugh P. ‘‘Mini-mental state.’’ A practical method for grading the cognitive state of patients for the clinician. J Psychiatr Res. 1975;12:189–198.
30. Mioshi E, Dawson A, Mitchell J, Arnold R, Hodges JR. The Addenbrooke’s Cognitive Examination Revised (ACE-R): a brief cognitive test battery for dementia screening. Int J Geriatr Psychiatry. 2006;21:1078–1085.
31. Sebasti ́an-Galle ́s N, Mart ́ı M, Cuetos F, Carreiras M. L ́exesp: L ́exico Informatizado Del Espan ̃ol. Barcelona: Ediciones de la Universidad de Barcelona; 1998.
32. Taylor W. ‘‘Cloze procedure’’: a new tool for measuring read ability. Journal Q. 1953;30:415–433.
33. Calvo M, Meseguer E. Eye movements and processing stages in reading: relative contribution of visual, lexical, and contextual factors. Span J Psychol. 2002;5:66–77.
34. Rayner K. The perceptual span and peripheral cues in reading. Cogn Psychol. 1975;7:65–81.
35. Boston M, Hale J, Vasishth S, Kliegl R. Parallelism and syntactic processes in reading difficulty. Lang Cogn Process. 2011;26: 301–341.
36. Rottach K, Riley D, DiScenna A, Zivotofsky A, Leigh R. Dynamic properties of horizontal and vertical eye movements in Parkinsonian syndromes. Ann Neurol. 1996;39:368–377.
37. Mosimann U, Muri R, Burn D, Felblinger J, O’Brien J, McKeith I. Saccadic eye movement changes in Parkinson’s disease dementia and dementia with Lewy bodies. Brain. 2005;128: 1267–1276.
38. Walker R, Husain M, Hodgson T, Harrison J, Kennard C. Saccadic eye movement and working memory deficits following damage to human prefrontal cortex. Neuropsycho- logia. 1998;36:1141–1159.
39. Fletcher WA, Sharpe JA. Saccadic eye movement dysfunction in Alzheimer’s disease. Ann Neurol. 1986;20:464–471.
40. Kliegl R, Grabner E, Rolfs M, Engbert R. Length, frequency, and predictability effects of words on eye movements in reading. J Cogn Psychol. 2004;16:262–284.
41. Rosler A, Mapstone M, Hays A, Mesulam M, Rademaker A, Gitelman D, et al. Alterations of visual search strategy in Alzheimer’s disease and aging. Neuropsychology. 2000;14: 398–408.
42. Wei W, Li X, Pollatsek A. Word properties of a fixated region affect outgoing saccade length in Chinese reading. Vision Res. 2013;80:1–6.
43. Hikosaka O, Takikawa Y, Kawagoe R. Role of the basal ganglia in the control of purposive saccadic eye movements. Physiol Rev. 2000;80:953–978.
44. Leigh RJ, Zee DS. The Neurology of Eye Movements. Oxford: Oxford University Press; 2006:151–186.
45. Scudder CA, Kaneko CS, Fuchs AF. The brainstem burst generator for saccadic eye movements. A modern synthesis. Exp Brain Res. 2002;142:439–462.
46. Wurtz RH, Goldberg ME. The Neurobiology of Saccadic Eye Movements. In: Wurtz RH, Goldberg ME, eds. Reviews of Oculomotor Research. Vol. 3. Amsterdam: Elsevier; 1989:257– 284.
47. Liversedge SP, Findlay JM. Saccadic eye movement and cognition. Trends Cogn Sci. 2000;4:1–11.
48. Douglas P, Munoz I, Armstrong K, Hampton D, Kimberly D. Moore altered control of visual fixation and saccadic eye movements in attention-deficit hyperactivity disorder. J Neuro- physiol. 2003;90:503–514.
  Downloaded From: http://iovs.arvojournals.org/pdfaccess.ashx?url=/data/journals/iovs/932983/ on 02/17/2017
Lund University Cognitive Science 2004
Eye movement patterns
and newspaper design factors. An experimental approach
Master’s Thesis · Nils Holmberg
Supervisor: Kenneth Holmqvist (LU) Co-advisor: Bengt Engwall (NT)
Table of contents
1. Introduction 3
2. Previous research 5
2.1 Newspaper studies
2.2 Integration of text and pictorial content
3. Hypotheses 7
4. Method 8
4.1 Participants 4.2 Apparatus 4.3 Materials 4.4 Procedure
5. Results 12
5.1 Global eye movement data
5.2 Experiment 1: Information graphics 5.3 Experiment 2: Article placement 5.4 Experiment 3: Scan paths
6. Discussion 24 7. References 27
Abstract
Readers’ eye movements were recorded as they participated in a newspaper reading experiment. The stimuli consisted of newspaper prototypes that were manipulated systematically according to two experimental conditions: (a) utilization of information graphics in conjunction with article text, and (b) variations of article placement on newspaper spreads. Viewing time of target articles was measured and compared between experimental conditions and control conditions. Readers tended to spend more time viewing articles with information graphics. They also tended to spend more time viewing articles in preferred spatial locations. Order of attention for target articles was also measured and compared between conditions. Readers tended to observe articles with information graphics later than in control conditions; articles with preferred placement was observed earlier than in control conditions. Suggestions about the pictorial properties and cognitive function of information graphics are made. Potential factors determining the spatial structure of scan paths are discussed.
2
1. Introduction
Eye movements are an integral part of newspaper reading. During news reading continuous shifts in visual focus are required, and these attentional shifts are accompanied by corresponding changes in gaze position. Overt changes in gaze position can be measured and recorded as eye movements. Eye movements basically consist of saccades and fixations. Saccades are rapid, ballistic relocations of the eye’s pupil, whereas fixations are stable states when visual information is allowed to pass through the pupil and hit the retina (particularly the fovea). With the aid of eye-tracking technology, a graphical representation of the reader’s successive changes in gaze position can be superimposed over an image of the corresponding visual field. The resultant representation of these two layers provides an objective and detailed record of the moment-to-moment unfolding of the reading process.
When combined with a behavioural methodology, the eye-tracking approach to newspaper reading puts us in a position to study how changes in the visual layout of newspaper pages affect readers’ eye movement patterns during reading. According to the behavioural paradigm newspaper layout can be regarded as a stimulus, which is taken to elicit eye movement responses in the reader. By isolating and altering various elements in the formal arrangement that constitute a given newspaper layout it is possible to infer (by trial-and-error) what arrangements lead to optimal reading patterns with respect to some predefined editorial goal (e.g. increase the amount of time readers spend on editorial material). In order to yield reliable results this procedure should be encompassed within a fully balanced experimental design.
Although eye-tracking research fits nicely within a behavioural paradigm, eye- tracking can also be coupled with a cognitive approach that sets out to investigate relations between eye movements and higher-order cognitive processes such as attention, memory, and attitude. Within the newspaper trade this approach might be interesting when evaluating advertising effectiveness and newspaper content. However, the full impact of the cognitive approach within eye-tracking research probably awaits progress in the brain sciences that could provide better explanatory models of mental phenomena in general. An independent advantage of concentrating on low-level visual cognition (that could be characterized as behaviour) is that these basic aspects of eye movements tend to reflect near-constants of the human visual system. By definition such properties can be generalized over large populations while cognitive aspects of eye movements may be subject to considerable individual variation.
Thus, whereas readers’ interest, motivation, and expectations with respect to newspaper content may vary widely between subjects, the responses of the visual system toward the graphic interface tend to be more uniform. This makes newspaper design an interesting area to study experimentally. Newspaper usability with regard to human visual function is a common motivational force behind experimental manipulations of newspaper design. Editorial re-conceptualization and economic considerations centered on advertising effectiveness are also plausible pretexts for experiments on newspaper design. In terms of usability, optimal newspaper design appears to be a compromise between built-in properties of the human visual system and the conventions that constrain the possible manifestations of newspapers considered as a mass medium. When this compromise is made the visual system has an opportunity to adapt to the informational environment of the newspaper, and communication between editors and readers is facilitated on the level of form.
3
In the present study the general motivation has been to determine what impact newspaper design has on reading behaviour. Both newspaper reading and newspaper design are complex phenomena, which need to be analyzed into constituent parts in order to be useful. Starting with newspaper reading, this process could provisionally be analyzed into scanning and reading. Scanning typically consists of short fixations and large saccades, whereas reading is characterized by comparatively small and controlled eye movements. The scanning component in newspaper reading behaviour seems to be more reliant on exogeneous perception than reading. Thus, scan paths could be an interesting correlative of the “cognitive ergonomics of a given newspaper layout” (Hansen, 1994).
In order to segment the complex concept of newspaper design, applied editorial units known as design factors have been utilized. Drawing on previous newspaper research, a limited number of design factors qualified as eligible for further investigation. Although lacking precise categorical definitions, the function of these design factors on reading patterns has been a recurrent topic in discussions within the newspaper trade. A cursory list of design factors could be generated with reference to Holmqvist and Wartenberg (forthcoming b).
• article placement • article axiality
• picture size
• picture content
• colour
• fact boxes
• drop quotes
• information graphics • headline typography • briefs
More specifically, the present study has concentrated on article placement and information graphics. Concerning article placement, we hypothesized that certain positions on a newspaper spread would prove to be more “attractive” in terms of eye movements than other positions. This attractiveness should be reflected by newspaper readers giving earlier attention and proportionately larger amounts of reading time to editorial text placed in these locations. This hypothesis is motivated by findings in earlier newspaper studies which showed that the total amount of reading time over large populations has a tendency to be unequally distributed over the surface area of newspapers. A plausible explanation to such a placement preference or placement heuristic is that long-standing newspaper conventions has conditioned readers to look for desired news at specific locations on the spread.
As for information graphics, our hypothesis was that co-occurrence of article text and graphics would enhance readers’ ability to construct mental models of textual content. If the relation between these two modes of information is complementary rather than competitive it would facilitate article comprehension and thus increase the probability of test subjects reading target articles in greater depth. The behavioural index of such a positive correlation between reading interest and information graphics would be greater total fixation times on articles featuring information graphics as compared to conditions in which information graphics were removed. Again, this hypothesis is motivated by similar but inconclusive findings in previous analyses of newspaper reading patterns.
Returning to the inventory of design factors, it will be noted that some design factors are local in nature (e.g. information graphics) whereas others are global (e.g. newspaper format, typography). The implication is that local design factors can be
4
applied selectively to an individual article while global design factors tend to affect overall newspaper design. Consequently, contrastive variations of global design factors may involve practical difficulties. Another (gradual) distinction that cuts across the inventory of design factors is whether design factors interact primarily with higher- order cognition or with lower-level reading behaviours. In accordance with this distinction it could be argued that e.g. fact boxes, information graphics, and picture content tend to inform higher-order cognitive processes whereas e.g. article placement and typography probably are a correlative of low-level exogeneous visual perception. Bearing in mind the ad hoc nature of these distinctions, they make up a two dimensional space in which design factors could be plotted.
Quantitative psychological testing of design factors is very scarce. Hitherto, only five major studies have been undertaken in which eye-tracking is applied to newspaper reading (Sternvik, 2003). Among the design factors given in the table above only colour has previously been tested under experimental conditions (Garcia and Stark, 1991). In the absence of precise behavioural metrics a qualitative method has often been preferred. Over the last years, however, eye movement measures have proved to yield such a metrics and eye movements during newspaper reading has emerged as an experimental paradigm in which newspaper design can be tested for usability.
An opportunity to enrich our understanding of the cognitive and behavioural mechanisms underlying newspaper reading has appeared on account of a recent research cooperation between the LUCS Eye-tracking Laboratory and Norrköpings Tidningar. The assumption behind this cooperation is that design factors can guide and attract readers’ attention. We believe that systematic knowledge of secondary reading abilities associated with newspaper reading has to emerge from quantitative measurement of eye movements combined with a counterbalanced experimental design. Our preliminary study of Norrköpings Tidningar complies with these criteria.
The target of the present study is to determine what impact local and global design factors have on newspaper reading behaviour. Indeed, such questions have been articulated earlier, but critical information about this interrelation has been largely inaccessible due to technical and/or methodological reasons. As for technology, this study employs non-invasive head-mounted eye-tracking equipment together with powerful data analysis software. Methodologically, our study investigates design factors in repeated and counterbalanced conditions presented to test subjects as authentic newspaper material. As our study incorporates these features, it manages to meet with experimental criteria as well as with demands on ecological validity. Alongside experimental testing of design factors, this study also utilizes a case study approach in order to describe other aspects of newspaper reading patterns.
2. Previous research
2.1 Newspaper studies
Newspaper reading is a great challenge to experimental research on reading behaviour. The difficulty of arriving at predictive theories of newspaper reading is partly due to conditions that are inherent to the communicative situation of newspapers. The fact that newspapers are a clear instance of commercial mass communication, and thus present a wide variety of information in conjunction with advertising makes it probable that any individual reader will experience an information overload in which desired and
5
undesired content has to be differentiated to some extent. In this situation readers seem to acquire a set of secondary reading abilities that typically involve strategies for scanning and integrating text and pictorial information.
Ever since eye-tracking technology became available in the late 1960’s, eye movement measures have proved to be an important component in attempts to construe a theory of reading. Understandably, eye-tracking methodology was first applied to what might be called primary reading abilities, which roughly correspond to linguistic decoding of letters ordered linearly into a text. In terms of eye movements, primary reading has become a well-defined process (Rayner, 1978; 1998).
Considering the frequent usage of pictures in conjunction with text, surprisingly little is known of how primary reading patterns extend to other areas of reading, e.g. scanning of newspaper content or integrating text and pictorial information. An eye- tracking study conducted by Garcia and Stark (1991) was one of the first inquiries that tried to validate editorial presumptions against eye movement data. A robust finding of their study was that pictures attract very early attention in the reader and that colour has a relatively small impact on overall reading patterns. Notably, Garcia and Stark did not address the question of interaction between text and picture content.
Another major insight emerging from Garcia and Stark’s study was that readers skipped a large proportion of editorial text and that scanning was a predominant acquisition strategy in the context of newspaper reading. This insight caused them to hypothesize that readers’ cognitive system utilize certain visual cues when scanning a newspaper page and that the amount of scanning is correlated to the visuo-spatial saliency and structure of these cues. A good newspaper layout should convey a hierarchical sequence of entry points that enable readers to stop scanning and start reading. In our terminology visual cues generally translate into design factors. Previous research indicate that design factors such as newspaper format, headline typography, and advertisement size could be utilized so as to approximate a match between the mechanisms of human visual perception and overall newspaper design.
More recently Holmqvist et al. (Holmqvist et al., 2003; Holmqvist and Wartenberg, forthcoming) has elaborated on the research initiated by Garcia and Stark, and has replicated some of their findings in the context of Scandinavian newspaper reading (SND/S). By using efficient analysis software (SMI iView) that allows eye movement data to be filtered through areas of interest (AOI), Holmqvist et al. have been able to provide a more precise account of the interrelation between eye movements and newspaper design. Our eye-tracking analysis software also puts us in a position to address the prevailing question of scan paths, i.e. whether news pages are scanned according to some inter-subject or intra-subject habituated order. Barthelson (2002) has extended this research into the domain of electronic news sites on the Internet. Lundqvist and Holmqvist (2001) found that an increase in advertisement size had a positive effect on how readers recognized and evaluated newspaper advertisements.
2.2 Integration of text and pictorial content
Even if pictures may serve as possible entry points into editorial text (Garcia and Stark, 1991), evidence from relevant eye-tracking research strongly suggest that text and pictures tend to be processed separately. As a consequence significant interaction between these two modes of information usually fail to occur. This segregation probably reflects that acquisition of text and pictorial information operates on radically different time scales. According to a rough model of the temporal unfolding of reading, pictures
6
could be decoded during an early processing phase (150–600 ms) while semantic processing of text occurs at a later stage (Radach et al., 2003).
     Processing mechanism
  Time period
  Initial selection
 • peripheral saliency • automated scanpath • visual search
 • pre fixation: extrafoveal
• processing concurrently with other information
  Early processing
 • skimming over text • fast picture scanning
 • first few fixations, 150 ms to 600 ms
  Late processing
 • reading of text and scanning of picture detail
• semantic processing
  • 600 ms up to several seconds
    Studies on print advertisements show that test subjects go for the text first in text-picture hybrid scenes, but that subjects would return to inspect picture details after reading the text if they received some kind of instruction (Rayner et al., 2001), or if the pragmatic relation between text and picture was implicit rather than straight forward (Radach et al., 2003). Interestingly, retention of ads was significantly better in conditions where subjects performed re-inspection of picture detail.
In conclusion, evidence suggests that pictures have positive effects on how readers build mental models of textual content if subjects manage to exploit both channels of information (Hyönä et al., 1999). However, integration of text and pictorial information does not tend to be overly utilized by readers. A probable explanation is that repeated gaze alteration and processing synchronization between text and graphics are taxing on cognitive resources (in particular short term memory).
Consequently, there are good reasons why newspaper editors would want to facilitate integration of text and pictorial content by means of design factors, e.g. information graphics in body text or implicit pragmatics between text and pictures. Such design factors could be tested and evaluated inductively through eye-tracking experiments.
3. Hypotheses
In previous case studies on newspaper reading behaviour, Holmqvist and Wartenberg (forthcoming b) have found evidence that information graphics has a strong but poorly understood effect on reading patterns. When information graphics is presented in conjunction with article text, viewing time of the article as a whole is increased considerably. Unlike other pictures, information graphics seems to be deeply involved in the processing of textual content. The first hypothesis in the present study is intended to verify the positive correlation between information graphics and viewing time under experimental conditions.
The second hypothesis also concerns information graphics, but on a different level of processing. This hypothesis is based on long-standing evidence that pictures attract early attention, and it states that information graphics will conform to this general pattern and elicit early attention in newspaper readers. Holmqvist and Wartenberg (forthcoming b) found evidence that suggested that information graphics deviated from other kinds of pictures by failing to attract readers’ early attention. The second hypothesis is intended to verify the relationship between information graphics and attention.
7
It is often asserted that article placement is not a neutral factor. On this view, articles placed in some areas of the newspaper spread are more likely to be observed and read. The second pair of hypotheses is intended to substantiate the intuition that some locations on the spread are more “favourable” or “attractive” than other locations. This issue has been investigated by Holmqvist and Wartenberg (forthcoming b), and their findings indicate that article placement does not affect the amount of viewing time spent on target articles. The third hypothesis in the present study is intended to clarify the function of article placement by stating that placement will affect the amount of viewing time spent on articles. This hypothesis runs counter to Holmqvist and Wartenberg’s findings.
The forth hypothesis is in consonance with Holmqvist and Wartenberg’s research. It is a corollary of the third hypothesis, and it states that articles with favourable placement will receive earlier attention than articles in less favourable locations.
H1: Articles featuring information graphics will elicit longer viewing time as compared to articles without information graphics.
H2: Articles featuring information graphics will elicit earlier attention as compared to articles without information graphics.
H3: Articles placed in favourable locations will elicit longer viewing time as compared to articles with unfavourable placement.
H4: Articles placed in favourable locations will elicit earlier attention as compared to articles with unfavourable placement.
4. Method
4.1 Participants
Twelve test subjects with normal or corrected-to-normal vision participated in the experiment on a voluntary basis. Test subjects were mainly recruited from the Lund University community and consisted of two women and ten men. Participants were aged 21–56 years (with an average age of 30 years). All participants stated no prior access to Norrköpings Tidningar during the week preceding the experiment. One test subject had participated in an eye-tracking experiment earlier but stated no prior knowledge as to the objectives of this particular experiment.
4.2 Apparatus
Eye movements were recorded with an iView head-mounted eye-tracker from SensoMotoric Instruments (SMI). Although viewing was binocular, only movements of the right (dominant) eye were monitored. Infrared corneal reflex (940 nm) video-based technology was used by the system to keep track of pupil relocations. A Polhemus head- tracker was used in an electromagnetic virtual reality model to compensate for head movements. The system calculated true gaze position as relative to the reading area by means of a vector analysis, which combined the vectors of eye movements and head movements. Eye positions were sampled at 50 Hz. As output, the system generated two kinds of data: MPEG video and eye movement coordinates.
8
The eye-tracker is attached to a bicycle helmet. This unit was fitted to the participants and the system was calibrated using a 13-point diagram until the average error in gaze position was less than 0.5°. The system was run on a Pentium 1700 MHz, using software from SMI to calculate numerous eye movement indices. During the reading session, participants were seated in front of a reading table that was slightly tilted towards the reader (approximately 10°).
4.3 Materials
The stimuli consisted of 16 broadsheet newspaper spreads (ca. 80 cm × 60 cm) that were distributed systematically and compiled into two eight-spread authentic-looking newspaper prototypes (dated the same day as the experiment and presented to the test subjects as the current issue of Norrköpings Tidningar). Thus, each newspaper prototype contained 8 unique test spreads + identical front page and back page. Each test spread contained target articles that had been manipulated according to one of two experimental conditions: (1) article placement, and (2) utilization of information graphics in conjunction with article text. These experimental conditions were repeated 4 times and presented in a sandwiched order within each newspaper prototype (2 conditions × 4 repetitions = 8 spreads). The experimental conditions were also counter- balanced between newspaper prototypes (2 conditions × 4 repetitions × 2 prototypes = 16 spreads). In effect, newspaper prototypes contained an eight-spread series of target articles, displaying 4 article placement conditions and 4 information graphics conditions. As these conditions were counterbalanced between prototypes, a crossed experimental design was maintained.
The newspaper content as a whole consisted of articles that had appeared in Norrköpings Tidningar previously and which were then re-used in this experimental context.
4.4 Procedure
Participants were randomly assigned to one of the two newspaper prototypes. Half read one prototype, half read the other. The newspaper prototypes were placed in front of the test subjects on a reading table. Before initiating the reading session all test subjects received an instruction (until they had received an instruction the newspaper was covered by a blank slate). Participants were told to imagine that they were sitting in a train station waiting room, and that they had come across the current issue of Norrköpings Tidningar. They were told to imagine that their train departure was due in twenty minutes and that they had an opportunity to read the newspaper during this period of time. After the instruction was finished the blank slate was removed and the reading session commenced. When 20 minutes had elapsed the reading session was terminated. Thus, a twenty-minute time limit was implemented in the experiment.
Participants viewed the newspaper pages at their own pace and in their own preferred order while their eye movements were recorded. If all pages were turned, participants were exposed to all 8 test spreads (4 article placement condition, and 4 information graphics condition). After the reading session participants filled in a short de-briefing questionnaire.
9
  Figure 1
Representative viewing patterns superimposed on newspaper spreads in the information graphics experiment. Panel A shows viewing patterns elicited by an article in the text-and-graphics condition, and Panel B shows viewing patterns elicited by an article in the counterbalanced text-only condition. Rectangles around target articles represent areas of interest (AOI). Viewing time spent within an AOI (target articles) can be computed as proportional to spread viewing time.
10
  Figure 2
Representative viewing patterns superimposed on newspaper spreads in the article placement experiment. Articles in the favourable placement condition are presented on the left page, and articles in the unfavourable placement condition are presented on the right page. The configuration of article placement and article content is counterbalanced between Panel A and Panel B. Rectangles around target articles represent areas of interest (AOI).
11
 Figure 3
The newspaper spread has been partitioned into 9 objects. A fairly typical initial scan path has been superimposed on the spread. The scanning sequence is initiated at the blue dot, and terminated at the green dot. Saccades are traced as red strokes, and fixations as small red dots. As the reader fixates the objects on the spread, a temporal ranking order is created between the objects. In this case, the actual scanning sequence can be expressed as: 7, 3, 1, 4, 9, 5, 8. Object 7 ranks as #1; object 3 ranks as #2 etc. A prototypical conventional scan path would be initiated at object 1 on the left page, and proceed linearly in the reading direction. Thus, it would be expressed as: 1, 2, 3, 4, 5, 6, 7, 8, 9. A prototypical perceptual scan path would adhere to the relative prominence of design factors. This scan path would be expressed as: 7, 3, 8, 1, 6, 2, 4, 5, 9. Mean ranking numbers from actual scan paths can be correlated with prototypical scan paths in order to reveal similarities.
5. Results
Eye movement coordinates were analyzed with SMI iView Analysis software. In both experiments, target articles on each spread were defined as areas of interest (AOI). Eye movement data from all test subjects were filtered through these AOIs. All fixation coordinates over 100 ms that fell within the rectangle of a relevant AOI were accumulated and expressed as a proportion of the total viewing time of a corresponding spreads. Eye movement durations originating from saccades and fixations under 100 ms were not included in subsequent calculations.
Absolute and relative viewing time were computed for each target article in the information graphics experiment and the article placement experiment. This procedure allowed us to compare the viewing times of text-only articles vs. text-and-graphics articles, and articles with favourable placement vs. unfavourable placement. In both experiments, two-sample t-tests were performed to test significant differences in viewing time between target articles (c.f. Figure 1–2).
12
A different approach was needed when calculating relative order of attention between target articles in both experiments. In this context, the accumulated amount of viewing time was an irrelevant measure. Instead, the iView Analysis tool was utilized in order to partition each newspaper spread into approximately 9 objects. Test subjects’ initial scanning sequence was then expressed as the order in which they fixated (attended to) each of the 9 compartments of the spread. The initial scanning sequence was terminated when test subjects initiated reading. In this analysis reading was defined quantitatively as 20 or more successive fixations on one single object. When the initial scanning sequence was determined, it was possible to see in which relative order target articles were attended to, both as compared to each other and as compared to other objects on the spread. As this analysis generated ordinal data, a Wilcoxon Signed Rank test was used to test significant differences in order of attention (c.f. Figure 3).
In the third experiment an attempt was made to analyze the spatial structure of the initial scan paths that readers performed on the newspaper spread. Some putative factors determining the structure of scan paths were also investigated.
5.1 Global eye movement data
Newspaper reading involves both scanning and reading. This combined activity will be referred to as viewing. The distinction between scanning and reading was applied to the editorial content of the newspaper prototypes in our experiment (advertisements excluded). It was found that approximately 88% of all the material (defined as 9 objects covering the newspaper surface area) was seen, whereas 38% was read. Seeing was defined as 1 or more fixations during scanning or reading. Reading was defined as “having read article brief and/or started to read body text”. The quantitative counterpart of this definition was 20 or more successive fixations on one single object. The proportion of objects that were read in depth, i.e. 200 or more successive fixations to one single object, was estimated to 9.7%.
Another global analysis showed that the first couple of spreads in the newspaper tended to receive considerably more viewing time. A decrease in viewing time was consequently associated with latter spreads. This “diachronic” variation in viewing time is given in Figure 4.
Figure 4
 250
200
150
100
50
Spread viewing time
                 0
1 2 3 4 5 6 7 8 9 10
Spreads
            13
Time (s)
The spatial distribution of eye movements on the surface area of the newspaper spread. The spread area was divided into 16 areas of interest, and then the entire set of eye movement data was filtered through this grid. The viewing time of each such AOI was calculated as a percentage of the total amount of viewing time of the whole spread. This distribution is given in Figure 5. Red areas signal large proportions of eye movment traffic.
Figure 5
Left page: 61,365 Right page: 38,635
A pronounced left-page dominance in viewing patterns is immediately visible. The left page receives a total of 61.3% of the viewing time, whereas the right page gets a mere 38.6%. This viewing pattern is not uncommon in newspapers, and to a certain extent it is explained by the fact that advertising often appear in the bottom right corner. Holmqvist and Wartenberg (forthcoming b) obtained similar results in the SND/S study they conducted.
5.2 Experiment 1: Information graphics
The purpose of this experiment was to isolate information graphics as an independent design factor and describe the effect that it exerts on general newspaper viewing patterns. The stimuli consisted of pairs of identical newspaper spreads that differed only as to whether or not information graphics appeared in conjunction with target article text. Articles in these conditions occurred in 4 counterbalanced versions.
Concerning information graphics we hypothesized that the presence of this design factor in conjunction with body text would lead to greater interest in the reader and that this increase in interest would be reflected as longer viewing times of target articles. Indirectly, information graphics would elicit a greater quantity of fixations on the target article. As a corollary, a null-hypothesis would be confirmed if information graphics failed to produce any differences in viewing patterns.
In Table 1 viewing times of target articles in the information graphics experiment are presented in the text-only condition and in the text-and-graphics condition. In both conditions mean values of absolute viewing time (M) with standard deviation (SD) is presented first. Absolute viewing times are also presented as a percentage of the total viewing time spent on the corresponding spread (Relative). On average, 7577 ms were spent on articles in the text-only condition (7.7% of spread viewing time), whereas 18996 ms were spent on articles in the text-and-graphics condition (20.9% of spread viewing time).
    6,949
  4,908
      5,529
     3,328
   8,751
 4,695
   6,124
   4,589
   11,707
 7,961
    7,100
   4,695
   9,417
  6,976
    4,624
     2,645
        14
Table 1
Absolute and relative viewing times spent on articles in information graphics conditions. Absolute viewing times are in milliseconds and relative viewing times are given as a percentage of spread viewing time.
  Text only
M SD
1 13658 18388
Text and graphics
Relative M SD 8.7% 19297 33801
18.4% 4784 2564 2.7% 34879 33171 3.2% 19076 37374
7.7% 18996
Relative
12.1% 9.1% 56.8% 22.0%
20.9%
        2 13619 3 632 4 1837
M 7577
21320
 534
2699
  Generalizing over all instances of information graphics (1–4), a
variance t-test showed that information graphics elicit significantly longer relative viewing times for target articles (p<0.01). This is also true of absolute viewing times, although this measure yields a lower level of significance (p<0.05). When treated separately, each individual instance of information graphics does not replicate the general pattern. In three of four cases (1, 3, 4) a higher percentage of viewing time is spent on articles with information graphics, but only in one case (3) is this effect significant (p<0.001). In one case (2) the general viewing pattern is reversed, but not significantly so (p=0.204). Separate analyses of cases 1 and 4 (p=0.109) suggest a non- significant positive trend exerted by information graphics on viewing patterns.
A complication in this analysis is that articles featuring information graphics necessarily occupies a larger surface area than the same article in the counterbalanced version, i.e. text without information graphics (c.f. Figure 1). Consequently, it had to be determined if the positive effect of information graphics on viewing patterns (fixation times) was caused by the trivial fact that information graphics in combination with body text constituted a larger spatial element than body text only. This scenario would undermine any hypothesis about an intrinsic relation of mutual reinforcement (a tandem effect) between body text and information graphics.
Previous research findings indicate that spatial size in itself is a powerful factor in graphical design (Lundqvist and Holmqvist, 2001). In order to exclude differences in size as the main cause for the association between information graphics and an increase in viewing durations, a secondary analysis was performed on our data. This analysis was supposed to compensate for differences in size between corresponding articles with and without information graphics. To this end, viewing times were calculated as relative to object area.
Articles containing information graphics were measured along with articles in the counterbalanced text-only versions. The respective areas of these objects were calculated in terms of screen pixels. On average, the area of text-only articles was about 65% of the area of text-and-graphics articles. This difference made it reasonable to believe that the amount of time spent on articles in the text-and-graphics condition was exaggerated because of larger surface area. Finally, viewing times were computed in terms of milliseconds per pixel. The result of this proportional analysis is given in Table 2. As comparative figures, absolute viewing times are provided in this analysis also.
15
two-sample equal
Mean values (M) and standard deviation (SD) are given of absolute viewing times. Relative viewing times are given as milliseconds per pixel.
Table 2
Absolute and relative viewing times spent on articles in information graphics conditions. Absolute viewing times are in milliseconds and relative viewing times are given as milliseconds per pixel.
     M
1 13658 2 13619 3 632 4 1837
M 7577
Text only
SD
 18388
 21320
  534
  2699
Relative
0.4934 0.2614 0.0171 0.0255
Text and graphics
M SD
19297 33801 4784 2564 34879 33171 19076 37374
Relative
0.3706 0.0726 0.6112 0.2131
0.3155
     0.2127 18996
  This analysis shows considerable difference in fixation density between conditions in the information graphics experiment. The results conform to the previous analysis as it reveals that text-and-graphics articles (0.3155 ms/pixel) are read to a greater extent than text-only articles (0.2127 ms/pixel). The viewing density values provided by this analysis are compatible with previous calculations of the same measure. In a study concerning looking times toward print advertisements, Rayner et al. (2001) established this measure in the range between 0.112 and 0.020 ms/pixel. The fact that our figures are slightly higher probably reflects that printed newspaper material has a higher inherent informational density than full-page advertisements studied by Rayner et al.
When comparing viewing times as relative to article area, the general positive effect on viewing times previously associated with information graphics failed to reach significance (p=0.190). However, a non-significant positive trend was maintained. Looking into each instance of information graphics separately, a significant positive effect occurs only in case 3 (p<0.01).
Since the two proportional analyses on relative viewing times of information graphics articles yielded similar but not completely converging results, a closer examination of the interaction between text and graphics in articles featuring these two modes of information was performed. The specific distributional pattern of viewing time over these two elements would indicate how the previous results should be interpreted and evaluated.
Articles containing information graphics were divided into two constituent parts (text and graphics), and the individual areas of these constituent parts were calculated in terms of pixels. On average, the graphics portion of the article was approximately 59% of the size of the text portion. Absolute viewing times were then computed separately for each of these areas. Finally, relative viewing times for text regions and graphics regions respectively were computed, both as viewing time per unit area, and as a percentage of total viewing time spent on the relevant spreads. The results of the latter analysis are given in Table 3. As in previous analyses, mean values (M) and standard deviation (SD) are given of absolute viewing times. Absolute viewing times are also presented as a percentage of spread viewing time (Relative).
16
Table 3
Absolute and relative viewing times spent on text and graphics respectively. Absolute viewing times are in milliseconds and relative viewing times are given as a percentage of spread viewing time.
  Text
M SD
1 8776 18390 2 3419 2121 3 18247 21044 4 13034 24765
M 10433
Relative
4.1%
7.3% 24.4% 10.6%
11.5%
Graphics
M SD
Relative
         10522 15961 6.0% 1365 515 3.3%
16632 16549
6042 12625 5.2%
22.6% 8563 9.4%
   On average, the distribution of fixation time between the text portion and the graphics portion was quite equal. A t-test showed no differential effects and thus hinted at an equal distribution of fixation time between text and graphics (p=0.320). A small predominance can be detected on account of the text portion (11.5% of spread viewing time) as compared to the graphics portion (9.4% of spread viewing time).
Interestingly, the viewing ratio between text and graphics is reversed when viewing times are measured as relative to object area. In this analysis the predominance is on the graphics portion (0.3527 ms/pixel) as compared to the text portion (0.2895 ms/pixel). This difference is not significant, but higher values of viewing density for graphics could reflect the fact that the graphics area tended to be only half the size of the text area.
The second hypothesis concerning information graphics was that articles featuring this design factor would receive earlier attention by readers as compared to articles in the counterbalanced text-only conditions. In order to test this hypothesis, each newspaper spread in the information graphics experiment was partitioned into 9 separate objects (c.f. Figure 3), one of which consisted of information graphics target articles. Then, the initial scanning sequence that readers performed when turning page to an information graphics newspaper spread was measured. When this scanning sequence was coupled with the nine-object array on the newspaper spread, it was possible to calculate what position information graphics articles occupied in the scanning sequence. The order index of articles in the text-and-graphics condition was then compared to the order index of articles in the text-only condition. Results from this analysis are given in Table 4. Mean ranking values (M) and standard deviation (SD) for target articles in the text-only condition and the text-and-graphics condition. Lower values indicate early attention.
17
Table 4
Mean position in scanning sequence for articles in information graphics conditions.
   Text only
M
1 9.7
2 4.8
3 3.6
4 7.8
M 6.6
Text and graphics
SD M SD
3.3 11.0 0.0 3.2 7.0 4.2 3.6 5.8 3.8 3.7 10.0 0.0
8.2
          As is evident from the mean values, our hypothesis concerning early attention of information graphics articles was not supported. Rather counter-intuitively, it turned out that text-only articles were fixated earlier (6.6) than counterbalanced text-and-graphics articles (8.2) on a scale from 1–10. A Wilcoxon Signed Rank test showed that this difference in scanning sequence was non-significant (p=0.171). Thus, although actual viewing patterns in this analysis run contrary to our second hypothesis concerning information graphics, this pattern did not reach significance. As a consequence, null- hypothesis stating no significant difference in scanning order in the information graphics condition was supported in this experiment.
5.3 Experiment 2: Article placement
In the article placement experiment we sought to determine if general newspaper viewing patterns are biased in the sense that editorial material is read to a greater extent if placed in certain favourable spatial locations on the newspaper spread. Conversely, we wondered if unfavourable article placement elicited less viewing times for these articles. Spatial location on a spread was defined as favourable or unfavourable on the basis of previous newspaper research that has found evidence of left-hand page predominance in terms of accumulated viewing time as compared to the right-hand page. According to this evidence, left-hand placement in general would be favourable. However, as these findings build on low-resolution statistical analyses of large amounts of eye movement data, we designed this experiment so as to provide a more detailed picture of how article placement operates as a design factor.
Our specific hypotheses concerning article placement was that favourable placement would (a) elicit longer viewing times (total fixation duration), and (b) elicit earlier attention in readers as they initiated a scanning sequence of relevant newspaper spreads. A null-hypothesis would be confirmed if none of these predictions were evidenced in viewing patterns.
The stimuli in this condition consisted of pairs of identical newspaper spreads. In one spread a certain article would appear at a favourably biased spatial location and another article would appear at an unfavourable position. In the second spread, this layout would be counterbalanced so that the unfavourable article appeared at the favourable location and vice versa (c.f. Figure 2). The article placement experiment was designed in 4 separate counterbalanced versions. Version 1 tested viewing bias in vertical placement; version 2 tested medial placement; version 3 tested horizontal placement, and version 4 tested diagonal placement.
18
In Table 5 viewing times of articles in the placement experiment are presented in the favourable condition and in the unfavourable condition. For both conditions, mean values of absolute viewing time (M) and standard deviation (SD) are presented in the first columns. Absolute viewing times are also presented as a percentage of the total viewing times of corresponding spreads (Relative). On average, 11415 ms were spent on articles in the unfavourable condition (6.1% of spread viewing time), whereas 14994 ms were spent on articles in the favourable condition (10.2% of spread viewing time).
Table 5
Absolute and relative viewing times spent on articles in the placement conditions. Absolute viewing times are in milliseconds and relative viewing times are given as a percentage of spread viewing time.
   Unfavourable placement
Favourable placement
M SD
    M SD
Relative
8.2% 3.3% 8.6% 4.4%
6.1%
Relative
     1
2
3
4 9221 16056
9702 19039 5.8% 7810 13324 7.4%
14994
17631 29831 4357 7145 15243 25957
33391 39447
8762 13537 4.8%
M 11415
22.4% 10.2%
   Generalizing over all instances of article placement (1–4), a two-sample equal variance t-test showed that favourable placement elicit significantly longer relative viewing times for target articles (p<0.05). When treated separately, three of four cases (2, 3, 4) show a higher percentage of viewing for articles placed at favourable positions, although this increase in viewing time fails to reach significance, except in case 3 (p<0.05). In one individual case (1) the general pattern is reversed, but not significantly so (p=0.243).
Thus, in the article placement experiment a similar picture to that of information graphics emerges. At a general level, favourable article placement elicit significantly longer viewing time of target articles, but at an individual level, only one instance of favourable article placement replicates this viewing pattern to a significant extent; in the other cases a non-significant positive trend prevails.
Unlike the information graphics experiment, the article placement experiment does not call for a secondary analysis to counter differences in size between target objects. In the placement experiment articles were almost equal in size, and consequently no proportional analysis was necessary to avoid skewing effects.
The second hypothesis concerning article placement stated that articles placed at favourable locations would elicit earlier attention in the reader as compared to articles located at unfavourable locations. Collapsing all instances of the placement experiment into one general Wilcoxon Signed Rank computation of order preference predictably yielded a strong significant effect in favour of articles located in the biased left-hand page of the newspaper spread (p<0.0001). In order to achieve a more fine-grained analysis of how article placement is reflected in readers’ initial scanning sequence of a target spread, each instance of the article placement condition was also treated separately. Results from this analysis are given in Table 6. Mean ranking value (M) and
19
standard deviation (SD) for target articles in the favourable and unfavourable placement conditions. Lower values indicate early attention.
Table 6
Mean position in initial scanning sequence for articles in placement conditions
Unfavourable placement Favourable placement
M SD M SD
           1 5.3
2 9.2
3 6.9
4 9.2
M 7.6
2.1 1.9 3.7 2.7
3.1 2.4 6.0 3.4 2.8 2.2 5.5 3.7
4.3
   Three of four instances in the article placement condition reached significance independently when the placement variable was tested against order of attention. With one exception, articles placed in favourable locations received earlier attention. In the vertical version (1), this means that the topmost article was attended to earlier (p<0.05). In the medial version (2), the article located at the center of the spread was attended to earlier than the article in the lower right (distal) corner (p<0.05). In the horizontal version (3), the article with left axial placement received earlier attention than the article with right axial placement (p<0.05). Finally, in the diagonal version (4) the article placed in the lower left corner of the spread was attended to earlier than the article in top right corner, although this difference was non-significant (p=0.139).
5.4 Experiment 3: Scan paths
Many researchers in the eye-tracking field have attested to the difficulty of quantifying scan paths (Rayner et al., 2001; Josephson et al., 2002; Barthelson, 2002). A scan path is a generalized series of brief fixations during visual search and similar tasks, and this series can be analyzed at different levels of complexity. A complete analysis of scan paths would account for exact fixation locations on a visual stimulus, exact spatial orientation and spatial extent of consecutive saccades, and exact temporal sequence of fixations. To control only one of these variables is labourious, and to control all of them is almost unmanageable. The fact that the human visual system accomplishes about 3 saccades per second contributes to a rapid growth of scan path information that has to be systematized.
In this study, scan paths during newspaper reading are analyzed at a moderate level of complexity. Scan paths are defined as the temporal order in which objects on the newspaper spreads are fixated (and, by extension, attended to). Thus, constituent fixations during scan paths are not analyzed in terms of exact coordinates, but in terms of hits on graphical objects, usually corresponding to articles. The sequential order in which these objects receive attention is accounted for. As there is no specific experimental condition involved in this analysis, it becomes more of a case study, and results should be interpreted accordingly.
The purpose of this analysis is to describe the eye movements that occur between the moment that readers turn page to a new spread, and the moment when readers start reading an article on that spread (reading is defined as 20 or more successive fixations
20
to one single object). It is reasonable to believe that this initial scanning sequence is critical when readers orient themselves on the newspaper spread and decides where to start reading. It is also probable that readers rely extensively on graphical entry points such as pictures, headlines, and drop quotes during this initial phase in the reading process.
As a preliminary measure, the temporal duration of initial scan paths was measured and calculated over all participants. On average, this scanning sequence lasted during 11.2 seconds. When compared to the average amount of viewing time that readers spent on each spread (115 sec), it appears that the initial scanning sequence occupies about 10% of average spread viewing time. During the initial scanning sequence, readers attended to about 51% of the objects on the spread.
A precondition of the scan path analysis was that every newspaper spread was partitioned into a roughly equal number of objects (7–9). This operation was performed in the iView Analysis tool, in which all objects were defined as areas of interest. There was no one-to-one correspondence between articles and AOIs. Instead, one AOI could include several articles, and conversely, an AOI could be defined by a graphical element smaller than an article (c.f. Figure 3). When scan paths from individual readers were superimposed on this array of objects, it was possible to determine on which object the scan path was initiated, and the order in which subsequent objects were fixated. Thus, in this analysis a scan path is equal to temporal ranking order between objects on a newspaper spread.
When considering the general structure of newspaper scan paths, a logical point of departure is to begin by inquiring about the starting point of these scan paths in relation to the surface area of the spread. Thus, in the first spatial analysis on scan paths, it was investigated whether readers’ first fixation landed on the left or right page of the spread. This analysis necessarily induced a binominal distribution of the scan path data. Scan paths that were initiated by a fixation on the left page were simply marked “1”, whereas fixations on right page were marked “0”. Subsequently, a chi-square test was applied in order to determine if the frequency of left-initiated scan paths was significantly different from that of right-initiated scan paths.
With reference to previous newspaper research, it was expected that a significant right-page predominance would emerge (Holmqvist and Wartenberg, forthcoming b; Hansen, 1994; Garcia and Stark, 1991). However, it turned out that scan paths were initiated equally often on the right page as on the left page, and the difference was non- significant (p=1). This result is surprising since a stable finding in earlier newspaper studies has indicated a definite right-side predominance. It has been suggested that the manual movement with which pages are turned subjects scan paths to a physiological constraint that necessitates initial right page fixations. Results derived from the present data do not support this lateral bias in initial scanning sequences.
Having established no significant differences in left-right distribution of initial fixations in scan paths, the unit of analysis was extended from first fixations to include all fixations in initial scan paths. In this analysis, it was queried whether a difference in left-right distribution could be detected over the whole initial scanning sequence. Since each scan path consisted of the temporal rank order in which objects on the corresponding spread had been fixated, it was easy to partition each scan path on a left- right basis in order to determine whether scan paths as a whole were concentrated to the left or right page of the spread.
21
Since the left-page and right-page portions of scan paths consisted of ordinal numbers, the data were submitted to a two-sample Wilcoxon Signed Rank test. In this analysis, scan paths were utilized in order to test the rank order of all the objects/articles on the left page against the rank order of all the on the right page. Generalizing over all data, the results indicate that objects on the left page were viewed earlier than objects on the right page. This effect was strongly significant (p<0.0001). To some extent, these results echo and reinforce the results from the placement analysis, albeit on the level of scan paths.
As yet, scan paths have been dismembered and discussed in terms of general distributional patterns, but nothing has been said of the spatial structure of scan paths. In the final analyses, an attempt will be made to answer some questions concerning the properties of objects that elicit early attention during the initial scanning sequence. In other words, this analysis will target the various factors that determine the structure of entire scan path sequences, and the relations between constituent fixations.
It will be assumed that the initial scanning sequence that readers perform upon turning to a new newspaper spread consists of a conventional component and a perceptual component. The first component could be perhaps be characterized as a habituated left placement preference, and it attempts to capture the consistent preference (early attention) of objects with left placement on the spread. This component suggests that scan path formation may have been influenced by an editorial convention of placing prioritized news articles in the top left corner of the spread. As a reaction to this convention readers may have responded by subjecting initial scanning to endogeneous control (e.g. a rule stating: “look top left first”). Such a heuristics would account for the left dominance recurrent in scan path data. The second component determining the structure of initial scan paths would be a hard-wired perceptual component, which reacts primarily on graphical properties of newspaper layout (design factors). In contrast to the first component, this component would be an intrinsic property of human visual function, which dictates certain eye movement responses to certain visual stimuli through exogeneous control (Klein et al., 1992). These two components (or factors) probably contribute to the formation of scan paths, although on two different levels of cognitive control (top-down vs. bottom-up). If this conjecture holds true, it should in principle be possible to disentangle the respective influences that these factors exert on initial scanning sequences.
In an attempt to separate these two components, two prototypical scan path orders were stipulated; one conventional, and the other perceptual. The conventional order started in the top left corner of the newspaper spread, and proceeded more or less linearly in the reading direction until the bottom right corner was reached. By contrast, the perceptual order was constructed with reference to design factor entry points such as pictures, large headlines, and drop quotes. The perceptual ranking order was established by calculating the relative perceptual saliency of a number of graphical elements that appeared in the newspaper layout. This ranking order is given in Table 7.
22
Table 7
Relative order of attention between design factors
   Design factor
1. Pictures
2. Large headline
3. Articles
4. Short paragraphs
5. Drop quotes
6. Information graphics 7. Advertisements
M SD
1.3 0.5 1.5 0.5 2.9 0.6 4.0 0.8 4.4 2.2 5.0 1.4 5.2 0.8
     Through a best match procedure, the actual scan paths performed by readers were then correlated with the two prototypical rankings in order to determine whether actual scan paths were dominated by a conventional component or a perceptual component. High correlation numbers between actual scan paths and the prototypical conventional ranking order would imply that initial scanning was determined by conventional, endogeneous control, whereas a high correlation between actual scan paths and the prototypical perceptual ranking order would imply that initial scanning was determined by perceptual, exogeneous control. Correlation numbers between actual scan paths and the two prototypical scan paths are given in Table 8. Higher correlation numbers indicate a better match between actual scan paths and the respective prototypical scan path. As every spread has a unique layout structure, this analysis was performed separately on each spread (1–8).
Table 8
Correlations between readers’ actual scan paths and two prototypical scan paths
  Correlation with conventional scan path
1 0.6803
2 0.9122
3 0.6391
4 0.6568
5 0.45
6 0.4833
7 0.936
8 0.6167
M 0.6718
Correlation with perceptual scan path
0.3024 0.6495 0.7456 0.7633 0.65 0.6667 0.3931 0.6167
0.5984
     Keeping in mind that these correlation numbers are derived through a case study approach and not in a controlled experiment, the results indicate that actual scan paths are slightly biased toward a conventional scanning sequence (0.672) as opposed to a perceptual scanning sequence (0.598). However, a t-test revealed that this difference was non-significant.
23
6. Discussion
In the experiment concerning information graphics, readers tended to spend more time viewing articles in the experimental text-and-graphics condition than articles in the text- only control condition. On average, readers spent 7.6 seconds on text-only articles, which is about 7.7% of the time spent on corresponding spreads. By contrast, readers spent a mean viewing time of 19 seconds on articles featuring information graphics, which is about 20.9% of the time spent on corresponding spreads. Thus, articles with information graphics elicited significantly longer viewing times both in absolute and proportional measures than articles without information graphics. This is a general trend, which is not necessarily replicated in each condition of counterbalanced information graphics articles.
The same general trend was evident when viewing time of information graphics target articles was calculated as relative to article area in both conditions. However, in this analysis the general positive effect elicited by information graphics failed to reach significance, except in one individual instance of counterbalanced target articles. On average, the viewing density of text-only articles was 0.21 milliseconds per pixel, as compared to 0.32 milliseconds per pixel for text-and-graphics articles.
Arguably, the fact that the second analysis yielded weaker results in favour of information graphics could be attributed to differences in size between articles in the control condition and in the experimental condition. Text-only articles measured about 2/3 the size of text-and-graphics articles. When this size difference was accounted for, the general positive effect on viewing time elicited by information graphics was diminished. This means that a size factor probably acts a component that is involved in the overall positive effect elicited by articles with information graphics. However, calculating viewing time as relative to article area did not cancel out the increase in viewing times completely. This suggests that although bigger size could be an effective factor participating in articles with information graphics, this factor remains subordinate to the positive effect elicited by information graphics as a complement to article text, and as a design factor in its own right.
When testing the effect that information graphics exerted on early attention, the results were decidedly negative (although statistically non-significant). This finding means that articles with information graphics received later attention than articles without information graphics. When all articles on information graphics newspaper spreads were divided into 9 objects, text-only articles ranked better during readers’ initial scan path sequence (6.6) than text-and-graphics articles (8.2). This result is very intriguing since it suggests that information graphics elicit a completely reversed eye movement pattern in comparison with other kinds of pictures, such as ordinary photos. Photographical pictures in conjunction with text invariably lead to very early attention (i.e. early fixations in readers’ initial scan path). In this study, articles with photographic pictures received a ranking order index of 1.25 on a scale from 1–8. It appears that the general content and function of information graphics pictures is decoded pre-attentively through peripheral vision, and that foveal attention is somehow deferred with regard to these elements.
Granted that information graphics can be identified as such through peripheral vision, it is possible that this design factor signals complex information that readers judge as inherently interesting but demanding on cognitive resources. Thus, as readers try to maximize informational intake during a limited period of time (sub-optimal
24
reading conditions), news with more accessible contents is attended to first, and information graphics articles are dealt with if there is time left.
The combined results on information graphics show that this design factor elicits longer viewing time, but later attention. These results exactly parallel previous research results obtained by Holmqvist and Wartenberg (forthcoming b). In a large study involving several Nordic newspapers (SND/S), they found evidence that supported significantly longer viewing times and significantly later attention for articles with information graphics. In the present study, significant results is only replicated in terms of viewing time, but the general tendency in the case of attention toward information graphics is very similar. Using a case study approach rather than a controlled experiment, Holmqvist and Wartenberg compared the effect of information graphics with the effect of other kinds of pictorial information (maps, drawings, and photos) and found that information graphics elicited significantly longer viewing time of associated textual information than any other type of picture.
They conclude with the following remark: “The special status of information graphics is intriguing: They are seen later, but increase the observation time more than any other of the three types of images [maps, drawings, photos]. Obviously, information graphics is an interesting topic to study further: Does it compete with the text it is supposed to complement, for instance?” (forthcoming b).
In an attempt to answer the last question posed by Holmqvist and Wartenberg, the present study has sought to elucidate the relation in terms of eye movements between textual and pictorial information in information graphics articles. When dividing information graphics compound articles into a text portion and a picture portion, it was found that on average the text portion elicited a viewing time of approximately 10.4 seconds (11.5% of the viewing time spent on information graphics spreads as a whole), whereas the graphics portion elicited about 8.6 seconds (9.4% of spread viewing time). Even if these figures reveal a small predominance on account of the text portion, the general pattern indicates a more or less equal distribution of viewing time between text and graphics. This is an interesting finding since it implies that textual and pictorial information can co-exist within the same article. If text and graphics competed for viewing time, then, arguably, a more skewed relation between text and graphics would emerge, and total viewing time of information graphics articles would be negatively affected.
There is another interesting aspect on the equal distribution of viewing time between text and graphics. In an eye-tracking experiment concerning looking behaviour toward print advertisements, Rayner et al. (2001) addressed the question of how text and pictorial information are integrated in terms of eye movements. The stimuli in the experiment consisted of full-page advertisements with textual information in conjunction with background pictures, and the analysis of eye movement data involved a division of advertisements into separate portions of text and pictures. An important finding was that the text portion received considerably more looking time (70–80%), and that looking behaviour was text-dominated rather integrative in this respect. Rayner et al. concluded that pictures could be decoded in a only a few fixations, whereas text needed more fixations because of a higher informational density. These results are interesting since they diverge so clearly from the results obtained in the present study concerning interaction between text and information graphics. And the reason for this divergence probably lies in the unique pictorial properties of information graphics. In
25
distinction to other kinds of pictures, utilization of information graphics in conjunction with article text results in comparable viewing times between text and picture portions.
Turning to article placement, this experiment showed that readers tended to spend more viewing time on articles in the favourable placement condition than articles in the unfavourable placement condition. On average, readers spent 11.4 seconds on articles with unfavourable placement, which is about 6.1% of the viewing time spent on corresponding spreads. In the favourable placement condition readers spent 15 seconds on target articles, which is equivalent to 10.2% of the spread viewing time. The common denominator of articles with favourable placement is that they reside on the left page of the newspaper spread. This experiment shows that in terms of eye movements there is real motivation behind a nomenclature such as “favourable” vs. “unfavourable” placement: favourable placement leads to more viewing time of target articles.
Another aspect of the favourable vs. unfavourable distinction is the relative order of attention between articles in these conditions. In this experiment the placement variable was substantiated in this respect also. Thus, articles with favourable placement were attended to earlier than articles with unfavourable placement. When all articles on the relevant spreads were divided into 9 objects, articles with favourable placement ranked considerably better (4.3) during readers initial scanning sequence than articles with unfavourbale placement (7.6).
The placement variable reached significance both in terms of viewing time and in terms of order of attention. These results are reflected on a more general level of analysis. When the entire set of eye movement data was analyzed as relative to 16 areas of interest covering the surface area of one newspaper spread, a strong tendency of left lateralization emerged in general viewing behaviours. The left page (i.e. the 8 leftmost AOIs) received 61.4% of the accumulated viewing time, whereas the right page received a mere 38.6%. To a certain extent, this skewed proportionality can be explained by the fact that advertisements tend to be placed on the right page, and advertisements usually receive low levels of eye movement traffic. However, on the newspaper spreads that were included in the article placement experiment, advertising was very scarce. Thus, the left placement preference evident in the current data probably did not emerge as a consequence of readers trying to evade right-page advertising and thereby ending up on the left page. It is more plausible that the “ad- evasion behaviour” has become generalized in readers, and that this behaviour also affects newspaper spreads that are free from advertising.
Interestingly, left lateralization of viewing behaviour was not always pronounced on newspaper spreads that tested article placement. When advertising was absent in the bottom right corner of the spread, readers used a fair amount of time to inspect editorial material in these locations. This might indicate that previously referred to “ad-evasion behaviour is rather flexible and not yet a fixed habit. In the SND/S study conducted by Holmqvist and Wartenberg (forthcoming b), a distributional analysis was made in which eye movement data relating to pages with advertising were removed. The fact that this procedure caused the lateralization effect to vanish also points in the direction that readers tend to inspect all locations on the newspaper spread that are free from advertising. An indication in the opposite direction is given in Hansen (1998). Test subjects’ eye movements were recorded during reading of the Yellow Pages telephone directory, and a left lateralization effect was evidenced in spite of an arbitrary spatial
26
distribution of advertising. These results suggest that lateralized viewing behaviours are not entirely contingent on graphical structure.
The experiment concerning scan paths could only hint at possible factors determining the spatial structure of readers’ initial scanning sequence on newspaper spreads. No evidence was found of any left-page or right-page preference regarding the initial starting point of scan paths. Starting points seemed to have an equal left-right distribution. However, as scan paths were extended beyond the starting point, a clear left preference emerged. Objects on the left page were attended to significantly earlier than objects on the right page. This finding consolidates the results concerning article placement.
Relative order of attention between objects on the spread was also calculated on the basis of the perceptual cues (e.g. pictures, large headlines, and drop quotes) that objects were furnished with. Objects with pictures ranked best (1.3) in this analysis, which means that objects with pictures received earlier attention during initial scan paths than any other object. Large headlines were a close runner up (1.5). These findings are consistent with previous research.
Since it appeared that the structure of readers’ initial scanning sequence was determined both by left placement of objects, and by perceptually salient design factors, it was necessary to perform a secondary analysis on the scan path data in order to establish which determining factor that exerted the strongest influence on scan path structure. The left placement preference was considered a conventional artifact, reflecting editorial habits of placing articles with high news value in the top left corner of the spread. Thus, the left placement preference was expressed as a prototypical conventional scan path, which proceeded from top left to bottom right in the reading direction. By contrast, the attraction exerted by design factors was considered a perceptual constant of the human visual system. This behavioural bias was expressed as a prototypical perceptual scan path, which targeted the most prominent design factors in the same order as they were ranked (i.e. pictures first, then large headlines etc.).
Organized by different principles, this pair of distinct prototypical scan paths was then compared to actual scan paths performed by readers. As it turned out, a marginally higher correlation prevailed between actual scan paths and the conventional prototype. Thus, left placement had a greater predictive power than design factors as concerning the spatial structure of scan paths, but this difference did not prove statistically significant. A considerable spatial overlap between conventional and perceptual scan paths made it difficult to disentangle these prototypes from each other.
What conclusions can be drawn from the present results concerning eye movements and newspaper reading? Generally speaking this study shows that eye movements actually are sensitive and responsive to various design factors that are applied in building the graphical layout of newspapers. Consequently, reading behaviour can be affected by newspaper design. Under these circumstances the critical concern might be to ensure that reading behaviour is affected in the right manner. That is, eye movement behaviour should be affected in way that is meaningful for the reader; all influences on eye movements are not necessarily beneficial.
Concerning information graphics the present study suggests that readers actually take advantage of this pictorial mode of information. Presumably, this utility enables readers to process corresponding textual information more effectively. However, the
27
methodology applied in this study does not provide evidence for such interpretations. If it is proved that information graphics has the cognitive function of allowing better comprehension of textual content, then newspaper designers seem to dispose over a graphical device that increases usability and reading value of newspapers.
Concerning article placement the results obtained in this study seem to substantiate an enduring intuition of newspaper designers, namely that placement of articles on a newspaper spread is all but a neutral design factor. It appears that articles with left-page placement elicit longer viewing time and earlier attention than articles placed in other locations. A plausible explanation would be that readers rely on some kind of conventionalized heuristic causing them to look for important news in the top left corner. Newspaper designers’ knowledge of such reader heuristics or reader strategies could motivate efforts to guide readers’ attention toward other areas of the spread that receive less eye movement traffic. In this way, a larger portion of the newspaper’s surface area could be put to use.
In conclusion, this study should be an incitement to pursue more research concerning the behavioural and cognitive functions of design factors. A highly relevant question might be to ask how design factors are utilized within readers’ viewing strategies. Another relevant question pertains to the pattern of interaction between various design factors. In the future, it is likely that the newspaper trade will have to take a more conscious stance concerning which designing solutions that are optimal with regard to readers’ eye movement patterns.
7. References
Barthelson, M. (2002). Behaviour in online news reading. Master’s thesis. Department of Cognitive Science, Lund University.
Carroll, D.W., (1999). Psychology of language. Pacific Grove, Calif.: Brooks, Cole Publications.
Garcia, M. and Stark, P. (1991). Eyes on the news. St Petersburg, FL: The Poynter Institute for Media Studies.
Gazzaniga, M., Ivry, A., Mangun, G. eds. (2002). Cognitive neuroscience: The biology of the mind. New York: Norton.
Hansen, J.P. (1994). Analyse av læsernes informationsprioritering. Kognitiv systemgruppen. Forskningscenter Risø, Roskilde.
Hansen, J.P. (1998). Reading in Yellow Pages. Presentation at the Lund-Risø eye- trackning meeting in November 1998.
Hegarty, M. (1992). The mechanics of comprehension and comprehension of mechanics. In: K. Rayner (ed.), Eye movements and visual cognition: Scene perception and reading (pp. 428–443). New York: Springer-Verlag.
28
Henderson, J.M. and Hollingworth, A. (2000). Eye movements, visual memory, and scene representation. Michigan State University Eye Movement Laboratory Technical Report, 5, 1–17.
Holmqvist, K. et al. (2003). Reading or scanning? A study of newspaper and net paper reading. In: J. Hyönä, R. Radach, and H. Deubel (eds.), The mind’s eye: cognitive and applied aspects of eye movement research (pp. 657–670). Amsterdam: North Holland.
Holmqvist, K. and Wartenberg, C. (forthcoming a). Daily newspaper layout – designers’ predictions of readers’ visual behaviour: A case study. Research report for SND/S 2003.
Holmqvist, K. and Wartenberg, C. (forthcoming b). The role of local design factors for newspaper reading behaviour – an eye-tracking perspective. Unpublished research report.
Hyönä, J. et al. (1999). Utilization of illustrations during learning of science text book passages among low- and high-ability children. Contemporary Educational Psychology, 24, 95–123.
Josephson, S. and Holmes, M.E. (2002). Visual attention to repeated internet images: Testing the scan path theory on the world wide web. ETRA’02, New Orleans, Louisiana.
Klein, R. et al. (1992). Orienting of visual attention. In: K. Rayner (ed.), Eye movements and visual cognition: Scene perception and reading (pp. 46–62). New York: Springer- Verlag.
Küpper, N. (1989). Recording of visual reading activity: Research into newspaper reading behaviour. Available as PDF-file from http://calendardesign.de/leseforschung/Eyetrackstudy.pdf.
Lundqvist, D. and Holmqvist, K. (2001). Bigger is better: How size of newspaper advertisement and reader attitude relate to attention and memory. Unpublished research report.
Radach, R. et al. (2003). Eye movements in the processing of print advertisements. In: J. Hyönä, R. Radach, and H. Deubel (eds.), The mind’s eye: cognitive and applied aspects of eye movement research (pp. 609–632). Amsterdam: North Holland.
Rayner, K. (1978). Eye movements in reading and information processing. Psychological Bulletin, 85, 618–660.
Rayner, K., ed. (1992). Eye movements and visual cognition: Scene perception and reading. New York: Springer-Verlag.
Rayner, K. (1998). Eye movements in reading and information processing: 20 years of research. Psychological Bulletin, 124, 372–422.
29
Rayner, K. (2001). Integrating text and pictorial information: Eye movements when looking at print advertisements. Journal of Experimental Psychology: Applied, 7(3), 219–226.
Shaughnessy, J.J. and Zechmeister, E.B. (1997). Research methods in psychology. New York: McGraw-Hill.
Sternvik, J. (2003). Ögonrörelsestudier och dagstidningsläsning – en forskningsöversikt. PM från dagspresskollegiet, 52, Gothenburg University.
Wells, W.D., ed. (1997). Measuring advertising effectiveness. Mahwah, N.J.: Lawrence Erlbaum.
        Roman Bednarik, Teresa Busjahn, Carsten Schulte (Eds.)
Eye Movements in
Programming Education: Analyzing the Expert’s Gaze
 Publications of the University of Eastern Finland
Reports and Studies in Forestry and Natural Sciences
        reports and studies | 18 | Bednarik, Busjahn, Schulte (Eds.) | Eye Movements in Programming Education
Eye Movements in Programming Education: Analyzing the Expert's Gaze
ROMAN BEDNARIK, TERESA BUSJAHN, CARSTEN SCHULTE (EDS.)
Eye Movements in Programming Education: Analyzing the Expert's Gaze
Proceedings of the First International Workshop
Publications of the University of Eastern Finland Reports and Studies in Forestry and Natural Sciences No 18
University of Eastern Finland Faculty of Science and Forestry School of Computing Joensuu, Finland
2014
Grano Oy
Joensuu, 2014
Editor Prof. Pertti Pasanen, Prof. Pekka Kilpeläinen, Prof. Kai Peiponen, Prof. Matti Vornanen Distribution:
Eastern Finland University Library / Sales of publications P.O.Box107, FI-80101 Joensuu, Finland
tel. +358-50-3058396 http://www.uef.fi/kirjasto
ISSN (nid): 1798-5684
ISBN (nid): 978-952-61-1538-2
ISSN-L: 1798-5684
ISSN (PDF): 1798-5692
ISBN (PDF): 978-952-61-1539-9
Bednarik, Roman; Busjahn, Teresa; Schulte, Carsten (Eds.)
Eye Movements in Programming Education: Analyzing the Expert's Gaze. Itä-Suomen yliopisto, School of Computing, 2014
Publications of the University of Eastern Finland. Reports and Studies in Forestry and Natural Sciences, no 18
ISSN (nid.): 1798-5684
ISSN (PDF): 1798-5692
ISSN-L: 1798-5684
ISBN (nid): 978-952-61-1538-2
ISBN (PDF): 978-952-61-1539-9
Eye Movements in Programming Education:
Analyzing the Expert's Gaze
Proceedings of the First International Workshop
 at the 13th KOLI CALLING INTERNATIONAL CONFERENCE ON COMPUTING EDUCATION RESEARCH, 2013
School of Computing, UEF, Joensuu, Finland November 13th - November 14th, 2013
Welcome to the proceedings of the “Eye Movements in Programming Education: Analyzing the Expert's Gaze” workshop.
Code reading is an essential part of program comprehension and a common activity in debugging, maintenance and learning a programming language. Nevertheless, Computer Science Education Research and Teaching mostly focus on code writing. Better insights in code reading are valuable to support programmers from novice to expert. The first international workshop “Eye Movements in Programming Education: Analyzing the Expert's Gaze” is an approach to gain deeper understanding of the comprehension processes behind observable eye movements during code reading.
The workshop was organized in association with the 13th KOLI CALLING Conference in Computing Education and took place November 13th - November 14th, 2013 at the School of Computing, UEF, Joensuu, Finland. A total of 15 people participated in the workshop, four of them remotely. The event was supported by the Joensuu University Foundation.
Before the workshop, participants were given two sets of eye movement records of expert programmers reading Java. The data can be downloaded from www.mi.fu- berlin.de/en/inf/groups/ag-ddi/Gaze_Workshop/koli_ws_material. We asked the participants to analyze and code these records with a provided scheme. Based on this analysis position papers have been written describing the eye movement data and commenting on the coding scheme, as well as on the application of eye movement research in computer science education. The coding scheme concerned code areas in different level of detail, observable eye movement patterns and presumed comprehension strategies. The scheme was revised following suggestions given in the position papers and during the workshop. Additionally, several group members developed visualization tools both for eye movements and the results of the coding process and provided them in their position papers.
This technical report contains the position papers. Furthermore it includes the workshop call, the eye movement materials used, the revised coding scheme, and a list of participants.
We would like to thank all participants for the great work,
Roman Bednarik, Teresa Busjahn and Carsten Schulte

Contents
Eye Movements in Programming Education. Analyzing the 1 Expert's Gaze
Maria Antropova, Galina Shchekotova
Analyzing Programming Tasks 4
Andrew Begel
 nalysis of two eyetracking renders of source code reading 7
Katerina Gavrilo
Towards Automated Coding of Program Comprehension 9 Gaze Data
Michael Hansen, Robert L. Goldstone, Andrew Lumsdaine
Notes on Eye Tracking in Programming Education 13
Petri Ihantola
Eye Movements in Programming Education: Analyzing the 16 expert’s gaze
Suzanne Menzel
Visual evaluation of two eye-tracking renders of source code 20 reading
Paul A. Orlov
Finding Patterns and Strategies in Developers’ Eye Gazes on 24 Source Code
Bonita Sharif and Sruthi Bandarupalli
Eye movements in programming education: Analysing the 27 expert’s gaze
Simon
Workshop call 30 Sample visualizations of gaze data 32 Revised coding scheme 36 List of participants 42
Eye Movements in Programming Education. Analyzing the Expert's Gaze
Maria Antropova Research Team Lead at JetBrains Russia, Saint Petersburg Universitetskaya nab.7-9-11, k.5, lit.A +7-921-311-4431
maria.antropova@gmail.com
ABSTRACT
There are two main strategies of subject behavior during eye- movement experimental research. The first pattern is based on the inductive approach and the second one is based on the deductive.
Keywords
Eye-movement analysis, code-reading patterns.
1. SUBJECT DESCRIPTION
1.1 The first subject
The first subject spent much more time on learning the program, he was unsparing in his efforts for the methods analysis and matching parameters in methods and constructor.
In the top of patterns behavior for this subject there are many pairs like:
‘Main’→ 'Height'
'Width' → 'Constructor'
'Constructor' → ' Main '
‘Constructor’ → ‘Height’
'Width' → 'Constructor' → 'Width'
‘Height’ → 'Constructor' → ‘Height’ 'Constructor' → 'Width' → 'Height' → 'Constructor'
This means that the subject was trying to actually understand how the method works and which parameters were used and how they were used in each method. For the ‘Height’ and ‘Width’ block he spent 25% of the time. This behavior explains his task description (to answer on certain question about program output).
It seems like the subject was following combined strategy: Scan strategy is more valuable than JumpControl and LineScan.
The subject was learning the correspondence between input parameters and variables in constructor. That is why in the top of patterns we see a lot of pairs like 'Constructor' → 'Main' and 'Main' → 'Constructor'. Time which the first subject spent for Constructor block is less than the second subject, because the first one looked at the Constructor all the time very briefly, only for understanding of the parameters order.
1
Galina Shchekotova Analyst at JetBrains
Russia, Saint Petersburg Universitetskaya nab.7-9-11, k.5, lit.A +7-921-763-7648 gshchekotova@gmail.com
 1.2
The second subject
The second subject spent less time than the first subject for the task completion (40% less than the first one). The reason is that second subject had a different task and had to answer multiple- choice questions. Also he used other technique, which is more convenient and fast for the short program and this technique is based on scan strategy. In the top of patterns behavior for this subject are:
'Constructor' → 'Width'
'Main' → 'Constructor'
'Area' → 'Main'
'Area' → 'Main' → 'Constructor' 'Constructor' → 'Area' → 'Main'
The second subject probably uses Linear or LineScan (or combined) strategy because of the task description.

1.3 Comparison table
Take a look at the table comparison of the two subjects by several metrics related to the code scheme and the subject’s behavior: Attributes, Main, Constructor, Height and Width blocks, Actual Parameter List in Main, Return, Pattern, Duration.
Table 1. Comparison table for considered subjects
1.4 Eye movement flow
This is an example of eye movement flow, which presents big difference between subjects behavior. First subject moves between different code blocks a lot, second one has different behavior and has not too many jumps.
       Code
     First Subject
    Second Subject
     Main
   Often but with short duration
   Rarely but with long duration
     Constructor
  Often and with long duration in the first part of the session. It needs for the area calculation
  Rarely and not very intensively, only for understanding how the object is created. The second subject spent on 10% more time for Constructor block than the first one.
     Height and Width blocks
    The first subject spent on these blocks 25% of the time.
    The second subject spent on these blocks 13% of the time; it is 12% less than the first one.
     Actual Parameter List
in Main
  Learning the parameter list close to the end of the session for the area calculation, then moving to Constructor and object methods for calculation
  Looked at the parameter list in the end of the session to keep in mind two created objects (rectangles).
     Return
    Looked at Return block very often to calculate the value of the area
    Looked at the Return block only for understanding how does method work
     Pattern
   Combined:
more valuable is Scan strategy, than JumpControl and less LineScan.
   The main strategy is LineScan - the best one for the general understanding of the short code.
     Duration
  The first one spent more time than the second one (the reason is strategy or experience difference).
                     2
2. THE CODING SCHEME
In the presented coding scheme there is no Area block.
The presented coding scheme is too detailed for the research of small code length. For example, researching current two subjects we didn’t need such tier of the scheme as Strategy (Debugging, etc).
Based on Block analysis we can conclude that there are two main strategies of code reading: from the special to general and the other way around.
The first strategy is more effective in case of big program with many modules of a code (or in case of specific task), the second strategy is more effective in case of short code (in this case more experienced user can guess what is going on in the program). This is a very important difference that should be taken into consideration in the experiment design. Other parameters should be looked in other experiments with code of different length and with subjects with different strategies.
3. MAIN QUESTIONS
What yields the tagging of “primitive” events? What ideas/thoughts/associations did arise?
It helps to identify general patterns more clearly. Also it gives some clue about actual cognitive task if we don’t know about it. Also, probably it helps to follow the order of task stages.
We know about global understanding strategies from program comprehension research (data flow, control flow, top-down, bottom-up, as-needed...). Do we find those in the gaze?
In case of having only gaze data it is possible to find it only if we have a very simple program on one screen, because it becomes to be impossible to recognize the strategy if we don’t know anything about scrolling.
Otherwise it is possible but still difficult because we have to mark on the gaze file points with coordinates changing (if the program is more than one screen).
What patterns did you find and what are suitable names for them?
We have found two main patterns: for the first subject we can call it inductive approach (from the special to general), for the second subject it is more deductive way (from the general to special). It very depends on the task, which the subject has to solve (and probably depends on subject experience, type of program paradigm, etc).
How are patterns connected to cognitive strategies? Which patterns are indicative of which strategies?
Patterns are practical realization of cognitive strategies in process of task making. Different cognitive strategies probably have different patterns. For the inductive approach combined strategy is more typical (mix of Scan strategy, JumpControl and LineScan). For the deductive strategy LineScan is more typical.
Are there further strategies? And what would be suitable names for them?
As we mentioned above, the strategy depends on the task. There are many types of tasks: debugging, code review, refactoring, etc.
3.1 Application of Eye Movement Research in Computer Science Education
The best way of implication is conscious use of code reading strategies depending on code size, program structure and other parameters.
The explication of code reading strategies and deliberate usage of these strategies depending of situations helps students to make their strategies more effective.
With high probability, there are also differences in code reading process due to the approach to programming (OOP, Functional, Procedural), that should be also considered in the education process.
4. ACKNOWLEDGMENTS
Our thanks to Eye-movement workshop organizators for allowing us to participate in this event.
3
Analyzing Programming Tasks
Andrew Begel Microsoft Research
One Microsoft Way Redmond, WA, USA andrew.begel@microsoft.com
1. INTRODUCTION
In this position paper, I first describe the eyetracking pat- terns of the two participant videos I watched and coded. Next, I reflect on the methods and validity of manual coding and interpretation, and finally, I add my own thoughts on the utility of eyetracking data for understanding and helping programmers create and maintain software.
1.1 Task Segment 1
Participants were asked to understand what the area() method would do. The first participant spent 22 seconds linearly reading the code from top to bottom. He then went back- wards and read through the class methods and constructor for 6 seconds. Then he explored a constructor call from the main(), and spotted similarly named instance variables throughout the rest of the code. Then it seemed that he switched to tracing the code in each of the instance meth- ods, flipping back and forth from the constructor call to the instance method in order to figure out which values were being used in the computations. Finally, he traced through the execution of the rect2.area() method call, jumping from the this.width() call to the definition of width() and from the this.height() call to the definition of height().
Diving a bit deeper from 00:33 – 00:36, the subject explored the meaning of the width() method. Triggered by the call to width() in the area() method, he read the width() method body from start to finish, then traced the definition of this.x1 to the constructor call where this.x1 was assigned. He then traced that back to the parameter list which contained an x1 parameter. Then he jumped back down to the Rectangle constructor call in main() to see which value was passed in as the first argument to the Rectangle constructor call.
While this could be characterized as a strategy of execution tracing in reverse (a.k.a. debugging), I think the user was re- ally executing a pattern by tracing similar words backwards through the code file. So, he saw x1 in width(), saw it again in the Rectangle body, and then again in the parameter list. Afterwards, he used the notion of parameter-argument posi- tions to find the appropriate value passed into the Rectangle constructor from the main() method.
If we wanted to identify when the subject was tracing in a debugging strategy vs. pattern matching words, we could modify the study instrument and change the Rectangle con- structor parameter names to be di↵erent than the instance variable names. Similarly, we could also create a second con-
structor in which the positions of the parameters (and group- ing) are permuted from the first, to see if actual knowledge of method calls was being used to spot the correspondence between the caller and the callee arguments.
1.2 Task Segment 2
The second participant read linearly through the class and constructor until he reached the width() method. Then he traced the definition of each used instance variable to the constructor. He did the same when reading the height() method, but switched back to linearly reading the code when he reached the area() method. This took about 25 seconds. Then he started tracing the first constructor call to see how each argument was assigned to a particular parameter and assigned into a similarly named instance variable. Finally, answering the question, he looked at the rect2.area() method call, read the definition, and then presuming he understood the code correctly, computed the math in his head to figure out the rectangle’s area, and finished the task.
The second participant worked more quickly than the first to minimize code scanning and concentrate more directly on the rect2.area() method call. Between 00:40 — 00:55, he traced the Rectangle constructor call that created rect2. He first connected the first argument to the first parameter, then to the first instance variable assignment. Then he read the next line of the constructor call and worked backwards to the parameter and to the argument to the constructor call to validate some internal hypothesis about which argument values were assigned into each instance variable.
2. REFLECTIONS
The process of coding eyetracking data can be divided into two parts: segmentation/identification and interpretation. For programming tasks, the first part is automatable, pro- vided the subjects’ IDEs can be queried to turn (x, y) pixel positions o↵ered by the eyetracking device into program con- structs at various levels of textual, lexical, syntactic, and semantic abstraction. The second part is subjective, requir- ing the observer to interpret the rationale behind the user’s eye movements. This is easiest to do when the user thinks aloud (and the narration is recorded in sync with the user data). But, interpretation can easily be biased by the ob- server’s prior knowledge of programming and pedagogy. We can mitigate this by having many independent observers in- terpret the same data, allowing unsupported inferences to be detected, negotiated, and eliminated [1]. Tailoring the
4
research questions requiring interpretation towards purely observable phenomena can also help.
After segmenting and coding the data using the observable measures, I have several thought about the process:
1. ELAN has some awkward user interface constructs that make it di cult to process multiple related tiers of codes. One specific example is that some clearly hi- erarchical code tiers should have corresponding start and end time stamps in each tier, but the system does not automatically align the annotation boundaries for you.
2. I do not trust my annotation timestamps to be accu- rate within one second. I would trust an automated la- beler much more. The implication here is that I would not feel comfortable trusting many quantitative anal- yses based on annotation times or lengths. I would trust an analysis based solely on the order of annota- tions within a single tier.
3. Without think aloud, I can only o↵er speculations on the programming strategies employed by the partici- pants. Even on a smaller time scale, there are so many things that could be going through the participants’ heads while they code that influence where their eyes are pointed. Before I would believe anyone else’s spec- ulations, I would conduct an experiment to confirm the theories found in the Empirical Studies of Program- mers workshop series through some carefully designed code comprehension experiments [3, 2, 4].
With respect to what I found the participants to be do- ing, it was possible to see what I thought were eye move- ments (saccades) influenced by various semantic and opera- tional properties of the code (all timestamps for first video): data flow (following a single object in memory as its value changes through the program, e.g. 00:46–00:50), intrapro- cedural control flow (scanning lines of code in program ex- ecution order (real or simulated), interprocedural control flow (following call-chains in real or simulated execution, e.g. 00:52–00:59), word (pattern) matching (simple visual pattern matching, e.g. 00:26, 00:27.8–00:28.2), linear scan- ning at the block level and the line level (reading through the textual lines of code, e.g. 00:02–00:20, 00:26.3–00:27.6), and reverse data flow data (tracing assignments backwards through control flow in service of debugging and/or program execution comprehension, e.g. 00:23–00:26).
I do not think the two examples we saw were intricate enough to help us understand much about program comprehension strategies, and certainly nothing about programming or de- bugging strategies. I o↵ered suggestions in the previous sec- tions describing the two segments as to how to alter these examples to validate any theoretical concepts related to dif- ficulty, confusion, or fatigue.
One major complaint about the 1980s Empirical Studies of Programming work is that most of the program compre- hension theories were derived from experiments on students reading tiny programs away from a computer where they could code or run them. Thus, the lowest level strategies
used by experts in real work may appear similar, but there will be evidence of higher and higher-level strategies and plans in in situ empirical data (should we have some) that will confound the simpler theories.
3. THE BIGGER PICTURE
Software developers continue to make mistakes when writ- ing code, despite improvements in programming languages, high-level abstractions, better development tools, better com- munication tools, more responsive development methodolo- gies, and even the availability of Internet search. Mining software repositories (MSR) research correlates empirical data about the software and the process by which it was de- veloped to discover attributes that indicate poor code qual- ity and/or poor productivity. However, this research does not explain why mistakes are made, but only where they occur most often.
Developers do take steps to mitigate the risk pointed out by MSR analyses. For example, they might more rigorously test code that has been implicated in prior bugs. However, I feel that to improve the basic situation, we need to go to the root of the problem, when developers are actively reading, writing, and modifying code. In a pilot study I conducted last year with my colleague, Thomas Fritz, from the Univer- sity of Zurich, we recorded 6 Microsoft software engineers working for five minutes to modify some code we gave them and for five minutes on their own task they had that day. We found each developer expressed (via think aloud proto- col) temporary confusion, and got lost (re: navigation) in their code several times in that short time span, even when working on their own code with which they were very fa- miliar. Perhap, developers make more mistakes when they are confused or lost (and do not make mistakes when they are not). Thus, if we could detect and/or stop them from programming in these emotional states, we could improve code quality and productivity.
In the last year, I have been using eyetracking, electro- dermal activity sensors, and EEG sensors with professional programmers doing comprehension tasks (very similar to the ones in this workshop) to identify correlations between the biometric sensor readings and programmer confusion, task di culty, and surprise. My goals are to discover which sen- sors correspond most precisely to these emotional attributes, which combination of sensors are easiest to deploy and o↵er the best online prediction accuracy, and correlate the sensor readings to areas of the code where developers cause bugs or experience lowered productivity.
Ultimately, I would like to use instantaneous measurement of biometric data and design an appropriate analysis to en- able the design of IDE-based programmer interventions that could stop developers from making bugs before they make it into the source code. For instance, EDA readings can help determine when someone is not paying attention to their work (e.g. they just had lunch) and warn them if they try to edit a region of the code known to be at high risk for bugs.
During my work, I have had to learn a lot about experimen- tal design of small comprehension tasks, biometric sensor measurements, analysis of noisy human-sourced data, and
5
still find ways to discover significant results with non-trivial e↵ect sizes. I hope to find others at this workshop to trade tips and tricks for this experimental data, and develop a set of practical methods for design and implementing experi- ments and analyses. I would also like to find out how best to adapt experimental methods and analyses from the med- ical and cognitive psychological fields for tasks that involve many fewer, yet much more complex (related to more areas of the brain) activities that are representative of computer science tasks and skills.
4. REFERENCES
[1] B. Kitchenham, D. I. K. Sjøberg, O. P. Brereton,
D. Budgen, T. Dyb ̊a, M. Ho ̈st, D. Pfahl, and
P. Runeson. Can we evaluate the quality of software engineering experiments? In Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement, ESEM ’10, pages 2:1–2:8, New York, NY, USA, 2010. ACM.
[2] G. M. Olson, S. Sheppard, and E. Soloway, editors. Empirical studies of programmers: second workshop. Ablex Publishing Corp., Norwood, NJ, USA, 1987.
[3] E. Soloway, B. Shneiderman, and S. Iyengar, editors. Empirical Studies of Programmers: First Workshop. Greenwood Publishing Group Inc., Westport, CT, USA, 1986.
[4] S. Wiedenbeck and J. Scholtz, editors. ESP ’97: Papers presented at the seventh workshop on Empirical studies of programmers, New York, NY, USA, 1997. ACM. 608977.
6
Аnalysis of two eyetracking renders of source code reading
Katerina Gavrilo
Saint-Petersburg, Russia katrinaalex@gmail.com
ABSTRACT
In this paper, the specific and subjective description of two short segments of data is given. Author proposes some thoughts on the usage of the eye movement data in computer science education research.
Keywords
Eye movement, source code review, eye-tracking metrics, cognitive strategies, program comprehension, pattern
1. INTRODUCTION
Connection between comprehension processes and eye movement data has been analyzed for many years now. Although this field of computer science is rather young, we should not underestimate the results we have already got. A lot of different researches have been made in this field. Some studies have more physical specification [1], another ones — cognitive [2]. Recently a number of programming orientation works has been written [3,4].
Having initial data (two subjects per program) and various annotations to program, a number of particular observations is given as a result of assignment.
2. GENERAL INTERPRETATION
According to our goal, which is to find existing connection between eye movement data and cognitive processes during programming, we analyzed data we have. For a review at our disposal we have one Java program and two subjects. To each of them two different comprehension questions about the same program were given. As a result we recorded two video fragments with different subjects.
3. DATA, PATTERNS, STRATEGIES
As raw material we have two videos from where we get information about subjects’ eyes behavior, several characteristics (fixations, location and duration of those fixations) and saccades amplitudes.
For making the analysis we apply a number of given rules which describe some of the eye-movements. These rules represented as digest of attributable patterns for eye gazes and also strategies, which are based on existing patterns.
In our research we address to some patterns as scan pattern, liner,
7
retrace declaration pattern [4], retrace reference pattern [4]. As for strategies we will try to derive them from patterns we found.
Even though the source code for two participants is the same, patterns and strategies are noticeably different. That occurs because comprehension task, which were given to the subjects before the source code was shown, are not the same. Let us discuss the gaze data step by step.
3.1 Figure 1
The task for this figure was to say the return value of ‘rect2.area()’ after the program was executed. The whole fragment takes 1,5 minutes.
First 19 seconds we consider as a scanning process through the whole source code. During this part time participant becomes acquainted with it. At the end of the program probationer facing finds the line with where he find ‘rect2.area()’, the — data he has to know the value of. So then he is going back to place where variables for x’s and y’s were declared. From that place he descends reads the code again. And again the moment participant he reaches the ‘area()’ method description participant he looks step by step “go” through the previous places where the variables has have been recently referred. For example he faced the ‘area()’ method and sees there ‘width()’ and ‘height()’ methods, so he goes to them returns to them. After he gets what is in there the values of these variables, next gaze he stops his sight is at the constructor, where x and y values are defined. This route is repeated a number of times with some insignificant deviations.
At some point we have three blocks of code between which gazes are travelling. These are area with given parameters (5, 5, 10, 10), which have to be counted for having output value, area between where width and height methods are described and constructor with parameter definitions. After a sequence of brief fixations longer fixations appear. That is caused by some cognitive processes. Presumably there the participant counts the result, because he is looking at the entering parameters (5, 5, 10, 10).
3.2 Interpretation of Figure 1
We found scan pattern in the beginning of the video. For the most part of the whole test there were a lot of oft-recurring saccadic eyesight jumps between places where variables had been recently referred to or declared. Those patterns we call retrace declaration[4] pattern and retrace reference pattern[4]. As for strategies, we would define here DesignAtOnce and Trial&Error.
We would propose to describe the mix of patterns and strategies as a process when you first check the risk of any deal. Like before transport some goods through the unknown route, first one go there without merchandise and see where to turn and where the traffic lights are. And when one is sure about everything he takes the goods with him to finish the deal. Comparing this example with our task, the route here is the algorithm while the goods are the parameters. This strategy is called touchstone.
3.3 Figure 2
The task for this figure was to find a way to give an answer to a multiple-choice question about the algorithmic idea. The whole fragment takes 56 seconds.
First 25 seconds we can define as detailed and thoughtful scanning. During scanning the participant equally pays attention to signatures, lists of parameters, body of functions. We can notice that when the similar description of a method or a variable appears, fixation time is much shorter. For example after the participant has examined the width method long enough he did not spend much time examining the height method, because they are similar. When probationer reaches output commands he spends there quite a long period of time (the sum of fixation intervals is bigger compared to other blocks), thinking and analyzing the type of information he will have as an output. Also we can see a moment when the participant was comparing the ‘rect1.area()’ to ‘rect2.area()’. Next “block” of his action is juxtaposition of entered parameters (5, 5, 10, 10) to how they are described in the constructor. Afterwards eyesight is coming back to ‘public static void main’ with predominant attention on line 20. The fixation duration is getting noticeably longer. Then participant has his eyes directed to the ‘area()’ method. This part seems like he is attentively investigating what the method is doing. After all sights are going back to the ‘public static void main’ zone and last 6 second we observe that the fixations are longer and they gathered just near the end of the code.
3.4 Interpretation of Figure 2
It is uncertain if the scan pattern is appropriate in this case, because usually it means that the participant is briefly looking over the source code and then coming back to parts, which he thinks deserve more attention. It could be perceived as slow motion scan pattern. Beside that we can distinguish the linear pattern. So we come to strategies DesignAtOnce and ProgrammFlow, when the subject’s intention is to understand the general idea of algorithm and figure out the outcome of the program.
3.5 Comparison between figures
Compared to the Figure 1, Figure 2 was more consistent, regular and calm, so to say. In the Figure 1 the whole picture was assembled by the participant from the pieces, which where all around the code in random places. The participant of Figure 2 made his picture very accurate. It seems that the second
participant was memorizing information during the reading the code, from the first step. That is logically explained by the task he was given.
4. POTENTIAL USE
Each person has their own model of cognitive comprehension and by studying them we can individualize the material we have. That could be used in computer science education to improve quality of studying materials.
There are several areas of application for the eye movement data analysis, if it were researched more thoroughly. For example, that could be used for finding “bugs” in the code. This can be observed and as the results we could have some rules of differences between novices and professionals (which are actually already observed) with which it is possible to check the candidates for some job for example.
Also this method could be used as a great base in education. For instances, as some special aspects for IDE interface design, that could be even auto-tuned with live eye-tracking data. If some parameters are getting too low or too high that means that the person has some problems in this particular block of code, therefore some tooltips, hints or buttons could appear. So that certainly could be used for creating IDE for learning programming. No doubt that this field has a great potential for educational field.
5. REFERENCES
[1] Gippenreiter Y.B. (1978) Movement of human eye. Moscow: Moscow University publisher (in Russian).
[2] V elichkovsky B.M. (2006) Cognitive Science: The Foundations of Epistemic Psychology. Moscow: Smysl/ Academia (in two volumes, in Russian).
[3] CROSBY, M. E., AND STELOVSKY, J. 1990. How Do We Read Algorithms? A Case Study. IEEE Computer 23, 1, 24– 35.
[4] Uwano, H., Nakamura, M., Monden, A., Matsumoto, K. (2006) Analyzing individual performance of source code review using reviewers' eye movement. In Proceedings of the 2006 Symposium on Eye Tracking Research &Amp; Applications (San Diego, California, March 27 - 29, 2006). ETRA '06. ACM, New York, NY, pp. 133–140.
     8
Towards Automated Coding of Program Comprehension Gaze Data
Michael Hansen Indiana University School of Informatics and Computing
2719 E. 10th Street Bloomington, IN 47408 USA mihansen@indiana.edu
ABSTRACT
Gaze data collected during program comprehension provides insight into programmers’ thought processes. Manual coding of this data, however, can be tedious and subjective. We de- fine and demonstrate an automated coding scheme for most categories in this workshop’s coding scheme. We discuss potential sources of error when abstracting from fixations to areas of interest and patterns, and consider alternative definitions for some codes. For the high-level Strategy cat- egory, we inform coding decisions with metrics computed over a rolling time window.
Categories and Subject Descriptors
H.1.2 [Information Systems]: User/Machine Systems— software psychology
1. INTRODUCTION
Gaze data collected during program comprehension provides an insight into programmers’ thought processes that is dif- ficult to gain using common performance measures [1]. The process of interpreting and coding this gaze data, however, is tedious and highly subjective. To aid in the discovery of strategies for use in programming education, automated coding can be done with fixation data obtained directly from the eye-tracker. By building on the abstraction gained from lower-level automated coding – e.g., from fixations to blocks, lines, parameter lists, etc. – we demonstrate that codes from most categories in this workshop’s coding scheme can be au- tomatically and reasonably assigned.
Automated coding requires precise definitions of each cat- egory and code. At a low level, this means defining areas of interest (AOIs) based on syntax or semantics, and then deciding to which AOI (if any) each fixation belongs. Sec- tion 2 discusses the details of AOI creation and fixation as- signment. These details must be explicit because the process
9
Robert L. Goldstone Indiana University Dept. of Psychological and Brain Sciences
1101 E. 10th Street Bloomington, IN 47405 USA rgoldsto@indiana.edu
Andrew Lumsdaine Indiana University School of Informatics and Computing
2719 E. 10th Street Bloomington, IN 47408 USA lums@indiana.edu
of quantizing fixations introduces new potential sources of error. Section 3 defines all automatically-assigned codes in terms of AOI rectangles or lower-level codes. These defini- tions fit the authors’ intuitions, but should not be taken as absolute or final. To aid in the manual assignment of Strat- egy codes, we make use of several fixation metrics computed over rolling time windows in each trial (Section 4).
2. QUANTIZING FIXATIONS
Fixations are quantized gaze positions over time. To ab- stract further, we draw rectangles around areas of interest (AOIs) and assign each fixation to zero or more AOIs. For simplicity, we assume the AOI rectangles in the Block, Sub- Block, Signature, and MethodCall categories do not over- lap. Codes in these categories, therefore, are mutually ex- clusive (not the case for Pattern).
Figure 1: Example assignment of a fixation to an AOI. A circle is drawn around the fixation point, and the AOI with the largest overlap is assigned.
To determine whether or not a fixation belongs to an AOI, we do the following: (1) draw a circle around the fixation point with radius R, and (2) choose the AOI rectangle with the largest area of overlap (Figure 1). The choice of R de- pends on the size of the experiment screen and how far away the participant was sitting. Using R = 20 pixels, Figure 2 shows a timeline for subject 1’s trial where each fixation has been quantized by line. Particular high-level patterns, such as Scan (highlighted), become readily apparent with such plots. Caution must be exercised, however, because noise at the lowest levels (raw gaze data) may result in a wrong AOI or code assignment.
3. CODING SCHEME DEFINITIONS
To facilitate automation of the coding process, we must pre- cisely define each portion of the coding scheme. Even for very basic codes, such as Body from SubBlock, di↵erent rea- sonable definitions are possible. For example, should a fixa- tion be coded as Body if it hits an opening curly brace ({)? For functions defined with K&R style braces, the opening brace is part of the signature line, and would likely not be considered part of the body:

             Figure 2: Timeline of line fixations for subject 1 (en- tire trial). The automatically identified Pattern:Scan portion is highlighted (2.034-18.642s).
public Rectangle(int x1, int y1, int x2, int y2) {
// constructor body
}
With more compactly defined functions, such as width(), the separation between body and signature is not as clear:
public int width () { return this.x2 - this.x1 ; }
We suggest the following definitions for SubBlock. The open- ing brace is counted as part of the signature, whether or not the function is defined on a single line. To be consistent, the closing brace (}) is never considered part of the body. Figure 3 shows areas of interest overlaid on the rectangle program according to these definitions.
Figure 3: SubBlock areas of interest for constructor and width method. Signature and body are consis- tently separated.
3.1 Signature and MethodCall
Both Signature and MethodCall have Name, Type, and pa- rameter list codes. For a signature like main’s:
public static void main (String[] args) {
// ...
}
While the type and name of a method call are distinct lin- guistically (e.g., System.out and println), they are phys- ically combined as a single “word” (System.out.println). Unlike signatures as well, the types and names of method calls are both in the same grammatical category (identi- fiers), as opposed to being in separate categories (keywords and identifiers). For these reasons, we do not separate type from name for MethodCall (Figure 4). Lastly, we do not code nested calls hierarchically (e.g., foo(bar())) because it would cause within-category overlap of the AOIs.
Figure 4: MethodCall areas of interest for main method. We do not distinguish between Name and Type.
3.2 Pattern
The most basic pattern, Linear is defined as the subject fol- lowing at least 3 lines in text order. We follow this definition with one caveat: blank lines are not taken into account. For example, fixations on lines 1, 2, then 4 for the rectangle program are coded as Linear because line 3 is blank.
The JumpControl pattern, while seemingly simple, hides a great deal of complexity. Whether or not a transition be- tween two lines follows execution order depends on where the subject is in evaluating the program! For example, a transition between line 11 (width() definition) and line 15 (area() definition) follows execution order only if the sub- ject is currently evaluating the call to this.width() in the body of area(). For now, we code any line transition that could follow execution order as JumpControl. Future defini- tions of this code should take previous fixations into account in order to guess where the subject is in the call stack.
LineScan is defined in English as the subject reading the whole line in “rather equally distributed time.” For simplic- ity, we operationalize this definition by splitting each line into a set of equally-sized rectangles (Figure 5). A LineS- can is coded for any set of consecutive fixations that hit at least 3 distinct rectangles on a single line. While this does not explicitly address the “equally distributed time” portion of the English definition, it assigns codes that match the au- thors’ intuitions for the sample data. Another option would be to use the rolling metrics discussed in Section 4 – e.g., fixation spatial density and duration.
Building on LineScan, we can simply define Signatures as a line scan of a signature line (SubBlock:Signature) im- mediately followed by a fixation inside the corresponding function/constructor body (SubBlock:Body). With this def- inition, we identify two instances of the pattern in subject 2’s trial (width starting at 7 seconds and the constructor starting around 26 seconds).
The Scan pattern, inspired by results from Uwano et al. [4], can be operationalized using two sets of constraints. A Scan
 we consider public static void to be the type, main to be the name, and the arguments plus surrounding parentheses to be the formal parameter list. When coding method calls, however, we only consider Name and ActualParameterList.
10
 starts the first time a fixation moves down the screen rel- ative to the previous fixation, and stops when one of two conditions is met: either (1) more than 3 fixations move up the screen, or (2) more than 1.5 seconds are spent on the same line. The highlighted portion of Figure 2 has been identified using this definition, and matches well with the authors’ intuitions.
Figure 5: A single line split into equally-sized rect- angles. We code a LineScan if 3 or more distinct rectangles are fixated consecutively.
4. STRATEGIES & ROLLING METRICS
Codes from the categories described above can be assigned based (mostly) on observation. The Strategy category of codes, however, requires more interpretation. To aid in the identification and interpretation of strategies, we compute three fixation metrics over the course of each trial using a rolling window. Windows are 4 seconds in size and are shifted by 1 second during each step. On average, a single time window will contain about a dozen fixations.
Our first two metrics are simply fixation count and mean fixation duration [3]. Respectively, they are the total num- ber of fixations in a time window and the mean duration of those fixations. Our third metric, fixation spatial den- sity [2], is computed as follows: (1) divide the screen into a grid, and (2) calculate the proportion of cells in the grid which contain at least one fixation. We divide the portion of the screen containing code vertically into 10 equally-sized rectangles. A spatial density of 1, therefore, means that all 10 rectangles were fixated at least once in a time window.
Figure 6 shows our three rolling metrics computed for sub- ject 1’s trial (time windows with no fixations were dropped). Troughs in spatial density (solid blue line) correspond to windows in which subject 1 was concentrating on one or two lines. In some cases, this was correlated with an increase in fixation count (dashed green line), which may be useful for distinguishing between the Debugging and TestHypothesis strategies. The sharp increase in mean fixation duration just after the 70 second mark (dashed-dotted red line) cor- responds with the subject focusing on the final line of the program:
System.out.println(rect2.area ());
The subject’s task in this trial is to obtain the value of rect2.area(). Given the increased fixation duration and drop in both fixation count and spatial density at this point (at approximately 65-75 seconds), we hypothesize that the subject is performing the necessary mental calculation to compute the area of rect2. There are several o↵-screen fix- ations at 70-75 seconds in the video, supporting this hypoth- esis. While we may not be able to pinpoint shifts in strategy using this kind of visualization, we can quickly identify in- teresting time windows to investigate further.
11
Figure 6: Rolling fixation metrics for subject 1 (en- tire trial) with a window size of 4 seconds and a step size of 1 second.
5. CONCLUSION & FUTURE WORK
We have defined and demonstrated an automated process for coding non-Strategy categories from the workshop’s coding scheme. In most cases, this process assigns codes that match well with the authors’ intuitions. In the context of pro- gramming education, automated coding helps researchers quantify di↵erences between experienced and novice pro- grammers. Such di↵erences could inform the design of an automated tutor capable of providing highly-contextualized feedback to a student. For example, alternative strategies could be presented to students who fail to locate a bug in an exercise.
Automated coding also forces the coder to think precisely about areas of interest and how to define high-level codes, increasing confidence in subsequent analyses. Because the process is automated, it can be run with di↵erent, compet- ing code definitions. Multiple quantitative cognitive mod- els could also be used to inform coding (e.g., JumpControl), with deviations from expectations helping to refine the mod- els.
For future work, we would like to achieve automated cod- ing of the Strategy category in a way that agrees with hu- man coders. This may not be possible without more precise definitions of Debugging, DesignAtOnce, etc. Previous psy- chology of programming research, combined with focused eye-tracking studies where only one strategy is likely to be used, will be crucial to achieving this goal.
6. ACKNOWLEDGMENTS
We would like to thank the workshop organizers for their e↵orts in constructing the coding scheme and providing the gaze data. All software will be made available online af- ter the workshop. Grant R305A1100060 from the Institute of Education Sciences Department of Education and grant 0910218 from the National Science Foundation REESE sup- ported this research.

7. REFERENCES
[1] R. Bednarik, N. Myller, E. Sutinen, and M. Tukiainen. Program visualization: Comparing eye-tracking patterns with comprehension summaries and performance. In Proceedings of the 18th Annual Psychology of Programming Workshop, pages 66–82, 2006.
[2] L. Cowen, L. J. Ball, and J. Delin. An eye movement analysis of web page usability. In People and Computers XVI-Memorable Yet Invisible, pages 317–335. Springer, 2002.
[3] A. Poole and L. J. Ball. Eye tracking in human-computer interaction and usability research: Current status and future. In Prospects, Chapter in C. Ghaoui (Ed.): Encyclopedia of Human-Computer Interaction. Pennsylvania: Idea Group, Inc, 2005.
[4] H. Uwano, M. Nakamura, A. Monden, and K.-i. Matsumoto. Analyzing individual performance of source code review using reviewers’ eye movement. In Proceedings of the 2006 symposium on Eye tracking research & applications, pages 133–140. ACM, 2006.
12
Notes on Eye Tracking in Programming Education
ABSTRACT
Eye tracking is an interesting approach to trace how pro- grammers read source code. Although it is relatively straight- forward to find out where a programmer focus his or her eyes and how focus travels, interpreting this is much more di - cult. Why a programmer looks at something and why his eyes move to something else? In this report, I describe my interpretations of two short eye traces where experienced programmers have read a short Java program to find out what it does. I briefly discuss potential pitfalls of interpret- ing eye tracking data and possible avenues of future research.
Categories and Subject Descriptors
K.3.4 [Computer and Information Science Education]: computer science education, information systems education
General Terms
Experimentation, Human Factors
Keywords
eye tracking, code reading, computing education
1. INTRODUCTION
Eye tracking is measurement of eye activity combined with information about the surrounding reality. This in- cludes measuring where a person looks at, how his or her gaze travels as a function of time, and even how the diame- ters of pupils reacts to di↵erent stimuli. Eye tracking data is gathered with eye tracking devices. These can be divided be- tween head mounted (e.g. special glasses) and remote ones (e.g. a monitor with with an accurate camera measuring users eye focus).
In programming education, eye tracking has been used to analyze both novice and expert programmers since early 90’s. Since that, as illustrated in Figure 1, an increasing number of studies has been carried out.
Copyright held by the authors.
13
Eye traces are rarely su cient by themselves. Thus, to better support reasoning about the cognitive processes re- lated to reading source code, eye tracking data is often ac- companied with, for example, think aloud and retrospective think aloud information. The latter is created by replay- ing the eye tracking videos to the subjects after they have been recorded and asking participants to explain what they did, why they looked certain parts of the code, why they navigated the source code with their gazes as they did, etc.
In this short essay, I have analyzed two eye tracking record- ings where experienced programmers read code in order to understand what it does. Recordings were created by using a mobile eye tracking device attached to a monitor. This re- sults to a video where the screen view is on the background and eye traces are drawn on top, as illustrated in Figure 2. Because of the setup, there is no information what partic- ipants look at when they do not look at the screen. The original data did not include any think aloud information or other interpretations about what the participants eye gazes.
2. DESCRIPTIONS OF THE TRACES
In this section the behavior of both participants is briefly described. Before diving into the stories, I advice my readers to read the program in Figure 1 by themselves, and find out what it does.
2.1 Participant A
Participant A started by skimming through the defini- tions of instance variables and the constructor. After that, he or she went straight into the main method and skimmed through it. Perhaps the participant found out from the main method that reading the whole would be beneficial, as he or she next linearly skimmed through all the methods (de- clared before the main). After the last method, the partic- ipant started to refer back to the code what he just went through. First, perhaps because the area method, that was the last method, uses width and height methods, the par- ticipant went to look at them. After that, perhaps because width and height methods used the instance variables, the participant went back to the constructor.
Towards the end of the session, the participant does more and more jumping and looking back and forth in the code. It may be that he starts tracing the creation of the rectangle from main method, but after tracing what the constructor does, he or she continues to other method definitions instead of returning to the main method, as the execution does. This could be to find out what the methods will return with this particular rectangle object. This is possible to find out al-
Petri Ihantola
Aalto University
Department of Computer Science and Engineering Finland petri.ihantola@aalto.fi
                         1990 1992 1994 1996 1998 2000
Figure 1: Number of publications per year matching to “eye tracking” and “programming” query in Google Scholar. Numbers are not accurate because the search engine may misclassify publication years, not all publications (especially older ones) are digitally available, etc.
Year
2002 2004 2006 2008 2010 2012
 Figure 2: A screenshot from the eye gaze data an- alyzed in this study. The red circle shows where the participant looks at the moment. Blue lines and circles provide information about the history where the participant looked at before.
ready at this point because there are no methods that would change the state of an object. Indeed, when the participant returns to the main method, he or she does not need to start
14
tracing when the area method is called.
2.2 Participant B
Participant B starts reading the code linearly from the first line, that is the class definition. During the first 8 seconds he or she goes briefly and linearly through defini- tions of the instance variables and the constructor. After that, during the next 6 seconds, he or she goes through the width method. While reading this one line method, parti- pant scans the line back and forth and also quickly checks how the variables used in this method were initialized ear- lier in the constructor. The next method (i.e. height()) is almost the same as the previous width method and the par- ticipant just skims it though very briefly. The participant actually starts reading the method backwards from the end of the line – perhaps because the the two consecutive lines are so similar that it is su cient to check how the variables used in this height method di↵er from the previous width method. The next method (i.e. area()) is di↵erent than the previous two methods methods and the participant spends a couple of seconds on scanning this line back and forth.
Finally there is the main method from where the execution starts. This main method has two very similar segments where a rectangle object is first created and then the area of that rectangle is printed on the screen. The participant first goes back and forth the lines inside the main method – perhaps to ensure that there is nothing wrong locally in that method. Finally, perhaps to ensure that the Rectangle really works as the participant expects, he or she seems to trace the execution related to the creation of one of the rectangles and calling the area method of that object.
3. DISCUSSION
3.1 Different Strategies in Reading the Code
As described in the previous section, participants A and B used slightly di↵erent methods in reading the code. In addition to di↵erences in what participants looked at, the time they needed to find out what the program does di↵ered. Participant A spend about one and a half minutes reading the code, whereas B read did that in about a minute. A
#Publications
0 500 1500 2500
significant di↵erence between the approaches of participants A and B is that A did a lot more long jumps and backwards referencing in the code. At some point, participant A seems to look almost everything at the same time. Participant B’s approach, on the other hand, was very linear. He started from the beginning and he or she referred back to previously read sections only a few times – typically not more than once to same blocks.
At the end of the sessions, both participants started trac- ing what happens when an object is created. After that, participant B continued the tracing by returning to main and after that to the area method as it was called. Par- ticipant A did not return to main method but continued directly to other methods from the constructor.
3.2 What is the Task
There are di↵erent use cases when programmers read source code. For example, programmers read code of their own and code written by others. In the latter case, programmer may or may not know who has written code. In addition, pro- grammers may or may not have some trust on that person. I argue that when reading code of others, it makes a di↵erence if an experienced programmer is reviewing a patch from an unknown source, if he or she is reviewing code from someone trusted. This is why all eye tracking studies should report the context in details. It is an interesting avenue for future research to study how much and how the contex a↵ects code reading strategies of experts.
I also assume that size of the code base a↵ects to how (experienced) programmers start reading it. However, it looks like that so far most eye tracking research has focused on small programs only.
4. CONCLUDING REMARKS
I analyzed two short recordings of eye gaze data where ex- perienced programmers were asked to find out what a small Java program does. I did not have previous experience from this kind of manual annotation of eye traces and I found the task quite laborious. Some of my tasks were something that should be automated. However, despite my lack of experi- ence in analyzing eye tracking data, I found it possible to observe di↵erences, but also similarities, in how participants read the code. As there were only two samples, I did not find annotating the data as useful as viewing them side by side.
15
Eye Movements in Programming Education: Analyzing the expert’s gaze
A position paper for a workshop at
Koli Calling 2013: International Conference on Computing Education Research
ABSTRACT
This position paper describes the author’s experience with the ELAN tool for annotating the recorded eye movements of two expert programs during a code-reading exercise. From observable patterns in the gaze, strategies that the subjects may have been employing are inferred. Ideas for future re- search directions and the possible applications to improving Computer Science education by explicitly teaching reading skills to novices is discussed.
1. INTRODUCTION
This project attempts to infer the high-level cognitive pro- cesses at work during the reading of a simple Java program by an expert programmer, where the reading behavior is en- coded as eye movement data. For this phase, the data for two subjects was provided as an animation.
Both subjects read the same simple 18-line Java program, but were given di↵erent instructions regarding the question they would be asked following the reading. The first subject read for 1 minute and 32 seconds, with the knowledge that the follow-up question would involve the return value of a specific method call. The second subject read for only 56 seconds, and expected to be asked a multiple choice question regarding the algorithmic idea. Both subjects were told that the code was free of errors, thereby eliminating the need to verify “compiler level” details.
2. ANNOTATIONS
Time segments in each animation were coded, using mul- tiple tiers, in the ELAN Linguistic Annotator tool [1]. A controlled vocabulary was used to limit the set of possible annotations appearing in a given tier. From the observable positions and patterns, the author attempted to infer the
16
problem-solving strategy being employed by the program- mer, i.e., to see what was going on “behind the eyes”.
3. EXPERIENCE WITH ELAN
The tiers and vocabulary were created by the workshop or- ganizers and provided to the participants, although we were encouraged to adapt the template to our needs. Thus, my primary interaction with ELAN was to “mark up” time seg- ments in the given animations with given annotations. Al- though there is ample documentation of the system available online, the acclimation to the system could have been faster and easier had a brief tutorial of the annotation procedure been provided.
Initially, I was unclear as to how detailed the annotations should be, how much coverage was reasonable, and how ex- acting should be the start and end points. Also, I wanted to complete the annotations for one subject in a single sitting, so I desired a ballpark estimate of how much time it could be expected to take. I sought guidance from one of the or- ganizers, Teresa Busjahn, who shared with me her personal approach to doing the annotations and told me that it took her about two hours per video. I gratefully adopted her pro- cedure. This was to proceed in two passes. During the first pass, only Blocks are annotated. This identifies the basic code segment the reader is concerned with during each time period. The remaining levels were covered in the second pass.
The tiers for SubBlock, Signature, and MethodCall allow for fine-tuning the description of the observable events. Gener- ally, I didn’t find these helpful, especially those that distin- guished between Name and Type. This was largely due to a lack of confidence that developed in knowing the precise word corresponding to the gaze point. In the instructions to participants, we had been warned by the organizers that“the gaze point might be somewhat askew (due to head move- ments etc.) and that an area of several characters around the middle of the fixation can be perceived. The perceived information may span about a thumbnail around the cen- ter of the fixation.” There were times when I debated my decision about the line of text that was being scanned, and making a contingent decision regarding the word on the line
Suzanne Menzel
School of Informatics and Computing Indiana University
150 S. Woodlawn Ave. Bloomington, IN 47405 menzel@indiana.edu
seemed like a stretch.
Each video was annotated in a single session. The first took about four hours. The second video was shorter, had fewer high-level transitions, and I was more practiced with the ELAN system, so it took me under three hours.
The most interesting and important tiers are Pattern and Strategy, as this is where I relied on my intuition (garnered over three decades of teaching programming) to speculate on how the subject had decided to go about the task of comprehending the program. I am sure that I relied, at times, on my own expectation of how I would have read the program myself and where I would have proceeded next from a given point. Because there were times when it seemed that there were overlapping strategies in play, I added two additional tiers, SecondaryPattern and SecondaryStrategy. I had no trouble selecting one strategy as the dominant force guiding the subject, which is why I labeled the recessive strategy as Secondary.
4. INTERPRETATIONS
It is likely that the prompt influenced the subjects’ approach to the reading, with the first person focused entirely on pro- gram execution and output, whereas the second needed to recognize the program’s algorithm. In some real sense, the cognitive load on the first subject was less than that on the second. It is a mechanical process to trace a given program (to “be the computer”), whereas the second subject had the additional burden of formulating an abstract understanding of the code.
The two subjects exhibited vastly di↵erent behaviors, most notably in the duration of time spent in one area before moving on. An interesting statistics might be to calculate the total distance traveled by each subject.
4.1 Impressions of Subject1
This subject was “all over the place”, with many sporadic jumps and short visits to code blocks. This is evidenced by the comparatively large number of Block annotations (92) and the frequent use of the Trial&Error strategy.
Given the concrete “what does this Area method return” prompt, I was surprised at the small amount of time spent tracing the code and viewing the Area method. This subject seemed to be overly concerned with syntax. A good deal of time was spent reading the Height method, and wandering from place to place. The e↵ort exerted on a Debugging strategy is surprising given that the subject was informed, in advance, that the program contained no syntactic or run- time errors.
4.2 Impressions of Subject2
This subject’s gaze was characterized by a careful, methodi- cal, top-down scan of the code, followed by a DesignAtOnce and ProgramFlow strategies. Compared to the first subject, the gaze is more controlled and less fragmented. The to- tal number of Block annotations is just 21. The systematic top-down reading is broken with the occasional brief TestHy- pothesis, which appear to be used to reinforce or confirm prior assumptions.
17
After the initial line by line reading, the transitions generally seem to follow the program execution. The gaze seems to pick up where it left o↵ in the reading when returning to a code block for further review. Some annotations are clearly just stops on the way to someplace else, which would be better coded as JustPassingThrough.
This subject exhibited concentrated and localized e↵ort. Not only were the Block annotations longer, the gaze would linger on a single line for a sustained period.
Sometimes the gaze would indicate close reading of whites- pace. For example, from about 0:52 to the end shows the subject studying a blank area in the lower right. This makes me wonder if the calibration is too error prone to allow re- liable coding of tokens within a line. Perhaps this could be mediated by using a larger font and smaller code segments.
5. VISUALIZATIONS
Mike Hansen, one of the workshop participants, created some wonderful visualizations of the eye movement data, showing which program lines the subjects fixated on.
It might be interesting to overlay a “heat map” on top of the code that shows the fixations. In cases where the subject is given a prompt to evaluate an expression, one might expect a more uniform coating than if the subject was trying to extract algorithmic meaning from the code.
6. FUTURE EXPERIMENTS
Java has a lot of “noise”. It might be more interesting for run experiments using a language such as Scheme, which packs an algorithmic punch in a small amount of code. I would rather identify successful readership skills to discern the “al- gorithmic gist” of a program, as opposed to the syntactic structure.
Consider, for example, the following simple recursive proce- dure. The reader would be asked to evaluate, say, (mystery ’(4 7 3 8 5 2)), and also told that the evaluation does not result in an error (so as to lighten the cognitive load). It would be interesting to note whether subjects notice the cddr in the else clause.
(define (mystery ls) (cond
[(null? ls) ’()]
[(even? (car ls)) (mystery (cdr ls))]
[else (cons (car ls) (mystery (cddr ls)))]))
Another interesting possibility is to ask the subject to em- ploy a Think Aloud strategy, as much as possible, and then collect audio during the reading, as well as the gaze data. This could be used in a control group to help refine the cat- egories in the Strategy tier.
7. CODING SCHEME
Some observations about the coding scheme:
1. The coding scheme provided by the organizers, and the corresponding ELAN template, omitted a code inside
the Block tier for Area. I was certain this was an oversight, so I just added that tag to the vocabulary. Also, the organizers described a TestHypothesis code for the Strategy tier in their provided materials. This was inadvertently omitted from the ELAN template.
2. The Type code in the MethodCall tier is confusing be- cause method calls do not include type information. If the intent is to annotate the time when the gaze is over a declaration, then Decl is a better identifier. However, it seems that the assignment is the more in- teresting artifact, as in Rectangle rect1 = new ..., and in that case I’d suggest the code Assignment.
3. ProgramFlow was perhaps the easiest strategy to iden- tify with confidence.
4. When I performed the annotations, I was unaware of the fact that participants had been assured of the error-free nature of the code they were reading. Thus, I made an assumption about them being in Debugging mode when they appeared to be carefully checking a line character by character or when they flickered from one place to another, quickly, as if verifying a small de- tail. In retrospect, some of these later cases may have been better categorized as TestHypothesis.
5. It is interesting to speculate how the subjects may have altered their usual reading strategies to accommodate for the fact that they knew the code was error-free. Professional programmers hardly ever have this luxury and it is probably second nature for them to verify syntax during reading. I suspect that they would not have been able to entirely suspend this behavior.
It seems a bit of a misnomer to classify this activity as Debugging. After all, there are no bugs! I would call this AttentionToDetail. In most cases, there is a slowness to AttentionToDetail, but the subject could also be verifying a global property, such as that argu- ment/parameter types agree or that the semi-colons are present in the right places.
6. The Debugging strategy seems to be characterized by very small jumps, where the subject is presumably val- idating the syntax. In contrast, DesignAtOnce is cap- turing high-level algorithmic thinking, thus, features rather large steps as the gaze sweeps over the text.
7. I associated the TestHypothesis code with Worry. I imagined that subject might have found the need to corroborate some assumption, as in “Wait, did I un- derstand that correctly...”. This is di↵erent from De- bugging (or the proposed AttentionToDetail) in that there is a connection between what was being read pre- viously and what is being checked, and that the gaze will return to the original point.
8. I found the Trial&Error identifier a bit di cult to grasp. At some point, I translated this in my mind to Wandering, and that seemed to help, although it might be better to have this be a separate strategy. I used this code for times when it appeared that the subject was backtracking, seemingly searching for a point to resume the reading after a particular path of reasoning had been exhausted—essentially a transition period or a brief rest between bursts of e↵ort.
18
8. REFLECTION
I am reminded of the work done by Matt Jadud to try to extract students’ cognitive processes from their compilation behaviors [2].
If we can gain insights into how experts read code, per- haps those concrete code-reading skills could be explicitly taught to learners in CS1. Using observable low-level be- havior avoids the pitfalls of relying on human testimonials. In many cases, the strategies being employed by the expert may be so ingrained and practiced that the person is not even aware of them on a conscious level.
The idea that expert knowledge sometimes needs to be teased out and made concrete is something that has been studied, in the context of undergraduate education, for some time at Indiana University. A technique known as “Decoding the Discipline” was developed, initially for History [3][5], but later applied to other disciplines including Computer Sci- ence. In [4], the authors state that “Since faculty did not learn to think like historians through explicit instruction, they find it di cult to articulate what it means to think like historians.” and “We present history as a model for other disciplines. They too need to uncover their ways of knowing and to teach them explicitly to students”.
The cornerstone of the technique involves an intelligent non- expert interviewing the expert to discern the “tacit knowl- edge” that is inherent in the field, thereby bringing it to the surface. Once the hidden knowledge is made concrete, ap- propriate ways of developing similar skills in the new learner can be addressed. The interesting aspect of this project with the eye movements is the prospect of taking the human out of the loop because, many times, the human is unable or unwilling to honestly self-reflect. I suspect that expert pro- grammers may have a di cult time articulating exactly how they go about reading a program, even while they are doing it, because they are so skilled at the task that they make many rapid, unconscious decisions and may fail to discern the discrete steps that form their overall strategy. They may also fail to report the “dead ends” or “false starts” in their lines of reasoning, something that would be preserved in the gaze data.
I find this to be a very exciting and rich research direction. I am eager to hear what others at the workshop think about the potential application to Computer Science education. I can imagine that this work might lead to the creation of a tool for teaching reading skills that shows the student where to look.
9. REFERENCES
[1] ELAN. http://tla.mpi.nl/tools/tla-tools/elan/. A professional tool for the creation of complex annotations on multimedia resources.
[2] M. C. Jadud. Methods and tools for exploring novice compilation behaviour. ICER, September 2006.
[3] J. K. Middendorf and D. Pace. Decoding the disciplines: A model for helping students learn disciplinary ways of thinking. New Directions for Teaching and Learning, (98), Summer 2004.
[4] L. Shopkow, A. Diaz, J. K. Middendorf, and D. Pace. From bottlenecks to epistemology in history. Changing
the Conversation about Higher Education, pages 17–37,
2012.
[5] L. Shopkow, A. Diaz, J. K. Middendorf, and D. Pace.
The history learning project “decodes” a discipline: The union of research and teaching. Scholarship of Teaching and Learning In and Across the Disciplines, 2012.
19
 Visual evaluation of two eye-tracking renders of source code reading.
Paul A. Orlov
ABSTRACT
In this paper, I describe the reading process of source code. By analyzing gaze data during code reading processes, were defined eye-movement patterns which are essential part of programming comprehension. Software Visual Evaluation Tool (VETool) was developed for visual evaluation of eye-tracking data and renders. Two general patterns of eye-movements were found. The Jump Control pattern was at the beginning for both subjects. And for second subject was normal to use the Line Scan pattern.
Categories and Subject Descriptors
D.3.3 [Programming Languages]: Language Constructs and Features – abstract data types, classes and objects, control structures. H.5.2 [User Interfaces] : Interaction styles, Theory and methods.
General Terms
Human Factors, Measurement, Languages.
Keywords
Eye-tracking, source code reading, data visualization, pattern, strategy.
1. INTRODUCTION
The analysis of eye - movements is used for understanding of human behavior and different psychological aspects. In his basic work Yarbus describes, that eye-movement patterns and vision strategy depend on the task (Yarbus, 1965). His thesis actual not only for strong visual tasks like visual searching or reading. More abstract tasks also determined visual strategy. For example, when experimenter gives first task about age evaluation of the person and second task about evaluation of emotional conditions of the person on picture. In that two tasks both visual strategy and eye- movements patterns are different.
The measurements of eye movements are very common for understanding of humans activity, visual information processing and comprehension of objective reality. And the problem is how to interpreted eye movements. In psychology of programming (PoP) eye-movements patterns mostly corresponds with the meaning of stimuli. It means that we should assume, that if subject look at the source code element, like variable definition, he should think about this variable. This hypothesis plays vital role if we try to understand mental process through eye-movements, for example, source code comprehension.
Workshop at the 13th KOLI CALLING INTERNATIONAL CONFERENCE ON COMPUTING EDUCATION RESEARCH. Joensuu, Finland, November 13th - November 14th, 2013
2. OBSERVATIONS AND
INTERPRETATIONS
In two video renders subjects have to understand a Java program and to answer questions about them. Eye movements (gaze fixation, saccades and gaze path ) were visualized for evaluation. Both participants are experts in programming. First part of task context was the instruction. How often programmers should answer questions about source code in real live? The physical experiment context rebuild usual Integrated Development Environments (IDE) view, size, colors and formatting of source code. In definition of “task” we have to agree that current conditions relevant only for laboratory study.
For visual evaluations were used ELAN software and were build new software tool for dynamic visualization of ELAN and eye- tracking data Visual Evaluation Tool (VETool). VETool is an open source software. Source code of VETool can be found here: https://bitbucket.org/orlovpa/visual-evaluation-tool-vetool .
2.1 Comparing by “primitive” events
2.1.1 Attributes (Block), Type (Signature annotations), Formal Parameter List (Signature annotations), Name (Signature annotations)
Both subjects looks at these elements at the beginning of reading and rarely after middle of total time. First points of fixation are in the physical center of stimuli, but then, both subjects moves their gaze to class attributes. Also duration of looking at class attributes and at the name of signature are quite same. Times for each block are different.
Attributes and Type elements are shown on Figure 1.
2.1.2 Constructor
Similar situation could be found in duration of seeing on Constructor block. For this block of source code there are two activity intervals: at the 5 sec (at the beginning) and at 25 - 40 sec. First subject looks more often back to this block than second one. Constructor element shown on Figure 2.
2.1.3 Area
Participants look at this block first time at about 20 sec and then, they back to this block once or twice.
2.1.4 Width
This block shows different looking times and in different time intervals. First subject backs here several times for about 1 sec each. Second subject looks here twice and first time quite long. But total time of looking on this block could be similar.
University of Eastern Finland Yliopistokatu 2. P.O. Box 111 FI-80101 Joensuu, Finland paul.a.orlov@gmail.com
20
     Figure 1: Time cyclogram for Attributes and Type elements. Letter A shown first subject. Letter B shown second subject.
  2.1.5 Main (Block), Body (Sub-Block), Actual Parameter List and Names (Method calls) and Method returns element.
The Main block is the popular for looking for both subjects. And
the similarity is in the interval of seeing, both subject looking here from the middle time. They spend at this block much time (more
than 20 sec). In the Main block they look at Body sub-block and at method calls inside sub-block. That is why these three elements are very similar for total time and interval. All these blocks are interesting for subjects gaze after the middle of total spending time and at the end. Main element shown on Figure 3.
Figure 2: Time cyclogram for Constructor element. Letter A shown first subject. Letter B shown second subject.
 21
In other “primitive” events I could not find any interesting moments. All of them are quite individual for participants.
2.2 Comparing by patterns
I found two general patterns that were used by subjects. The Jump Control pattern was at the beginning. This situation is the same for both subjects. And for second subject was normal to use the Line Scan pattern. One interesting moment could be the way of Line
Scan. There are reading from right to left also. This “back” reading should not be the same with backwards saccades (regressions) in normal reading. Line scan patterns shown on Figure 4.
  Figure 3: Time cyclogram for Main element.
Letter A shown first subject. Letter B shown second subject.
    Figure 4: Time cyclogram for Line Scan pattern. Letter A shown first subject. Letter B shown second subject.
  22
 3. INTERPRETATIONS AND DISCUSSION
In PoP eye-movements patterns should also depend on task. The term “task” should be given a definition. The task is a mental construction in humans mind that formed by instruction and context (Gippenreiter, 1978; Rayner, 1998). Humans can be instructed in different forms, like verbal or visual. Instructions comes from objective reality, but human does their interpretation through the individual context. Context can be influenced by subjective emotional factors, previous experience, social and physical factors (Muller et al., 2012). In PoP there are different aspects of context also, but numerous papers skips context (except studies with gaze controlled systems and gaze contingent systems).
Visual evaluation of these two eye-movements shows that there is not one significant picture. Even though subjects are both experts in programming, they have mostly different eye-movements patterns. Only at the beginning of reading (working) they have, may be, Jump Control, but then they go in individual ways. It seems, that it is necessary to take into consideration more factors to determine the context of the task.
Finally, if we would like to identify the Strategy, we have to go back to the Task definition. In current study subjects are expert in programming, and they decide (may be at first gaze fixation), what kind of strategy is necessary to use. If they got instruction like this: “There is a bug, find it!”, they will use different strategy. Subjects were informed about the task, before they read the code. Subject1 was told, that there will be a question about the return value of “rect2.area()” after the program was executed. And for subject2 the information was, that there will be a multiple-choice question about the algorithmic idea. So, they both use Program Flow and both try to not only understand, but remember this program to answer questions. Of course, that kind of task and problems in real professional life are not exactly the same.
This situation shows that there are so much interesting finding in this field of science in future!
4. ACKNOWLEDGMENTS
I thank all organizers ( Roman Bednarik, Teresa Busjahn & Carsten Schulte) of Workshop at the 13th KOLI CALLING INTERNATIONAL CONFERENCE ON COMPUTING EDUCATION RESEARCH. I like to express profound gratitude to Teresa Busjahn for the idea to build such interesting materials for the workshop.
5. REFERENCES
[1] Gippenreiter, Y. B. (1978). Движения человеческого глаза [Movements of the human eye] (p. 256). Moscow: Изд-во Московского государственного университета.
[2] Muller, M. G., Kappas, A., & Olk, B. (2012). Perceiving press photography: a new integrative model, combining iconology with psychophysiological and eye-tracking methods. Visual Communication, 11(3), 307–328. doi:10.1177/1470357212446410
[3] Rayner, K. (1998). Eye Movements in Reading and Information Processing : 20 Years of Research. Psychological Bulletin, 124(3), 372–422.
[4] Yarbus, A. L. (1965). Роль движений глаз в процессе зрения [Eye movements and vision]. (p. 173). Moscow: Изд- во “Наука.”
23
Finding Patterns and Strategies in Developers’ Eye Gazes on Source Code
Bonita Sharif and Sruthi Bandarupalli Software Engineering Research and Empirical Studies Lab Department of Computer Science and Information Systems Youngstown State University
Youngstown, Ohio 44555 bsharif@ysu.edu, sbandarupalli@student.ysu.edu
Abstract—This paper presents observations on patterns and strategies expert developers use while reading source code. An interpretation of two code segments of two expert developers is given in the context of a coding scheme. Results indicate that the method of reading source code varies based on the task however some similarities are noted. Implications of these results to Computer Science education are presented.
Keywords—eye tracking, source code reading, program comprehension strategies, computer science education
1. INTRODUCTION
Computer Science is currently being taught at most major Universities with a focus on code writing without really introducing methods on how to first read the code. Reading code is important because it is the first thing developers do as part of most software tasks such as bug fixing and impact analysis. Glass [1] states that we should approach learning a programming language the same way we learn any other language. First, a child learns how to read a language and later develops writing skills.
How do we read and comprehend code? In order to answer this question, a group of researchers at Freie Universitat Berlin and the University of Eastern Finland conducted a workshop at the Koli Calling 2013 conference dedicated to provide some insight into this question. They provided workshop participants with a two short videos of expert developer’s eye gaze and a coding scheme. The workshop participants were required to annotate the eye gaze using ELAN and find patterns and strategies based on their annotations. Table 1 gives the task given to each subject. The subjects were expert developers. These tasks were given to the subjects before the source code was even shown to them. Both subjects were given the same 23 lines of source code.
In order to answer the above question on how programmers read and comprehend code, we introduce specific research questions based on the types of activities software developers are engaged in while they are reading the code. The research questions we attempt to address are:
  RQ1: What specific parts of the program do programmers
look at most/least?
  RQ2: What comprehension strategies are used together?
24

RQ3: Does the eye movement depend on the task being solved?
RQ4: What are the similarities and differences in eye gaze between different tasks?
We do not generate any hypotheses for the above research questions since this is a purely observational study and reflection of our interpretation of the results. The videos are qualitatively assessed in a somewhat structured manner based on the coding scheme given.
The next section gives our interpretation of the eye gaze in the two code segments. In Section III, we present some discussion about the coding scheme used and modifications we made. Finally we conclude with how eye movements can be used in computer science education.
2. SOURCE CODE INTERPRETATIONS AND OBSERVATIONS
The coding was done by the second author of the paper using the coding scheme provided to the workshop participants. The ELAN files that represent our coding can be downloaded from http://www.csis.ysu.edu/~bsharif/koliworkshop13/.
The main coding events fall into four main categories: Block, SubBlock, Signature, and MethodCall. Each of these categories are further decomposed into codes based on the identifiers, methods, and functions in the program given to the two programmers.
A. Subject 1 – Specific Task
The first subject was told that they would be asked about the return value of rect2.area() as shown in Table 1. We categorize this as a specific task because they knew about a specific method that they would be asked about.
The subject spent the first 20 seconds scanning [2] the program from top to bottom. The latter part of the time was spent reading the constructor and the three methods and mapping actual parameters to the formal parameters for the rect2 object. The highest number of fixations were on the rect2 object and the height() method. We could come to the conclusion that the subject was trying to calculate the area of rect2 based on his/her eye gaze. This is indicative of the task given. In other words, the subject was trying to trace the program and the eye movements were scattered between the main function and the methods called from main showing that the subject was trying to mimic the behavior of a compiler.

Table 1. Tasks, Patterns, and Strategies
    Subject
Task
  Time
 Scan Time
 Patterns
   Strategies
    Subject 1
Asked about the return value of rect2.area()
  92 secs
 20 secs
 First 20 seconds: LineScan, Linear
Later: JumpControl, LineScan
 First 20 seconds: DesignAtOnce, Trial&Error
Later: DesignAtOnce, ProgramFlow, Debugging
 Subject 2
Multiple choice question about the algorithm
   56 secs
 26 secs
 First 26 seconds: LineScan, Linear
Later: JumpControl, LineScan
    First 26 seconds: DesignAtOnce, TestHypothesis
Later: ProgramFlow
   The pattern evident in the first 20 seconds was mainly LineScan and Linear. In the latter part of the eye gaze, the patterns that emerged were many alternating JumpControl and LineScans. The strategy evident in the first 20 seconds was DesignAtOnce and Trial&Error. In the latter part of the video the strategy that emerged were DesignAtOnce, ProgramFlow, and Debugging in that order.
Subject 1 was asked afterwards: “What is the return value of rect2.area()?” and gave the correct answer – 25.
B. Subject 2 – General Task
The second subject was told that they would be asked about the algorithmic idea in multiple choice form. We categorize this as a general task because they were not told about anything specific to look for a priori.
The subject spent the first 26 seconds scanning the program from top to bottom. We do not observe any mapping of actual to formal parameters. The number of fixations on the rect2 object was the highest. There was equal emphasis with respect to number of fixations on the width() and area() method definitions. From the latter part of the eye gaze after the first scan, the subject focused again on the Rectangle constructor, looked at how rect2 was being instantiated, and checked the area method again before looking at the rect2 object. We believe that this subject focused more on rect2 because it was the last object in the program.
The pattern evident in the first 26 seconds was also mainly LineScan and Linear. In the latter part of the eye gaze, the patterns that emerged were also JumpControl and LineScan with very little context switching between the two patterns.
The strategy evident in the first 26 seconds was DesignAtOnce and TestHypothesis in that order. In the latter part of the video the strategy that emerged was ProgramFlow. Each of these patterns and strategies are mentioned in Table 1.
Subject 2 was asked the following multiple choice question and chose a, which was incorrect. The correct answer was b. This program
a) computes the area of rectangles by multiplying their width (x1-x2) and height (y1-y2)
b) computes the area of rectangles by multiplying their width (x2-x1) and height (y2-y1)
c) computes the area of rectangles by multiplying their width (x1-y1) and height (x2-y2)
d) I'm not sure.
25
C. Other Observations and Caveats
This small experiment on two subjects shows that if the subject does not know what to look for it is difficult for them to remember details like parameter ordering as seen by the answer given by Subject 2.
The differences in strategies begin to appear after the initial scan. It could be possible that small differences in the initial scan could cause the second phase to follow different strategies. However, since we only got one video for each task, we were not able to consider this possibility.
We noticed that the eye movements of Subject 2 were much more focused compared to Subject 1. In other words, we did not detect many stray glances in Subject 2’s data. We define a stray glance as something they look at that does not necessarily involve comprehension or something that we cannot explain. Subject 2 quickly read the code and tried to trace it. There were not many places where Subject 2 re-reads the lines (regressions) – at 7 seconds through 10 seconds the subject re- reads the width method. We also have to point out that Subject 1 did have three sections of the video where the eye data was not available so the above observation should be considered with caution.
With respect to the two videos in question, it is not possible to generalize or come to any conclusions without more data about how the number of fixations might relate to how difficult the task is. We could conjecture that the more fixations a line has, the more difficult it is to comprehend but more studies are needed to validate this claim. It is also possible that fixation duration or pupil diameter might be a better alternative. Another possibility could be to look at smaller time windows of 10 or 15 seconds instead of looking at fixations in the entire dataset.
D. Possible Threats to Validity
First, we cannot come to any general conclusions based on only one data point for each task. Both subjects had a high number of fixations on the object rect2 (line 20). It would be interesting to see how the fixations would change if subject 1 was asked about the return value of rect1 instead of rect2. A possible explanation here would be that because rect2 is the last object in the program, subject 2 also focused on it more than rect1. The specific durations of the fixations were not provided. It is possible for a method to have few fixations but have longer durations of them indicating higher cognitive load.
E. Preliminary Insights into Research Questions
We give some preliminary insights and start to answer the research questions posed in the Introduction based on the two videos. These will be further refined in future work.
  RQ1: What specific parts of the program do programmers
look at most/least? – The programmers looked at private member variables sparingly and only in the beginning. They focused mainly on the constructor and the main function body. All the methods in this program were a single line and the programmers looked at each of those as well although with less frequency.
  RQ2: What comprehension strategies are used together? – We answer this question based on the coding scheme given. The DesignAtOnce was used during the scanning phase along with (Trial&Error and TestHypothesis). In the latter phase DesignAtOnce, ProgramFlow, and Debugging were observed.
  RQ3: Does the eye movement depend on the task being solved? – Yes it is very evident from the two videos that the low-level eye gaze behavior follows different trends for the two tasks involved.
  RQ4: What are the similarities and differences in eye gaze between different tasks? – Both the programmers first scanned the entire code from top to bottom indicated by the DesignAtOnce strategy. Both also tried to understand how the program executes (ProgramFlow). The differences were evident in the period after the initial scan. The subject with the specific task focused on trying to find the answer to the method call area() whereas the subject with the general task focused on understanding the Rectangle constructor and the area() method without a need to find the specific result of the method calls.
3. MODIFICATIONS TO CODING SCHEME
It is highly possible that two different coders will code the same video in entirely different ways. There are some subtle differences between the strategies presented. With respect to Trial&Error, it is hard but not impossible to gauge the reading speed from the videos.
We found the coding scheme quite comprehensive overall covering all scenarios found in the two videos. We made some modifications to the coding scheme and list them below.
  Added class, main, area codes to the Block Tier.
  Added visibility code to the Signature Tier. This will determine if they looked at the visibility of the methods such as the keyword public or private.
  Added PrintLine code to the MethodCall Tier.
  We added a child tier FormalParameterList for Signature to be more specific when a person looks at the formal parameter list. This child tier has four codes x1, x2, y1,
and y2.
  In order to annotate two patterns at the same time we
added two child tiers to Pattern (Pattern1 and Pattern2)
We did not find the Pattern Signatures in the two videos provided. Note that we did not specifically comment on the use of the PrintLine or visibility code that we added in the
26
strategies above, leaving that as future work. This does not mean that they will not be useful in another type of task such as a bug fixing task, an impact analysis task or new feature task.
In future work, we plan to have the videos coded by another coder, compare the findings and calculate the inter coder reliability rating.
4. FUTURE WORK ON EYE MOVEMENTS IN COMPUTER SCIENCE EDUCATION
The use of eye movements has tremendous potential in Computer Science education. First, beginning programmers can be shown eye tracking videos of expert programmers performing tasks such as the ones presented here. The novices get to see firsthand how code is supposed to be read. This increases their awareness while they read code by themselves.
Second, beginning programmers can track themselves while they are solving a task and later analyze in retrospect what they were thinking while solving the task. All this builds self-awareness that eventually teaches a beginner how to learn to read code efficiently. Of course an eye tracker would be required for this purpose.
There are several research questions we pose with respect to using eye tracking for Computer Science education:
  What task is the most difficult for beginning
programmers?
  How do beginning programmers write code after they read
and comprehend it?
  What strategies are used in program debugging?
  What tools could help beginning programmers increase
their productivity?
In order to answer the above questions, a systematic family
of empirical studies need to be carried out in a way that they can be replicated in the future thereby adding to the body of knowledge and evidence of computer science education.
REFERENCES
[1] R. L. Glass, Facts and Fallacies of Software Engineering: Addison-Wesley Professional, 2002.
[2] H. Uwano, M. Nakamura, A. Monden, and K. Matsumoto, "Analyzing individual performance of source code review using reviewers' eye movement," in 2006 symposium on Eye tracking research & applications (ETRA), San Diego, California, 2006, pp. 133-140.
Eye movements in programming education: analysing the expert’s gaze
ABSTRACT
This is Simon’s contribution leading up to the workshop on eye movements in programming education that is to be held in conjunction with Koli Calling 2013. It encompasses brief descriptions of two short segments of gaze-tracking data, thoughts about the coding scheme, and general thoughts about the use of gaze tracking in computing education research. The contribution has been revised in response to comments by the workshop leaders.
Categories and Subject Descriptors
K3.2 [Computers and education]: Computer and Information Science Education – computer science education
General Terms
Measurement
Keywords
Gaze analysis, computing education, programing education, eye tracking
1. INTRODUCTION
Two expert programmers were invited to read the same short piece of code in the expectation of being asked a question about it. The code is a class representing a rectangle, with two pairs of x-y coordinates as its attributes, and methods to return its length, width, and area. The main method declares two rectangles and prints the area of each.
There are clear differences between the approaches of the two participants. They were in fact told to expect different questions, one involving tracing the code and one involving the algorithm. However, the differences in gaze appear deeper than this difference in what they were expecting to be asked, and suggest that different readers read code in markedly different manners.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org.
Koli Calling ’13, November 14-17 2013, Koli, Finland. Copyright 2013 ACM 978-1-4503-2482-3/13/11...$15.00. http://dx.doi.org/...
27
2. SAMPLEDESCRIPTIONS
Reader 1’s gaze might politely be described as erratic. Considered in real time, it flashes wildly about the code, generally spending very little time on any one point. Viewed over time, there is a clear pattern of returning to certain focal points, points that are pertinent to the question that the reader was told to expect; but the gaze fixations are so brief as to leave the analyst wondering whether it is possible to gain any comprehension of the code. For example, in the ten seconds between about 52s and 1m02s, gaze shifts more than a dozen times between the main method, the constructor, and the width, height, and area methods, typically spending less than a second on each point of interest.
Reader 2, by contrast, appears to read the code slowly and methodically. There are elements of linear scanning, and gaze remains far longer on areas of interest. By contrast with the ten- second span described above for reader 1, between about 34s and 44s reader 2 focuses on just one line of code, the declaration of rectangle2. After one-second glances at height and width, there is a steady four seconds on area followed by another eight seconds on the declaration of rectangle2. The impression is of a slow and deliberate analysis of the code, suggesting that most of it is understood the first time it is considered.
It is tempting to suggest that reader 1 is unlikely to have understood the code in the time during which the gaze was recorded. However, both readers are professional programmers, so this seems unlikely – unless it turns out that this reader was unable to correctly answer the subsequent question.
Reader 1 was expecting a question about the output of rect2.area(). It is clear that the scanning was indeed addressing this particular question: for example, the gaze returns frequently to the declaration of rect2, and very seldom to the declaration of rect1. The gaze also frequently returns to the area() method, and to the height() and width() methods that are called by area. The reader has clearly identified the relevant parts of the code and is working on absorbing them; yet there is no hint of the methodical linear (or rather, flow of control) reading that one might expect to be associated with code tracing.
On the other hand, reader 2 was told to expect a question about the overall algorithm. This is a broader question that would entail comprehension rather than tracing; yet reader 2 displays more of the flow-of-control reading style that one might think would be associated with tracing.
It is clear that these two expert programmers have markedly different code-reading styles. As a code reader myself, I have no difficulty seeing how reader 2’s approach could lead to program comprehension; for me the challenge is to hypothesise a way in which reader 1’s erratic reading can lead to the same outcome.
Simon
University of Newcastle, Australia
simon@newcastle.edu.au

After making these observations I learnt that reader 1 was asked ‘What is the return value of rect2.area()?’, and gave the correct answer of 25. Reader 2 was given the multiple-choice question:
This program
a) computes the area of rectangles by multiplying their width (x1-x2) and height (y1-y2)
b) computes the area of rectangles by multiplying their width (x2-x1) and height (y2-y1)
c) computes the area of rectangles by multiplying their width (x1-y1) and height (x2-y2)
d) I'm not sure.
In response to this question, reader 2 chose option a, whereas the correct answer is option b. Option a expresses a correct outcome, but not the exact implementation that leads to that outcome. A generous interpretation would be that reader 2 understood the nature and purpose of the algorithm, but did not remember its detail.
By contrast, reader 1’s wild flashing about the code does seem to have taken it in, as reader 1 correctly answered the question.
3. THECODINGSCHEME
The coding scheme consists of a number of ‘tiers’, each of which can be coded with a choice of values. The tiers are summarised below.
Block indicates in which block of code the participant’s gaze is working. The simple code used for this example has six basic code blocks: the (rectangle) class attributes, the constructor, the main method, and three further methods, height, width, and area. Therefore Block has six possible coding values, one for each of these.
SubBlock: some of the blocks have identifiable sub-blocks in which a reader’s gaze might rest. The sub-blocks that have been coded are Signature, Body, and Return. While the return statement is part of the body of a method, its particular importance means that it is likely to be the focus of some concentration by the reader. Another part of the body that is similarly likely to receive attention is method calls, which are coded in a separate tier. It is interesting that a method signature is coded as a sub-block and additionally in its own tier, whereas method calls are coded just in their own tier. I can see the benefit of the separate tier, which permits the coding of which part of the signature is occupying the reader’s gaze. But at the same time I fell that body is potentially a huge sub-block, if we analyse gaze on more substantial code, with many other statement types that would each merit their own tier, types such as assignment, iteration, selection, and more.
Perhaps my confusion here is that this is a coding scheme specific to this single piece of code, whereas I have been trying to envisage it applied to bigger and more varied code passages. When I do see it applied to different passages, I expect that my confusion will dissipate.
Signature: when gaze rests on the signature sub-block, this tier further indicates whether it dwells on the method name, its type, or its formal parameter list.
MethodCall: when gaze rests on a method call, this tier is used to indicate whether it is focusing on the method name or its actual parameter list. The method’s type is also included in this tier, but we note that this information is not included in a
28
method call, so this value will never be used, and should be removed from the scheme.
Pattern attempts to describe the gaze sequence by associating it with similar sequences that have been previously identified. The sequences identified to date are JumpControl, in which gaze follows the order of code execution; Linear, in which the gaze follows at least three lines (of any type) sequentially, regardless of order of execution; LineScan, in which gaze concentrates on a single line in it entirety; Scan, in which gaze reads a sequence of lines briefly, then returns to concentrate on points of interest; and Signatures, in which gaze covers a number of method signatures before moving to the bodies of the methods. There would seem to be scope for many further patterns. Two possible patterns that I have identified are Flicking, in which the gaze moves back and forth between two related items, such as the formal and actual parameter lists of a method call; and Thrashing, in which the gaze moves rapidly and wildly in a sequence that appears to make no particular sense.
Strategy is the crux of the analysis. It is in this tier that the analyst tries to determine what the reader was thinking while reading the code. DesignAtOnce, typically associated with Linear and Scan patterns, suggests reading through part or all of the code in a linear manner, intending to acquire an overall understanding of it. Debugging is similar, but with gaze time more evenly distributed over the elements, and suggests a search for syntactic or semantic errors. ProgramFlow follows the expected sequence of program control, with the apparent intention of simulating program execution. TestHypothesis involves repetition of a pattern of gaze, and suggests further concentration in order to better understand a particular detail. Trial&Error, somewhat like DesignAtOnce but with faster reading, irregular jumps, and repetition, suggests a search for some part of the code that will lead to an initial understanding. As with patterns, there would seem to be scope for further strategies. For example, I could envisage a use for a FlowCycle strategy, in which the same program flow sequence might be followed several times; the intent might be to gain a first understanding of the flow, strengthening an reinforcing it with repeated examinations of the same code. The Flicking pattern might then suggest the simplest level of the FlowCycle strategy. In addition, the Debugging strategy might in fact be broader than its name suggests, as we might see similar gaze patterns (but infer different intentions) in readers who are trying to comprehend a piece of code that is not believed to contain bugs.
4. REFLECTIONS
The two gaze-tracking examples considered here lead to the observation that different experts have markedly different code- reading styles.
How would one use gaze tracking in computing education – for example, in the teaching of introductory programming? One approach might be to examine the gaze of programming novices and determine how closely it resembles that of experts: the more expert-like the novice’s gaze, the more expert-like the novice would appear to be. Unfortunately, the very small sample of expert programmers examined in this work suggests a major flaw: that experts do not examine code in the same way; that there are at least two, and possibly many more, different ways of examining code in order to successfully comprehend it.
While there is much literature linking the ability to read and comprehend code with the ability to write it, there is also
substantial evidence that many programming novices have not yet acquired the ability to read and comprehend program code. This suggests another weakness in the idea of comparing the gaze of novices with that of experts: the experts are presumed to be able to read code, whereas novices are not.
There is still clearly value in analysing students’ patterns of gaze. This technique could be used, for example, to determine when students are looking at entirely the wrong section of code, or to determine that they are looking wildly all over the code without ever settling on any particular piece. But this is not necessarily the same as comparing their gaze with that of experts.
5. ACKNOWLEDGEMENTS
This work would never have been done without the impetus and inspiration of Teresa Busjahn, Carsten Schulte, and Roman Bednarik. For this I am deeply indebted to them.
6. REFERENCES
[1] None at this point . . .
29
 Eye Movements in Programming Education
Analyzing the Expert's Gaze
Workshop at the 13th KOLI CALLING INTERNATIONAL CONFERENCE ON COMPUTING EDUCATION RESEARCH
Joensuu, Finland, November 13th - November 14th, 2013
Organizers: Roman Bednarik (University of Eastern Finland), Teresa Busjahn & Carsten Schulte (Freie Universität Berlin)
Computer Science Education Research and Teaching mainly focus on writing code, while the reading skills are often taken for granted. Reading occurs in debugging, maintenance and the learning of programming languages. It provides the essential basis for comprehension. By analyzing behavioral data such as gaze during code reading processes, we explore this essential part of programming.
This first workshop gives participants an opportunity to get insights into code reading with eye movement data. However, as this data only reflects the low level behavioral processes, the challenge to tackle is how to make use of this data to infer higher order comprehension processes. We will take on this challenge by working on a coding scheme to analyze eye movement data of code reading. The links between low and high level behaviors will help computing science educators to design, realize and reflect on the teaching of code reading skills.
Furthermore, we aim to open discussion about the ways of explicit teaching of readership skills in computing education. Therefore we will discuss the role of reading skills in teaching programming, facilitated by position papers of each participant.
To participate send a mail to teresa.busjahn@fu-berlin.de. It is possible to participate independent of attending Koli Calling. Participants will get eye movement data of reading and comprehension processes of expert programmers, and a coding scheme for annotating the process. You will annotate the video, and reflect on the (perceived) intentions behind the visible pattern. Applying and refining the coding scheme on the data gives insight into the higher order comprehension strategies of the reader.
 30
A short individual reflection and position paper of the results and perspectives for teaching programming is required by the participants [max. 2-3 pages]. As a result, participants will jointly prepare a paper with the data and the refined coding scheme.
IMPORTANT DATES
  Making data and tools available for participants, as well as instructions for
coding and position paper: beginning of September 2013
  Deadline for submissions: October 14, 2013
  Workshop: November 13th (evening) – November 14th
Visit www.mi.fu-berlin.de/en/inf/groups/ag-ddi/Gaze_Workshop/expert/ for details.
 31
Sample visualizations of gaze data
The eye movement data was recorded using Ogama (www.ogama.net) and an SMI RED-m Tracker (120 Hz). Visualizations were done with eyeCode (http://eyecode.synesthesiam.com/stories/koli-calling.html).
SUBJECT 1
Instruction given to participant before the source code was shown:
You will be asked about the RETURN VALUE of 'rect2.area ()' after the program was executed.
Comprehension task given after the source code was shown: What is the return-value of 'rect2.area ( )'?
Subject's answer: 25
Scanpath - Subject 1
  32
 Fixations per lines - Subject 1
 33
Timeline - Subject 1
SUBJECT 2
Instruction given to participant before the source code was shown:
You will be given a MULTIPLE CHOICE question about the algorithmic idea. Comprehension task given after the source code was shown:
This program
a) computes the area of rectangles by multiplying their width (x1-x2) and height (y1-y2)
b) computes the area of rectangles by multiplying their width (x2-x1) and height (y2-y1)
c) computes the area of rectangles by multiplying their width (x1-y1) and height (x2-y2)
d) I'm not sure. Subject's answer: a
 34
Scanpath – Subject 2
 Fixations per lines - Subject 2
 35
Timeline - Subject 2
Revised coding scheme1
    Tier/ Category
    Codes
  Description
    Classification
   (Lexical) Element
 Public1, Double1
(Lexical) element on which the fixation occurs, e.g. an operator or identifier
  Observable
    Line
    Line1, Line2 ...
  Line on which fixation occurs
    Observable
   Block
   Attributes, Con- structor, Height, Main, Width, Area
 General area in which fixation occurs, e.g. the height-method, the main-method etc.
   Observable
   SubBlock
 Body, Return, Signature
Specific region in which fixation occurs, e.g. a signature or a line containing a return-statement. Can be nested. Granularity de- pends on structures of interest.
  Observable
    Signature
    FormalParameter- List, Name, Type, Visibility
  Precise code section
    Observable
   Formal- Parameter- List
 x1, x2, y1, y2
Precise code section
  Observable
    MethodCall
    ActualParameter- List, Name, Print- Line
  Precise code section
    Observable
   Pattern
 Flicking, JumpControl, JustPassing- Through, LinearHorizontal, LinearVertical, Retrace- Declaration, RetraceReference, Scan, Signatures, Thrashing, Word(Pattern)- Matching
 Flicking: The gaze moves back and forth between two related items, such as the formal and actual parameter lists of a method call.
JumpControl: Subject jumps to the next line according to execu- tion order.
JustPassingThrough: Fixations are on a blank spot and clearly just stop on the way to some- place else.
  Observable
 1 The scheme was developed with the specific Rectangle program in mind.
36
           LinearHorizontal: Subject reads a whole line either from from left to right or right to left, all elements in rather equally dis- tributed time.
LinearVertical: Subject follows text line by line, for at least three lines, no matter of pro- gram flow, no distinction be- tween signature and body.
RetraceDeclaration: Often- recurring jumps between places where variable is used and where it had been declared (Uwano et al. 2006). Form of Flicking.
RetraceReference: Often- recurring jumps between places where variable is used and where it had been recently re- ferred to (Uwano et al. 2006). Form of Flicking.
Scan: Subject first reads all lines of the code from top to bottom briefly. A preliminary reading of the whole program, which occurs during the first 30 % of the review time (Uwano et al. 2006).
Signatures: Subject looks at all signatures first, before looking into method/constructor body.
Thrashing: The gaze moves rapidly and wildly in a se- quence that appears to make no particular sense.
Word(Pattern)Matching: Sim- ple visual pattern matching.
      37
    Strategy
    AttentionTo-Detail, DataFlow, Debugging, Deductive, DesignAtOnce, FlowCycle, Inductive, Interprocedural- ControlFlow, Intraprocedural- ControlFlow, StrayGlance, TestHypothesis, Touchstone, Trial&Error, Wandering
   AttentionToDetail: Readers are trying to comprehend a piece of code that is not believed to con- tain bugs. In most cases, there is a slowness to AttentionToDe- tail, but the subject could also be verifying a global property, such as that argument/ parame- ter types agree or that the semi- colons are present in the right places.
DataFlow: Following a single object in memory as its value changes through the program. Can also occur backwards through control flow in service of debugging and/or program execution comprehension.
Debugging: Similar to Design- AtOnce, but more equally dis- tributed fixation durations, and more equally distributed time of fixation for all text elements. Based on pattern LinearHori- zontal and LinearVertical. The subject's intention is to find syntactical or semantic errors. Very small jumps, where the subject is presumably validating the syntax. (Note: Maybe de- bugging is more a goal, than a strategy.)
Deductive: From general to special, from definition to use, typically includes LinearHori- zontal.
    Interpretation
  38
           DesignAtOnce: LinearHorizon- tal or Scan, hardly any jumps back. The subject's intention is to understand the general or algorithmic idea, without hav- ing the need to go into details. Aiming at understanding by linear reading of the complete (needed) code. Can easily be confused with excessive de- mand/trial and error, might also include TestHypothesis on local levels. Captures high-level algo- rithmic thinking, thus features rather large steps as the gaze sweeps over the text typically associated with Linear and Scan patterns. Suggests reading through part or all of the code in a linear manner, intending to acquire an overall understand- ing of it.
FlowCycle: The same program flow sequence is followed sev- eral times, the intent might be to gain a first understanding of the flow, strengthening and rein- forcing it with repeated exami- nations of the same code. The Flicking pattern might then suggest the simplest level of the FlowCycle strategy.
Inductive: From the special to general, from context to defini- tion, typically combined strate- gy (mix of Scan, JumpControl and LinearHorizontal).
      39
           InterproceduralControl-Flow: The subject follows call-chains in real or simulated sequence of control flow. Intention is to understand the execution or to get the outcome of a code sec- tion. Focus is on execution be- tween blocks.
IntraproceduralControl-Flow: The subject scans lines of code in real or simulated program execution order. Intention is to understand the execution or to get the outcome of a code sec- tion. Focus is on execution on block level.
StrayGlance: A glance where something is looked at that does not necessarily involve compre- hension or something that we cannot explain.
TestHypothesis: Repetition of a pattern or gaze path. Occurs in connection with DesignAtOnce or ControlFlow. The subject's intention is to check for some details in understanding. Hints at some issue where either the person was distracted, or which is more difficult to comprehend. Involves repetition of a pattern of gaze, and suggests further concentration in order to better understand a particular detail.
      40
           Touchstone: Analogue to checking the risk of any deal. Before transporting some goods through an unknown route, first you go there without merchan- dise and see, where to turn and where the traffic lights are. And when you are sure about every- thing, you take the goods with you to finish the deal. Compar- ing this example with program comprehension, the route is the algorithm, while the goods are the parameters.
Trial&Error: Similar to Design- AtOnce, but with higher read- ing speed, and some irregular jumps and repetitions in read- ing. The subject's intention is to cope with cognitive overload and to try to find some place to start the understanding process. Connected to JustPassing- Through and Wandering.
Wandering: It appears that the subject was backtracking, seem- ingly searching for a point to resume the reading after a par- ticular path of reasoning had been exhausted, essentially a transition period or a brief rest between bursts of effort.
      41
List of participants
         Name
     Mail
    1
 Antropova, Maria
 maria.antropova@gmail.com
    2
   Bednarik, Roman
   roman.bednarik@uef.fi
    3
     Begel, Andrew
     andrew.begel@microsoft.com
    4
 Busjahn, Teresa
 busjahn@inf.fu-berlin.de
     5
      Gavrilo, Katerina
      katrinaalex@gmail.com
     6
  Hansen, Michael
  mihansen@umail.iu.edu
    7
   Ihantola, Petri
   petri@cs.hut.fi
     8
    Menzel, Suzanne
    menzel@indiana.edu
    9
     Orlov, Paul
     paul.a.orlov@gmail.com
    10
   Schulte, Carsten
   arsten.schulte@fu-berlin.de
     11
  Sharif, Bonita
  bsharif@ysu.edu
    12
   Shchekotova, Galina
   intendia@gmail.com
    13
   Simon
   simon@newcastle.edu.au
    14
     Vrzakova, Hana
     vrzakova.hana@gmail.com
    15
 Wang, Peng
  pwang@student.uef.fi
  42
Remote participants
 Roman Bednarik, Teresa Busjahn, Carsten Schulte (Eds.) Eye Movements in Programming Education: Analyzing the Expert’s Gaze
 This is the proceedings of an international workshop on the emerging topic of eye movement data analysis in programming education. The  rst workshop edition focused on “Analyzing the expert’s gaze”. It was held in November 2013 at the School of Computing, University of Eastern Finland.
       Publications of the University of Eastern Finland
Reports and Books in Forestry and Natural Sciences
isbn: 978-952-61-1538-2 (nid.) isbn: 978-952-61-1539-9 (pdf) issnl: 1798-5684
issn: 1798-5684
issn: 1798-5692 (pdf)
 reports and studies | 18 | Bednarik, Busjahn, Schulte (Eds.) | Eye Movements in Programming Education
Journal of Eye Movement Research 2(5):2, 1-10
Eye Movements in Reading: Models and Data
Keith Rayner Department of Psychology University of California, San Diego
Models of eye movement control in reading and their impact on the field are discussed. Differences between the E-Z Reader model and the SWIFT model are reviewed, as are benchmark data that need to be accounted for by any model of eye movement control. Predictions made by the models and how models can sometimes account for counterintuitive findings are also discussed. Finally, the role of models and data in further understanding the reading process is considered.
Keywords: Reading, Eye Movements, Models of Eye Movements.
deal with more surface properties of eye movements, and little attempt was made to infer mental processes from eye movement data. Classic work by Tinker (1946) on reading and by Buswell (1935) on scene per- ception was carried out during this era. Tinker’s (1958) final review ended on the rather pessimistic note that almost everything that had been learned about reading via eye movements (given the technology of the time) had been discovered. As I remarked earlier (Rayner, 1998), this opinion seems to have been widely held because very little research on eye movements during reading was undertaken between the late 1950’s and the mid-1970’s.
The third era began in the mid-1970’s and was marked by both a better description of language and rapid technological advances that resulted in marked improvements in eye movement recording systems (and computer systems) that allowed measurements to be more accurate and more easily obtained. These ad- vances in technology also made possible the develop- ment of innovative techniques in which the visual dis- play could be changed contingent on the eye position. In this gaze-contingent display change paradigm (Mc- Conkie & Rayner, 1975; Rayner, 1975; Reder, 1973), eye movements are monitored and changes are made in the visual display that the reader is looking at contingent on when the eyes move (or at some other time during the fixation). During this third era, great strides were made as a result of researchers designing interesting and informative studies using gaze contingent change (as well as other) techniques.
As noted at the outset of this article, it is now fairly obvious that we have entered a fourth era of research on eye movements during reading, beginning in the late 1990’s, characterized by the development of com- plex and sophisticated models (in the form of imple- mented computer simulations). Much of the research on eye movements during reading over the past ten years has been focused on either providing validity for
1
DOI 10.16910/jemr.2.5.2 This article is licensed under a ISSN 1995-8692 Creative Commons Attribution 4.0 International license.
The study of eye movements during reading has a long and rich history dating back to the latter part of the 19th century. Elsewhere, I (Rayner, 1978, 1998) have argued that there have been three eras of research on eye movements during reading. But, it now is appar- ent that we have entered a new, fourth, era in which sophisticated computational models of eye movement control are dominating the field. Given this new trend, it is perhaps appropriate to consider exactly what role these models, in contrast to empirical data from ex- perimental manipulations and corpus-based analyses, should play in furthering our understanding of the reading process.
The first era extended from Javal’s initial observa- tions concerning the role of eye movements during reading (see Huey, 1908) until about 1920. During this first era, many of the basic facts about eye movements during reading were discovered. In addition, issues such as the perceptual span (the region of effective vi- sion), saccade latency (the time it takes to initiate an eye movement), and saccadic suppression (the fact that we do not perceive information during an eye movements) were of central interest.
The second era, which coincided with the behavior- ist movement in experimental psychology, tended to
This article is based on the keynote address delivered by the author at the Fourteenth European Conference on Eye Movements at Potsdam, Germany in August 2007. A version of the talk was also presented at the International Congress of Psychology at Berlin, Germany in July, 2008. Thanks to Reinhold Kliegl, Simon P. Liversedge, Erik D. Reichle, and an anonymous reviewer for comments on a prior draft.
Correspondence to Keith Rayner, Department of Psychol- ogy, University of California, San Diego, La Jolla, CA 92093.

2 RAYNER
or disconfirming these models. Table 1 provides a list of the currently implemented models. Some might ar- gue that the model provided by Just and Carpenter (1980; see Thibadeau, Just, & Carpenter, 1982 for actual simulations) was the first such model, and I wouldn’t argue with such a claim. However, it also seems fairly clear that the E-Z Reader model was the impetus for the development of the competing models listed in Table 1. Although not the first such model (as Mr. Chips pre- ceded it by a year), E-Z Reader has arguably attracted the most attention, largely because of the transparency of the model, the clear predictions that it makes, and the fact that it accounts for a tremendous amount of eye movement data. SWIFT is generally regarded as the main competitor to E-Z Reader1.
In this article, I will primarily use E-Z Reader as the context for various points to be made (though I will also discuss SWIFT as a contrast point). At a conference in Tianjin, China in 2006, Reinhold Kliegl (one of the ar- chitects, along with Ralf Engbert, of the SWIFT model) made the following provocative claim: ”Models of eye movement control in reading are more sophisticated and realistic than models in any other area of psychol- ogy”. Undoubtedly there are many who would dispute this claim because there are highly sophisticated mod- els (generally implemented) in many areas of psychol- ogy (especially cognitive psychology), yet his claim has special force when one considers his additional proviso concerning being ”realistic”. Virtually all of the eye movement models produce data that fit directly with the human data. That is, the models produce eye fix- ations durations in milliseconds (and not number of processing cycles or some other more indirect metric) and they likewise produce number of fixations, saccade length, skipping rates, and so on. The predictions made by the models are also in terms of real units (words) rather than regions of interest.
My goal in the present article is threefold. First, I will discuss the data that models of eye movement control in reading should be able to handle. Second, I will summarize some predictions that follow from E-Z Reader and SWIFT and point out how they can provide accounts of what otherwise might be difficult data to explain (as well as discuss how the models have been changed and extended). Third, I will argue that models and data should go hand-in-hand to further our under- standing of reading. Prior to discussing these issues, a brief overview of E-Z Reader will be provided, as well as some comments on what distinguishes the models.
The E-Z Reader Model
The details of E-Z Reader have been presented else- where and here I will only provide a brief and cur- sory overview. In actuality, E-Z Reader is a family of models with the first five versions discussed in Reichle, Pollatsek, Fisher, and Rayner (1998) and subsequent versions presented by Reichle, Rayner, and Pollatsek
(2003), Rayner, Ashby, Reichle, and Pollatsek (2004), and Pollatsek, Reichle, and Rayner (2006). In all of these versions of the model, lexical processing is the engine that drives eye movements during reading. The most recent version of the model, E-Z Reader 10 (Re- ichle, Warren, & O’Connell, 2009) presents an attempt to include the influence of higher order variables on eye movements during reading. The model has also been extended to Chinese readers (Rayner, Li, & Pollat- sek, 2007) and older readers (Rayner, Reichle, Stroud, Williams, & Pollatsek, 2006).
According to E-Z Reader, two stages of lexical pro- cessing are underway when readers fixate a word. An initial stage, referred to as L1, can be characterized as the system determining if it is likely to know what the fixated word is in the next few milliseconds. Once a cer- tain threshold is exceeded, a signal is sent to the oculo- motor system to program a saccade to the next uniden- tified word in the text. This decision of where to go next is also influenced by a low level attentional scan oper- ating in parallel with lexical processing that is more or less surveying the terrain of what is coming up (and where word boundaries are). The second stage of lexi- cal processing, L2, is akin to lexical access for the word. When L2 completes, attention moves to word n+1, and the two stages begin running for that word. On some occasions, L1 and L2 will both complete for word n+1 (such as when it is a highly predictable word) and at- tention will shift to word n+2; the saccade to word n+1 will also be cancelled and a saccade programmed to word n+2 (provided that the prior saccade program has not passed a point of no return). Given that the two stages are influenced by the frequency and predictabil- ity of word n, lexical processing is the engine driving the eyes through the text according to E-Z Reader.
How Do the Models Differ?
It is generally recognized that there are two primary ways in which the various models differ (see Reichle et al., 2003 for a more extended discussion). Here, I’ll focus on how E-Z Reader and SWIFT differ. First, E-Z Reader has been categorized as a serial attention shift (SAS) model, whereas SWIFT has been categorized as a gradient by attention guidance (GAG)2 model. The main distinction is that in E-Z Reader serial lexical pro- cessing (word n is identified then word n+1 and then word n+2, in order) is invoked whereas in SWIFT par- allel lexical processing is possible (so that more than one word can be processed lexically at any given point in an eye fixation). There has been considerable dis- cussion about this issue lately (see Kennedy & Pynte,
1 With all due apologies to the designers of the other mod- els listed in Table 1 for the possible ego-centric nature of this claim.
2 Chuck Clifton recently pointed out that E-Z Reader wins out if one simply goes by better sounding or more appealing acronyms.
 DOI 10.16910/jemr.2.5.2 This article is licensed under a ISSN 1995-8692 Creative Commons Attribution 4.0 International license.
JOURNAL OF EYE MOVEMENT RESEARCH, VOLUME 2(5):2 3
Table 1
Models of Eye Movement Control in Reading. The type of model is in parentheses behind the name of the model.
 Model E-Z Reader (SAS)
SWIFT (GAG) Mr. Chips (Ideal Observera ) EMMA (SAS within ACT system) Glenmore (GAG within Connectionist System) SERIF (POCb) Competition/Activation (POC) SHARE (POC)
Key References
Reichle, Pollatsek, Fisher, & Rayner, 1998 Reichle, Rayner, & Pollatsek, 2003 Pollatsek, Reichle, & Rayner, 2006
Engbert, Longtin, & Kliegl, 2002
Engbert, Nuthmann, Richter, & Kliegl, 2005
Legge, Klitz, & Tjan, 1997
Legge, Hooven, Klitz, Mansfield, & Tjan, 2002
Salvucci, 2001
Reilly & Radach, 2006
McDonald, Carpenter, & Shillcock, 2005
Yang & McConkie, 2001 Yang, 2006
Feng, 2006
  aIdeal Observer models do not attempt to mimic human reading.
bIn POC (Primarily Oculomotor Control) models, most of the variation in eye fixation times is due to properties of the oculomotor system, and lexical variables only influence very long fixations. There is some variation across the models in terms of how early in a fixation lexical processing can affect the duration of the current fixation. In SERIF, perceptual processing of words has a more immediate effect on when they eyes move than in the Competi- tion/Activation model or SHARE.
2006, 2008; Kliegl, 2007; Kliegl, Nuthmann, & Engbert, 2006; Rayner, Reichle, Drieghe, Slattery, & Pollatsek, 2007; Rayner, Pollatsek, Liversedge, & Reichle, 2009; Reichle, Liversedge, Pollatsek, & Rayner, 2009) and no clear consensus has yet emerged. However, it may be fair to say that, in principle, much (or perhaps some) of the time GAG models function as serial models, though the advocates of GAG models would also claim that there are existence proofs that more one than word can be processed at once.
The second way in which the models differ relates to the influence of cognitive/lexical properties of the fix- ated word on how long readers remain on that word. In E-Z Reader, as noted above, cognitive/lexical pro- cesses are the engine that drives the eyes through the text; in SWIFT, saccades are autonomously generated but with occasional cognitive influences. Both models thus allow for the influence of variables like word fre- quency and word predictability on fixation times and both models allow for the early influence of such vari-
ables. On the other hand, some models (such as the Competition Inhibition model) generally posit that cog- nitive/lexical variables can only have an effect on eye movements very late in an eye fixation. In my view, there are rather striking data (to be discussed below) that render this claim fairly implausible.
Benchmark Data
Valid models need to account for what is known about the basic properties of eye movements during reading. There are five central facts that any model needs to account for (see Rayner, 1998, 2009 for re- views). First, the average fixation duration in read- ing is about 200-250 ms (though there is considerable variability with some fixations under 100 ms and some over 500 ms). Second, the average saccade length (for alphabetic writing systems) is about 7-9 letter spaces (again with considerable variability as some saccades are less than 1 letter space and some are as long as
DOI 10.16910/jemr.2.5.2 This article is licensed under a ISSN 1995-8692 Creative Commons Attribution 4.0 International license.
4 RAYNER
over 20 letter spaces). Third, various measures of pro- cessing time, such as first fixation duration (the du- ration of the first fixation on a word), single fixation duration (the duration of the fixation when only one fixation is made on a word), and gaze duration (the sum of all fixations on a word prior to moving to an- other word) are sensitive to certain properties of the fixated word (to be discussed below). Fourth, readers skip (do not directly fixate) about 25-30% of words in text (with word length strongly influencing skipping rates as short words are skipped much more than long words; word predictability and word frequency, to a lesser extent, also influence skipping rates). Fifth, read- ers make regressions (saccades backwards to look at words that occurred earlier in the text) about 10-15% of the time. Many of the implemented models are able to account for these findings (though in many cases, some of them are hard-wired in the model). The most problematic issue for most models with respect to the above facts is that they do not do a particularly good of a job in accounting for regressions3. This is perhaps be- cause it is quite difficult for researchers to control when readers regress (as well as when they skip words).
In addition, five other findings are generally agreed upon as being valid characteristics of eye movements in reading (see Rayner, 1998, 2009), and hence, need to be accounted for by models of eye movement con- trol. First, the perceptual span (or region of effective vision during a fixation) extends 3-4 letter spaces to the left of fixation to 14-15 letter spaces to the right of fixation. Second, readers obtain useful preview bene- fit from the word to the right of fixation; evidence for this finding comes from numerous studies using the boundary paradigm (Rayner, 1975) in which a preview word changes to a target word during a saccade. It is generally accepted that the source of the preview ben- efit effect is from orthographic, phonological, and ab- stract letter codes. Interestingly, semantic preview ben- efit is not obtained in English; thus, a semantically re- lated word like song as a preview for tune doesn’t pro- vide preview benefit (Altarriba, Kambe, Pollatsek, & Rayner, 2001; Rayner, Balota, & Pollatsek, 1986). Third, the amount of time that readers look at words varies as a function of variables such as word frequency and word predictability. Indeed, the fact that first fixation durations on a word are influenced by lexical vari- ables, in my opinion, renders models that do not al- low for such early effects quite implausible and such models are seriously compromised by these findings4. Fourth, there are effects associated with the launch site of a saccade and the landing position of the saccade (McConkie, Kerr, Reddix, & Zola, 1988; Rayner, 1979). Specifically, readers tend to fixate about halfway be- tween the beginning of a word and the middle of the word (Rayner, 1979). Fifth, somewhat paradoxically, single fixation durations are longer when the eyes land in the middle of a word, the Inverted Optimal View-
ing Position (IOVP) effect; Vitu, McConkie, Kerr, & O’Regan, 2001) than when they land on the end letters of a word (though frequency effects remain constant). Once again, the models do a fairly good job of account- ing for these effects5 (again, with some of the effects hard-wired in the model).
Predictions from the Models
Models are generally evaluated, not only for how well they handle benchmark data, but also by how effectively they make unique predictions. Both E-Z Reader and SWIFT make unique predictions. Four such predictions will be discussed here. First, E-Z Reader predicts that when readers skip a word, the fix- ation prior to the skip (and to a lesser degree, the fixa- tion after the skip) should be inflated. This prediction flows from the central assumption in E-Z Reader that words are lexically identified in a serial fashion. On this issue, the data are somewhat mixed as some stud- ies (Rayner, Ashby, Pollatsek, & Reichle, 2004; Drieghe, Rayner, & Pollatsek, 2005; Pollatsek, Rayner, & Balota, 1986; Kliegl & Engbert, 2005; Rayner, Juhasz, Ashby, & Clifton, 2003) have found inflated fixations6, while oth- ers (Engbert, Longtin, & Kliegl, 2002; Radach & Heller,
3 SWIFT perhaps does a better job than E-Z Reader with respect to accounting for the frequency of regressions, but neither model does particularly well in accounting for regres- sions due to higher-order processes (though see Reichle, War- ren, & O’Connell, 2009)
4 Indeed, a particularly compelling finding that is quite problematic for models that do not allow for early influ- ences of lexical processing on eye movements is the disap- pearing text studies (Liversedge, Rayner, White, Vergilino- Perez, Findlay, & Kentridge, 2004; Rayner, Liversedge, White, & Vergilino-Perez, 2003; Rayner, White, & Liversedge, 2006). In these studies, on each fixation the fixated word is masked or disappears at a certain point in the fixation. As long as the reader sees the word for 50-60 ms, reading proceeds quite normally. More interesting, how long the readers’ eyes re- main in place is strongly influenced by the frequency of the word. Thus, although the word is no longer there, its fre- quency determines how long the eyes remain in place. This is striking evidence for the early influence of lexical variables on fixation times.
5 SWIFT accounts for single fixation IOVP effects, while E- Z Reader basically accounts for the first of several fixations. While the effect with single fixations has not been demon- strated using E-Z Reader, the mechanism that accounts for the IOVP effect with the first of two fixations (i.e., an auto- matic refixation assumption) should also produce the effect (at least to some degree) with single fixations.
6 It should be noted that although Kliegl and Engbert did find inflated fixations prior to skips the data did not necessar- ily conform to what would be predicted by E-Z Reader. They reported that fixations before skipped words were shorter be- fore short or high-frequency words and longer before long or low-frequency words in comparison with fixations on the same words when they were followed by fixations on the next word (i.e., when the next word was not skipped).
 DOI 10.16910/jemr.2.5.2 This article is licensed under a ISSN 1995-8692 Creative Commons Attribution 4.0 International license.
2000) have not (though those that have not are often corpus based analyses). The data on this issue may be noisy due to the fact, as noted above, experimenters can’t control when readers skip a word. There are also possible influences on skipping related to deliberate versus accidental (due to motor error) skips.
Second, E-Z Reader predicts that readers generally should not obtain preview benefit from word n+2. That is, the model predicts that in the normal course of events, readers should obtain preview benefit from the word to the right of fixation (word n+1), but not word n+2 (unless word n+2 is the target of the next saccade from word n). Again, this prediction follows from the serial architecture of E-Z Reader. For the most part, a series of studies originating with Rayner, Juhasz, and Brown (2007) have generally confirmed the prediction that there is not preview benefit for word n+2 (see also Angele, Slattery, Yang, Kliegl, & Rayner, 2008; Kliegl, Risse, & Laubrock, 2007; McDonald, 2006). SWIFT does predict that there should be preview benefit ef- fects from word n+2, and it may be the case that readers can obtain preview benefit from word n+2 when word n and word n+1 are both short, high frequency words. Such a situation could be seen as an existence proof for SWIFT, but E-Z Reader can account for the occasional preview benefit from word n+2 (i.e., when word n+1 is identified and skipped). Overall, the general pattern of results on experiments dealing with the lack of pre- view benefit for word n+2 might suggest that SWIFT can best handle the results if it is assumed that much of the time the model is acting like E-Z Reader (or an SAS type model).
Third, SWIFT predicts that there should be parafoveal-on-foveal effects wherein the characteristics of word n+1 influence the amount of time that readers look at word n. According to E-Z Reader, parafoveal- on-foveal effects should be limited to orthographic effects, and there should not be lexical effects. Thus, the frequency of word n+1 should not influence the amount of time that readers look at word n accord- ing to E-Z Reader, whereas it should according to SWIFT. The entire issue of parafoveal-on-foveal effects is highly contentious (see Rayner & Juhasz, 2004; Rayner, White, Kambe, Miller, & Liversedge, 2003 for reviews). In alphabetic writing systems, lexical parafoveal-on-foveal effects are typically not found via the boundary paradigm, though there are exceptions (i.e., Kliegl et al., 2007 reported a parafoveal-on-foveal effect). On the other hand, parafoveal-on-foveal effects are routinely reported for corpus analyses of eye movements in reading (Kennedy & Pynte, 2005; Kliegl, Nuthmann, & Engbert, 2006; see Kliegl, 2007; Rayner, Pollatsek, Drieghe, Slattery, & Reichle, 2007 for further discussion). Compared to preview benefit effects, parafoveal-on-foveal effects tend to be weak and the direction of the effect is not consistent across studies7. It should also be noted that the lack of semantic preview benefit effects is problematic for SWIFT.
In E-Z Reader, parafoveal-on-foveal effects are generally explained in terms of mislocated fixations (Drieghe, Rayner, & Pollatsek, 2007; Rayner, Warren, Juhasz, & Liversedge, 2004). That is, due to noise in either the oculomotor system or the eyetracking equip- ment, where the eyes are fixating and where attention is (or more specifically, which word was actually be- ing processed) could be in different places. Thus, for example, due to undershoot in the oculomotor system, the eyes could be on word n while attention (and hence word processing) is on word n+1. Such a mislocated fixation account would explain many parafoveal-on- foveal effects. It is quite interesting that Nuthmann, En- gbert, and Kliegl (2005) have used modeling techniques to convincingly demonstrate the frequency with which mislocated fixations occur, but they (as proponents of the SWIFT model) do not consider mislocated fixations to be the main cause of parafoveal-on-foveal effects.
A final prediction comes from an observation that Reingold (2003) generated concerning E-Z Reader. He noted that experimental manipulations that disrupt early encoding of visual and orthographic features of the fixated word without affecting subsequent lexical processing should influence the processing difficulty of the fixated word without affecting the processing of the next word. Reingold and Rayner (2006) then explicitly tested this prediction and found results consistent with E-Z Reader.
One could compute a score card of how well empir- ical studies support the predictions of E-Z Reader ver- sus SWIFT. In doing so, it may be that beauty is in the eye of the beholder. My goal in this section has not really been to provide a scorecard, but rather to doc- ument some of the predictions the models make. Per- haps the most unbiased statement that could be made is that each model has support, but that there are prob- lems with extant data for each model as well. But, more importantly, my guess is that many experiments that have been reported over the past few years would not have been done had it not been for the explicit predic- tions made by the models. To this extent, the models have been extremely important in driving forward em- pirical work in the field of eye movements and reading.
7 Interestingly, Yang, Wang, Xu, and Rayner (2009) recently reported a parafoveal-on-foveal effect with Chinese, sugges- tion that readers of Chinese obtain some information of the word to the right of the fixation that influences the current fixation duration. And, Yan, Richter, Shu, and Kliegl (2009) recently reported semantic preview benefit effects for Chi- nese. It may be that such differences are due to the fact that any given Chinese character in a word will be closer to fixa- tion on average than letters within words in English. Thus, there is much more possibility that semantic preview benefit effects and parafoveal-on-foveal effects may occur in a char- acter based written language like Chinese than in an alpha- betic horizontally spatially extended language like English.
JOURNAL OF EYE MOVEMENT RESEARCH, VOLUME 2(5):2 5
 DOI 10.16910/jemr.2.5.2 This article is licensed under a ISSN 1995-8692 Creative Commons Attribution 4.0 International license.
6
RAYNER
Explaining Counterintuitive Findings
An interesting finding recently emerged in our lab- oratory; we found that gaze durations on a noun were longer when the preceding adjective was low fre- quency than when it was high frequency but were ac- tually shorter when the adjective was long than when it was short (Pollatsek, Juhasz, Reichle, Machacek, & Rayner, 2009). This counterintuitive finding is one that caused us to shake our heads in frustration for some time, but when we actually implemented a simulation of the experiment using E-Z Reader, we found that the model accounted for the data. Likewise, Nuthmann, Engbert, and Kliegl (2007) compellingly documented how the counterintuitive IOVP effect mentioned above emerged in SWIFT simulations.
My point in this section is that computational mod- els can be very informative, not only with respect to generating interesting predictions for researchers to test, but they can also sometimes account for otherwise seemingly unexplainable findings that might emerge from empirical investigations. Thus, the models can sometimes offer researchers assistance in accounting for interesting and counterintuitive findings.
Changes to the Models
An interesting issue with respect to the develop- ment of models is the extent to which any given model should be changed as a result of convincing evidence that is inconsistent with original assumptions of the model. Some purists might consider it a cheat to make changes to a model once it has been presented. In- deed, charges of a model being a moving target are of- ten made when the model is changed. Yet, it seems quite rational that a given model would be changed when there are data that emerge that the architects of the model deem valid. For example, results re- ported by Rayner, Ashby et al. (2004) resulted in a change to E-Z Reader. An original assumption in E- Z Reader was that frequency and predictability share a multiplicative function. However, data from Rayner, Ashby et al. (2004) found additive (or weakly interac- tive) effects of frequency and predictability in an ex- periment in which the two variables were orthogonally varied. Thus, the equation relating frequency and pre- dictability was changed to an additive function to ac- count for the data pattern (see also Miellet, Sparrow, & Sereno, 2007). It is also worth noting that changes have been made to SWIFT as a result of empirical data that emerged. Thus, the original version of SWIFT (Engbert et al. 2002) differs from the current version (Engbert et al., 2005) and the changes were implemented as a result of data deemed to be in conflict with the original ver- sion. Such changes seem to be entirely appropriate and valid.
Extending the Models
As noted earlier, E-Z Reader has been extended to older readers and to Chinese readers. Research has demonstrated that elderly readers make longer fixation durations, shorter saccades, but skip words more of- ten and regress back to those words more often than younger readers (Laubrock, Kliegl, & Engbert, 2006; Rayner, Castelhano, & Yang, 2009; Rayner, Reichle, Stroud, Williams, & Pollatsek, 2006). Such effects have been modeled in the context of both E-Z Reader (Rayner et al., 2006) and SWIFT (Laubrock et al., 2006). Also, a considerable amount of recent research has ex- amined the characteristics of eye movements of Chi- nese readers. And, E-Z Reader has been extended to account for these data (Rayner, Li, & Pollatsek, 2007).
The Role of Models and Data
It should hopefully be apparent that models of eye movement control in reading have successfully ac- counted for much of the important available data. But, it is important to realize that the models can only be tested via good experiments. Thus, the models and good data go hand in hand in advancing the field. Al- though we are now in a fourth era of eye movement research in reading where the models have been ad- mittedly quite dominant (for reasons discussed above) one shouldn’t lose sight of the fact that there should always be an important role for empirical data. Fur- thermore, the models are not perfect and there are as- pects of the eye movement record that they do not do a particularly good job of capturing, such as regressions (Mitchell, Shen, Green, & Hodgson, 2008), and the ef- fect of higher order influences on fixation times (see Re- ichle et al., 2009).
For the most controversial aspects of eye movements during reading, such as (1) parafoveal-on-foveal ef- fects, (2) word skipping, and (3) regressions, there seems to be a division between experimental studies and corpus-based studies. While it is clearly the case that corpus-based analyses are best suited for certain is- sues, such corpus-based studies are being increasingly used in cases where experimental control is quite possi- ble. Experimenters in the field need to ask themselves which empirical approach to data acquisition should be trusted more? There isn’t always a simple answer to this question (cf., Kliegl et al., 2006; Kliegl, 2007; Rayner et al., 2007), but one thing is certain and that is data ob- tained via corpus-based analyses need to be confirmed via experimental studies, and vice versa.
As a caveat, let me note the following. I often read papers submitted to journals (and sometimes in print) saying: These data provide evidence against E- Z Reader or SWIFT. In my opinion, this is not a wise statement! It would be more accurate to say that the data are not consistent with the model as currently im- plemented. In many cases where it is argued that the
DOI 10.16910/jemr.2.5.2 This article is licensed under a ISSN 1995-8692 Creative Commons Attribution 4.0 International license.
data are inconsistent with the model, it is simply the case that the model hasn’t tried to account for the data. And, you can’t know if the model doesn’t account for an effect until you do the simulation (e.g., see Pollatsek et al., 2006). Fortunately, in the case of both E-Z Reader and SWIFT, the models are publicly available (along with information on how to use the code8), and thus re- searchers other than the architects of the model can use them to run simulations and thereby generate/test pre- dictions (see Miellet et al., 2007 for an example). In part, the issue here is that some researchers have adopted the practice of making ”predictions” about the models based on their (sometimes faulty) understanding of the model, run experiments that provide results that are inconsistent with the ”predictions”, and then conclude that the model has been falsified. In such situations, the predictions aren’t really valid to begin with because the experimenters have not done what needs to be done– namely, that the novel prediction needs to be derived and tested from the model. Basically, this amounts to researchers making claims about what the model can or cannot do without first checking to see if such claims are true.
Another issue has to do with how models of eye movement control are tested against each other. Three points are relevant. First, as discussed above, there is a well established set of benchmark phenomena that all of the serious researchers involved with eye move- ment control models have adopted and use in model evaluation. Second, while it might be argued that it would be desirable to have a fixed test suite of cor- pora for model evaluation, the fact that several differ- ent corpora (and not just one corpus) are being used in model evaluation can be seen as a strength, and not a weakness of the modeling endeavor. For exam- ple, this allows researchers to evaluate model general- izability across writing systems and languages. Finally, cross-validation procedures have been explicitly used in some cases (see McDonald, Carpenter, & Shillcock, 2005), and rough equivalents of such procedures (e.g., using the same model and parameter values to fit data from completely independent experiments) have also been widely used (e.g., Rayner et al., 2005).
Finally, it is worth noting that there are now a num- ber of models of eye movement control in scene percep- tion and visual search that have appeared recently (see Rayner, 2009). However, these models focus primarily on where we move our eyes and say little about when we move our eyes. In general, work on eye movements in scene perception and visual search has lagged be- hind work in reading. This is not surprising since with reading the stimulus characteristics are easier to spec- ify and the task is more apparent. However, the future looks promising with respect to models of eye move- ment control in scene perception/visual search, and hopefully architects of these models will soon move to- wards explaining not only where viewers move their eyes in scene perception and reading, but also attempt
to explain the when decision with respect to moving the eyes.
Summary
Models of eye movement control in reading have stimulated a lot of research. While these models have, in some sense, dominated the field recently, there is still an important place for well-designed experiments and empirical data. The models and data should go hand- in-hand in furthering our understanding of reading.
References
Altarriba, J., Kambe, G., Pollatsek, A., & Rayner, K. (2001). Semantic codes are not used in integrating information across eye fixations in reading: Evidence from fluent Spanish-English bilinguals. Perception & Psychophysics, 63, 875-890.
Angele, B., Slattery, T.J., Yang, J., Kliegl, R., & Rayner, K. (2008). Parafoveal processing in reading: Ma- nipulating n+1 and n+2 previews simultaneously. Visual Cognition, 16, 697-707.
Buswell, G.T. (1935). How people look at pictures. Chicago: University of Chicago Press.
Drieghe, D., Rayner, K., & Pollatsek, A. (2005). Eye movements and word skipping during reading revisited. Journal of Experimental Psychology: Human Perception and Performance, 31, 954-969.
Drieghe, D., Rayner, K., & Pollatsek, A. (2008). Mis- located fixations can account for parafoveal-on- foveal effects in eye movements during reading. Quarterly Journal of Experimental Psychology, 61, 1239-1249.
Engbert, R., Longtin, A., & Kliegl, R. (2002). A dynam- ical model of saccade generation in reading based on spatially distributed lexical processing. Vision Research, 42, 621-636.
Engbert, R., Nuthmann, A., Richter, E., & Kliegl, R. (2005). SWIFT: A dynamical model of saccade generation during reading. Psychological Review, 112, 777-813.
Feng, G. (2006). Eye movements as time-series random variables: A stochastic model of eye movement control in reading. Cognitive Systems Research, 7, 70-95.
8 See the following URL for E-Z Reader: http://www.pitt.edu/ ̃reichle/ezreader.html and for SWIFT see: http://www.agnld.uni- potsdam.de/ ̃ralf/ swift/
JOURNAL OF EYE MOVEMENT RESEARCH, VOLUME 2(5):2 7
 DOI 10.16910/jemr.2.5.2 This article is licensed under a
Creative Commons Attribution 4.0 International license.
ISSN 1995-8692
8 RAYNER Huey, E.B. (1908). The psychology and pedagogy of
reading. New York: Macmillan.
Just, M.A., & Carpenter, P.A. (1980). A theory of reading: From eye fixations to comprehension. Psychological Review, 87, 329-354.
Kennedy, A, & Pynte J. (2005). Parafoveal-on-foveal effects in normal reading Vision Research, 45, 153- 168.
Kennedy, A., & Pynte, J. (2008). The consequences of violations to reading order: An eye movement analysis. Vision Research, 48, 2309-2320.
Kliegl, R. (2007). Toward a perceptual-span theory of distributed processing in reading: A Reply to Rayner, Pollatsek, Drieghe, Slattery, and Reichle (2007). Journal of Experimental Psychology: General, 136, 530-537.
Kliegl, R., & Engbert, R. (2005). Fixation durations before word skipping in reading. Psychonomic Bulletin & Review, 12, 132-138.
Kliegl, R., Nuthmann, A., & Engbert, R. (2006). Tracking the mind during reading: The influence of past, present, and future words on fixation durations. Journal of Experimental Psychology: General, 135, 12-35.
Kliegl, R., Risse, S., & Laubrock, J. (2007). Preview benefit and parafoveal-on-foveal effects from word n+2. Journal of Experimental Psychology: Human Perception and Performance, 33, 1250-1255.
Laubrock, J., Kliegl, R., & Engbet, R. (2006). SWIFT explorations of age differences in eye movements during reading. Neuroscience and Biobehavioral Reviews, 30, 872-884.
Legge, G.E., Hooven, T.A., Klitz, T.S., Mansfield, J.S., & Tjan, B.S. (2002). Mr. Chips 2002: New insights from an ideal-observer model of reading. Vision Research, 42, 2219-2234
Legge, G.W., Klitz, T.S., & Tjan, B.S. (1997). Mr. Chips: An ideal-observer model of reading. Psychological Review, 104, 524-553.
Liversedge, S.P., Rayner, K., White, S.J., Vergilino- Perez, D., Findlay, J.M., & Kentridge, R.W. (2004). Eye movements while reading disappearing text: Is there a gap effect in reading? Vision Research, 44, 1013-1024.
McConkie, G.W., Kerr, P.W., Reddix, M.D., & Zola, D. (1988). Eye movement control during reading: I.
The location of initial fixations in words. Vision Reseach, 28, 1107-1118.
DOI 10.16910/jemr.2.5.2 This article is licensed under a ISSN 1995-8692 Creative Commons Attribution 4.0 International license.
McConkie, G. W., & Rayner, K. (1975). The span of the effective stimulus during a fixation in reading. Perception & Psychophysics, 17, 578586.
McDonald, S.A. (2006). Parafoveal preview benefit in reading is only obtained from the saccade goal. Vision Research, 46, 4416-4424.
McDonald, S.A., Carpenter, R.H.S., & Shillcock, R.C. (2005). An anatomically-constrained, stochastic model of eye movement control in reading. Psychological Review, 112, 814-840.
Mitchell, D.C., She, X., Green, M.J., & Hodgson, T.L. (2008). Accouting for regressive eye-movements in models of sentence processing: A reappraisal of the Selective Reanalysis hypothesis. Journal of Memory and Language, 59, 266-293.
Miellet, S., Sparrow, L., & Sereno, S.C. (2007). Word frequency and predictability effects in reading French: An evaluation of the E-Z Reader model. Psychonomic Bulletin & Review, 14, 762-769.
Nuthmann, A., Engbert, R., & Kliegl, R. (2005). Mislo- cated fixations during reading and the inverted optimal viewing position effect. Vision Research, 45, 2201-2217.
Pollatsek, A., Juhasz, B.J., Reichle, E.D., Machacek, D., & Rayner, K. (2008). Immediate and delayed effects of word frequency and word length on eye movements during reading: A reversed delayed effect of word length. Journal of Experimental Psychology: Human Perception and Performance, 34, 726-750.
Pollatsek, A., Rayner, K., & Balota, D. A. (1986). Inferences about eye movement control from the perceptual span in reading. Perception & Psychophysics, 40, 123-130.
Pollatsek, A., Reichle, E.D., & Rayner, K. (2006). Tests of the E-Z Reader model: Exploring the interface between cognition and eye movements. Cognitive Psychology, 52, 1-56.
Radach, R., & Heller, D. (2000). Relations between spatial and temporal aspects of eye movement control. In A. Kennedy, R.Radach, D. Heller, and J. Pynte (Eds.), Reading as a perceptual process (pp. 169-192). Oxford, UK: Elsevier.
Rayner, K. (1975). The perceptual span and peripheral cues during reading. Cognitive Psychology, 7, 65-81.
Rayner, K. (1978). Eye movements in reading and information processing. Psychological Bulletin, 85, 618660.
Rayner, K. (1979). Eye guidance in reading: Fixation locations in words. Perception, 8, 2130.
Rayner, K. (1998). Eye movements in reading and information processing: 20 years of research. Psychological Bulletin, 124, 372-422.
Rayner, K. (2009). Eye movements and attention in reading, scene perception, and visual search. Quarterly Journal of Experimental Psychology, in press.
Rayner, K., Ashby, J., Pollatsek, A., & Reichle, E.D. (2004). The effects of frequency and predictability on eye fixations in reading: Implications for the E-Z Reader model. Journal of Experimental Psychology: Human Perception and Performance, 30, 720-732.
Rayner, K., Balota, D. A., & Pollatsek, A. (1986). Against parafoveal semantic preprocessing dur- ing eye fixations in reading. Canadian Journal of Psychology, 40, 473-483.
Rayner, K., Castelhano, M.S., & Yang, J. (2009). Eye movements and the perceptual span in older and younger readers. Psychology and Aging, in press.
Rayner, K., & Juhasz, B.J. (2004). Eye movements in reading: Old questions and new directions. European Journal of Cognitive Psychology, 16, 340- 352.
Rayner, K., Juhasz, B.J., Ashby, J. & Clifton, C. (2003). Inhibition of saccade return in reading. Vision Research, 43, 1027-1034.
Rayner, K., Juhasz, B.J., & Brown, S.J. (2007). Do read- ers obtain preview benefit from word n+2? A test of serial attention shift versus distributed lexical processing models of eye movement control in reading. Journal of Experimental Psychology: Human Perception and Performance, 33, 230-245.
Rayner, K., Li, X., & Pollatsek, A. (2007). Extending the E-Z Reader model of eye movement control to Chinese readers. Cognitive Science, 31, 1021-1034.
Rayner, K., Liversedge, S.P., & White, S.J. (2006). Eye movements when reading disappearing text: The importance of the word to the right of fixation. VisionResearch,46,310-323.
Rayner, K., Liversedge, S.P., White, S.J., & Vergilino- Perez, D. (2003). Reading disappearing text: Cognitive control of eye movements. Psychological Science, 14, 385-389.
Rayner, K., Pollatsek, A., Drieghe, D., Slattery, T.J., & Reichle, E.D. (2007). Tracking the mind during reading via eye movements: Comments on Kliegl, Nuthmann, and Engbert (2006). Journal of Experimental Psychology: General, 136, 520-529.
Rayner, K., Pollatsek, A., Liversedge, S.P., & Reichle, E.D. (2009). Eye movements and non-canonical reading: Comments on Kennedy and Pynte (2008). Vision Research, in press.
Rayner, K., Reichle, E.D., Stroud, M.J., Williams, C.C., & Pollatsek, A. (2006). The effect of word fre- quency, word predictability, and font difficulty on the eye movements of young and older readers. Psychology and Aging, 21, 448-465.
Rayner, K., Warren, T., Juhasz, B.J., & Liversedge, S.P. (2004). The effect of plausibility on eye movements in reading. Journal of Experimental Psychology: Learning, Memory, and Cognition, 30, 1290-1301.
Rayner, K., White, S.J., Kambe, G., Miller, B., & Liversedge, S.P. On the processing of meaning from parafoveal vision during eye fixations in reading. In J. Hyona, R.Radach, & H.Deubel (Eds), The mind’s eye: Cognitive and applied aspects of eye movements (pp. 213-234). Oxford: Elsevier.
Reder, S.M. (1973). On-line monitoring of eye po- sition signals in contingent and noncontingent paradigms. Behavior Research Methods and Instru- mentation, 5, 218-228.
Reichle, E.D., Liversedge, S.P., Pollatsek, A., & Rayner, K. (2009). Encoding multiple words simultane- ously in reading is implausible. Trends in Cognitive Science, in press.
Reichle, E.D., Pollatsek, A., Fisher, D.L., & Rayner, K. (1998). Toward a model of eye movement control in reading. Psychological Review, 105, 125-157.
Reichle, E.D., Rayner, K., & Pollatsek, A. (2003). The E-Z Reader model of eye-movement control in reading: Comparisons to other models. Behavioral and Brain Sciences, 26, 445-476.
Reichle, E.D., Warren, T., & McConnell, K. (2009). Using E-Z Reader to model the effects of higher- level language processing on eye movements during reading. Psychonomic Bulletin & Review, 16,
JOURNAL OF EYE MOVEMENT RESEARCH, VOLUME 2(5):2 9
DOI 10.16910/jemr.2.5.2 This article is licensed under a ISSN 1995-8692 Creative Commons Attribution 4.0 International license.
10
RAYNER
1-21.
Reilly, R., & Radach, R. (2006). Some empirical tests of an interactive activation model of eye movement control in reading. Cognitive Systems Research, 7, 34-55.
Reingold, E.M. (2003). Eye-movement control in reading: Models and predictions. Behavioral and Brain Sciences, 26, 500-501.
Reingold, E.M., & Rayner, K. (2006). Examining the word identification stages hypothesized by the E- Z Reader model. Psychological Science, 17, 742-746.
Salvucci, D.D. (2001). An integrated model of eye movements and visual encoding. Cognitive Systems Research, 1, 201-220.
Thibadeau, R., Just, M. A., & Carpenter, P.A. (1980). A model of the time course and content of human reading. Cognitive Science, 6, 101-155.
Tinker, M.A. (1946). The study of eye movements in reading. Psychological Bulletin, 43, 93-120.
Tinker, M.A. (1958). Recent studies of eye movements in reading. Psychological Bulletin, 55, 215-231.
Vitu, F., McConkie, G.W., Kerr, P., & O’Regan, J.K. (2001). Fixation location effects on fixation durations during reading: an inverted optimal viewing position effect. Vision Research, 41, 3513- 3533.
Yan, M., Richter, E., Shu, H., & Kliegl, R. (2009). Readers of Chinese extract semantic information from parafoveal words. Psychonomic Bulletin & Review, in press.
Yang, J., Wang, S., Xu, Y., & Rayner, K. (2009). Do Chi- nese readers obtain preview benefit from word n+2? Evidence from eye movements. Journal of Experimental Psychology: Human Perception and Performance, in press.
Yang, S. (2006). A oculomotor-based model of eye movements in reading: The competi- tion/activation model. Cognitive Systems Research, 7, 56-69.
Yang, S., & McConkie, G.W. (2001). Eye movements during reading: a theory of saccade initiation times. Vision Research, 41, 3567-3585.
DOI 10.16910/jemr.2.5.2 This article is licensed under a ISSN 1995-8692 Creative Commons Attribution 4.0 International license.
     REVIEW ARTICLE
published: 17 March 2014 doi: 10.3389/fpsyg.2014.00210
Eye movements when viewing advertisements
Emily Higgins*, Mallorie Leinenger and Keith Rayner
Department of Psychology, University of California, San Diego, CA, USA
                     Edited by:
Jukka Hyönä, University of Turku, Finland
Reviewed by:
Agnieszka Konopka, Max Plank Institute for Psycholinguistics, Netherlands
Stevan Adam Brasel, Boston College, USA
*Correspondence:
Emily Higgins, Department of Psychology, 0109, University of California, San Diego, 9500 Gilman Drive, La Jolla, CA 92093, USA e-mail: ehiggins@ucsd.edu
In this selective review, we examine key findings on eye movements when viewing advertisements. We begin with a brief, general introduction to the properties and neural underpinnings of saccadic eye movements. Next, we provide an overview of eye movement behavior during reading, scene perception, and visual search, since each of these activities is, at various times, involved in viewing ads. We then review the literature on eye movements when viewing print ads and warning labels (of the kind that appear on alcohol and tobacco ads), before turning to a consideration of advertisements in dynamic media (television and the Internet). Finally, we propose topics and methodological approaches that may prove to be useful in future research.
Keywords: advertising, eye movements, visual attention, saccades, marketing
Eye movements are of interest, with respect to viewing advertisements and more generally, because they provide fine- grained information about patterns of visual attention. Because we cannot process detailed information far beyond the fovea, the central region of the retina spanning about 2◦ of visual angle, we must move our eyes from one location to the next, sequentially fixating (or looking directly at) areas of interest (Rayner, 1998, 2009). Saccade targets are determined, in large part, by our imme- diate cognitive or perceptual requirements. Eye movements are thus an important way in which we exercise active selection over our complex visual environments (Findlay and Gilchrist, 2003). By inspecting the eye movement record we can, consequently, make inferences about how viewers selectively attend to the visual world, whether they are reading, viewing natural scenes, search- ing for a target item, or, as is of primary concern here, viewing advertisements.
It is important to note, at this point, that eye position and the locus of visual attention are not precisely identical con- cepts, since it is possible to disengage attention from the current point of fixation (Posner, 1980). Indeed, our attention gen- erally shifts to the next location we will fixate shortly before we actually move our eyes (Rayner et al., 1978; Kowler et al., 1995; Deubel and Schneider, 1996). However, attention and eye movements are typically quite closely coupled (and, when they do become separated, it is generally in the systematic man- ner just described, so that the eyes will soon “catch up” with the focus of attention). Therefore, fixation distributions provide detailed information about which regions of a display most effec- tively capture visual attention. Furthermore, the duration spent fixating each location provides information about the amount of cognitive and perceptual processing devoted to that region (Rayner, 1998, 2009).
Research on eye movements and advertisements can provide general theoretical insights (Rayner et al., 2001; Wedel and Pieters, 2008b). For instance, the domain is well-suited for investigat- ing the relationships between eye movements and higher-level
phenomena, such as memory and preference. Furthermore, work in this area can shed light on how we integrate text and images as we inspect our visual environments, as ads are often complex stimuli, composed of both elements. As Buswell (1935) noted in his classic study of eye movements and scene perception, this research may be also be useful from an applied perspective (see Duchowski, 2002 for a general review of applied eye movement research).
There are several reasons why eye tracking may be useful to those who design advertisements or public policy notices such as warnings on alcohol and tobacco products. First, eye move- ments can provide insight into the fast and detailed dynamics of visual attention that may simply not be available for intro- spection or verbal report (Pieters and Wedel, 2008). Second, eye tracking can be done in real time during ad viewing with- out interfering with ongoing processing (Russo, 1978; Wedel and Pieters, 2008a; Glaholt and Reingold, 2011). Third, the technique seems less prone to biasing subsequent responses of interest (e.g., choice of product or brand memory) than ver- bal protocols. Fourth, eye tracking can provide an efficient means of pinpointing which specific characteristics of an ad contribute to its success or failure in holding viewers’ atten- tion or driving consumer choices1. Of course, the technique is limited with regard to the kinds of information it can pro- vide: if a researcher or advertiser were primarily interested
1 Suppose, for instance, that two draft versions of an ad were created and that one was consistently viewed for longer than the other. If the ads differed in several respects (pictorial, headline, etc.), an eye tracking experiment could efficiently reveal which element of the favored (or, at least, longer-viewed) ad was driving the effect. This information could then be used to inform the creation of new ads. As another exam- ple, suppose that behavioral experiments revealed that the inclusion of a particular new element in an ad – a line of text, for example, or a “packshot” showing the product – failed to increase memory and preference for the brand or product in question. Eye tracking could reveal whether the element was viewed (but, presum- ably, deemed unpersuasive) or simply never fixated. This, in turn, could provide useful clues about how the element should be revised, e.g., by changing its message or simply making it more visually salient (Lohse, 1997).
  www.frontiersin.org
March 2014 | Volume 5 | Article 210 | 1
Higgins et al.
Eye movements when viewing advertisements
 in viewers’ conscious, emotional reactions to a given image, for example, soliciting verbal responses would be preferred. Used in conjunction with other approaches, however, includ- ing interviewing subjects, testing their memory for products or brands, and tracking their selections, the technique can contribute substantially to applied research on advertisements (Treistman and Gregg, 1979).
We begin by providing some background information on the basic properties of eye movements as well as their character- istics in reading, scene perception, and visual search. These topics are relevant because ads often consist of both text and scene-like information, and may also include a search com- ponent (if, for example, one is searching in a supermarket circular for a particular product of interest). Next, we will provide a more specific review of key findings concerning eye movements when viewing advertisements, including print ads, warning labels, and ads appearing on television (TV) and on the Internet2. Finally, we outline some topics that have, up to this point, remained relatively unexplored, as well as methodological approaches that may prove useful in future research.
BACKGROUND INFORMATION ON EYE MOVEMENTS
BASIC CHARACTERISTICS
While we can produce several different types of eye movements (see Rayner, 1998 for a review), only saccades are covered here, since they are most critical for the research reviewed. Sac- cades are fast, darting movements that we perform about three times each second (Schiller, 1998). They are interleaved with brief periods of relative stability, known as fixations, which last on average about 200–300 ms, depending on the task and the individual (Rayner, 1998, 2009). Saccades can reach veloc- ities as high as 500◦ of visual angle per second. While their duration is dependent on the distance covered and varies as a function of task, they generally last about 20–50 ms. During these movements, effective visual processing is largely suppressed (Matin, 1974; Campbell and Wurtz, 1978), such that useful visual information can only be gathered during the intervening fixations.
Saccades are executed, as was noted above, in order to bring the fovea, the central 2◦ of the visual field with high acuity and good color vision, into alignment with the region we wish to pro- cess. The region surrounding the fovea and extending up to 5◦ of visual angle from fixation is known as the parafovea, while the region that lies beyond the parafovea is known as the periph- ery (note, however, that acuity drops off in a continuous fashion with increasing distance from the fovea, so that no sharp dis- tinction should be drawn between the parafovea and periphery; Liversedge and Findlay, 2000). Although we make use of the lower resolution, parafoveal and peripheral information (e.g., to
2Please note that some important topics concerning eye movements and marketing lie beyond the purview of this article. For example, we do not cover point-of- purchase marketing here (e.g., consumer responses to supermarket shelf displays). However, this is an active area of research (see Wedel and Pieters, 2008a; Glaholt and Reingold, 2011; Orquin and Mueller Loose, 2013 for relevant reviews). The topic of roadside advertising and potential attendant distraction, while clearly a matter of great importance, is also beyond the scope of the present article.
begin to process an upcoming word when reading or to decide where to move the eye next), for most tasks requiring the rapid processing of detail, foveal processing is necessary (Rayner, 1998, 2009).
NEURAL BASIS OF SACCADE TARGETING
The neural underpinnings of saccade targeting span multiple cortical and sub-cortical structures involved in attention, visual processing, and motor planning. We present a brief overview of some of the important aspects of this system here (for reviews, see Gaymard et al., 1998; Schiller, 1998; Liversedge and Findlay, 2000; Pierrot-Deseilligny et al., 2004; Schall and Cohen, 2011).
A saccade occurs when the extraocular muscles, arranged in three opposing pairs around the eye, are appropriately stimulated by premotor structures in the brainstem. Regions of the superior colliculus (SC), located in the midbrain, are critical for controlling these saccades. One population of cells in the SC fires continually during fixation, ceasing to fire just before a saccade is executed and remaining inactive for much of the duration of the saccade. Another population of cells forms a map of the visual field. The level of neural activity at different locations in the map appears to code for the importance of the corresponding locations in the visual scene. Thus, this population of cells is sometimes referred to as a salience map, with areas of high activity (or “peaks”) mark- ing important positions that serve as candidate targets for the upcoming saccade (Findlay and Gilchrist, 2005).
Similar maps appear to exist in other, cortical areas of the brain that project to the SC, though they are sometimes known as prior- ity maps in these higher areas (Schütz et al., 2011). Maps in a region of the frontal cortex known as the frontal eye fields (FEF) may be important for directing endogenous, or top-down, saccades – i.e., saccades based largely on the goals of the viewer3. In contrast, the parietal eye fields (PEF) in the parietal lobe appear to be partic- ularly important for coding exogenous, reflexive, or bottom-up saccades, of the kind that might occur, for example, following the sudden onset of a stimulus. Other frontal regions may be involved in suppressing such saccades, however, when executing them would be undesirable for present purposes (Pierrot-Deseilligny et al., 2004).
Notably, when mild stimulation, insufficient to trigger a sac- cade, is applied to the SC or FEF, this leads to superior visual processing at the corresponding locations in the scene (see Noudoost et al., 2010 for a summary), indicating overlap between the visual attention system and the oculomotor system (see Des- imone and Duncan, 1995 for a review of visual attention in the brain).
While the basic principles of the oculomotor system hold true across tasks, it is important to note that eye movement measures in one task (e.g., reading) can differ substantially from those in other tasks (e.g., scene perception). This likely follows from dif- ferences both in the physical stimuli involved and in the nature of the viewers’ goals and cognitive processing across these differ- ent activities. Therefore, we outline the basic characteristics of eye
3 Many complexities of the system are necessarily omitted from this short review. For example, the FEF also have direct projections to the premotor areas of the brainstem that are not relayed through the SC (Gaymard et al., 1998).
   Frontiers in Psychology | Cognition
March 2014 | Volume 5 | Article 210 | 2
Higgins et al.
Eye movements when viewing advertisements
 movements during reading, scene perception, and visual search below.
READING
When reading, fixations tend to be on the order of 225–250 ms. Average saccade length is seven to nine letters in alphabetic lan- guages (Rayner, 1998, 2009). For speakers of English, and other languages written from left to right, most eye movements proceed in that direction, with regressions (i.e., saccades that move back- ward in the text) representing 10–15% of eye movements. Readers only fixate about 70% of the words in the text, skipping the other 30%.
Eye movements during reading provide an online index of the cognitive processes underlying language comprehension: in fact, how long the eyes remain fixated on a given word largely depends on how easy or difficult it is to process. Lexical variables such as word frequency and predictability have strong influences on fixation durations (for reviews, see Rayner, 1998, 2009), as does reading skill (Ashby et al., 2005) as well as typographical factors such as font difficulty (Rayner et al., 2006; Slattery and Rayner, 2010).
Though a large amount of text falls on the visual field during reading, readers are only able to obtain useful letter information from approximately 18–20 character spaces around fixation, and they do not use information from lines above or below the cur- rently fixated line (Inhoff and Briihl, 1991; Inhoff and Topolski, 1992; Pollatsek et al., 1993). This limited area of effective process- ing, known as the perceptual span, is asymmetrical in the direction of upcoming text (and attention), such that, for readers of English, it extends about three to four character spaces to the left of fixa- tion (McConkie and Rayner, 1976; Rayner et al., 1980) and 14–15 characters to the right of fixation (McConkie and Rayner, 1975; Rayner and Bertera, 1979).
While fixation location and visual attention coincide when we are processing a fixated word, they may become decoupled when processing of that word is complete. While the eyes remain fixated on the current word, attention can nonetheless shift to the upcoming word (located parafoveally, but within the percep- tual span) so that processing of this parafoveal word can begin. This preprocessing prior to actual fixation will facilitate foveal processing following a saccade to that word, giving rise to a pre- view benefit. Preview benefit is measured using a gaze-contingent boundary paradigm (Rayner, 1975), in which an initial preview of a target word is replaced with the word itself when the sub- ject’s eyes cross an invisible boundary during the saccade to the target (note that, because the display change occurs during the saccade, when vision is largely suppressed, subjects generally fail to notice it; Slattery et al., 2011). The preview may be identical to the target or may be a non-identical letter string. During reading, this preview benefit, defined as the reduction in foveal viewing time of the target following an identical vs. a non-identical pre- view, is about 30–50 ms (for reviews, see Rayner, 1998, 2009; Schotter et al., 2012).
SCENE PERCEPTION
During scene perception, viewers make both longer fixations and longer saccades than when reading text. Fixations last, on average,
about 300 ms, while saccades span approximately 4–5◦ of visual angle (though both figures vary depending on the specific features of the scene as well as the task at hand). Furthermore, the percep- tual span in scene viewing is substantially larger than in reading, though its precise extent is not as well understood as it is in read- ing (Rayner and Castelhano, 2008; Rayner, 2009). In addition, just as in reading, viewers obtain a preview benefit during scene per- ception (Pollatsek et al., 1984, 1990; Henderson et al., 1987, 1989; Henderson, 1992; Henderson and Siefert, 1999, 2001). The magni- tude of this benefit appears to be on the order of 100 ms (Rayner, 1998, 2009).
Within our very first fixation on a scene we are, rather impres- sively, able to extract its global meaning or gist, distinguishing, for example, an indoor from an outdoor scene or a forest from a mountain landscape (Henderson, 2003; see Oliva, 2005 for a review of gist processing). This first glimpse is thought to ori- ent the viewer and provide some guidance about subsequent eye movements (Rayner, 2009). When viewers do go on to inspect the rest of the scene, they do not fixate all regions with equal probability. Rather, they tend to selectively view those elements that are particularly meaningful or relevant. For instance, view- ers inspecting a scene of two figures walking in a garden would devote a great many more fixations to the people’s faces than to a nearby patch of plain grass (see Buswell, 1935 for a classic demonstration of this effect). In addition, if a region is visually distinctive or salient – for example, if it is of higher or lower intensity than its immediate surroundings – it will tend to draw a disproportionate number of fixations (Parkhurst and Niebur, 2003).
The goals of the viewer also affect eye movements during scene perception. Yarbus (1967), for instance, found that view- ers inspected a single painting, Repin’s The Unexpected Visitor, quite differently depending on their instructions. In the paint- ing, a man (the “visitor”) enters a domestic scene. When viewers were asked to decide how long the visitor had been away, for instance, fixations seemed to cluster mainly on the faces of the individuals in the room. When asked to determine the eco- nomic circumstances of the family depicted, however, viewers’ fixations appeared more widely dispersed, landing more upon objects in the room (such as pieces of furniture or clothing) that might provide information about prosperity than in the former condition.
Finally, one striking finding regarding scene perception is that, despite the common intuition that we monitor our visual environments quite closely (Levin etal., 2000), research indi- cates that we may miss even rather dramatic changes provided that they happen during a saccade or other visual disruption. Grimes (1996; see also McConkie and Currie, 1996), for example, investigated subjects’ sensitivity to dramatic changes in natural scenes introduced during saccadic eye movements. Even with prior warning that such changes might occur, subjects’ ability to detect them was surprisingly limited. For example, when a flock of birds in one scene dwindled in number by about a third during an eye movement, subjects reported noticing some- thing odd only about 10% of the time. Importantly, however, if the changing object is pre-cued (Rensink et al., 1997) or lies near the target of the critical saccade (i.e., the saccade during
 www.frontiersin.org
March 2014 | Volume 5 | Article 210 | 3
Higgins et al.
Eye movements when viewing advertisements
 which the change occurs), change detection rates improve (Hen- derson and Hollingworth, 1999). These findings highlight the critical role of attention in determining how we perceive our visual environments.
VISUAL SEARCH
Visual search is an important part of many everyday activities. We perform such searches, for example, when looking for tea at the grocery store or trying to find our keys on the way to work each morning. The basic parameters of fixations and saccades during visual search are quite variable. Overall, average fixation times are reported to be between 180 and 275 ms, while average saccade size tends to be intermediate between that of reading and that of scene perception, but can vary widely (Rayner and Castelhano, 2008). Such variability is perhaps to be expected since, as will be seen below, eye movement patterns during search exhibit a remarkable flexibility and sensitivity to the specific demands of the moment.
When we search for an item of interest, both bottom-up (or stimulus-driven) and top-down (goal-driven) factors guide our eye movements. Bottom-up guidance is evident when eye movements are drawn to a region that stands apart from its sur- roundings, irrespective of the qualities of the search target (see Itti and Koch, 2001 for a review of models that emphasize bottom-up effects on attention and eye movements). An item that stands out in a highly salient manner from all surrounding objects (e.g., a single tilted line amid a field of vertical lines) is said to “pop out” (Wolfe, 1994).
Top-down guidance is driven by the properties of the target and their relationship with various elements of the scene. For instance, if we are searching for a bright yellow car in a crowded parking lot, similarly bright cars will preferentially attract our eye movements (Pomplun, 2006). When we perform conjunctive visual search, i.e., search for a target that is defined by a pair of properties (e.g., being both round and red), fixations cluster preferentially on items belonging to the less frequent property in the display (Shen et al., 2003). This illustrates the remarkable sensitivity of our eye movement system to the relative informativeness of different stimulus features during search.
Top-down search also operates when our high-level expecta- tions about where a target object is expected to reside affect search behavior. For instance, when searching for a computer monitor in an office scene, eye movements will cluster on the desk, rather than along the floorboards (Neider and Zelinsky, 2006). In gen- eral, recent research suggests that, while bottom-up guidance plays a role in search, top-down guidance may be dominant during real- world search for meaningful objects (e.g., Chen and Zelinsky, 2006; Pomplun, 2006; Henderson et al., 2007; Peters and Itti, 2007).
VIEWING ADVERTISEMENTS
We now turn to examine research more specifically focused on eye movements when viewing advertisements. We discuss print advertisements, warning labels, and dynamic media (TV and the Internet) in turn.
PRINT ADVERTISING
Viewers obtain the gist of print advertisements very quickly, reliably discriminating them from editorial content – and, under
some conditions even identifying the advertised product – after exposures of only 100 ms (Pieters and Wedel, 2012). In this section, we examine some of the factors that guide attention after this ini- tial glimpse, as viewers begin to actively explore advertisements by shifting their gaze from one location to the next within the dis- play. We begin by considering the composition of ads, including basic visual properties (e.g., color and size) as well as higher- level, semantic cues. Next, we review effects of ad originality (or creativity) as well as repetition. We then consider how view- ers’ goals or tasks affect viewing behavior before turning, finally, to briefly review findings concerning the integration of text and picture processing when viewing print advertisements. At several points throughout the review, the relationship between eye move- ments and higher-level phenomena such as memory will also be discussed.
Ad composition
In this section, we review critical findings on the relationship between the composition of print ads and eye movement mea- sures. We begin by examining possible effects of basic, visual characteristics and then proceed to a consideration of higher-level, semantic aspects of advertisements.
Lohse (1997) tracked subjects’ eye movements as they viewed yellow page advertisements and selected products from various categories as if for purchase. Viewers were more likely to look at large ads than small ads (see also Pieters et al., 2007), though small display ads received more fixations per unit area than large display ads (see Peschel and Orquin, 2013 for a review of sur- face size effects on visual attention). Viewers were also more likely to fixate on color than black and white ads, and looked at color ads sooner (i.e., nearer the beginning of the fixation sequence) and for a longer duration. In addition, they spent marginally more time viewing ads that contained pictures than those that did not. The location of the ad was also important, such that ads near the end of the page were often skipped. Products that were subsequently selected also received considerably more visual attention than did those that were not. Lohse and Wu (2001) conducted a similar study, this time presenting a directory in Mandarin to Chinese subjects and replicated the main findings of the original study, suggesting that these effects are not culturally specific.
Other research has examined possible effects of the size of particular elements of advertisements, such as the text or pic- ture, on patterns of visual attention. When ads were presented as part of a competitive visual array (as in a supermarket cir- cular), Pieters et al. (2007) found that ads with larger pictures, but not larger text elements, were more likely to be fixated and were viewed for longer. In contrast, Pieters and Wedel (2004) found that when subjects inspected solitary advertisements in magazines, ads with larger text elements, but not larger pictures, were more likely to be fixated and viewed for longer. (The pres- ence of a picture, however, independent of its size, did appear to attract attention under these conditions.) Comparing these findings may suggest that sufficient picture size is particularly important for capturing and holding attention in competitive visual environments, while a sufficient amount of text may be especially important when ads are presented alone. However, the
 Frontiers in Psychology | Cognition
March 2014 | Volume 5 | Article 210 | 4
Higgins et al.
Eye movements when viewing advertisements
 results were obtained in separate studies using stimuli that differed in several respects (e.g., types of product advertised, the range of text and picture sizes), so no strong claim to that effect can yet be made.
Interesting findings have also been reported regarding brand elements (e.g., logos) of advertisements in particular. While intu- ition might suggest that viewers will be repelled by them, since they serve as a salient reminder that the stimulus is an ad rather than a piece of editorial content, some eye movement data sug- gest otherwise. First, Wedel and Pieters (2000) found that, among all ad elements, the brand received most fixations per unit of surface area (but see Ryu etal., 2009). Second, each fixation on the brand element predicted a greater improvement in per- formance on a subsequent recall test than did each fixation on the text or pictorial4. Third, increasing the size of the brand element did not reduce overall viewing times on ads, as one might expect on the theory that salient brand elements reduce attention to advertisements (Pieters and Wedel, 2004). How- ever, as will be noted below, the sustained presence of a central brand element in TV commercials is associated with ad skipping (Teixeira et al., 2010).
Visual competition or clutter, an issue of considerable impor- tance in many visually complex contemporary environments, has also been examined. Pieters etal. (2010) found that high lev- els of visual feature complexity in advertisements was associated with reduced viewing of the brand element. Visual competition is also a concern when designing “feature advertisements” (such as supermarket circulars), wherein multiple ads are displayed simul- taneously and must compete for viewers’ attention. Janiszewski (1998) found that items subject to greater visual competition by surrounding objects were viewed for less time and, in a sepa- rate experiment, remembered less well than items subject to less competition.
Janiszewski (1998) also proposed that the layout of feature advertisements could be optimized (from the perspective of the advertiser), without removing any items, in order to minimize visual clutter and maximize overall viewing time. Pieters etal. (2007) extended this line of inquiry, developing a model to min- imize visual competition (based on the Attention Engagement Theory; see Duncan and Humphreys, 1989, 1992). This optimized layout led to an increase in overall viewing time of the entire ad array when compared with the existing layout. Average time spent viewing a particular feature ad, given that it was fixated, was also higher in the optimized layout, though average probability of fix- ating an ad within the array declined. Furthermore, Zhang et al. (2009) developed a Bayesian model that, they argue, suggests that the layout of feature advertisements can affect sales and that this effect is mediated by visual attention on ads. However, confounds are, of course, a concern in correlational research of this kind (though Zhang et al., 2009 adopted a statistical approach designed to circumvent several concerns of this nature).
4However, it should be noted that the particular nature of the memory test used here, in which subjects had to identify the advertised brand based on a pixilated version of the ad, seems likely to confer a relative advantage on the brand element when compared with other components. Note, for instance, that the body text was not easily resolvable from the pixilated version of the ad. Thus, further examinations should attempt replicate this result using different types of recall tests.
Simola et al. (2013) examined both the semantic and the spatial relationships between ads and editorial material. They found that when the semantic content of ads was congruent with the text – for instance, a beer ad accompanying an article about beer – these ads were (at least when presented on the right) remembered better than were incongruent ads. Interestingly, however, incongruent ads received more visual attention (also when presented on the right) than did congruent ads (but see Hervet et al., 2011, discussed below). This difference only appeared in “second-pass” viewing of the ad (that is, on a return to the ad after having left it), suggest- ing that an initial fixation on the ad was required before effects of semantic congruency could influence eye movements. Simola et al. also found that ads received more visual attention and were recognized better when placed to the right of the editorial content.
Social cues contained within advertisements have also been examined. Hutton and Nolte (2011) recently demonstrated, for instance, that when a model in an advertisement looks at the product on display, rather than looking forward toward the viewer, subjects spend longer inspecting the product, the brand logo, and the advertisement as a whole.
Classic research has also found that the presence of a human form may affect viewing behavior (Nixon, 1925; see also (Kroeber- Riel, 1979) citing a study by Witt, 1977 concerning the level of undress exhibited by a figure in an advertisement). Research in scene perception indicates, however, that when attempting to dis- cover effects of high-level, semantic aspects of a stimulus, it is important to control for possible differences in low-level visual salience (see Rayner, 1998 for a discussion of such considera- tions). Future research could build upon these early studies, then, by determining and attempting to control for differences in low- level visual salience across ads, thus allowing us to draw stronger inferences about the possible role of these higher-level, semantic factors.
Originality
When ads are particularly creative or original, how do viewers respond? Radach et al. (2003) compared viewing behavior, affec- tive responses, and memory for “implicit” and “explicit” ads. The explicit ads featured text and images that were related to one another and to the product being advertised in a fairly straight- forward manner while, in the implicit ads, these relationships were more creative and less direct. The implicit ads were viewed for longer than their explicit counterparts and, while mean fix- ation duration and saccade amplitudes did not differ across ad types, the implicit ads received significantly more fixations than did the explicit ads. Subjects also liked the implicit ads better than the explicit ones and rated them to be more interesting than their explicit counterparts5. Overall, memory for the implicit and explicit ads was similar, but a detailed analysis suggested that there might have been a slight advantage for the implicit ads in some conditions (see also Pieters et al., 1999b).
5There is a typo in Table 6 of the chapter by Radach etal. (2003) suggesting that, in Experiment 2, the explicit ads were liked better and rated as more interesting. However, the main body of the text (with which the table conflicts) is correct in claiming that in both Experiment 1 and Experiment 2 the implicit ads were liked better and rated as more interesting (R. Radach, personal communication, October 17, 2013).
   www.frontiersin.org
March 2014 | Volume 5 | Article 210 | 5
Higgins et al.
Eye movements when viewing advertisements
 However, Pieters et al. (2002) pointed out that while consumers like original ads and view them for longer periods overall, they may attend selectively to the particularly creative or artistic aspects of the advertisements, potentially at the expense of the brand or product advertised. Thus, while such creative ads may please the viewer, they may not serve the interests of the advertiser if, indeed, they direct attention away from the advertised brand. Pieters et al. conducted an experiment that partially addressed this question by comparing viewers’ fixations on the brand elements (such as the logo) of original or creative ads with more typical ads. Brand elements in the creative ads tended to receive more, not fewer, fixations than those of their typical counterparts, suggesting that creative ads may not, in fact, divert attention from the advertised brand, but rather may serve to increase it.
Repetition
Another potentially important factor in real-world ad viewing is that a viewer may well be exposed to a particular ad repeatedly (if, for instance, it runs in multiple magazines). Pieters et al. (1996) addressed this topic, finding that when subjects were exposed to an ad three times over the course of an experimental session, viewing time decreased with additional exposures (see also Pieters et al., 1999a). More elements of the ad were also skipped in the third than in the first viewing. Furthermore, an effect of subject moti- vation on viewing time (to be described below) disappeared by the third exposure. Pieters etal. (1999a) maintained, however, that the probabilities of moving from each ad element (e.g., the headline) to each other element (e.g., the pictorial) on the next fixation remained stable over repeated exposures (see also Rosber- gen et al., 1997b). It is not yet clear, however, how well each of these findings will generalize to (arguably more naturalistic) con- ditions in which exposures to the ad are spaced out over longer intervals.
Finally, Pieters et al. (2002; see also Pieters et al., 1999b) inves- tigated the eye movement patterns associated with ads of varying prior familiarity. Ads rated as being more familiar (by trained raters not participating in the eye movement study) were fix- ated less frequently than were less familiar ads. The effect seemed mainly to be driven by a decline in fixation frequency on the text with increasing ad familiarity. However, if an ad was particularly original or creative, this ameliorated negative effects of familiarity.
Goals
As was discussed above, top-down factors concerning the viewer’s goal have long been known to affect eye movement behavior during scene perception and other visual activities. More recent research has also examined effects of goal or task when subjects view advertisements and has demonstrated that these factors can have a profound effect on viewing behavior.
Perhaps unsurprisingly, when subjects control viewing time, they inspect ads for longer when given instructions that encour- age deeper processing. An important implication of this general finding (to be discussed in more detail below) is that viewing behavior during laboratory tasks that promote deep engagement with advertisements is likely to differ substantially from real-world ad viewing, which is often quite cursory (Wedel and Pieters, 2000; Pieters and Wedel, 2004, 2007, 2008).
Pieters et al. (1996) compared behavior in a “high motivation” condition, in which subjects were instructed to view ads carefully and told they would later be allowed to select one of the advertised products, to that in a “low motivation” condition, in which sub- jects were simply told to evaluate the “draft versions” of the ads (see also Pieters et al., 1999a, Study 2). In early exposures to the ad, highly motivated subjects viewed ads for substantially longer, although, as was noted above, this difference disappeared by the third exposure. Similarly, Rayner et al. (2001) compared viewers’ responses to “critical” ads, those featuring a product to be evalu- ated as if for purchase, and “non-critical” ads, featuring products from another category. Critical ads were fixated more and viewed for significantly longer than were non-critical ads. Critical ads were also missed less, in a subsequent recognition memory test, than were non-critical ads (though no such advantage for criti- cal items appeared in a free recall test). In addition, Radach et al. (2003) found that when subjects were asked to decide how much they liked an ad, they viewed it for substantially longer than when they were asked to paraphrase the message of the ad. Subtle dif- ferences in task, however, may not be sufficient to drive this effect, as Rayner et al. (2008) found no significant differences in total ad viewing time when subjects were instructed to evaluate an ad for its effectiveness or decide how much they liked it.
The total time spent viewing an ad (presented in isolation) can, of course, be measured perfectly well without eye tracking. However, eye movement data can also reveal more fine-grained differences across tasks. In particular, some eye tracking research suggests that viewers’ goals affect the proportion of time they allocate to different ad elements, such that tasks that require con- sidering the brand or product advertised in a fairly deep manner may favor the text, while tasks that encourage more shallow pro- cessing, or making judgments about the quality of the ad itself, may favor picture viewing.
First, Radach et al. (2003) found that when subjects were asked to evaluate an advertisement, they viewed the picture longer than the other components and subsequently recalled more informa- tion about the picture. When subjects were asked to paraphrase the message of an ad, however, viewing time on the picture substan- tially declined. In addition, Pieters et al. (1996) found interesting differences in text and picture viewing between high and low motivation conditions. However, the effects were only significant in the second of three presentations of the ad, so they should perhaps be viewed as tentative at this time. In the second expo- sure to an advertisement, low motivation subjects spent a greater proportion of time viewing pictures than did those in the high motivation group. Conversely, high motivation subjects spent a greater proportion of time viewing the text than low motivation subjects.
Pieters and Wedel (2007; see also Wedel et al., 2008 for fur- ther analyses of these data) also found that body text and picture viewing were affected differently by task. Subjects spent most time viewing the text in a task that required subjects to learn about the advertised brand. In contrast, viewers’ eye movements were drawn preferentially to the picture in conditions that required subjects to memorize the ad or view it freely as they would at home.
Comparing the findings of Rayner et al. (2001), in which sub- jects were instructed to consider one of the types of advertised
 Frontiers in Psychology | Cognition
March 2014 | Volume 5 | Article 210 | 6
Higgins et al.
Eye movements when viewing advertisements
 products for purchase, and Rayner et al. (2008), wherein subjects made judgments about the ads themselves (whether they liked them and how effective they were) also suggests that different goals may affect text and picture viewing patterns differently. In Rayner et al. (2001), text elements were viewed for a great deal longer than the pictures, while in the latter study, the pictures were viewed longer than the text (though the effect failed to reach statistical significance in an analysis that controlled for differences in sur- face area across elements). Furthermore, early looks tended to be drawn toward text in the 2001 study (on average, the text was reached by the third fixation) but toward the picture in the 2008 study.
Rayner et al. (2008) compared data obtained in the two exper- iments, considering only the subset of stimuli that were used in both. Based upon this analysis, they suggested that differences in subject instructions did likely contribute, to some extent, to the differences in viewing behavior across studies. This interpreta- tion should not be viewed as conclusive, however, since the data compared were collected in separate experiments. It should also be noted that, when text and picture viewing for critical and non- critical ads were compared within the Rayner et al. (2001) study, no clear interaction of the expected type (i.e., showing a text advan- tage for critical ads and a picture advantage for non-critical ads) emerged6 .
Rosbergen et al. (1997a) obtained related results using latent class analysis to segment viewers into three distinct groups. While task was not manipulated in this study, subjects’ attitudes about the advertised products were recorded and compared with the eye movement data. The picture (as well as the headline) was favored by the subject group who spent the least time viewing the ad over- all and deemed the advertised product to be particularly low in risk (i.e., they thought that choosing incorrectly would not be a costly error; Jain and Srinivasan, 1990, as cited in Bearden and Netemeyer, 1999). The only group to spend a substantial portion of the time viewing the body text was that which spent the most time viewing the ad overall, perhaps indexing deeper considera- tion of the advertised product. Additionally, subjects in this group viewed the product as more risky than did those in the other groups. Overall, then, the evidence suggests that deep engage- ment with the product advertised (and its attendant risks) may bias subjects toward the text, while more casual viewing, or eval- uation of the advertisement itself, may bias viewers toward the picture.
Integrating text and picture viewing
We now consider research on how viewers integrate text and pic- ture elements while inspecting print ads. Rayner et al. (2001) found that average fixation duration when viewing the picture in an ad (about 266 ms) was significantly longer than when viewing the text (about 226 ms). Viewers also made longer saccades on aver- age (about 4.5◦ of visual angle) when examining a picture than when reading the text (about 3.1◦). These findings were repli- cated in Rayner et al. (2008) and are also quite consistent with the
6 More specifically, the text was viewed longer and more often than the picture in this study for both critical and non-critical ads. For one of the ad types only (depicting cars), however, the text advantage was greater when those ads were critical than when they were not.
broader literature on differences in eye movements when viewing text and pictures (Rayner and Castelhano, 2008).
Rayner et al. (2001, 2008) also found that viewers generally did not quickly alternate between fixating the text and the picture but rather tended to remain on one component or the other for several fixations in a row. More specifically, given that a fixation was on the picture, the next fixation would also be on the picture about 78% of the time; if a fixation was on the text, the following fixation would remain on the text about 77% of the time (Rayner et al., 2008). Pieters et al. (1999a) reported similar findings.
However, Radach et al. (2003) reported (somewhat informally) that viewers tended to look back and forth fairly frequently between different elements of the ad, including the text and the picture. They suggested that this may have been due to the rela- tively high demands placed on subjects in their study. Indeed, as we have seen, the goal of the viewer can substantially affect view- ing behavior. However, another possibility is that the nature of the stimuli, and in particular the text used within the ads, may have differed across experiments. In particular, many of the ads used by Rayner et al. contained somewhat lengthy passages of “body text.” If the stimuli used by Radach et al. (2003) contained shorter snippets of text (in the form of headlines or brief slogans), one might imagine that this could lead to more alternating between text and pictures if readers adopted a “sampling” approach rather than a reading approach toward the text. This idea is, of course, purely speculative, but it could be tested experimentally in future research.
In summary, then, a number of factors appear to guide eye movements when viewing print advertisements. These include size, color (Lohse, 1997; Lohse and Wu, 2001), and visual clut- ter (Janiszewski, 1998), as well as higher-level social cues, such as the direction of a model’s gaze (Hutton and Nolte, 2011). Cre- ative or original ads are also fixated more than typical ads, and are liked better, and deemed more interesting (e.g., Radach et al., 2003). Repeated exposures to a given ad reduce viewing times, at least when these exposures occur in short succession (Pieters et al., 1996). However, the transition matrices between ad elements, indexing the probability of making a saccade from one element to another, remain fairly stable across multiple viewings (Pieters et al., 1999a). In addition, the beneficial effects of a particularly creative ad may ameliorate the negative influences of repetition (Pieters et al., 2002). The goal or task of the viewer also strongly influences how long we view ads (e.g., Rayner et al., 2001) and may, further- more, change the proportion of time spent viewing specific ad elements (such as the text vs. the picture). Research on eye move- ments when viewing text and pictures in ads mirrors the broader eye movement literature in that both fixations and saccades are longer when viewing pictures than when reading text (Rayner et al., 2001). Somewhat mixed findings have emerged on the question whether viewers tend to skip back and forth between text and pictures or remain on one element for a more extended period (compare Radach et al., 2003 with Rayner et al., 2001, 2008). How- ever, two possible explanations for these discrepancies have been proposed (one concerning differences in task and the other con- cerning differences in stimuli), and future research may resolve this question. Finally, in some of the studies reviewed, eye movement measures were correlated with subsequent measures of memory
  www.frontiersin.org
March 2014 | Volume 5 | Article 210 | 7
Higgins et al.
Eye movements when viewing advertisements
 for the advertised product or brand. In the upcoming sections of the article, reviewing eye movements when viewing warning labels as well as ads presented on TV or the Internet, we will continue to explore issues of eye guidance, as well as the relationship between eye movements and higher-level phenomena such as memory.
WARNING LABELS
When studying how viewers inspect advertisements, we are often interested in what elements of an ad capture and hold viewers’ attention. While most information (pictorial or textual) is redun- dant in its attempt to persuade consumers and provide them with a favorable impression of the advertised product or brand, there is one clear-cut exception. The inclusion of health warnings on alco- hol and tobacco advertisements represents a clear case in which the information gleaned from viewing the advertisement varies as a function of which regions are viewed.
Across several studies investigating the viewing of alcohol and tobacco warning labels, the general finding is that these labels are often never viewed, and when they are viewed, it is for a very small percentage of the overall ad viewing time (e.g., Fischer et al., 1989; Fox et al., 1998; Thomsen and Fulton, 2007). Because, in the United States, these warnings are usually small in relation to the overall advertisement (taking up, for example, only 3.2% of the ad in a sample used by Fischer et al., 1989), entirely text-based, and black and white, they are unlikely to capture and hold viewers’ attention. Multiple lines of research have therefore investigated the viewing time and recall of warning labels in existing advertisements and compared them with those in which the salience of the warnings has been manipulated.
In one of the first such studies, Fischer et al. (1989) recorded the eye movements of adolescents viewing real cigarette and alcohol advertisements. They found that on 43.6% of trials, subjects never directly fixated the warning, and that on 19.8% of trials subjects looked at, but did not read the warning7. On average, subjects looked at the warning labels for only 750 ms, which corresponded to 8% of the total ad viewing time, and this time was unaffected by differences in content, position, or shape (though the stimulus set was small – only five advertisements were tested). Additionally, they found that performance in a subsequent masked recall test of warning label content (where subjects were shown the original ad with the warning label and other areas masked and asked to recall the content) was positively correlated with both mean looking and reading time.
To investigate the effects of various cues on attentional capture and ease of identification, Laughery and Young (1991) manipu- lated the saliency of warning labels by including pictorials, icons, colors, borders, or combinations of these four cues, and measured the time it took subjects to locate the warning label (i.e., the time from image onset to the first fixation on the warning label), as well as the time it took them to determine that the information was a warning (measured by the time from first fixation on the label until
7Reading time was calculated as the sum of all fixations with durations of 100 ms or more, not by a qualitative assessment of the eye movement patterns in relation to the text. Individual fixations shorter than 100 ms were counted in looking time, but not reading time. If a subject made no fixations over 100 ms in duration, they were deemed not to have read. A more detailed investigation of the eye movement data was not included.
a button was pressed). Time to locate the warning was numerically shorter when any of the saliency manipulations were included, and significantly shorter when the pictorial cue, the color cue, or all four cues combined were included. Similarly, the time to deter- mine that the label was a warning was significantly shorter when a pictorial was included, either alone or combination with other cues. However, since the subject’s goal was to determine whether or not a warning was present in each advertisement, the procedure was, in fact, a visual search task. Thus, it is unclear whether the results would generalize to a more naturalistic, passive viewing of advertisements.
To answer this question, Krugman et al. (1994) compared the eye movements of subjects viewing ads with standard, federally mandated cigarette warnings to novel warnings, which were the same size and shape, but could differ in text, color, graphics, and print type. To keep ecological validity high, the subjects were asked to view the advertisements as they would in a magazine. Novel warnings attracted more attention (i.e., were fixated by more subjects) and attracted attention sooner (i.e., were fixated more rapidly) than the standard warnings. Additionally, Krugman et al. (1994) found that the time spent viewing the warning was positively correlated with masked recall performance for content of the new ads (note that they did not measure masked recall of the standard ads because of subject familiarity).
More recently, Thomsen and Fulton (2007) examined the eye movements of adolescents viewing alcohol ads with moderation messages (e.g., “drink responsibly”). They found that, on aver- age, subjects only fixated the moderation message for 350 ms, which corresponded to 7% of the total viewing time, and that in 75% of the ads with small moderation messages, that message was the least fixated area of the advertisement. However, when the moderation message was a central theme, subjects viewed the message significantly longer (on average 710 ms, compared to 170 ms when the message was not a central theme). In general, recall for even general concepts of the moderation messages was poor even among subjects who fixated them, but, as in the stud- ies by Fischer et al. (1989) and Krugman et al. (1994), there was a positive correlation between fixation time and masked recall performance.
Finally, Peterson et al. (2010) found that American adolescents viewed Canadian-style cigarette warnings, containing graphic images (e.g., of diseased tissue) and novel text warnings, for about 2.5 times as long as traditional, American warnings (including only text delivering the Surgeon General’s warning). Subjects also recalled the graphic messages more accurately in a subsequent memory test. Strasser et al. (2012) observed similar responses to graphic warnings on tobacco products among adult, American smokers.
Overall, then, the data seem quite clear that small, text-based warnings on advertisements receive little visual attention and are poorly recalled. However, by manipulating the salience (and the novelty) of such ads by, e.g., adding graphic images, attention and memory may be improved8.
8For an additional example of research using eye tracking to examine the effective- ness of public health messages, see O’Malley et al. (2012), which concerns visual attention when viewing osteoporosis prevention ads.
   Frontiers in Psychology | Cognition
March 2014 | Volume 5 | Article 210 | 8
Higgins et al.
Eye movements when viewing advertisements
 DYNAMIC MEDIA
Recent research has expanded beyond the realm of print adver- tising to examine eye movements when viewing ads presented via dynamic media, including websites and TV. While print adver- tisements can only use static cues, websites and TV also afford advertisers the opportunity to use sound and motion to guide viewers’ attention. Research that specifically examines viewers’ responses to dynamic media is essential for developing a com- plete understanding of the effects of sound and motion on attentional capture, memory, and preference. Several important findings regarding eye movements when viewing dynamic media are reviewed below.
Television advertisements
While research using eye tracking to examine the effectiveness of TV ads in capturing visual attention and affecting recall is rela- tively limited at this time, several interesting and potentially useful findings have nonetheless emerged from this literature (see also Wedel and Pieters, 2008a for a review).
First, in one early line of research, d’Ydewalle and colleagues (d’Ydewalle et al., 1988; d’Ydewalle and Tamsin, 1993) measured attention to and subsequent memory for advertisements appearing on billboards at a soccer field during a televised game. In both studies, subjects viewing the game on video spent less than 4% of the total time fixating the billboards. Perhaps unsurprisingly, given how little time was spent inspecting the ads, d’Ydewalle and Tamsin (1993) found that subjects recalled on average only 1.2 brands out of the 42 that were presented and were at chance for brand recognition. Thus, TV ads that are embedded within the primary content of a sporting event may not attract substantial visual attention or lead to strong memory representations of the advertised brand.
Other research has analyzed visual attention to more stan- dard TV ads, typically presented during commercial breaks and interspersed with the primary content. Brasel and Gips (2008b) compared viewing behavior for TV shows and commercials. They found, first, that viewers exhibited a strong tendency to fixate near the center of the screen when viewing both kinds of con- tent. They also conducted a frame-by-frame analysis of variability in fixation locations across subjects and found that variability was higher when viewing commercials than when viewing the primary program. Furthermore, variability of fixation locations was par- ticularly high when the commercials contained brand elements. Finally, familiarity with a given commercial (manipulated by pre- senting it several times over the course of an experimental session) was also linked with increased variability of fixation locations. Brasel and Gips speculated that lack of engagement with the ad, driven by repeated presentations, could, perhaps explain the ten- dency for subjects’ eyes to wander more widely in later exposures to the ad.
Two studies by Teixeira and colleagues also examined variabil- ity in fixation locations across subjects, this time in connection with ad avoidance. Critically, if viewers do not wish to view TV ads (and video-based ads more broadly), they are often able to avoid them entirely, by muting them, temporarily turning off the device, or even blocking or skipping the commercials. The topic of ad avoidance is, consequently, an important one in the domain
of TV advertising. Teixeira et al. (2010) found that higher vari- ability in fixation locations across subjects predicted greater ad skipping. They suggested that high variability may indicate a fail- ure, on the part of the advertiser, to sufficiently shape viewers’ engagement with the advertisement and guide attention to key aspects of the scene from one moment to the next. In addition, they found that the sustained presence of a central brand element on the screen predicted ad skipping9. However, brand “pulsing,” a strategy wherein the brand is shown for the same duration overall, but for shorter intervals each time, was found to ameliorate this effect. To explain this finding, Teixeira et al. speculated that puls- ing, unlike the sustained, central presence of the brand, may leave the narrative of the commercial relatively intact, thus supporting effective guidance of viewers’ visual attention and preventing ad skipping.
Building up on these findings, Teixeira et al. (2012) examined the relationships among emotion, as measured by viewers’ facial expressions, variability in fixation locations, and commercial skip- ping10. They found that measures of apparent joy and surprise were linked with reduced variability in fixation locations across subjects. These emotions, in addition, were found to reduce ad skipping, both via a direct route (when controlling for fixation concentration effects) and via an indirect route, by concentrating fixation locations across viewers.
Quite recently, Brasel and Gips (2013) investigated the effect of subtitles on visual attention to and memory for ads. They found that same-language subtitles attracted visual attention, as subjects spent a greater percentage of frames looking at the sub- title region when subtitles were present than when they were absent. In addition, same-language subtitles also improved recall for the brand and for verbal information that was presented redundantly (i.e., both vocally and within the subtitles). Subti- tles did not improve all aspects of memory, however: indeed, they decreased recall of information presented only visually, leading to reduced memory for brands that were not verbally named (and were therefore not included in the subtitles). The eye-tracking data and the memory data were collected from dif- ferent subject groups, however, so it is not possible to correlate a given subject’s fixations on subtitles with subsequent recall performance.
Finally, Janiszewski and Warlop (1993) found evidence that attention to ads may be improved via a conditioning procedure. In the study, TV commercials were always presented in a specific order such that a conditioned stimulus (clip of the soda being advertised) always preceded an unconditioned stimulus (a clip of an enjoy- able activity). This conditioning procedure led to increased (and more rapid) attention to the conditioned brand during subsequent exposure, suggesting that associative learning about a given brand can enhance attention to that brand.
In summary, research on TV ad viewing suggests, first, that embedded advertisements, in the form of billboards appearing
9See Brasel and Gips (2008a), however, for results suggesting that a central brand element may be beneficial for memory for brands viewed in fast-forwarded commercials.
10 The ads tested in this study were, in fact, Internet ads. However, they are included in this section because they represent video-based ads and are similar in form to television advertisements.
  www.frontiersin.org
March 2014 | Volume 5 | Article 210 | 9
Higgins et al.
Eye movements when viewing advertisements
 during sporting events, may not be effective in capturing visual attention or influencing subsequent memory (d’Ydewalle etal., 1988; d’Ydewalle and Tamsin, 1993). When considering more tra- ditional TV commercials, in which ads are interleaved with the primary content during commercial breaks, ad skipping is a central concern. Interestingly, when fixation locations are quite variable across subjects, more frequent ad skipping occurs (Teixeira et al., 2010), perhaps suggesting a lack of engagement with the narra- tive of the ad. Measures of joy and surprise are linked with more homogeneous viewing behavior across subjects and reduced brand skipping (Teixeira et al., 2012). In contrast, repeated exposures to an ad lead to increased variability in fixation locations across subjects (Brasel and Gips, 2008b). Including subtitles with TV ads is also associated with improved memory for certain kinds of information presented in the ads (Brasel and Gips, 2013). Finally, conditioning procedures can increase attention to brand elements in TV commercials (Janiszewski and Warlop, 1993).
Internet advertisements
As in TV advertising, ad avoidance is a topic of considerable interest in the domain of Internet advertising. Unlike most TV ads, banner and “skyscraper” ads (i.e., vertical banners) that appear on websites must often compete directly with sur- rounding editorial content for visual attention (see Drèze and Zufryden, 2000). As will be discussed below, viewers are thought to routinely avoid such ads when viewing websites, a phe- nomenon known as “banner blindness” (Benway, 1998, 1999; see also Owens etal., 2011 for similar findings regarding text ads). Several lines of research have manipulated the location, animation, onset, and relevance of Internet ads, simultane- ously recording viewers’ eye movements to determine when the ads capture visual attention and when “banner blindness” takes place.
In one early study of eye movements during Internet search, Drèze and Hussherr (2003) found that subjects searching web sites fixated just under half of the banner ads presented. Since the prob- ability of fixation was less than one would predict on the basis of ad size and location alone, Drèze and Hussherr concluded that viewers were able to identify banner ads in the visual periphery and, subsequently, intentionally avoid fixating them. Additionally, only 46.9% of subjects remembered seeing any banner ads during the experiment, and a recognition memory test revealed that sub- jects could not accurately discriminate ads that had been present on the website from foils that had never appeared.
Since certain Internet ad locations are consistent and thus predictable, however, users may not need to identify ads in the periphery in order to avoid them, but rather may be able to learn where they tend to appear and simply avoid fixating those loca- tions. Lapa (2007) provided evidence that viewers do, in fact, learn the locations of banner ads over time and sometimes use this infor- mation to avoid fixating them. However, Burke et al. (2005) found that even when ad locations were not predictable, subjects only fixated the banners in 11.7% trials11. This suggests, as Drèze and
11This 11.7% estimate is an upper bound since in 70% of these trials, the ad was fixated following the first eye movement and in 54% of this subset, the ad actually appeared in the location of the first fixation after the eyes had already moved.
Hussherr (2003) proposed, that subjects are, indeed, also able to recognize banner ads in peripheral vision and avoid fixating them. While it appears that Internet ads may receive little attention in general, certain factors may be manipulated with the aim of attracting or holding viewers’ attention: these include location, animation, onset, and relevance. Kuisma et al. (2010) manipu- lated both ad location (horizontal, banner ads on the top of the display vs. vertical, “skyscraper” ads on the right side of the dis- play) and animation (both static, both animated, or one of each). There was a main effect of ad location, such that more fixations landed on the skyscraper ad on the right side of the display than on the banner ads along the top. Animation was also found to increase fixations on skyscraper ads and decrease fixations on banner ads. Furthermore, including multiple animated advertise- ments resulted in fewer fixations on the ads than including only a single animated ad. Somewhat surprisingly, recognition memory results did not mirror the eye movement data. Rather, animation increased recognition memory for banner ads, but had no effect on the recognition memory for skyscraper ads. Findings on the relationships among memory, animation, and visual attention to Internet ads become even less clear when we consider the results of Burke et al. (2005), who found that memory (though very poor
overall) was better for static banner ads than animated ones.
In a study similar to that of Kuisma etal. (2010), Simola et al. (2011) also manipulated both location (banner, skyscraper) and animation (both static, both animated, one of each), but additionally included different ad onset delays from 0 to 12 s. Consistent with the findings of Kuisma et al. (2010), they reported that animation increased attention to the skyscraper ads to the right of the text (especially when one ad was animated and the other remained static), and that the skyscraper ad was fix- ated more often and for longer than was the banner ad above the text. They also found that abrupt onset captured attention, as ads that appeared abruptly were fixated more often, though this effect was modulated by ad location, with skyscraper ads in close proximity to the text capturing attention more immedi- ately, and banners located in the periphery capturing attention less quickly (see also Day etal., 2006 for evidence that even without capturing overt attention, ads flashing in the periphery can increase arousal and result in more efficient primary task
performance).
Extending these findings, Simola et al. (2011) varied the task
(reading for comprehension vs. browsing according to subjects’ own interests) and found that subjects were more likely to view the ads and looked at them for longer during browsing than during reading for comprehension, thus providing evidence that a user’s goals can exert “a strong top-down influence on attentional allo- cation” (p. 189) during online processing of information and ads. Additionally, during browsing, they found a correlation between ad onset and first fixation time for ads at both locations. However, in the reading task, there was only a correlation for the ad to the right of the text (which was in close proximity to the ends of the lines of text) and not to the peripheral banner ad, suggesting that users can selectively allocate attention to the task-relevant portions of the screen. Critically, in both tasks, self-reports of attention were correlated with actual eye movement data, such that participants who reported attention to ads also looked at the ads more often and
  Frontiers in Psychology | Cognition
March 2014 | Volume 5 | Article 210 | 10
Higgins et al.
Eye movements when viewing advertisements
 for longer periods of time. This led Simola et al. to suggest that attentional capture by ads is related to overt rather than covert attention, a conclusion that seemingly runs counter to the studies suggesting that ads are recognized peripherally via covert atten- tion (e.g., Drèze and Hussherr, 2003; Burke et al., 2005; Day et al., 2006).
Hamborg et al. (2012) examined the time course of attention to banner ads when subjects were given a primary task requiring that they extract information from an accompanying article. Sig- nificantly more subjects looked at a continuously animated than a static banner ad, in seeming contrast to some of the findings described above. Interestingly, these banner ads also attracted most fixations near the beginning or end of the primary task, suggesting that bottom-up salience may be more likely to interfere with top-down processing during these early and late periods of information search (see also Wang and Day, 2007). More details about the animated ads than the static ads were also recalled in a subsequent memory test.
Finally, some research has manipulated relevance of the ad to the subject’s task as well as the relationship between the ad and the editorial content. Lapa (2007) manipulated ad relevance by including ads that were either related or unrelated to the subject’s search task. He found that relevance did not influence ad viewing time, suggesting that users may assume banner ads to be irrelevant to their goals and the primary content. Relatedly, Hervet etal. (2011) found that congruency between text ads and surrounding web page content did not influence fixation probability or total viewing time on the ads, though congruent ads were remembered better than incongruent ones12.
In summary, viewers may tend to avoid fixating advertisements on websites, both by identifying them peripherally (Drèze and Hussherr, 2003; Burke et al., 2005) and by learning the locations in which they are likely to appear (Lapa, 2007). Some evidence also suggests that skyscraper ads, presented to the right of the primary content, are more likely to be fixated across a variety of tasks than are banner ads, presented on top of the primary text (Kuisma et al., 2010; Simola et al., 2011). Furthermore, the likelihood of fixating such skyscraper ads may be increased if they are animated (Kuisma et al., 2010; Simola et al., 2011) or appear suddenly (Simola et al., 2011). Effects of animating banner ads, however, are somewhat less clear (compare Hamborg et al., 2012 with Simola et al., 2011 and Kuisma et al., 2010). A mixed pattern of findings has also been reported concerning the relations among memory, anima- tion, and eye movements when viewing Internet ads. In general, however, the data indicate that memory for Internet ads is rather poor (Drèze and Hussherr, 2003; Burke et al., 2005). The relevance of Internet ads (Lapa, 2007) or their relationship with surrounding content (Hervet et al., 2011) do not appear to affect ad viewing, suggesting that users may assume that such ads will be irrelevant to their primary goals. Finally, some evidence suggests that when viewers are engaged in a primary task, they are more likely to view banner ads near the beginning or the end of this task, when they
12 As discussed above, however, Simola et al. (2013) found that newspa- per ads that were semantically incongruent with primary content received more attention than those that were semantically congruent in second-pass viewing.
may be more susceptible to bottom-up influences on oculomotor behavior (Wang and Day, 2007; Hamborg et al., 2012).
CONCLUSION
In this article, we reviewed critical findings on eye movements when viewing advertisements, including in print, on TV, and on websites. A number of factors were found to guide eye movements when viewing prints ads, ranging from basic visual properties of advertisements (e.g., size and color), to social cues (e.g., the direc- tion of a model’s gaze), to the goals of the viewer. The literature regarding warning labels on tobacco and alcohol ads revealed that the plain, black-and-white text warnings currently used in the United States draw little visual attention and are often forgotten. However, manipulating the visual salience (and novelty) of these warnings – by, for example, including graphic images – improved both visual attention to and memory for such warnings. Research on ads in dynamic media has also produced several noteworthy findings, revealing, for example, that subjects appear to avoid viewing banner ads in some cases, using both peripheral process- ing and canonical ad locations as cues. Across multiple domains, eye movement measures were often (though not always) found to predict subsequent memory for the advertised product, warning, or brand.
Although a substantial body of research has now been produced on eye movements while viewing advertisements, several avenues remain largely unexplored. First, relatively little is known about the guidance of eye movements when viewing dynamic, video- based ads (but see Itti, 2005 for a model of bottom-up effects on dynamic scene viewing). We expect that this will be an impor- tant area for future research to examine in greater depth. The relationships among eye movements, memory, and preference are also ripe for further investigation. The potentially complex causal relationships among these variables are of considerable theoret- ical interest13. Such research could also be quite useful from an applied perspective. For example, it would be helpful to determine how or whether specific eye movement measures might predict memory for a brand or product over an extended period of time (e.g., multiple days or weeks). As was noted above, tracking eye movements seems less likely to bias subsequent measures (such as product recall) than does soliciting verbal responses from sub- jects. Therefore, if eye movements are indeed a robust predictor of brand memory over some duration, this may be very helpful to applied researchers.
Several methodological approaches may also prove useful. First, though the point may seem rather a minor one, we strongly believe that settling on a common, codified set of terms to refer to the same, underlying eye movement measures (e.g., the total duration spent viewing a given element within a trial) will enable findings to be shared and compared much more efficiently across labora- tories in the upcoming years. At present, the terminology used for such measures appears to be somewhat variable in the advertising literature.
Second, the gaze-contingent display change paradigm (McConkie and Rayner, 1975; Rayner, 1975) may prove useful
13Note that related issues have been addressed in some detail in the visual decision- making literature (see, e.g., Glaholt and Reingold, 2011).
   www.frontiersin.org
March 2014 | Volume 5 | Article 210 | 11
Higgins et al.
Eye movements when viewing advertisements
 in future research. As was noted above, this approach, which consists in dynamically updating the display based on the eye movements of the viewer, has been quite useful in research on reading, visual decision-making, etc., allowing us to investigate topics such as parafoveal preview and the perceptual span in detail. The technique is useful because it affords experimenters precise control over subjects’ visual input, based on current eye position, while allowing subjects to inspect the scene freely. Gaze-contingent designs could, we believe, take on an impor- tant role in upcoming research on eye movements when viewing advertisements.
Third, and most broadly, further controlled, experimental designs could be used in future research to complement some existing correlational findings. A number of important studies in the field have used an approach that is at least partly correlational, presenting viewers with an assortment of real advertisements that vary naturally along dimensions of interest (e.g., the size of each ad element) and then measuring associated eye movement variables. This approach has advantages: notably, ecological validity is high. However, confounds are also a risk in such studies14. Therefore, it would be useful to determine if experimental studies, requiring systematic manipulation of independent variables of interest, will produce consistent results.
Finally, compared with the literature concerning eye move- ments in reading, scene perception, and visual search, it seems that research on eye movements while looking at advertisements is in its infancy. Consequently, a large number of interesting and useful avenues of research (of which only a few are mentioned above) remain available for future researchers to explore.
ACKNOWLEDGMENTS
We would like to thank Agnieszka Konopka and Stevan Adam Brasel for helpful comments on a previous draft of this article. This submission was partially supported by the University of California at San Diego Open Access Fund.
REFERENCES
Ashby, J., Rayner, K., and Clifton, C. (2005). Eye movements of highly skilled and average readers: differential effects of frequency and predictability. Q. J. Exp. Psychol. 58, 1065–1086. doi: 10.1080/02724980443000476
Bearden, W. O., and Netemeyer, R. G. (eds). (1999). Handbook of Marketing Scales: Multi-item Measures for Marketing and Consumer Behavior Research. Newbury Park, CA: Sage Publishing.
Benway, J. P. (1998). “Banner blindness: the irony of attention grabbing on the World Wide Web,” in Proceedings of the Human Factors and Ergonomics Society 42nd Annual Meeting, Vol. 2, 463–467.
Benway, J. P. (1999). Banner Blindness: What Searching Users Notice and Do Not Notice on the World Wide Web. Unpublished doctoral dissertation. Rice University, Houston, TX.
Brasel, S. A., and Gips, J. (2008a). Breaking through fast-forwarding: brand information and visual attention. J. Mark. 72, 31–48. doi: 10.1509/jmkg.72.6.31
Brasel, S. A., and Gips, J. (2008b). Points of view: where do we look when we watch TV? Perception 37, 1890–1894. doi: 10.1068/p6253
Brasel, S. A., and Gips, J. (2013). Enhancing television advertising: same-language subtitles can improve brand recall, verbal memory, and behavioral intent. J. Acad. Mark. Sci. doi: 10.1007/s11747-0358-1
14For example, suppose that brands that sold particularly intriguing products also tended, on average, to use large pictures in their advertisements. If we found longer gaze durations associated with larger pictures, then, it may be attributable to underlying characteristics of the product rather than the size of the picture.
Burke, M., Hornof, A., Nilsen, E., and Gorman, N. (2005). High-cost banner blind- ness: ads increase perceived workload, hinder visual search, and are forgotten. ACM Trans. Comput. Hum. Interact. 12, 423–445. doi: 10.1145/1121112.1121116
Buswell, G. T. (1935). How People Look at Pictures. Chicago: University of Chicago Press.
Campbell, F. W., and Wurtz, R. H. (1978). Saccadic omission: why we do not see a grey-out during a saccadic eye movement. Vision Res. 18, 1297–1303. doi: 10.1016/0042-6989(78)90219-5
Chen, X., and Zelinsky, G. J. (2006). Real-world visual search is dominated by top down guidance. Vision Res. 46, 4118–4133. doi: 10.1016/j.visres.2006.08.008
Day, R., Shyi, G. C., and Wang, J. (2006). The effect of flash banners on multiattribute decision making: distractor or source of arousal? Psychol. Mark. 23, 369–382. doi: 10.1002/mar.20117
Desimone, R., and Duncan, J. (1995). Neural mechanisms of selective visual atten- tion. Annu. Rev. Neurosci. 18, 193–222. doi: 10.1146/annurev.ne.18.030195. 001205
Deubel, H., and Schneider, W. X. (1996). Saccade target selection and object recog- nition: evidence for a common attentional mechanism. Vis. Res. 36, 1827–1837. doi: 10.1016/0042-6989(95)00294-4
Drèze, X., and Hussherr, F.-X. (2003). Internet advertising: is anybody watching? J. Interact. Mark. 17, 8–23. doi: 10.1002/dir.10063
Drèze, X., and Zufryden, F. (2000). Internet advertising: the medium is the difference. Consumption Mark. Cult. 4, 23–37. doi: 10.1080/10253866.2000. 9670347
Duchowski, A. T. (2002). A breadth-first survey of eye-tracking applications. Behav. Res. Methods Instrum. Comput. 34, 455–470. doi: 10.3758/BF03195475
Duncan, J., and Humphreys, G. (1992). Beyond the search surface: visual search and attentional engagement. J. Exp. Psychol. Hum. Percept. Perform. 18, 578–588. doi: 10.1037/0096-1523.18.2.578
Duncan, J., and Humphreys, G. W. (1989). Visual search and stimulus similarity. Psychol. Rev. 96, 433–458. doi: 10.1037/0033-295X.96.3.433
d’Ydewalle, G., Abeele, P. V., Rensbergen, J. V., and Coucke, P. (1988). “Incidental processingofadvertisementswhilewatchingsoccergamesbroadcasts,”inPractical Aspects of Memory: Current Research and Issues, Vol. 1, eds M. M. Gruneberg, P. E. Moris, and R. N. Sykes (Chichester, NY: Wiley), 478–483.
d’Ydewalle, G., and Tamsin, F. (1993). “On the visual processing and memory of incidental information: advertising panels in soccer games,” in Visual Search 2: Proceedings of the 2nd International Conference on Visual Search, eds D. Brogan, A. Gale, and K. Carr (London: Taylor & Francis), 401–408.
Findlay, J. M., and Gilchrist, I. D. (2003). Active Vision: The Psy- chology of Looking and Seeing. Oxford: Oxford University Press. doi: 10.1093/acprof:oso/9780198524793.001.0001
Findlay, J. M., and Gilchrist, I. D. (2005). “Eye guidance in visual search,” in Cognitive Processes in Eye Guidance, ed. G. Underwood (Oxford: Oxford University Press), 259–281. doi: 10.1093/acprof:oso/9780198566816.003.0011
Fischer, P. M., Richards, J. W., Berman, E. J., and Krugman, D. M. (1989). Recall and eye tracking study of adolescents viewing tobacco advertisements. JAMA 261, 84–89. doi: 10.1001/jama.1989.03420010094040
Fox, R. J., Krugman, D. M., Fletcher, J. E., and Fischer, P. M. (1998). Adoles- cents’ attention to beer and cigarette print ads and associated product warnings. J. Advert. 27, 57–68. doi: 10.1080/00913367.1998.10673563
Gaymard, B., Ploner, C. J., Rivaud, S., Vermersch, A. I., and Pierrot-Deseilligny, C. (1998). Cortical control of saccades. Exp. Brain Res. 123, 159–163. doi: 10.1007/s002210050557
Glaholt, M. G., and Reingold, E. M. (2011). Eye movement monitoring as a process tracing methodology in decision making research. J. Neurosci. Psychol. Econ. 4, 125–146. doi: 10.1037/a0020692
Grimes, J. (1996). “On the failure to detect changes in scenes across saccades,” in Vancouver Studies in Cognitive Science, Vol. 5, Perception, ed. K. Akins (Oxford: Oxford University Press), 89–109.
Hamborg, K. C., Bruns, M., Ollermann, F., and Kaspar, K. (2012). The effect of banner animation on fixation behavior and recall performance in search tasks. Comput. Hum. Behav. 28, 576–582. doi: 10.1016/j.chb.2011.11.003
Henderson, J. M. (1992). Identifying objects across saccades: effects of extrafoveal preview and flanker object context. J. Exp. Psychol. Learn. Mem. Cogn. 18, 521– 530. doi: 10.1037/0278-7393.18.3.521
Henderson, J. M. (2003). Human gaze control during real-world scene perception. Trends Cogn. Sci. 7, 498–504. doi: 10.1016/j.tics.2003.09.006
  Frontiers in Psychology | Cognition
March 2014 | Volume 5 | Article 210 | 12
Higgins et al.
Eye movements when viewing advertisements
 Henderson, J. M., Brockmole, J. R., Castelhano, M. S., and Mack, M. (2007). “Visual saliency does not account for eye movements during visual search in real-world scenes,” in Eye Movements: A Window on Mind and Brain, eds R. P. G. van Gompel, M. H. Fischer, W. S. Murray, and R. L. Hill (Oxford: Elsevier), 539–562.
Henderson, J. M., and Hollingworth, A. (1999). The role of fixation position in detecting scene changes across saccades. Psychol. Sci. 10, 438–443. doi: 10.1111/1467-9280.00183
Henderson, J. M., Pollatsek, A., and Rayner, K. (1987). Effects of foveal priming and extrafoveal preview on object identification. J. Exp. Psy- chol. Hum. Percept. Perform. 13, 449–463. doi: 10.1037/0096-1523. 13.3.449
Henderson, J. M., Pollatsek, A., and Rayner, K. (1989). Covert visual attention and extrafoveal information use during object identification. Percept. Psychophys. 45, 196–208. doi: 10.3758/BF03210697
Henderson, J. M., and Siefert, A. B. (1999). The influence of enan- tiomorphic transformation on transsaccadic object integration. J. Exp. Psychol. Hum. Percept. Perform. 25, 243–255. doi: 10.1037/0096-1523. 25.1.243
Henderson, J. M., and Siefert, A. B. (2001). Types and tokens in transsaccadic object identification: effects of spatial position and left-right orientation. Psychon. Bull. Rev. 8, 753–760. doi: 10.3758/BF03196214
Hervet, G., Guérard, K., Tremblay, S., and Chtourou, M. S. (2011). Is banner blindness genuine? Eye tracking Internet text advertising. Appl. Cogn. Psychol. 25, 708–716. doi: 10.1002/acp.1742
Hutton, S. B., and Nolte, S. (2011). The effect of gaze cues on attention to print advertisements. Appl. Cogn. Psychol. 25, 887–892. doi: 10.1002/acp.1763
Inhoff, A. W., and Briihl, D. (1991). Semantic processing of unattended text during selective reading: how the eyes see it. Percept. Psychophys. 49, 289–294. doi: 10.3758/BF03214312
Inhoff, A. W., and Topolski, R. (1992). Lack of semantic activation from unat- tended text during passage reading. Bull. Psychon. Soc. 30, 365–366. doi: 10.3758/BF03334090
Itti, L. (2005). Quantifying the contribution of low-level saliency to human eye movements in dynamic scenes. Vis. Cogn. 12, 1093–1123. doi: 10.1080/13506280444000661
Itti, L., and Koch, C. (2001). Computational modeling of visual attention. Nat. Rev. Neurosci. 2, 194–203. doi: 10.1038/35058500
Janiszewski, C. (1998). The influence of display characteristics on visual exploratory search behavior. J. Consum. Res. 25, 290–301. doi: 10.1086/209540
Janiszewski, C., and Warlop, L. (1993). The influence of classical conditioning procedures on subsequent attention to the conditioned brand. J. Consum. Res. 20, 171–189. doi: 10.1086/209342
Kowler, E., Anderson, E., Dosher, B., and Blaser, E. (1995). The role of attention in the programming of saccades. Vision Res. 35, 1897–1916. doi: 10.1016/0042- 6989(94)00279-U
Kroeber-Riel, W. (1979). Activation research: psychobiological approaches in consumer research. J. Cons. Res. 5, 240–250. doi: 10.1086/208736
Krugman, D. M., Fox, R. J., Fletcher, J. E., Fischer, P. M., and Rojas, T. H. (1994). Do adolescents attend to warnings in cigarette advertising? An eye-tracking approach. J. Advert. Res. 34, 39–52.
Kuisma, J., Simola, J., Uusitalo, L., and Öörni, A. (2010). The effects of animation and format on the perception and memory of online advertising. J. Interact. Mark. 24, 269–282. doi: 10.1016/j.intmar.2010.07.002
Lapa, C. (2007). Using Eye Tracking to Understand Banner Blindness and Improve Website Design. Rochester, NY: RIT Digital Media Library, Rochester Institute of Technology.
Laughery, K. R., and Young, S. L. (1991). “An eye scan analysis of accessing product warning information,” in Human Factors and Ergonomics Society Annual Meeting Proceedings, Vol. 35, 585–589.
Levin, D. T., Momen, N., Drivdahl S. B. IV, and Simons, D. J. (2000). Change blindness blindness: the metacognitive error of overestimating change-detection ability. Vis. Cogn. 7, 397–412. doi: 10.1080/135062800394865
Liversedge, S. P., and Findlay, J. M. (2000). Saccadic eye movements and cognition. Trends Cogn. Sci. 4, 6–14. doi: 10.1016/S1364-6613(99)01418-7
Lohse, G. L. (1997). Consumer eye movement patterns on yellow pages advertising. J. Advert. 26, 61–73. doi: 10.1080/00913367.1997.10673518
Lohse, G. L., and Wu, D. (2001). Eye movement patterns on Chinese yellow pages advertising. Electron. Mark. 11, 87–96. doi: 10.1080/101967801300197007
Matin, E. (1974). Saccadic suppression: a review and an analysis. Psychol. Bull. 81, 899–917. doi: 10.1037/h0037368
McConkie, G. W., and Currie, C. B. (1996). Visual stability across saccades while viewing complex pictures. J. Exp. Psychol. Hum. Percept. Perform. 22, 563–581. doi: 10.1037/0096-1523.22.3.563
McConkie, G. W., and Rayner, K. (1975). The span of the effective stimulus dur- ing affixation in reading. Percept. Psychophys. 17, 578–586. doi: 10.3758/BF03 203972
McConkie, G. W., and Rayner, K. (1976). Asymmetry of the perceptual span in reading. Bull. Psychon. Soc. 8, 365–368. doi: 10.3758/BF03335168
Neider, M. B., and Zelinsky, G. J. (2006). Scene context guides eye movements during visual search. Vision Res. 46, 614–621. doi: 10.1016/j.visres.2005.08.025
Nixon, H. K. (1925). Two studies of attention to advertisements. J. Appl. Psychol. 9, 176–187. doi: 10.1037/h0073291
Noudoost, B., Chang, M. H., Steinmetz, N. A., and Moore, T. (2010). Top- down control of visual attention. Curr. Opin. Neurobiol. 20, 183–190. doi: 10.1016/j.conb.2010.02.003
Oliva, A. (2005). “Gist of the scene,” in Neurobiology of Attention, eds L. Itti, G. Rees, and J. K. Tsotsos (San Diego, CA: Elsevier), 251–256. doi: 10.1016/B978- 012375731-9/50045-8
O’Malley, D., Latimer, A., and Berenbaum, E. (2012). Using eye tracking technology to determine the most effective viewing format and content for osteoporo- sis prevention print advertisements. J. Appl. Biobehav. Res. 16, 167–186. doi: 10.1111/j.1751-9861.2011.00072.x
Orquin, J. L., and Mueller Loose, S. (2013). Attention and choice: a review on eye movements in decision making. Acta Psychol. 144, 190–206. doi: 10.1016/j.actpsy.2013.06.003
Owens, J. W., Chaparro, B. S., and Palmer, E. M. (2011). Text advertising blindness: the new banner blindness? J. Usability Stud. 6, 172–197.
Parkhurst, D. J., and Niebur, E. (2003). Scene content selected by active vision. Spat. Vis. 16, 125–154. doi: 10.1163/15685680360511645
Peschel, A. O., and Orquin, J. L. (2013). A review of the findings and the- ories on surface size effects on visual attention. Front. Psychol. 4:902. doi: 10.3389/fpsyg.2013.00902
Peters, R. J., and Itti, L. (2007). “Beyond bottom-up: incorporating task-dependent influences into a computational model of spatial attention,” in Proceedings of IEEE Computer Vision and Pattern Recognition, Minneapolis, MN.
Peterson, E. B., Thomsen, S., Lindsay, G., and John, K. (2010). Adolescents’ attention to traditional and graphic tobacco warning labels: an eye-tracking approach. J. Drug Educ. 40, 227–244. doi: 10.2190/DE.40.3.b
Pierrot-Deseilligny, C., Milea, D., and Müri, R. M. (2004). Eye movement control by the cerebral cortex. Curr. Opin. Neurol. 17, 17–25. doi: 10.1097/00019052- 200402000-00005
Pieters, R., Rosbergen, E., and Hartog, M. (1996). Visual attention to advertising: the impact of motivation and repetition. Adv. Consum. Res. 23, 242–248.
Pieters, R., Rosbergen, E., and Wedel, M. (1999a). Visual attention to repeated print advertising: a test of scanpath theory. J. Mark. Res. 36, 424–438. doi: 10.2307/3151998
Pieters, R., Warlop, L., and Wedel, M. (1999b). The Influence of Advertisement Familiarity and Originality on Visual Attention and Brand Memory. Research Report. Leuven: Katholieke Universiteit Leuven.
Pieters, R., Warlop, L., and Wedel, M. (2002). Breaking through the clut- ter: benefits of advertisement originality and familiarity for brand atten- tion and memory. Manag. Sci. 48, 765–781. doi: 10.1287/mnsc.48.6. 765.192
Pieters, R., and Wedel, M. (2004). Attention capture and transfer in adver- tising: brand, pictorial, and text-size effects. J. Mark. 68, 36–50. doi: 10.1509/jmkg.68.2.36.27794
Pieters, R., and Wedel, M. (2007). Goal control of attention to advertising: the Yarbus implication. J. Consum. Res. 34, 224–233. doi: 10.1086/519150
Pieters, R., and Wedel, M. (2008). “Informativeness of eye movements for visual marketing,” in Visual Marketing. From Attention to Action, eds M. Wedel and R. Pieters (New York: Lawrence Erlbaum), 43–71.
Pieters, R., and Wedel, M. (2012). Ad gist: ad communication in a single eye fixation. Mark. Sci. 31, 59–73. doi: 10.1287/mksc.1110.0673
Pieters, R., Wedel, M., and Batra, R. (2010). The stopping power of adver- tising: measures and effects of visual complexity. J. Mark. 74, 48–60. doi: 10.1509/jmkg.74.5.48
 www.frontiersin.org
March 2014 | Volume 5 | Article 210 | 13
Higgins et al.
Eye movements when viewing advertisements
 Pieters, R., Wedel, M., and Zhang, J. (2007). Optimal feature advertis- ing design under competitive clutter. Manag. Sci. 53, 1815–1828. doi: 10.1287/mnsc.1070.0732
Pollatsek, A., Raney, G. E., Lagasse, L., and Rayner, K. (1993). The use of information below fixation in reading and in visual search. Can. J. Exp. Psychol. 47, 179–200. doi: 10.1037/h0078824
Pollatsek, A., Rayner, K., and Collins, W. E. (1984). Integrating pictorial information across eye movements. J. Exp. Psychol. Gen. 113, 426. doi: 10.1037/0096-3445.113.3.426
Pollatsek, A., Rayner, K., and Henderson, J. M. (1990). Role of spatial location in integration of pictorial information across saccades. J. Exp. Psychol. Hum. Percept. Perform. 16, 199. doi: 10.1037/0096-1523.16.1.199
Pomplun, M. (2006). Saccadic selectivity in complex visual search displays. Vision Res. 46, 1886–1900. doi: 10.1016/j.visres.2005.12.003
Posner, M. I. (1980). Orienting of attention. Q. J. Exp. Psychol. 32, 3–25. doi: 10.1080/00335558008248231
Radach, R., Lemmer, S., Vorstius, C., Heller, D., and Radach, K. (2003). “Eye movements in the processing of print advertisements,” in The Mind’s Eye: Cognitive and Applied Aspects of Eye Movement Research, eds J. Hyönä, R. Radach, and D. Heller (Amsterdam: North-Holland), 609–632.
Rayner, K. (1975). The perceptual span and peripheral cues in reading. Cogn. Psychol. 7, 65–81. doi: 10.1016/0010-0285(75)90005-5
Rayner, K. (1998). Eye movements in reading and information processing: 20 years of research. Psychol. Bull. 124, 372–422. doi: 10.1037/0033-2909. 124.3.372
Rayner, K. (2009). The thirty fifth Sir Frederick Bartlett Lecture: eye movements and attention in reading, scene perception, and visual search. Q. J. Exp. Psychol. 62, 1457–1506. doi: 10.1080/17470210902816461
Rayner, K., and Bertera, J. H. (1979). Reading without a fovea. Science 206, 468–469. doi: 10.1126/science.504987
Rayner, K., and Castelhano, M. S. (2008). “Eye movements during reading, scene perception, visual search, and while looking at print advertisements,” in Visual Marketing. From Attention to Action, eds M. Wedel and R. Pieters (New York: Lawrence Erlbaum), 9–42.
Rayner, K., McConkie, G. W., and Ehrlich, S. (1978). Eye movements and integrating information across fixations. J. Exp. Psychol. Hum. Percept. Perform. 4, 529–544. doi: 10.1037/0096-1523.4.4.529
Rayner, K., Miller, B., and Rotello, C. M. (2008). Eye movements when looking at print advertisements: the goal of the viewer matters. Appl. Cogn. Psychol. 22, 697–707. doi: 10.1002/acp.1389
Rayner, K., Reichle, E. D., Stroud, M. J., Williams, C. C., and Pollatsek, A. (2006). The effect of word frequency, word predictability, and font difficulty on the eye movements of young and older readers. Psychol. Aging 21, 448–465. doi: 10.1037/0882-7974.21.3.448
Rayner, K., Rotello, C. M., Stewart, A. J., Keir, J., and Duffy, S. A. (2001). Integrating text and pictorial information: eye movements when looking at print advertisements. J. Exp. Psychol. Appl. 7, 219. doi: 10.1037/1076-898X. 7.3.219
Rayner, K., Well, A. D., and Pollatsek, A. (1980). Asymmetry of the effective visual field in reading. Percept. Psychophys. 27, 537–544. doi: 10.3758/BF031 98682
Rensink, R. A., O’Regan, J. K., and Clark, J. J. (1997). To see or not to see: the need for attention to perceive changes in scenes. Psychol. Sci. 8, 368–373. doi: 10.1111/j.1467-9280.1997.tb00427.x
Rosbergen, E., Pieters, R., and Wedel, M. (1997a). Visual attention to adver- tising: a segment-level analysis. J. Consum. Res. 24, 305–314. doi: 10.1086/ 209512
Rosbergen, E., Wedel, M., and Pieters, R. (1997b). Analyzing Visual Attention to Repeated Print Advertising Using Scanpath Theory (Technical Report No. 97B32). Groningen: Research Institute SOM (Systems, Organizations and Management), University of Groningen.
Russo, J. E. (1978). Eye fixations can save the world: a critical evaluation and a com- parison between eye fixations and other information processing methodologies. Adv. Consum. Res. 5, 561–570.
Ryu, Y. S., Suh, T., and Dozier, S. (2009). “Effects of design elements in magazine advertisements,” in Engineering Psychology and Cognitive Ergonomics, ed. D. Harris (Berlin: Springer), 262–268.
Schall, J. D., and Cohen, J. Y. (2011). “The neural basis of saccade tar- get selection,” in The Oxford Handbook of Eye Movements, eds S. P. Liv- ersedge, I. P. Gilchrist, and S. Everling (Oxford: Oxford University Press), 357–381.
Schiller, P. H. (1998). “The neural control of visually guided eye movements,” in Cognitive Neuroscience of Attention: A Developmental Perspective, ed. J. Richards (London: Erlbaum), 3–50.
Schotter, E. R., Angele, B., and Rayner, K. (2012). Parafoveal processing in reading. Atten. Percept. Psychophys. 74, 5–35. doi: 10.3758/s13414-011-0219-2
Schütz, A. C., Braun, D. I., and Gegenfurtner, K. R. (2011). Eye movements and perception: a selective review. J. Vis. 11, 1–30. doi: 10.1167/11.5.9
Shen, J., Reingold, E. M., and Pomplun, M. (2003). Guidance of eye movements during conjunctive visual search: the distractor-ratio effect. Can. J. Exp. Psychol. 57, 76–96. doi: 10.1037/h0087415
Simola, J., Kivikangas, M., Kuisma, J., and Krause, C. M. (2013). Attention and memory for newspaper advertisements: effects of ad–editorial congruency and location. Appl. Cogn. Psychol. 27, 429–442. doi: 10.1002/acp.2918
Simola, J., Kuisma, J., Oörni, A., Uusitalo, L., and Hyönä, J. (2011). The impact of salient advertisements on reading and attention on web pages. J. Exp. Psychol. Appl. 17, 174–190. doi: 10.1037/a0024042
Slattery, T. J., Angele, B., and Rayner, K. (2011). Eye movements and display change detection during reading. J. Exp. Psychol. Hum. Percept. Perform. 37, 1924–1938. doi: 10.1037/a0024322
Slattery, T. J., and Rayner, K. (2010). The influence of text legibility on eye movements during reading. Appl. Cogn. Psychol. 24, 1129–1148. doi: 10.1002/acp.1623
Strasser, A. A., Tang, K. Z., Romer, D., Jepson, C., and Cappella, J. N. (2012). Graphic warning labels in cigarette advertisements: recall and view- ing patterns. Am. J. Prev. Med. 43, 41–47. doi: 10.1016/j.amepre.2012. 02.026
Teixeira, T., Wedel, M., and Pieters, R. (2012). To zap or not to zap: how to insert the brand in TV commercials to minimize avoidance. Mark. Sci. 29, 783–804. doi: 10.1287/mksc.1100.0567
Teixeira, T. S., Wedel, M., and Pieters, R. (2010). Moment-to-moment optimal branding in TV commercials: preventing avoidance by pulsing. Mark. Sci. 29, 783–804. doi: 10.1287/mksc.1100.0567
Thomsen, S. R., and Fulton, K. (2007). Adolescents’ attention to respon- sibility messages in magazine alcohol advertisements: an eye-tracking approach. J. Adolesc. Health 41, 27–34. doi: 10.1016/j.jadohealth.2007. 02.014
Treistman, J., and Gregg, J. P. (1979). Visual, verbal, and sales responses to print ads. J. Advert. Res. 19, 41–47.
Wang, J. C., and Day, R. F. (2007). The effects of attention inertia on advertisements on the WWW. Comput. Hum. Behav. 23, 1390–1407. doi: 10.1016/j.chb.2004.12.014
Wedel, M., and Pieters, R. (2000). Eye fixations on advertisements and mem- ory for brands: a model and findings. Mark. Sci. 19, 297–312. doi: 10.1287/mksc.19.4.297.11794
Wedel, M., and Pieters, R. (2008a). “A review of eye-tracking research in marketing,” in Review of Marketing Research, Vol. 4, ed. N. Malhotra (New York: M.E. Sharpe), 123–147. doi: 10.1108/S1548-6435(2008)0000004009
Wedel, M., and Pieters, R. (2008b). Eye Tracking for Visual Marketing. Hanover, MA: Now Publishers Inc.
Wedel, M., Pieters, R., and Liechty, J. (2008). Attention switching during scene perception: how goals influence the time course of eye movements across advertisements. J. Exp. Psychol. Appl. 14, 129. doi: 10.1037/1076-898X. 14.2.129
Wolfe, J. M. (1994). Guided search 2.0: a revised model of visual search. Psychon. Bull. Rev. 1, 202–238. doi: 10.3758/BF03200774
Yarbus, A. (1967). Eye Movements and Vision. New York: Plenum Press. doi: 10.1007/978-1-4899-5379-7
Zhang, J., Wedel, M., and Pieters, R. (2009). Sales effects of attention to feature advertisements: a Bayesian mediation analysis. J. Market. Res. 46, 669–681. doi: 10.1509/jmkr.46.5.669
Conflict of Interest Statement: The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.
 Frontiers in Psychology | Cognition
March 2014 | Volume 5 | Article 210 | 14
Higgins et al.
Eye movements when viewing advertisements
 Received: 15 November 2013; paper pending published: 23 December 2013; accepted: 24 February 2014; published online: 17 March 2014.
Citation: Higgins E, Leinenger M and Rayner K (2014) Eye movements when viewing advertisements. Front. Psychol. 5:210. doi: 10.3389/fpsyg.2014.00210
This article was submitted to Cognition, a section of the journal Frontiers in Psychology.
Copyright © 2014 Higgins, Leinenger and Rayner. This is an open-access article dis- tributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.
 www.frontiersin.org
March 2014 | Volume 5 | Article 210 | 15
Eye Tracking in Computing Education
Teresa Busjahn Freie Universität Berlin busjahn@inf.fu-berlin.de
Simon
University of Newcastle simon@newcastle.edu.au
Roman Bednarik University of Eastern Finland roman.bednarik@uef.fi
Carsten Schulte Freie Universität Berlin schulte@inf.fu-berlin.de
Andrew Begel Microsoft Research abegel@microsoft.com
Paul Orlov University of Eastern Finland paul.a.orlov@gmail.com
Bonita Sharif Youngstown State University bsharif@ysu.edu
Michael Hansen Indiana University mihansen@indiana.edu
Petri Ihantola
Aalto University petri.ihantola@aalto.fi
Galina Shchekotova Maria Antropova JetBrains JetBrains
gshchekotova@gmail.com maria.antropova@gmail.com
ABSTRACT
The methodology of eye tracking has been gradually mak- ing its way into various fields of science, assisted by the diminishing cost of the associated technology. In an inter- national collaboration to open up the prospect of eye move- ment research for programming educators, we present a case study on program comprehension and preliminary analyses together with some useful tools.
The main contributions of this paper are (1) an introduc- tion to eye tracking to study programmers; (2) an approach that can help elucidate how novices learn to read and un- derstand programs and to identify improvements to teaching and tools; (3) a consideration of data analysis methods and challenges, along with tools to address them; and (4) some larger computing education questions that can be addressed (or revisited) in the context of eye tracking.
Categories and Subject Descriptors
K.3.2 [Computers and Education]: Computer and Infor- mation Science Education—Computer science education
General Terms
Experimentation, Human Factors, Measurement
Keywords
CS Ed Research; Code reading; Computing education; Em- pirical research; Eye tracking; Gaze analysis; Program com- prehension; Programming education; Teaching programming
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
ICER’14, August 11-13, 2014, Glasgow, Scotland, UK. Copyright 2014 ACM 978-1-4503-2755-8/14/08 ...$15.00. http://dx.doi.org/10.1145/2632320.2632344.
1. INTRODUCTION
This paper introduces eye tracking as an instrument for computer science education research. The approach builds on the outcomes of the 1st International Workshop on Eye Movements in Programming Education: Analyzing the Ex- pert’s Gaze [4], held in conjunction with the 13th Koli Call- ing International Conference in Computing Education Re- search [15]. The workshop brought together educators and practitioners to analyze how eye tracking and the rich data it a↵ords could benefit programming education.
The observation of eye movements adds an objective source of information about programmer behavior to the collec- tion of research methods in computing education which can be used to facilitate the teaching and learning of program- ming. Alternative approaches for gaining insights about learners’ programming processes include analyzing the con- secutive versions of assignments submitted for automated assessment [1], instrumenting student programming envi- ronments to record snapshots from compilation [18], and recording keyboard events in text editors [11]. All of these methods complement one another; augmenting them with eye tracking would provide a comprehensive view of the pro- cess of learning programming. While our focus in this paper is on programming education, eye tracking is a valuable in- strument for other areas of computing education as well, e.g. understanding graphical data models [17].
This paper is organized as follows, the next section gives an overview of eye tracking technology, and presents previ- ous work. In section 3, we describe our case study and the eye movement coding scheme. Finally, to address challenges we discovered in our study, we o↵er a set of ideas and tools to advance eye tracking in computing education. We con- clude with an outline of ways in which eye tracking can help answer crucial questions in computing education.
2. 2.1
GAZE IN COMPUTING
A Brief Introduction to Eye Tracking
Eye trackers are used to capture a user’s eye movements when he looks at a stimulus while working on a task. In
computing education, a programmer would be given a pro- gramming task to solve while looking at relevant source code. There are two important types of eye movements: the fixa- tion, which is the settling of the eye gaze on an object of in- terest for a minimum period of time, and the saccade, which is a quick movement of the eyes from one location to another. Both fixations and saccades are voluntary, and indicate the location of the subject’s attention. A scan path is a directed path formed by saccades between fixations. Processing of vi- sual information occurs only during fixations [14], i.e., as a programmer looks at programming constructs in code, vari- ous mental processes are triggered to solve the task at hand.
Eye tracking is a source of rich and valuable information which cannot be obtained by other methods. Conventional measures retrospectively record the accuracy of the subject’s response and the time taken to obtain that response. For example, a programming educator will ask students to re- port their answers after debugging or tracing a program in a lab. This method records only the final outcome after the specific task has ended, neglecting information that might help understand how and why a student chose a particular (correct or incorrect) answer. Additionally, these measures raise a potential threat to the validity of the task, namely the di↵erence between student responses upon completion of a task and the reality the student experienced while per- forming that task. In other words, a student may misreport an experience at the end of a long task, or may forget to report it altogether.
Researchers can address this issue by asking programmers to record their observations while working towards their an- swers. However, this has the risk of interrupting their work on the main task at hand. This same drawback exists even if explicit methods such as think-aloud are used, since they still distract the programmers from their core task. More- over, subjects must be constantly reminded to verbalize their thoughts, since they do not often do so while they program in their natural setting. Even expert programmers find it di cult to state out loud exactly how they read a program. Many unconscious decisions go unreported, for example, en- countering logical dead ends while reading a program.
Much of the critical information that is lost with tradi- tional methods of measurement and assessment can be re- covered using an eye tracker. There is nothing the program- mer needs to wear in order for their eye movements to be recorded. Modern state-of-the-art eye trackers consist of a small hardware device that is placed near the program- mer’s monitor and can silently and unobtrusively document eye movements while the programmers looks at the screen. The additional data provides insights into the programmer’s thought processes, and achieves a finer granularity of data capture across space (across the program) and time (as the task progresses) because tacit knowledge and understanding is made more explicit.
Furthermore, eye tracking makes it possible to take ad- vantages of verbal accounts without the drawback of im- posing additional cognitive load and interfering with the comprehension process at hand. Combining retrospective think-aloud with eye tracking, study participants initially work purely on their task (e.g. understanding source code). Once the task is complete, they are prompted to verbalize their thoughts, watching their recorded eye movements to aid their recollection [24, 13]. This technique makes comple- mentary use of think-aloud and eye tracking and has been
found to induce higher quality comments about cognitive processes. We strongly believe that eye tracking synergizes with other methods of assessing comprehension, and used together provides additional insights.
2.2 Previous Work on Eye Tracking in Computer Programming
Eye tracking has been studied in non-computer fields such as chess, reading, piloting [12], mammography [21], and surgery [25]. Here, we present an overview of work that has used eye tracking in programming research.
Crosby and Stelovsky [9] were pioneers of using eye track- ing to study programmers. They found that programmers employed several distinct types of scan path patterns while they read an algorithm written in Pascal.
Uwano et al. [24] studied eye gaze patterns while five programmers detected code defects. They identified a pat- tern called scan, in which programmers appear to form an overview of the code. Approximately 70% of source code lines were viewed in the first 30% of the time spent reading the code. Sharif et al. [20] replicated this experiment with a larger sample of 15 programmers and found similar results. Programmers who spent less time to initially scan the code tended to take more time to find defects.
Fan [10] analyzed the eye gaze of programmers to learn about program comprehension processes used for beacons and comments in di↵erent tasks. Code scanning sequences were directly a↵ected by comments, enabling programmers to chunk larger code blocks. Fan concluded that eye gaze data is very useful in documenting and analyzing the pro- gram comprehension processes.
Busjahn et al. [7] used eye tracking to compare natural language text reading and code reading. They found a sig- nificant increase in both fixation duration and number of backward movements when subjects read source code, indi- cating the di↵erent demands of these two text types and the reading patterns that they induce.
Bednarik [3] studied the di↵erences between novices and experts during debugging using source code and graphical representations. He found that repetitive eye patterns were associated with less expertise; novices used both representa- tions with a lot of context switching.
Turner et al. [23] conducted a preliminary study on 38 students to assess how the choice of programming language a↵ected how programmers solve tasks. Looking at simple C++ and Python programs, they found a significant dif- ference between the two languages for the fixation rate on buggy lines of code.
There have been a few studies using eye tracking to study programming, but none comprehensively analyze the rela- tionship between raw eye movements and comprehension. It is extremely di cult to translate a person’s eye movements into insights about his or her mental state while reading and understanding program [6]. We are confident that there will be more of these studies because of the diminishing cost of eye tracking. One of our goals is to raise awareness of the opportunities and challenges a↵orded by this technology in computing education research.
3. CASE STUDY
We conducted a case study to determine whether the use of eye tracking in computing education was feasible, could provide rich data for analysis, and could lead to novel teach-
 ing ideas. For details beyond those in this paper, please read our technical workshop report [4].
3.1 Experimental Setup
The eye movement data analyzed for the workshop came from a study with professional software developers reading and understanding short Java programs. We recorded them in an o ce at the programmers’ company with an SMI RED- m 120 Hz eye tracker using the OGAMA tracking software.1
The recording sessions started with natural language texts followed by comprehension questions to familiarize the sub- jects with the instrument and the tasks. The subjects then moved on to examine Java code. After being informed that the code did not contain bugs, they were asked to read it, comprehend it, and answer a question to test their compre- hension. The code segments were short enough to fit on a single screen without scrolling, to simplify the connection between gaze location on the screen and in the code.
The program read by the subjects (shown in Figure 1) calculated the area of a rectangle. Subject 1 was told to expect to answer a question about the return value of the rect2.area() method, while Subject 2 was told to expect a multiple-choice question about the algorithm in the code.
Each trace was given to the workshop participants as an AVI video showing the subject’s current fixation location as a large red circle on top of the source code that the subject saw.2 The prior five fixations were marked by blue circles whose size indicated the duration of the fixation. Blue lines joining the circles represented the saccades. The eye move- ments were quite rapid (and usually are), so the researchers could slow down the video to see all the gaze locations.
The two traces are very di↵erent. One might consider subject 1’s gaze to be erratic. Viewed in real time, it flashes wildly about the code, generally spending very little time on any one point. However, when taken in total, there is a clear pattern of subject 1’s eye returning to certain focal points. These points are pertinent to the question the subject was told to expect, but the fixations are so brief as to leave the analyst wondering whether if it was at all possible to gain any comprehension of the code. For example, in one 10 second span, the gaze shifts more than a dozen times between every method on the screen, typically spending less than a second on each point of interest.
By contrast, Subject 2 reads the code slowly and method- ically, yet takes about 40% less time overall than Subject 1. We see evidence of linear scanning patterns, and very long gaze fixations on areas of interest. Whereas Subject 1 spent 10 seconds looking at every method on the screen, Subject 2 spent 10 seconds looking at a single variable declaration. In addition, after 1 second glances at the methods height() and width(), there was a steady 4 second gaze on area(), followed by another 8 seconds on the declaration of rect2. Our impression was that Subject 2 deliberately read through the code, understanding it the first time it was viewed.
3.2 The Workshop
We designed the Eye Movements in Programming work- shop to bring together a number of researchers to consider various approaches of inferring cognitive processes from eye
1http://www.ogama.net
2These videos are available at http://www.mi.fu-berlin.
de/en/inf/groups/ag-ddi/Gaze_Workshop/koli_ws_ material.
Figure 1: Source code used for the workshop – over- laid with eye movements
movements during source code reading. Prior to the work- shop, the participants were given access to the two gaze traces described above. The workshop organizers developed a coding scheme for the gaze traces based specifically on the two eye gaze trace videos and the specific Java program, in order to broaden our knowledge of program comprehension strategies. We made a fundamental decision to distinguish between objective and subjective behaviors. At the most ba- sic level, analysts using the scheme would objectively code the part of the program on which the programmer’s gaze is resting. At the next level, they would use subjective codes to describe their inferences of the patterns of eye movement and the strategies being employed by the programmer to comprehend the code.
Each workshop participant individually analyzed the eye movement records (without any audio) and coded it us- ing the ELAN video annotation software.3 They each then wrote a position paper to describe the traces, to reflect on the validity and utility of our coding scheme, and to discuss possible applications of eye movement research for computer science education. At the full day workshop, participants explored their findings with one another, using their discus- sions to refine the coding scheme and plan further research.
3.3 Coding Scheme
The coding scheme used in the case study captures the objective eye tracking data as well as the coder’s inferences about the programmer’s comprehension of the source code. We present the coding scheme here to illustrate the possible outcomes of employing eye tracking in computing; it is not
3http://tla.mpi.nl/tools/tla-tools/elan

directly generalizable and should not be taken as a central contribution by itself.
We revised the coding scheme according to suggestions given by the participants in their position papers, and fur- ther refined it during the workshop (see report [4]). The scheme consists of a number of ‘tiers’, each of which can be coded with a choice of values. The tiers are summarized below.
Line: indicates the line of code the participant’s gaze is on. Block: indicates the block of code that the line is in. In a typical short code segment, the blocks might be the class At- tributes, the Constructor, the Main method, or any other method.
SubBlock: some blocks have identifiable sub-blocks in which a reader’s gaze might rest. For example, a method may contain sub-blocks of Signature, Body, Method Call, and Return. While the latter two statements are part of the method body, we code them separately because their im- portance makes them likely to be the focus of the reader’s concentration.
Signature: when the gaze rests on the signature sub-block, this tier further indicates whether it dwells on the method Name, its Return Type, or its Formal Parameter List. Method Call: when the gaze rests on a method call, this tier is used to indicate whether it focuses on the method’s Name or its Actual Parameter List.
These first five tiers refer to the gaze location at a single point in time. They can be coded objectively and automat- ically based simply on the eye gaze location on the screen.
The next tier of the coding scheme, Pattern, identifies particular combinations of the observed fixations. For ex- ample, in the pattern we call Flicking, the gaze flicks back and forth between two (or possibly more) identified gaze points. Specific instances of this pattern may flick between the actual and formal parameter lists of a method call and its declaration, between the use and declaration of a variable, or between di↵erent locations where a variable is used. Other patterns include Linear Scan, in which the gaze moves lin- early through some part of the code; Jump Control, in which gaze follows the code in execution order; and Thrashing, in which the gaze leaps about all over the code with no dis- cernible intent. To date, we have identified 11 patterns. While patterns are observable in the eye gaze trace, coders must make a subjective decision about the number of fixa- tions to combine into a single pattern; thus, these patterns cannot be automatically identified without human interven- tion.
The final tier of the coding scheme, Strategy, relies on interpretation by the coder. The analyst uses these codes to determine the cognitive actions taken by a programmer comprehending the program. Some strategies tend to be as- sociated with particular patterns, but there is no one-to-one relation between them. For example, the Design at Once strategy is often associated with a linear pattern and sug- gests the programmer is reading sequentially through part, or all of the code, to acquire an overall understanding. In- traprocedural and Interprocedural Control Flow follow the expected control flow of the program, which implies that the programmers is simulating program execution. Test Hypothesis involves repetition of a gaze pattern, suggest- ing increased concentration is needed to better understand a particular detail of the program. Trial and Error is essen- tially a Linear Scan pattern with faster reading, irregular
jumps, and repetition. This code is used when the program- mer is searching for some part of the code that will lead to an initial understanding. So far, we have identified 14 distinct strategies.
3.4 Interpretation of Results
Our analyses of the two traces proved extremely inter- esting. Subject 1, whose gaze we described as erratic, cor- rectly answered the study question. Subject 2, whose gaze seemed to be more methodical, chose the wrong answer to a multiple-choice question about the code. Since both of these subjects were expert programmers, we think it is un- likely that the di↵erences in the accuracy of their answers are directly related to the di↵erences in their gaze patterns. Indeed, we would hope that Subject 2’s wrong answer indi- cated a simple slip, rather than a failure to comprehend the code. Perhaps Subject 1 got his answer correct because his task was more specific than Subject 2’s task. Subject 2 had to memorize more of the code and remember four specific variables (x1, x2, y1, and y2) in order to choose the cor- rect answer. Nevertheless, it is clear that di↵erent experts can display entirely di↵erent gaze patterns while reading the same code for comprehension. This diversity was apparent in the traces, even though the program was very short, sim- ple, and bug-free.
3.5 Lessons Learned
As we expected, we had to revise and refine the coding scheme during the course of the case study. Participants found using ELAN to code the low-level categories (e.g. Block) to be time-consuming and inconsistent, and wished for a tool to automate this step.
The distinction between patterns as objective, observable behavior and strategies as the associated cognitive processes was valuable. However, the coding process is necessarily subjective and the coders could not be definitive about the readers’ cognitive processes. Perhaps at this early stage of the research, it would be beneficial to complement the eye movement analysis with other methods, such as retrospec- tive think-alouds.
Our aim is to correlate the subjects’ cognitive strategies with observable patterns, so that we might use the pattern to identify the strategy being applied. This would make strategy coding less subjective, but is going to require a great deal more analytical work before it becomes feasible.
One threat to the validity of our coding scheme is that we based it on just two expert gaze traces of one program. However, we believe that our collaborative experiment has helped us establish a foundation that supports additional data analysis and the elaboration and development of more sophisticated analysis methods and materials. Future stud- ies that vary programs, problem domains, and test subjects will enable the research community to refine and improve our expanded understanding of the cognitive processes in- volved in program comprehension.
4. DATA ANALYSIS - CHALLENGES AND SOLUTIONS
In this section, we discuss the challenges raised in the case study and explain how we addressed them. We present several tools to support interpretation of eye movement data and of records annotated with the coding scheme. Even
though the interpretation of eye gaze data is not entirely straightforward, our workshop made it clear to us that the main challenges are already well understood.
Eye tracking videos are useful for spot-checking specific points in an experiment (e.g., did the subject look in re- gion X at time T), but it is not easy to get a big picture sense of the subject’s behavior or to compare it with other subjects’ behaviors. Static visualizations like heatmaps and fixation scatter plots can provide such global pictures, but these fail to capture the dynamics and nuances of subject be- havior. Accurate eye tracking data interpretation requires additional tools and methods to combine the various views that arise during the analysis.
4.1 Visualizing Annotated Gaze Records
Here we present several tools to help interpret data anno- tated with ELAN-assigned codes. First, we present an eye movement flow chart in Figure 2, made using the D3.js li- brary. This flow chart represents a graphical Markov chain of the elements in the coding scheme. Flow charts can help analysts find and understand eye gaze patterns, and can also be used in exploratory analysis.
Figure 2: Gaze transition flow chart for Subject 1
The curved lines in the chart represent transitions of the subject’s eye gaze from a source (left) to a target (right). The chart in Figure 2 shows two consecutive transitions. The width of the curved lines indicates the fraction of times (i.e., probability) the subject transitioned from the particu- lar source to that particular target. For example, Subject 1 looked at Main and then Height about twice as often as he did from Main to Area. However, after looking at Area, he switched back to Main and Height roughly the same frac- tion of times. We can use the flow charts to compare sub- jects and easily see the di↵erences in their transition prob- abilities. Subject 1’s flow chart shows many narrow tran- sitions between code locations, while Subject 2’s flow chart (see report [4]) shows fewer, wider lines between consecutive code locations reflecting his more methodical comprehension style.
Second, we introduce VETtool4 which can read a file of ELAN annotations (i.e., the codes from the coding scheme) and display them on a timeline based on the duration of the associated fixations. After using ELAN to assign low-level codes (e.g., Signature), VETool’s visualization can show the areas of the program that were visited during the trial.
4 VETool is GPLv2 software built on the Net- Beans Platform with JavaFX. The source code can be found at https://bitbucket.org/orlovpa/ visual-evaluation-tool-vetool.
Several codes can be presented together, allowing the analyst to see the timeline of the areas of interest (AOIs) overlaid with the patterns and strategies employed to understand them.
 Figure 3: VETool displaying fixation durations (or- ange) and Main codes (green) for Subject 1
VETool also enables analysts to correlate fixation counts and durations with various low-level, pattern, or strategy codes, helping determine whether the subject was just briefly inspecting an area or examining it at length. Visualizing these correlations make it easier to compare two subjects by highlighting di↵erences in their behavior. Figure 3 shows subject 1 concentrating on Main overlaid with fixation dura- tion.
4.2 Quantizing Fixations
Next, we describe two static visualizations that we devel- oped for understanding program comprehension. The first transforms fixations into coarse-grained areas of interest, such as single code elements, lines of code or blocks. The second plots the areas of interest (or metrics derived from them) on a timeline.
The workshop participants found coding the gaze location into code elements and blocks to be tedious and error-prone. Automatically coding these tiers would address a primary challenge of the eye tracking data analysis process [6]. So, we created a tool to draw virtual rectangles around areas of interest and assign each fixation to zero or more AOIs. Each AOI rectangle is then associated with the source code on the screen that represents each of the low-level codes in the coding scheme (e.g., Block, Line, Method Call, etc.). For simplicity, assume that the AOI rectangles for the Block, SubBlock, Signature, and Method Call categories do not overlap, making them mutually exclusive (this is not the case for Pattern).
Figure 4: Sample assignment of a fixation to an AOI
To determine whether or not a fixation belongs to an AOI, the tool draws a circle around the center of the fixation point with radius R, and chooses the AOI rectangle with the largest area of overlap (Figure 4). The choice of the parameter R influences accuracy and depends on other pa- rameters such as the size of the computer monitor and font size used in the experiment.
Once fixations from the eye gaze record have been assigned to AOIs (and thus the low-level codes in the coding scheme),

  Figure 5: Fixations for Subject 1 quantized by line
we plot them on a timeline. The timeline plot in Figure 5 shows Subject 1’s fixations, quantized by Line using R = 20 pixels. These plots provide a wealth of information about a participant’s behavior at a glance, enabling analysts to easily identify critical moments in the eye gaze record.
The AOI quantization tool is based on a graphical analy- sis of the eye gaze pixel locations on the screen. Its ability to track AOIs does not work so well on screens that contain scrolling or changing content. Applying it to IDEs in which the subject may scroll or type new code is simply infeasible for non-trivial programs and tasks [6]. Fortunately, there are IDE add-ons that can help, such as the iTrace plugin for Eclipse.5 iTrace provides the exact source code entity a pro- grammer looks at. Linking the eye tracker output directly to the IDE enables us to use the plugin to automatically annotate the eye gaze information with the correct program elements, solving the scrolling problem and the problem of adding or editing code.
Automatic code labeling facilitates aggregation of the data, which is valuable for group comparisons. In addition, if de- fined formally enough, pattern codes can be automatically derived from the low-level labels. For example, the Linear Scan pattern is readily apparently on a timeline plot. In Figure 5, we might say that Linear Scan describes the fix- ations between 2 and 20 seconds. We caution that someone should review the results because noise in the raw gaze data might have resulted in an incorrect classification of an AOI or low-level code, throwing o↵ the pattern detector.
Finally, automatically labeling codes from our scheme of- fers a foundation for comparing results from future studies of eye tracking in computing education. We invite computing education researchers to apply our coding scheme in their own studies.
4.3 Strategies and Fixation Metrics
To aid in the identification and interpretation of Strategy codes, we compute three fixation metrics over the course of each trial: fixation count, mean fixation duration [19], and fixation spatial density [8]. We calculate each using a moving average of 4 second time windows, shifted by 1 second at a time. Typically, a single 4 second time window will contain about a dozen fixations.
The first metric is simply the total number of fixations in a time window. The second is the average duration of these fixations. The third metric divides the screen into a grid, and calculates the proportion of cells in the grid
5http://www.csis.ysu.edu/~bsharif/iTrace
Figure 6: Fixation metrics for Subject 1
that contain at least one fixation. We applied a 10-cell, vertically-divided, rectangular grid to the source code editor, so a spatial density of 1 means that each rectangle in the grid was fixated upon at least once during the 4 second time window.
Finally, we plot the metrics (after removing time windows which contain no fixations) on a timeline. Figure 6 illus- trates the three metrics computed from Subject 1’s trial. Dips in the spatial density (shown on the red line) corre- spond to time windows in which Subject 1 focused on just one or two lines of code. Sometimes this corresponds with an increase of the number of fixations (blue line), we found to be useful to distinguish between Debugging and Test Hy- pothesis Strategy codes.
Subject 1’s erratic gaze is revealed by the very low mean fixation duration (green line), however, we see it increase sharply just after 70 seconds into the trial when Subject 1 focuses his gaze on the final line of the program: Sys- tem.out.println(rect2.area()); Recall that his task in this trial was to obtain the value of rect2.area(). Given the increased mean fixation duration, and the drop in fixa- tion count and spatial density at approximately 65-75 sec- onds, we hypothesize that Subject 1 is performing the neces- sary mental calculation to compute the area of rect2. While it may not be possible to pinpoint changes in strategy using this kind of visualization, we can quickly identify interesting time windows that we should investigate further.
5. PROSPECTS FOR PROGRAMMING EDUCATION
Eye tracking o↵ers opportunities for a great range of re- search questions in the areas of programming education and program comprehension. Possible research topics include
• the e↵ects of text-based, graphical, or UML program representations [26]; syntax and language features; pro- gramming paradigms;
• the behaviors and strategies of a learner’s reading, un- derstanding, writing and debugging tasks’
• challenges for learners, e.g., what makes tasks di cult for them, what obstacles impair their understanding and use of programming concepts;
• evaluation of tools for static and dynamic program vi- sualization [5], as well as for IDEs [3]; and
• gaze-related concerns, e.g., exploring the possibility of providing immediate feedback based on eye movements in programming environments [2].

These topics can lead to advances in teaching programming in the following areas.
5.1 Decoding the Learner
Eye tracking allows us to retrace how the novice goes about reading and understanding source code. We can ob- tain information about di culties, behavior and strategies, and develop new tools to assess learners. In addition, we can describe and evaluate an individual’s level of expertise on the basis of aggregated empirical studies of eye tracking that compare novice and expert programmers.
5.2 Advances in Teaching Material and Tools
The detailed data provided by eye trackers can also help to advance learning tools, such as IDEs for learning pro- gramming. Eye tracking is a well-established instrument in usability testing with a large corpus of analyses, metrics and examples of best practice. Applying eye tracking can help developers increase tool usability and lower the bar- riers to adoption. More sophisticated uses of eye tracking could provide highly-contextualized feedback to the learner, for example in an automated tutor. If an eye tracking metric moves past a particular threshold, it could indicate that the student is having di culties with the material, and could use a hint in order to make progress. Visual cueing could be employed in an IDE, if students look too long at the wrong section of code, or thrash their gaze over the entire program without focusing on any particular part.
5.3 New Perspectives for Teaching Code Reading
Eye tracking studies can be used to shed light on how in- dividuals conceptualize and discern the embedded process of computational representations. A crucial challenge yet to be solved is to explore the code reading characteristics of individuals. First a normative or generalized pattern needs to be established, e.g., a program flow gaze pattern for spe- cific source code examples. After taking into account the learner’s ability or level of understanding or the di culty of the task, individual deviations from this normative eye gaze pattern can be used to reveal meaningful information about the concrete learning process.
These normative eye gaze patterns can help identify di↵er- ences in the strategies adopted by programmers with various levels of expertise, domain knowledge, and skills. Given that the ability to read and comprehend code seems to be linked with the ability to write it, there is substantial evidence that many programming novices have not yet acquired the ability to e↵ectively read programs [16]. As with natural language, concrete code reading skills can explicitly be taught to learn- ers, addressing purposes like debugging code or working out why it is written that way. It is hard to do this now because reading and debugging strategies are so ingrained that peo- ple are not aware of them on a conscious level.
Eye tracking could be used to raise a novice’s awareness of how they go about reading code. A novice could track himself solving a task, and later understand how what he was thinking corresponded with where he was looking. This would build a novice’s self-awareness and facilitate the meta- cognition that eventually teaches a beginner to read code e ciently.
We can use eye gaze data to explore experts’ strategies and develop teaching materials demonstrating their application.
Novices observing expert programmers’ eye movements on a given task could get visual cues as to what is important [22]. This might lead to the creation of a tool for teaching read- ing skills that shows the student where to look, an approach that has proved successful in other domains [25]. If students could be taught to consciously use code reading strategies according to code size, program structure and other param- eters, they could see how to make their own approach more e↵ective. After they have been taught these reading tech- niques, teachers can use eye tracking to verify that students are using them, enabling better assessment of teaching in- terventions.
From our workshop examples, we saw that experts can read the same code using very di↵erent strategies. There- fore, the ones o↵ered to learners should be those that were used consistently by many expert programmers. Individual students could then adopt a strategy that fits well with their own personal approach.
6. CONCLUSIONS AND FUTURE WORK
In this paper, we have presented eye tracking as a method to enrich computing education research. Tracking a person’s gaze gives a record of their visual behavior on a super fine- grained scale in both space and time. We can build on the small, but growing, body of sound work on eye tracking in the context of programming to enhance programming educa- tion practice and research. Eye tracking has the potential to open up new methods of understanding how people program and learn to program, of corroborating existing empirical research, and of tackling currently unsolved questions. Its benefits will stretch across broad areas of research, such as program reading and comprehension, and a↵ord new teach- ing material for aspects of professional expertise that have not yet been analyzed.
Gaze analysis o↵ers intriguing prospects for further study. Eye tracking can record a person’s visual behavior during reading, without interruption, adding to his cognitive load, or requiring a subjective report. Thus, this research tool provides a new quality and directness, along with a much finer data granularity, to observe cognitive processing.
Gaze analysis can also serve as an additional source of data to corroborate studies carried out with other research methods. Such study replications will help to increase our collective evidence and sharpen our theories. With more and more of the challenges associated with eye tracking be- ing solved, with eye trackers becoming more a↵ordable, and with relevant analytical tools becoming increasingly avail- able, such studies might even become a standard in educa- tional research.
Our Eye Movements in Programming workshop and this paper are a first step towards making eye tracking more accessible to computing educators. When a dozen experts in computer science education and eye tracking can agree on the potential of using eye movement data in programming education, their position must surely have some merit.
7. ACKNOWLEDGMENTS
We would like to thank all workshop participants for their great work.
8. REFERENCES
[1] A. Allevato and S. H. Edwards. Discovering patterns in student activity on programming assignments. In ASEE Southeastern Section Annual Conference and Meeting, 2010.
[2] V. M. G. Barrios, C. Gu ̈tl, A. M. Preis, K. Andrews, M. Pivec, F. Mo ̈dritscher, and C. Trummer. Adele: A framework for adaptive e-learning through eye tracking. In Proc. of IKnow, volume 4, pages 1–8. Citeseer, 2004.
[3] R. Bednarik. Expertise-dependent visual attention strategies develop over time during debugging with multiple code representations. International J. of Human-Computer Studies, 70(2):143–155, 2012.
[4] R. Bednarik, T. Busjahn, and C. Schulte. Eye movements in programming education: Analyzing the expert’s gaze. Technical report, University of Eastern Finland, Joensuu, Finland, 2014.
[5] R. Bednarik, N. Myller, E. Sutinen, and M. Tukiainen. E↵ects of experience on gaze behavior during program animation. In Proc. of 17th Annual Workshop of the Psychology of Programming Interest Group, pages 49–61, Sussex University, 2005.
[6] R. Bednarik and M. Tukiainen. An eye-tracking methodology for characterizing program comprehension processes. In Proc. of the Symposium on Eye Tracking Research & Applications, pages 125–132. ACM, 2006.
[7] T. Busjahn, C. Schulte, and A. Busjahn. Analysis of code reading to gain more insight in program comprehension. In Proc. of the 11th Koli Calling International Conference on Computing Education Research, pages 1–9, Koli, Finland, 2011. ACM.
[8] L. Cowen, L. J. Ball, and J. Delin. An eye movement analysis of web page usability. In People and Computers XVI-Memorable Yet Invisible, pages 317–335. Springer, 2002.
[9] M. E. Crosby and J. Stelovsky. How do we read algorithms? A case study. Computer, 23(1):24–35, 1990.
[10] Q. Fan. The e↵ects of beacons, comments, and tasks on program comprehension process in software maintenance. PhD thesis, University of Maryland at Baltimore County, Catonsville, MD, USA, 2010.
[11] J. Helminen, P. Ihantola, and V. Karavirta. Recording and analyzing in-browser programming sessions. In Proc. of the 13th Koli Calling International Conference on Computing Education Research, pages 13–22, Koli, Finland, 2013. ACM.
[12] V. A. Huemer, M. Hayashi, F. Renema, S. Elkins,
J. W. McCandless, and R. S. McCann. Characterizing scan patterns in a spacecraft cockpit simulator: Expert vs. novice performance. Proc. of the Human Factors and Ergonomics Society Annual Meeting, 49(1):83–87, Sept. 2005.
[13] A. Hyrskykari, S. Ovaska, P. Majaranta, K.-J. Ra ̈iha ̈, and M. Lehtinen. Gaze path stimulation in retrospective think-aloud. J. of Eye Movement Research, 2(4):1–18, 2008.
[14] M. Just and P. Carpenter. A theory of reading: From eye fixations to comprehension. Psychological Review, 87:329–354, 1980.
[15] M.-J. Laakso and Simon, editors. Proceedings of the 13th Koli Calling International Conference on Computing Education Research, November 2013.
[16] R. Lister, C. Fidge, and D. Teague. Further evidence of a relationship between explaining, tracing and writing skills in introductory programming. SIGCSE Bulletin, 41(3):161–165, 2009.
[17] J. C. Nordbotten and M. E. Crosby. The e↵ect of graphic style on data model interpretation. Information Systems J., 9(2):139–155, 1999.
[18] C. Piech, M. Sahami, D. Koller, S. Cooper, and
P. Blikstein. Modeling how students learn to program. In Proc. of the 43rd ACM Technical Symposium on Computer Science Education, SIGCSE ’12, pages 153–160, NY, USA, 2012. ACM.
[19] A. Poole and L. J. Ball. Eye tracking in human-computer interaction and usability research: Current status and future. In Prospects, Chapter in C. Ghaoui (Ed.): Encyclopedia of Human-Computer Interaction. Pennsylvania: Idea Group, Inc, 2005.
[20] B. Sharif, M. Falcone, and J. Maletic. An eye-tracking study on the role of scan time in finding source code defects. In Proc. of the Symposium on Eye Tracking Research & Applications, pages 381–384, Santa Barbara, CA, 2012. ACM.
[21] S. Sridharan, R. Bailey, A. McNamara, and
C. Grimm. Subtle gaze manipulation for improved mammography training. In Proc. of the Symposium on Eye Tracking Research & Applications, pages 75–82, Santa Barbara, California, 2012. ACM.
[22] R. Stein and S. E. Brennan. Another person’s eye gaze as a cue in solving programming problems. In Proc. of the 6th international conference on Multimodal interfaces, pages 9–15, PA, USA, 2004. ACM.
[23] R. Turner, M. Falcone, B. Sharif, and A. Lazar. An eye-tracking study assessing the comprehension of C++ and Python source code. In Proc. of the Symposium on Eye Tracking Research & Applications, pages 231–234, Safety Harbor, Florida, 2014. ACM.
[24] H. Uwano, M. Nakamura, A. Monden, and K.-i. Matsumoto. Analyzing individual performance of source code review using reviewers’ eye movement. In Proc. of the Symposium on Eye Tracking Research & Applications, pages 133–140, San Diego, California, 2006. ACM.
[25] S. J. Vine, R. S. Masters, J. S. McGrath, E. Bright, and M. R. Wilson. Cheating experience: Guiding novices to adopt the gaze strategies of experts expedites the learning of technical laparoscopic skills. Surgery, 152(1):32–40, July 2012.
[26] S. Yusuf, H. Kagdi, and J. I. Maletic. Assessing the comprehension of UML class diagrams via eye tracking. In Proc. of the 15th IEEE International Conference on Program Comprehension, pages 113–122, 2007.
Generic Gaze Interaction Events for Web Browsers
Using the Eye Tracker as Input Device
Benjamin Wassermann Media University Stuttgart Mobile Media Group Nobelstraße 10 Stuttgart, Germany wassermann@hdm- stuttgart.de
ABSTRACT
In the last decade much research has been conducted on an- alyzing human eye and gaze movements using eye tracking technology, not only in the fields of neuroscience, psychol- ogy and marketing, but also in the field of human computer interaction. However, no flexbile framework exists to inte- grate eye tracking directly into web applications to easily create and test new interaction concepts. We have created a JavaScript library based on the latest HTML5 Web tech- nology and the jQuery library to close this gap. Facilitated by HTML5 WebSocket, the browser directly receives gaze input samples from an eye tracker, generating events that are similar to those of a mouse input device. Events like gazeOver/-Out or fixationStart/-End can be attached to any HTML element in the DOM tree. New custom events de- rived from the eye tracking data, e.g. blink or read, can easily be added. Using this library we have successfully implemented a number of Web applications, allowing the users to interact with their eyes. This paper also describes our gaze enabled Web-based eLearning environment. Our JavaScript library is used within the eLearning environment to capture and interpret eye gaze events for the purpose to support users in the acquisition of new knowledge.
Categories and Subject Descriptors
H.5.2 [User Interfaces]: Input devices and strategies, In- teraction styles - gaze; H.5.4 [Hypertext/Hypermedia]: Architectures, Navigation
General Terms
Human Factors, Design
Keywords
eye tracking, gaze events, gaze based interaction, gaze-enhanced web, gaze-enhanced eLearning
Adrian Hardt Eberhard Karls University Tübingen Wilhelm-Schickard-Institute Sand 14 Tübingen, Germany a.hardt@student.uni- tuebingen.de
Gottfried Zimmermann Media University Stuttgart Mobile Media Group Nobelstraße 10 Stuttgart, Germany gzimmermann@hdm- stuttgart.de
1. INTRODUCTION
Since the establishment of multi-touch devices in the mass market the acceptance and experimentation for new interac- tion designs has grown rapidly [24]. An upcoming approach is to model events by user intent in interaction (called Indie UI or former Intentional Events)[3, 4]. These technologies are envisioned for interacting with applications independent of input devices, giving the user the opportunity to choose their preferred input devices or allow people with disabil- ities to interact with assistive technologies. One of those technologies could be eye tracking, since it is continuously improving. In the near future this technology could reach the mass market, and eye gazing could be used as a means of user input. Based on this technology, using the gaze as input information, new interaction concepts are developed.
The ScienceCampus Tu ̈bingen [16] by the Knowledge Media Research Center [7] in Tu ̈bingen maintains a research line on the Design of Interactive Informational Environments. In the context of the pertaining project Adaptable and Adap- tive Multimedia Systems, we are interested in new interac- tion designs that support users in the acquisition of new knowledge and the pertaining regulation processes. We have created a dynamic and adaptive learning platform based on the eLearning Web platform ILIAS [13] and the JavaScript- library jQuery [20], and have included eye tracking as an input device for the eLearning environment.
This approach allows us to acquire a user’s eye gaze informa- tion in real-time that can be used to generate generic (i.e. not specific to a specific eye tracking device) gaze events, to analyse the data to deduce more higher-level events or to derive predictions about the user’s condition, e.g. if the user is bored or overstrained by the learning content.
Before describing our concept of the generic gaze interaction events, we will give an overview of related work in the field of eye tracking technology. After that we will introduce our framework, enabling us to transfer the gaze data from the eye tracker to the Web browser, followed by our concept about gaze interaction events. We will then describe our eLearning environment, and discuss using eye tracking as an input device. Finally, we will give a short conclusion and an outlook on future work.
2. RELATED WORK
Recently, eye tracking technology has grown mature. It has not only been used for many years for studies in the area of image scanning [15], eye movement while driving [14], read- ing [18] and solving arithmetic tasks [27], but also to build interactive systems using eye tracking as an input device [9, 10, 26]. Jacob was one of the first in introducing gaze- based interaction techniques. He discussed gaze interactions such as ob ject selection, ob ject movement and eye-controlled scrolling for text [10]. He also described the “Midas Touch” problem which refers to a user’s need to confirm their actions in order to interact with objects. To overcome this problem he introduced dwell-based activation. The gaze of the user has to remain over an object for a fixed time before it is ac- tivated. Since this dwell-time has an inherent latency, Jacob suggested allowing users to confirm their actions instantly using a keyboard as alternative.
A well-known problem is the accuracy of eye tracker sys- tems. Jacob, for example, has overcome this problem by us- ing sufficiently large targets for his experiments. He showed in his work that the eye can provide a pointing accuracy of approximately one degree [10]. Ware and Mikaelian [28] con- ducted studies to analyze different types of selection meth- ods with eye tracking, also taking target size into account. Their results showed that eye selection can be faster than mouse selection, provided that the target has a moderate size. Since then researchers have come up with many dif- ferent approaches for solving the problems of selecting and activating objects, even with low accuracy of the eye track- ing device.
In classical approaches the eye movement is typically an- alyzed in terms of fixations and saccades. A fixation is a longer gaze over informative regions of interest while the saccades are rapid movements between fixations. Typical metrics of eye tracking analysis includes fixation or gaze du- rations, saccadic velocities and amplitudes and transition- based parameters, e.g. between fixations and regions of in- terest [23]. By tracing these fixations one can analyze cogni- tive protocols [21, 22], since it is generally agreed upon that visual and cognitive processing occur during fixations [11]. However, the determination of fixations is not clear, because there is space for interpretation about when a fixation starts and when it ends.
Eye tracking can be used as input device and connected to an application in two ways: First, by a link between the eye tracker system and the application, and second by having the eye tracker emulate the pointing device of the operat- ing system. In the latter case, the cursor is controlled by the eye tracker, preventing in some cases the simultaneous usage of eye tracking and mouse. Drewes [5] addressed this problem by integrating eye tracking as additional interaction modality into a Web browser by using a special proxy, thus creating a powerful experimentation platform. Biedert [2] used a more flexible approach by embedding a Java applet to transfer eye movement data from the eye tracker into the Web browser. Drewes [6] also made first investigations for integrating eye tracking into eLearning environments. The eye movements bear more information than simple interac- tion events, and can therefore support personalization of the learning content and user interface.
3. FRAMEWORK
The framework consists of a communication adapter be- tween Web browser and eye tracker as shown in figure 1. To acquire data from the eye tracker system, a proprietary communication API is provided. Since the Web browser does not support proprietary eye tracker APIs, an adapter is required. This raises the question: Which kind of commu- nication technology, supported by the Web browser, should be used to transfer the eye tracker data to the Web browser?
I/O via custom protocol
 Web Browser with WebSocket support
iTrack JavaScript API
Communica on API + Adapter Connector Websocket Adapter Other Adapter
        Websocket Adapter Other Adapter Communica on API + Adapter Connector
iTrackServer
Generic Eye Tracker API Eye Tracker Adapter Connector
    SMI Tobii Adapter Adapter
Etc.
   I/O via proprietary protocols
Figure 1: This graphic shows the architecture of our framework. The Web browser is connected to the iTrackServer using the Communication API with the WebSocket Adapter and the custom protocol to communicate with the iTrackServer. By using the JavaScript API iTrack, Web applications within the Web browser can access the data from the eye tracker. The iTrackServer uses the Generic Eye Tracker API to communicate with the eye tracker hardware. Miscellaneous eye tracker hardware can be used via the Eye Tracker Adapters.
To answer this question we first have to take into account that we want the eye tracker data in real-time, which im- plies an active communication from the server to the client with low latency. A traditional client-server request does not seem appropriate since a client always has to start a re- quest to the server before receiving a reply from the server. There are some existing advanced techniques like reverse AJAX to circumvent this problem, but the latency is still quite high. A better solution could be the use of browser plugins, but this induces high implementation costs when multiple browsers have to be supported. Another possibility are embedded objects like flash or Java applets to communi- cate with external applications. This has been successfully
 SMI Hardware
 Tobii Hardware
 Other Hardware So ware
demonstrated by the Text 2.0 Framework [2]. A drawback of this solution is the inclusion of alien objects as interme- diates into the browser, thus limiting the flexibility in terms of configuration setups and security issues.
Therefore we propose to use HTML5 WebSockets [8] as communication means between an eye tracker and the Web browser. One reason for integrating WebSockets into browsers was to enable direct bidirectional communication between clients and servers with low latency. Optimal requirements for the transfer of data between an eye tracker and a browser. Another benefit is that the WebSocket protocol handles the security issues of network connections and data transfer. Since the HTML5 WebSocket API is a new web technology, it is not supported by old browsers. However, since many Web browsers are supporting WebSockets in their latest ver- sion [29], most people will have access to this technology.
An adapter is needed to bridge between the proprietary API of an eye tracker system and the WebSocket API as shown in the ITrackServer part of figure 1. Since we have an inter- process communication or even a network communication, we need to define a simple custom protocol for transferring eye movement data and properties from the eye tracker to the Web browser and vice versa remote commands in or- der to control the eye tracker from the Web browser. All data is passed on in the JSON format, because it can be efficiently parsed by Web browsers and in many program- ming languages. This protocol has been used as sub proto- col for the WebSocket API, but it can also be used for any other communication technology between an application and the eye tracker. This keeps our approach flexible enough to adapt for future Web technologies, and allows other appli- cations to connect to the eye tracker, e.g. by using a TCP network connection.
Given the custom protocol used as WebSocket sub protocol, adapters between a Web browser and any eye tracker system can be implemented. Our reference implementation is able to mediate between Web browsers and the SMI RED eye tracking system [25]. It consists of a separate application called iTrackServer. The iTrackServer was implemented in C++ and is using modular and object oriented concepts to easily extend the application by new eye tracker hardware.
4. GAZE INTERACTION EVENTS
Modern Web applications with dynamic user interfaces heav- ily rely on JavaScript and event driven design. Every HTML element in the source of a Website can be connected to event handlers. An event handler is a function that is called if a specific event is triggered. For example, one can connect a click handler to a button, so that the handler is called when the button is clicked with the mouse [17].
Every Web browser supports a standard set of possible events (with some variations between Web browsers though). They can be roughly classified into two groups: input events and system events. The input events are typically composed of mouse and keyboard interactions, while the system events are triggered from the Web browser, e.g. when the down- load of a Website has finished or the focus of an element has changed.
A mouse is an input device that transfers the movement of the hand of a user to the computer. Appropriate move- ments are applied by the computer to the mouse pointer on the screen. In combination with the mouse buttons, this en- ables the user to select or activate elements on the screen by moving the mouse pointer to a specific location and pressing a mouse button. A similar behavior can be considered for the eye gaze of a user. The movement of the pupils of the eyes of a user is recognized by the eye tracker and trans- formed into the screen position the user is looking at. It seems reasonable to consider both interactions as congruent in respect to the cursor movement.
Mouse events are among the most common events occur- ring in Web browsers, including click, dbclick, mousedown, mouseup, mouseover, mousemove and mouseout events and additionally the mouseenter and mouseleave events. The first four events are triggered if a user pushes a mouse but- ton. The events mousedown and mouseup allow a more sophisticated way to recognize click events, e.g. for the sup- port of Drag & Drop. The remaining events are triggered if the mouse pointer changes its position. The events mouseen- ter and mouseleave are separately listed, because they do not belong to the W3C Standard. We propose to use sim- ilar events for eye tracking and therefore we have defined following equivalent movement events: gazeOver, gazeMove, gazeOut, gazeEnter and gazeLeave. We have also added the gazeEnter and gazeLeave events for the sake of the Web developers’ habits.
Despite all the similarities between the mouse input device and the eye tracker, there are still many fundamental differ- ences. First, there is a difference in the accuracy between the two devices. A standard mouse device has a very high accuracy as opposed to the eye tracker, which is limited by the hardware used. There are additional factors influencing the accuracy of the eye tracker. One factor is the calibration of the system that varies from session to session. Without a calibration, only gross predictions of the gaze can be de- rived from the available data. Another problem is the posi- tion of the user: it changes over time, which also influences the accuracy of the tracked data. We also have to consider that the eye tracker produces a continuous stream of eye data samples, while the mouse device only produces data if the mouse has really been moved. This results in a con- tinuous triggering of the gazeMove event and under some circumstances in a poor performance of Web applications. Finally, a mouse provides click events. The eye tracker does not have any immediate counterpart for these interactions. However, approaches exist, to use eye blinks or gaze fixations in combination with dwell-time to simulate a click event[10]. We have added fixation events (fixationStart/-End) as more higher level events that can be derived from the basic gaze samples to simulate click events.
4.1 Implementation
Since we are able to receive eye tracker data samples from an eye tracker within the browser in real-time by using the WebSocket technology, we can use the data to generate in- teraction and custom events. For accessing the data in the Web browser we have created the JavaScript library iTrack. The library is composed of three subparts. First, a commu- nication interface similar to the one of the iTrackServer has
been created. With this interface JSON messages can be sent and received via WebSocket.
The second subpart is the eye tracker controller. It can open a connection to an eye tracker via the communication inter- face and interpret the custom protocol. Via the protocol the controller can remotely control the eye tracker, activat- ing the eye sample streaming or setting custom properties of the eye tracker. The controller provides an event interface to connect any function to a basic set of events: eyeUp- date, fixationStart/-End and propertyChange. The first two events are triggered when a new eye sample or a detected fixation from the eye tracker arrives. The remaining event is triggered if some configuration, state or property of the eye tracker has changed.
The last subpart is the event engine which generates the ba- sic set of gaze events: gazeOver, gazeMove, gazeOut, gazeEn- ter, gazeLeave and fixationStart/-End. The event engine is connected to the eye tracker controller events and uses the received data to generate the events on object-level. To re- duce the implementation overhead and to be cross-browser compatible, the event system of the jQuery JavaScript li- brary was used to realize the generic eye tracker events. Thus we can easily trigger these events on any element in the DOM tree. The event handling is the same procedure as for all other native browser events using the jQuery event system.
Since the gaze positions provided by the eye tracker are global screen coordinates, an additional adaptation is re- quired if the browser is not running in full screen mode to convert the eye tracker coordinates to the client coordinates. After the coordinate adaption is applied, the JavaScript function elementFromPoint can be used to find the under- lying element of the given position. Using this process, the event engine is able to trigger the corresponding gaze events for the given element.
4.2 Client Coordinate Evaluation
To apply the client coordinate adaptation, the screen po- sition of the client window, containing the content of the Webpage, has to be known. Until now, no native JavaScript function exists to acquire this information.To overcome this problem we used a mouse button click event. The mouse button click event provides as parameters the click position as both screen and client coordinates. Combining this infor- mation with the page offset (for scrolling), we can exactly compute the screen coordinates of the content window of the Website.
To evaluate the correctness of the client coordinate adap- tation, we have visualized the position of the gaze in the browser and the screen position by using the Experiment Center tool from the SMI eye tracker system as shown in figure 2. The experiment has shown that the screen posi- tion visualized by Experiment Center exactly corresponds to the client position visualized by the Web browser.
5. ADAPTIVE LEARNING ENVIRONMENT
We have built an adaptive eLearning environment extension that is based on the open source eLearning platform ILIAS [13]. This extension allows us to conduct the planned empir- ical studies within the Adaptable and Adaptive Multimedia Systems project. The extension is divided into two parts: The authoring environment for the creation and editing of the content for the lessons and the learning environment for the presentation of the created lessons to the learner.
Figure 3: This picture shows the integrated author- ing environment in ILIAS.
The authoring environment is directly built into the ILIAS platform. It uses the same concepts for creating and manip- ulating content as ILIAS for its own content objects. This reduces the barrier for authors that are familiar with the ILIAS platform. The authoring environment allows for the creation of modular lessons or learning units based on text, image, audio and video media types as shown in figure 3. A learning unit can contain an arbitrary number of content objects. Every learning unit additionally contains a set of properties for customization (e.g. choosing templates or se- lecting default content). To manage multiple lessons, the in- tegrated administrative objects of ILIAS, such as categories, folders and courses, can be used. For exchange between local
  Figure 2: The left picture shows the gaze visualiza- tion with adapted screen coordinates, the right pic- ture without. The green rectangle is visualized by the Web browser in client coordinates and the cir- cle by the SMI Experiment Center in global screen coordinates.
systems, import and export functionality has been added.
The learning environment or learning view presents the con- tent to the user. The layout is subdivided into three parts:
Navigation area Depending on the chosen template and settings of the author, a linear navigation (see B of figure 4) and a navigation tree (see A of figure 4) are displayed.
Content area This area is composed of slots or subareas where the content objects of the learning unit are dis- played (see C of figure 4). Depending on the author‘s settings, different kinds of layout with a varying num- ber of slots can be chosen.
Media shelf If not deactivated by the author, the media shelf shows all content objects of a learning unit as small icons (see D of figure 4). The mouseover event will show a preview of the content. Using Drag & Drop technology, an icon can be dragged to any slot in the content area and dropped to dynamically place the content into the slot.
Figure 4: This picture shows the learning environ- ment of the ILIAS plugin. A: Navigation tree; B: Linear navigation; C: Content area; D: Media shelf
The look and feel of the learning view can be adapted by templates supplied by authors. The view is implemented as Web application, dynamically loading the content via AJAX technology from the server. We have added eye tracker support by including our iTrack JavaScript library to the presentation view. Additionally tools for creating areas of interest within images and the DOM tree have been imple- mented. In combination with some analytic tools complex interaction relationships can be easily modeled. Thus the project can now conduct its psychological experiments, an- alyzing to what extent the eye tracking can be used to give the user feedback on relationships between different content elements, and to provide explicit hints about information the user may have missed.
6. DISCUSSION
As mentioned ealier, there are justifications for the use of an eye tracker as an input device, at least for people with
disabilities, because this technology is one of the few that can enable people with disabilities to interact with the com- puter. Since the eye tracker technology is currently still too expensive for the mass market (including most users with disabilities), the main application area is in (market) research. However, plans of companies producing an eye tracker system already exist to prepare this technology for the mass market by installing small eye tracker systems into normal notebooks [12, 19]. A further development can be found in free software that converts low cost hardware (e.g. Web cam) into eye tracker systems [1]. Unfortunately, the current low cost eye trackers suffer from low precision, much worse than the professional systems.
In combination with an eLearning environment, eye track- ing can certainly offer some benefits [6]. It can be used not only as an alternative for the mouse input in terms of In- die UI, but can also provide information about the learning condition of a user. Such information may include informa- tion on the cognitive load, boredom or learning progress of a user. First attempts in this direction are currently being researched in parallel pro jects.
7. CONCLUSION AND FUTURE WORK
Using eye tracker systems as input devices provides new possibilities for the creation of new interaction designs for applications. Eye tracking can substitute classical point- ing devices like a mouse to some extent, but also requires new interaction patterns. In this paper we have presented a framework that allows us to use an eye tracker system as input device for Web sites. Thus we can easily create new Web applications using new interaction design concepts. An adaptive eLearning environment was developed based on the eLearning platform ILIAS, including our framework. It is possible to let the user interact with the eLearning environ- ment by eyes, and to derive more complex information about the user by analyzing the eye movement.
We are planning to extend our framework by supporting other eye tracker systems and adding a filter interface to the iTrackServer to apply and test different approaches of preci- sion optimization algorithms for the eye tracker data and to search for patterns indicating special events. Additionally, we want to extend the iTrack JavaScript library to support calibration and validation within the Web browser. No ex- ternal application for calibration will be required, and re- calibration within a session can easily be performed without changing the application. As an additional side effect, the screen to client coordinate adaptation is not required any- more, since the eye tracker will be calibrated to the browser window and not to the whole screen.
In future studies we aim to find patterns and indicators in the eye tracker data to predict more sophisticated events like read or search, allowing new interactions, e.g. for auto scrolling while reading a document. We also plan to use machine learning techniques to find interaction patterns of users (also using mouse or keyboard input), which we could possibly use for user interface adaptations or for predicting some kind of user state, e.g. boredom or cognitive overload. Finally, we plan to analyze the usage of eye tracking for Indie UI.

8. ACKNOWLEDGEMENT
This work has been funded by the ScienceCampus of the Knowledge Media Research Center Tu ̈bingen [16].
References
[1] J. S. Agustin, H. Skovsgaard, E. Mollenbach, M. Barret, M. Tall, D. W. Hansen, and J. P. Hansen. Evaluation of a low-cost open-source gaze tracker. In Symposium on Eye-Tracking Research & Applications, pages 77–80, New York, 2010. ACM Press.
[2] R. Biedert, G. Buscher, S. Schwarz, M. Mo ̈ller, A. Den- gel, and T. Lottermann. The text 2.0 framework. In Workshop on Eye Gaze in Intelligent Human Machine Interaction, pages 114–117, New York, 2010. ACM Press.
[16] S. Pfeiffer. Sciencecampus. http://www. wissenschaftscampus-tuebingen.de/www/en/index. html?ref=folder5, Nov. 2011.
[17] T. Pixley. Document object model events. http://www. w3.org/TR/DOM-Level-2-Events/events.html, Nov. 2011.
[18] K. Rayner. Eye movements and cognitive processes in reading, visual search, and scene perception. In J. M. Findlay, R. Walker, & R. W. Kentridge (Eds.), Eye Movement Research: Mechanisms, Processes, and Ap- plications, pages 3–21, 1995.
[19] redOrbit.com. Tobii unveils eye-tracking laptops. http: //www.redorbit.com/news/technology/2004501/ tobii_unveils_eyetracking_laptops/index.html, Mar. 2011.
[20] J. Resig. jquery. http://jquery.com, Nov. 2011.
[21] D. D. Salvucci. Mapping eye movements to cognitive processes. Doctoral Dissertation, Department of Com- puter Science, Carnegie Mellon University, 1999.
[22] D. D. Salvucci and J. R. Anderson. Tracing eye move- ment protocols with cognitive process models. In Con- ference of the Cognitive Science Society, pages 923–928, Hillsdale, N.J., 1998. L. Erlbaum.
[23] D. D. Salvucci and J. H. Goldberg. Identifying fixations and saccades in eye-tracking protocols. In Symposium on Eye Tracking Research and Applications, pages 71– 78, New York, 2000. ACM Press.
[24] J. Scho ̈ning, A. Kru ̈ger, and P. Olivier. Multi-touch is dead, long live multi-touch. In Workshop on Multi- touch and Surface Computing, pages 1–5, New York, 2009. ACM Press.
[25] SMI. Red eye tracker. http://www.smivision. com/en/gaze-and-eye-tracking-systems/products/ red-red250-red-500.html, Nov. 2011.
[26] D. M. Stampe and E. M. Reingold. Selection by look- ing: A novel computer interface and its application to psychological research. J. M. Findlay, R. Walker, & R. W. Kentridge (Eds.), Eye Movement Research: Mech- anisms, Processes, and Applications, 19:467–478, 1995.
[27] P. Suppes. Eye-movement models for arithmetic and reading performance. E. Kowler (Ed.), Eye Movements and their Role in Visual and Cognitive Processes, pages 455–477, 1990.
[28] C. Ware and H. H. Mikaelian. An evaluation of an eye tracker as a device for computer input. In Conference on Human Factors in Computing Systems and Graphics Interface, pages 183–188, New York, 1987. ACM Press.
[29] Wikipedia. Websocket. http://en.wikipedia.org/w/ index.php?title=WebSocket&oldid=459013040, Nov. 2011.
[3] M. Cooper.
group charter. intentional-events-charter, Nov. 2011.
Intentional events working
http://www.w3.org/2011/08/ [4] M. Cooper. Indie ui working group charter. http://
www.w3.org/2011/11/indie-ui-charter, Feb. 2012.
[5] H. Drewes. Eye Gaze Tracking for Human Computer Interaction. Doctoral Dissertation, Media Informat- ics Group, Ludwig-Maximilians-Universita ̈t Mu ̈nchen, Germany, 2010.
[6] H. Drewes, R. Atterer, and A. Schmidt. Detailed moni- toring of user’s gaze and interaction to improve future e- learning. In Conference on Universal access in Human- Computer Interaction: Ambient Interaction, pages 802– 811, Heidelberg, 2007. Springer-Verlag.
[7] F. Hesse and S. Schwan. Knowledge media research center. http://www.iwm-kmrc.de/www/en/index.html, Nov. 2011.
[8] I. Hickson. The websocket api. http://dev.w3.org/ html5/websockets/, Nov. 2011.
[9] T. E. Hutchinson, K. P. White, W. N. Martin, K. C. Reichert, and L. A. Frey. Human-computer interaction using eye-gaze input. IEEE Transactions on Systems, Man, and Cybernetics, 19:1527–1534, 1989.
[10] R. T. K. Jacob. The use of eye movements in human- computer interaction techniques: what you look at is what you get. ACM Transactions on Information Sys- tems (TOIS), 9:152–169, 1991.
[11] M. A. Just and P. A. Carpenter. A theory of reading: From eye fixations to comprehension. Psychological Re- view 87, pages 329–354, 1980.
[12] O. Kharif. Eye-tracking technology for the masses. Bloomberg Businessweek, Mar. 2011.
[13] M. Kunkel. Ilias open source e-learning. http://www. ilias.de, Nov. 2011.
[14] M. F. Land and D. N. Lee. Where we look when we steer. Nature, (369):742–744, 1994.
[15] D. Noton and L. Stark. Scanpaths in saccadic eye move- ments while viewing and recognizing patterns. Vision Research, 11:929–942, 1971.
0,352       0D\              2SDWLMD  &URDWLD
Implementing Responsive Web Design for Enhanced Web Presence
S. Mohorovičić
University of Rijeka, Faculty of Maritime Studies, Rijeka, Croatia sanja@pfri.hr
Abstract - Modern companies, institutions, organizations, individuals, etc. have websites in order to extend their reach to audience or customers. However, it is not sufficient anymore just to have an appearance on web and to be recognized through various web search engines. People are increasingly using smartphones and tablets for accessing the Internet, not just desktop personal computers and notebooks, therefore websites need to be optimized for all these devices in order to provide the best user experience. Responsive web design provides a website with a flexibility to adapt to any of these devices, i.e. their resolutions. The paper presents statistics and predictions of market trends regarding the devices and user experiences in web browsing and m-commerce. Responsive web design is researched, along with its benefits and potential problems.
I. INTRODUCTION
A modern website is a must have tool for any company to increase its visibility towards potential customers. It is common for companies, institutions, organizations and individuals to have websites to reach audience or customers. However, it is not enough just to be present on web and available through web search engines anymore. People are spending increasing amount of time online and they are increasingly using smartphones and tablets for accessing the Internet, so websites need to be optimized for all these devices in order to provide the best user experience. Besides various screen sizes and resolutions, different web browsers and platforms, some differences also exist in the ways users interact with their devices: using a mouse, touching the screen or making movements.
Internet became accessible virtually to anyone and anywhere. However, a number of websites are still not optimized and viewable for all the devices that can be used for web browsing, mainly due to the technology used for website creation.
While some web technologies are near extinction (e.g. Flash), new technologies and web standards are in their rise (e.g. HTML5 and CSS3) to allow websites better adjustment and visibility.
In several articles and popular sites [1] [2] [3] that discuss web design trends, year 2013 has been declared as the year of responsive web design. Predictions are that such web design will be one of the biggest marketing trends in 2013. Responsive web design is a new web design approach that enables flexibility of a website to adapt to any device.
II. WHY DIFFERENT DESIGN IS NEEDED?
Following research results and predictions indicate that a change in traditional web design is needed to meet the market needs.
A. Device Market
According to Gartner's research report with shipments projections for smartphones, tablets, ultramobiles and PCs from 2012 to 2017 (Table I) [4], the combined shipments of PCs, tablets and mobile phones is expected to grow 9% to total 2.4 billion units in 2013 from 2012. In 2012, more tablets and smartphones were sold than PCs. Sale of desktop PCs and notebooks is expected to decline 7.6 % in 2013, while the shipments of tablets and smartphones are expected to grow. The most significant growth is projected for tablets: worldwide shipments are predicted to 197 million units in 2013, which is a 69.8% increase from 2012 shipments of 116 million units.
Device shipments are predicted to continue their growth, reaching more than 2.9 billion units in 2017.
With time, besides having the greater number of total devices, the proportion of particular devices will significantly change: there will be less PCs and more tablets and smartphones. There is a significant shift from PCs to mobile devices in these projections.
Similar research was conducted by International Data Corporation (IDC) [5]. Table II shows their results and predictions. They expect that tablet shipments will surpass desktop PC shipments in 2013 and notebook shipments in 2014.
TABLE I.
WORLDWIDE DEVICES SHIPMENTS BY SEGMENT, 2012- 2017 (THOUSANDS OF UNITS) [4]
  Device Type
  2012
 2013
  2014
  2017
 PC (Desk-Based and Notebook)
 341,263
 315,229
  302,315
 271,612
  Ultramobile
 9,822
 23,592
  38,687
 96,350
  Tablet
 116,113
 197,202
  265,731
 467,951
  Mobile Phone
  1,746,176
 1,875,774
  1,949,722
 2,128,871
 Total
 2,213,373
  2,411,796
  2,556,455
 2,964,783
             1206
TABLE II. SMART CONNECTED DEVICE MARKET BY PRODUCT CATEGORY, SHIPMENTS, MARKET SHARE, 2012-1017 (UNITS IN MILLIONS) [5]
III. MOBILE-FRIENDLY WEBSITE CREATION APPROACHES
Content on the web should be accessible everywhere, anytime and with any device: personal computers (desktops and netbooks), smartphones, tablets and televisions. How can it be achieved?
One approach to create mobile-friendly website is to create separate (two or more) versions of the same website, e.g. one for desktop computers, one for mobile phones and one for tablets. For each type of site, different URL is given. If a website, which has a mobile and a desktop version, is accessed on tablet, a mobile version of the website will most likely be loaded. Tablets are bigger than smartphones and that version of website is not optimized for them. When having multiple versions of websites, updates need to be executed on more than one place, thus a greater possibility for errors exists.
In most cases, a better approach would be to create one website based on responsive web design principles that works correctly on all devices and resolutions that exist today, and is also ready for the future devices. Once the code is written, it can be run on any device, which is a better option than building separate websites for each device. "A huge trend in 2013 will be that most e- commerce retailers will be using responsive web design." [14]
IV. RESPONSIVE WEB DESIGN – KEY FEATURES
The term "responsive web design" was first used and explained by web designer Ethan Marcotte in 2010 [15]. "Responsive web design is the approach that suggests that design and development should respond to the user’s behavior and environment based on screen size, platform and orientation." [16]
Responsive web design implies a different way of thinking, i.e. a slight change in web design philosophy. A responsive web page has one URL, one HTML code and its content is shown according to the defined CSS3 media queries on multiple devices (desktop PCs, notebooks, smartphones, tablets and televisions). It will automatically scale and adjust content to various screen sizes. The aim is to achieve readability and navigation on any device with minimum of resizing and scrolling.
Key features of responsive web design are:
            2012 Market Share
              2017 Market Share*
      2012— 2017 Growth*
  Product Category
    2012 Unit Shipments
  2017 Unit Shipments*
      Desktop PC
     148.4
        12.4%
   141.0
        6.0%
     -5.0%
      Portable PC
     202.0
        16.8%
   240.9
         11.0%
     19.3%
      Tablet
         128.3
         10.7%
        352.3
           16%
          174.5%
       Smartphone
   722.4
    60.1%
 1,516
    67%
  109.9%
      Total
     1,201.1
        100.0%
    2,250.3
          100.0%
      87.3%
   * Forecast estimates
IDC's and Gartner's results show small differences in their projected numbers but the trends of device market changes are common in both their projections.
B.
Web Browsing, M-commerce and User Experiences
Web browsing through mobile devices has covered 10.11% of website page views in May 2012 [6]. Mobile internet usage continues to grow. According to TechCrunch [7] and Gartner [8], more people globally will have access to the Internet through mobile devices rather than through desktop computers in 2013. IDC predicts that by 2015, more Internet users will be accessing the Internet through mobile devices than through PCs in the U.S. [9].
Consumers use a variety of devices to reach different goals on the Internet (e.g. browsing the Internet, shopping online, managing finances). Google's research shows that 90% of people use multiple screens sequentially [10], i.e. move between devices.
Since the increasing number of people own mobile devices, more and more companies adapt to mobile Internet usage. By 2016, mobile commerce is expected to grow to 5 times of its current size [8].
According to Google's research of users’ expectations
and reactions towards their mobile website experiences, 67% of users are more likely to buy a product from a mobile-friendly site so mobile-friendly sites can turn users into customers [11]. Mobile users are five times more likely to abandon the task if the site isn't optimized for mobile usage and 79% of mobile users will search for another site to complete the task [8]. If a company doesn't have a mobile-friendly site, both company's reputation and potential profit are affected, and users will turn to competitors' site.
More statistics about mobile commerce, mobile web, mobile marketing, consumers, mobile payment, etc. can be found in [12] and [13].
x x x
a flexible (fluid) grid, flexible images and CSS3 media queries [17].
1207
Flexible grid consists of columns expressed in relative widths (e.g. percentages, ems), as proportions of their containing element, rather than in fixed, inflexible pixels. The grid is resizing as the viewport (the area available for viewing the web page) is changing.
Flexible images move and scale proportionally (shrinking or enlarging) as their flexible container resizes. Besides the approach of resizing images according to the screen size, there is also an option to create multiple
versions of an image for different resolution ranges or an option to crop images.
Media queries are a module from the CSS3 specification that allow building multiple layouts using the same HTML documents. They are conditional statements that can identify not only media types (screen), but can also inspect the physical characteristics of the device and the browser that render targeted web page, e.g. the browser width, orientation. Style sheets are selectively served based on the device and browser features. Each media query has two components: a media type and the query. The query consists of the name of the feature and a corresponding value. All web browsers support CSS3 media queries. Some older web browsers lack media query support, but there are some alternative solutions that could be used for those particular browsers (JavaScript).
The most commonly used media feature is width. Figure 1 shows an example of a simple media query. Using max-width and min-width for checking resolution ranges below or above the certain breakpoints, enables conditional usage of parts of CSS designed for those ranges. Breakpoints are moments, i.e. certain pixel width ranges on which one responsive size range with corresponding layout changes to another [18]. Usually, browser's viewport dimension is taken into account, rather than device's screen size.
As the web browser width is changing, style, layout and proportions of the website content change as well so users can enjoy a seamless experience on any device [19]. Figure 2 shows a responsive website on several devices.
approach to building websites for today’s multi-screen world" [20]. The process of responsive website creation is less linear than traditional website creation. Designing and building a website based on responsive design involves more collaboration between designers and other team members (e.g. front-end developers) than before. Unlike traditional website designs, workflow of responsive web design is a somewhat different. Main phases (planning, design, development and delivery) are interlacing and iteratively repeating.
V. BENEFITS OF RESPONSIVE WEB DESIGN
Benefits of the implementation of responsive web design can be divided in three categories regarding to whom they are referred: benefits for webmasters, benefits for developers and benefits for end users.
Key benefits of implementation of responsive web design are one, content focused, device-independent website, long-term money and time savings, easy maintenance, better Search Engine Optimization (SEO) managing [21], more consistent user experience and usability.
Responsive websites have a single URL (one web) serving the same HTML for all devices so maintenance and updating content is easy. Changes and content editing can be made in one place (e.g. through a single CMS), unlike updating a separate desktop and mobile site. Having one URL is very important for sharing on various social networks, especially since social networks are often accessed through mobile devices. In that way, visitors can go directly to a website regardless of the device they are using.
Google recommends the industry best practice of using responsive web design [22], which is another reason for implementation.
With increasing number of devices and platforms for web browsing, building the site based on responsive design will maintain flexibility and better user experience. Responsive site enables consistency, keeping the same look and feel throughout all devices. Having a better user experience as a competitive advantage, more customers will become loyal, improving company's market share [23].
A.
Mobile First and Desktop First Responsive Web Design
There are two approaches for implementation of responsive web design. In mobile first and desktop first responsive web design approaches, designing starts at reference resolutions and with use of media queries adapting to other resolutions. Main resolution breakpoints and a content that will be shown on a web page in various ranges of resolutions should be defined.
In mobile first approach, by default, a layout appropriate for smaller screens is defined and then the design is progressively enhanced as the resolution increases. On the other hand, desktop first methodology takes desktop resolution as starting point and gracefully degrades design as the resolution decreases.
Modern websites are being designed for more devices and more resolutions than ever before. Responsive web design is "the only durable, flexible and future-proof
Figure 1. An example of a simple media query
Figure 2.
The Boston Globe's website loaded on several devices; an example of responsive web design
  1208
 VI. DOWNSIDES AND POTENTIAL PROBLEMS
Responsive web design projects are more time consuming to create and will cost about 10-20% more upfront than regular websites [24]. However, there is no need for a separate mobile site.
Some mobile devices and web browsers are not compatible with CSS media queries so alternative solutions should be taken into account for those devices. Web browsers are not uniformly supporting new web standards. Some sites have complex modules that won't function or would be hard to use on smaller devices. Responsive images are an unsolved problem of responsive web design and there have been many attempts to solve it [25]. One solution could be serving lower size images to smaller screens.
Most users expect a web page to load in less than 4 seconds [26]. Slow loading of a web page on mobile devices is the biggest performance problem for many responsive sites, mainly because of over-downloading (e.g. download and hide or shrink images, extra CSS download). Larger or unnecessary images can impact loading time. Page size and HTTP requests should be reduced for faster page load and better performance.
Guy Podjarny has conducted performance test on 347 responsive websites in March 2012 and 2013 [27]. Despite a fact that website changes its look across different screen sizes, the weight and load time of the website hardly changes. Figure 3 shows differences between page size on smallest and largest screens in the test conducted in 2013. Despite a fact that mobile version of responsive site was loaded on smallest screens, majority of the sites have roughly the same size (72%) on the smallest and on the largest screens. Comparing the results from 2012 and 2013 tests, the average page in 2012 weighted only 6% less on a small screen than on a large screen, compared to 9% in 2013 (Figure 4). The results are not good, but a slight improvement is evident.
During the designing and deployment process, the performance should be kept in mind, and website should be kept as lightweight as possible to ensure a good user experience [28].
Responsive web design is not so suitable for the
Figure 4.
Page size, results from 2012 and 2013 [27]
 Figure 3.
Page size, smallest resolution vs. largest resolution [27]
advertisers. It is harder to place banner advertisements within a responsive design [29] than in fixed width design.
In some cases, it is better to have a separate desktop and mobile site than having one responsive site. For most organizations, implementation of responsive web design will yield improved results and long-term savings and with other benefits will outweigh any negativity [30].
VII. CONCLUSION
It is hard to keep up with the different devices and resolutions on the market. Responsive web design adapts the web page to different screen sizes and it is also prepared for the future-devices that haven't been released yet. Along with greater number of mobile devices, the importance of responsive web design is also increased.
Mobile devices are changing the way that commerce works. It is important for a business or a commerce website to be optimized for optimal viewing experience, reading and navigation with minimum of resizing. Implementation of responsive web design can result with greater number of visitors, increased sales and customer satisfaction.
More work should be done in defining standards and best practices of responsive web design and finding the ways for improvements (e.g. better responsive web design performance and responsive images).
There are many examples of successful implementation of responsive web design, and its implementation increases on daily basis. Responsive web design is becoming the standard of present web design. It can be concluded that responsive web design has a promising future, and will continue to develop.
REFERENCES
[1] P. Cashmore, Why 2013 Is the Year of Responsive Web Design, 2012, http://mashable.com/2012/12/11/responsive-web-design
[2] J. A. Sanchez, Infographic: 2013 The Year of Responsive Design,
2012, http://www.uberflip.com/blog/infographic-2013-the-year- of-responsive-design
[3] J. Krenz-Kurowska, Web design trends for 2013, 2013, http://99designs.com/designer-blog/2013/02/21/web-design- trends-for-2013/
[4] Gartner Says Worldwide PC, Tablet and Mobile Phone Combined Shipments to Reach 2.4 Billion Units in 2013, 2013, http://www.gartner.com/newsroom/id/2408515
1209
[5] Worldwide Smart Connected Device Market Crossed 1 Billion Shipments in 2012, Apple Pulls Near Samsung in Fourth Quarter, According to IDC, 2013, http://www.idc.com/getdoc.jsp?containerId=prUS24037713#.UW G9CZNTB8E
[6] Mobile Internet and The Death of Desktop, http://shoutoutstudio.com/mobile-internet-takeover/
[7] Infographic: Responsive Web Design, http://www.artofdeveloping.com/2012/09/infographic-responsive- web-design.html
[8] M. Bishop, The Post-PC Revolution is Here – Don’t Panic!, 2013, http://blog.moovweb.com/2013/02/the-post-pc-revolution-is-here- dont-panic/
[9] IDC: More Mobile Internet Users Than Wireline Users in the U.S.
by 2015, 2012, http://www.businesswire.com/news/home/20110912005213/en/ID C-Mobile-Internet-Users-Wireline-Users-U.S.
[10] The New Multi-Screen World, 2012, http://services.google.com/fh/files/misc/multiscreenworld_final.pd f
[11] M. Fisch, Mobile-friendly sites turn visitors into customers, 2012, http://googlemobileads.blogspot.com/2012/09/mobile-friendly- sites-turn-visitors.html
[12] Industry Statistics, http://www.digby.com/mobile-statistics/
[13] Global mobile statistics 2012 Home: all the latest stats on mobile Web, apps, marketing, advertising, subscribers, and trends, http://mobithinking.com/mobile-marketing-tools/latest-mobile- stats
[14] M. Chowdhary, Responsive Web Design Will Be Expected On All Devices, 2012, http://multichannelmerchant.com/crosschannel/responsive-web- design-will-be-expected-on-all-devices-15092012/
[15] E. Marcotte, Responsive Web Design, 2010, http://alistapart.com/article/responsive-web-design
[16] K. Knight, Responsive Web Design: What It Is and How To Use It, 2011, http://coding.smashingmagazine.com/2011/01/12/guidelines-for- responsive-web-design
[17] E. Marcotte, Responsive Web Design, A Book Apart, New York, 2011
[18] The Savvy Marketer's Guide to Responsive Web Design, 2012, http://www.enablingideas.com/wp-
[19]
[20]
[21]
[22] [23]
[24] [25] [26]
[27]
[28] [29]
[30]
content/uploads/SavvyMarketersGuide_ResponsiveWebDesign_A ug-7.12.pdf
S. Pastore, "The Role of Open Web Standards for Website Development Adhering to the One Web Vision", International Journal of Engineering and Technology, vol. 2, no. 11, 2012, pp. 1824-1834.
E. Savitz, The Future Of The Web: The Case For Responsive Design, 2012, http://www.forbes.com/sites/ericsavitz/2012/12/27/the-future-of- the-web-the-case-for-responsive-design/
J. Taylor, 3 Reasons Why Responsive Web Design is the Best Option For Your Mobile SEO Strategy, 2013, http://searchenginewatch.com/article/2253965/3-Reasons-Why- Responsive-Web-Design-is-the-Best-Option-For-Y our-Mobile- SEO-Strategy
Building Mobile-Optimized Websites, 2012, https://developers.google.com/webmasters/smartphone-sites/
S. Gamble, How Responsive Web Design Improves Customer Loyalty, 2013, http://www.sweettoothrewards.com/blog/2013/04/16/how- responsive-web-design-improves-customer-loyalty/
J. Mazzei, "Responsive Web Design", Business NH magazine, 10/2012, pp.22-23.
T. Kadec, Implementing Responsive design, New riders, Berkley, 2013
2012 Mobile User Survey, 2012, http://www.keynote.com/docs/reports/Keynote-2012-Mobile- User-Survey.pdf
G. Podjarny, Real World RWD Performance – Take 2, 2013, http://www.guypo.com/uncategorized/real-world-rwd- performance-take-2/
J. Wisniewski, "Responsive Design", Online searcher, jan/feb 2013, pp.74-76.
D.Olson,Choosingbetweenresponsivewebdesignandaseparate
mobile site to improve mobile visitors’ experience, 2013,
http://www.foraker.com/choosing-between-responsive-web- design-and-a-separate-mobile-site-to-improve-mobile- visitors%E2%80%99-experience/
M. Crist, A. Francesconi, A. Paradise, and G. White, 2012, Responsive Design Guide, http://cantina.co/wp- content/uploads/2012/01/ResponsiveDesignGuideFnll0110121.pdf
IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 12, NO. 5, SEPTEMBER/OCTOBER 2006 853
Software Design Patterns for Information Visualization
Jeffrey Heer and Maneesh Agrawala
Abstract—Despite a diversity of software architectures supporting information visualization, it is often difficult to identify, evaluate, and re-apply the design solutions implemented within such frameworks. One popular and effective approach for addressing such difficulties is to capture successful solutions in design patterns, abstract descriptions of interacting software components that can be customized to solve design problems within a particular context. Based upon a review of existing frameworks and our own experiences building visualization software, we present a series of design patterns for the domain of information visualization. We discuss the structure, context of use, and interrelations of patterns spanning data representation, graphics, and interaction. By representing design knowledge in a reusable form, these patterns can be used to facilitate software design, implementation, and evaluation, and improve developer education and communication.
Index Terms—Design patterns, information visualization, software engineering, object-oriented programming
   1 INTRODUCTION
As recognition of the value of visualization has increased and the demand for visual analytics software has risen, visualization researchers have developed numerous software frameworks to meet these needs. By changing the cost structure governing the design and implementation of visualizations, such frameworks carry the potential to lower barriers to entry and increase the space of feasible visualization designs. Still, there is never a single tool or framework that is appropriate for all problems in a given domain. Developers often migrate between tools (e.g., when developing on a new platform) or build their own systems (e.g., to achieve functionality not available elsewhere). In either case, an understanding of the design solutions employed within existing tools could aid the programmer in learning and evaluating other frameworks and furthering their own development efforts. However, inspection of source code and design documents, if available, can prove difficult and tedious. Descriptions in the research literature often place more emphasis on novel features than on recurring design patterns. As a result, it can be difficult to identify, evaluate, and re-apply the design solutions implemented within existing frameworks.
Similar issues permeate any discipline steeped in design. Originally developed by Christopher Alexander and his colleagues in architecture [2], design patterns have proven to be a useful means of capturing time-tested design solutions and facilitating their reuse. Patterns aim to explicitly represent design knowledge that is understood implicitly by skilled practitioners.
Perhaps nowhere has the pattern approach been more effective than in software engineering. Gamma et al. [13] describe software design patterns as “descriptions of communicating objects and classes that are customized to solve design problems within a particular context.” Such patterns document object-oriented software design solutions in a fashion independent of specific programming languages. Patterns typically consist of a name, a purpose, a description of when and why to apply the pattern, structural diagrams, examples of use, and a discussion of interactions with other patterns.
• Jeffrey Heer is with the Computer Science Division of the University of California, Berkeley. E-Mail: jheer@cs.berkeley.edu.
• Maneesh Agrawala is with the Computer Science Division of the University of California, Berkeley. E-Mail: maneesh@cs.berkeley.edu.
Manuscript received 31 March 2006; accepted 1 August 2006; posted online 6 November 2006. For information on obtaining reprints of this article, please send e-mail to: tvcg@computer.org.
Schmidt [18] has noted a number of benefits gained from incorporating design patterns into the development process. He found that patterns enabled widespread reuse of software architecture designs, improved communication within and across development teams, facilitated training of new programmers, and helped transcend ways of thinking imposed by individual programming languages. Schmidt also recommends that practitioners focus on developing patterns that are strategic to a domain of interest, while reusing general-purpose patterns (e.g., those of [13]) as much as possible— an approach we now adopt for the design of information visualization software.
Previous research has applied the design pattern approach to visualization problems. Stolte et al. [21] introduce design patterns describing different forms of zooming within multi-scale visualizations. Chen [7] takes a more ambitious approach, suggesting high-level visualization patterns addressing general visualization concerns. He lists patterns such as Brushing, Linking, and Encoder, the latter encompassing in a single pattern the visual encoding principles (e.g., use of spatial position, color, size, and shape) advocated by Bertin [4], Cleveland [10], Mackinlay [17], and others.
In contrast, this paper considers patterns at a lower level of abstraction, building upon the approaches of Gamma et al. [13] and Schmidt [18] to focus specifically on software design patterns for information visualization applications. We present a set of twelve design patterns that have proven themselves in existing visualization frameworks, spanning issues of application structure, data handling, graphics, and interaction. The patterns were chosen based upon a review of existing frameworks and our own experiences building prefuse [14], an open source toolkit for building interactive visualizations. As Schmidt warns [18], not everything should be cast as a pattern, even if it is possible to do so. We have attempted to select only those patterns whose recurrence and/or significance warrants their inclusion.
2 DESIGN PATTERNS
W e describe a set of twelve design patterns for information visualization software. Each pattern consists of a name, a summary description (in italics), and a more detailed description describing the context of use, examples, and relations to other patterns. Each pattern is also accompanied by a structural diagram that uses the conventions of Gamma et al. [13] to depict the classes and the relations between them, such as inheritance, reference, and aggregation. Figure 1 provides a legend for interpreting the structural diagrams. The structural diagrams do not specify full-blown implementations; they provide an abstracted view to communicate the essence of the pattern.
 1077-2626/06/$20.00 © 2006 IEEE
Published by the IEEE Computer Society
854 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 12, NO. 5, SEPTEMBER/OCTOBER 2006
 Figure 1. Structural diagram legend. Classes are depicted as boxes, potentially listing attributes and method signatures. Abstract classes are titled using italics. Arrows indicate relationships between classes. These relations include inheritance, denoted by a large empty triangle; object reference, denoted by an unadorned arrow; aggregation, denoted by a diamond base; and creation, denoted by a dotted, labeled arrow. A circle at the arrow endpoint indicates a 1-to-many relation. Notes, depicted as a box with a folded corner, are used to communicate implementation information in pseudocode.
1.1 Reference Model
Separate data and visual models to enable multiple visualizations of a data source, separate visual models from displays to enable multiple views of a visualization, and use modular controllers to handle user input in a flexible and reusable fashion.
Information visualization application development requires balancing issues of data management, visual mappings, computer graphics, and interaction. Determining the right separation of concerns has serious consequences for the complexity, extensibility, and reusability of software architectures. The Reference Model pattern provides a general template for structuring visualization applications that separates data models, visual models, views, and interactive controls.
Figure 2. The Reference Model Pattern. A visualization manages visual models for one or more data sets, separating visual attributes (location, size, color, geometry, etc) from the abstract data. One or more views provide a graphical display of the visualization, while control modules process user input and may trigger updates at any level of the system.
The structure of the Reference Model pattern is shown in Figure 2. A DataSource component, such as a formatted file reader or a database connectivity interface, loads data sets to be visualized. One or more data sets can then be registered with a visualization. This mechanism allows abstract data to be separated from visual attributes such as location, size, shape, and color, thereby allowing a single abstract data set to be used in multiple visualizations. A common approach is to create visual items: lightweight components representing an interactive visual object in the visualization.
The Visualization, View, and Control classes employ the standard Model–View-Controller (MVC) pattern [16] of user interface development. A visual model can be shown in one or more views, with user input processed by controls that can affect change at any level of the system. One way of interpreting this pattern is as a tiered version of MVC, with the model divided into separate abstractions for the data and visual properties.
The Reference Model pattern has been widely used and advocated. Both Chi et al’s data state model [8] and Card et al’s infovis reference model [6] proscribe the use of this pattern. In their exploration of design choices for architecting visualizations, Tang et al. [22] also discuss the importance of separating data and visual models. Finally, numerous software frameworks adopt this template of application structure, including Advizor [11], Improvise [23], Polaris [20, 21], prefuse [14], and SAS/JMP [7].
The Reference Model pattern provides a high-level template for application design whose implementation can be informed by other patterns. The DataSource components may use the Abstract Factory or Factory Method patterns of [13] to instantiate objects without
specifying concrete classes. Update notifications can be passed between objects using the Observer (or Listener) pattern [13]. For example, Views can listen for update events issued by the Visualization. Control objects represent components in a Strategy pattern [13], as each represents a strategy for handling user input that can be added or removed at run-time. Each of the remaining patterns presented in this paper also contribute to the implementation of this model.
1.2 Data Column
Organize relational data into typed data columns, providing flexible data representations and extensible data schemas.
The most common data representation used in visualization is the relational model or “data tables” [6], with a table row representing a single data record (also called a relation or tuple) and table columns representing individual data fields. The description of a table’s column structure, including each column’s name and its contained data type, is called a schema. Database management systems regularly store relational data in row-major order, as keeping the contents of a relation in a contiguous block helps minimize disk usage when evaluating queries. Information visualization systems, however, primarily manage data sets in main memory and are faced with the design decision of internally grouping data by row or by column.
A row-based approach has the advantage of treating data records as individual objects, enabling easy grouping and sorting of relations. A column-based approach, however, often simplifies data management. Columns can be added and removed more easily, and columns can be shared across tables, facilitating data reuse. Each column can implement its own data management policy to more efficiently store data. When representational flexibility and extensible schemas are needed, the Data Column pattern can be applied to implement column-based relational tables.
Figure 3. The Data Column Pattern. Relational data tables are implemented as a collection of column objects. Data columns encapsulate data representation and can be shared among tables, propagating value updates using the Observer pattern [13]. A factory pattern can be used to facilitate column creation.
The structure of the Data Column pattern is shown in Figure 3. Tables are represented as an aggregation of column objects. Column objects can be shared across multiple tables, with update notifications provided to each enclosing table. Data representation strategies can vary by column (an instance of the Strategy pattern [13]). For example, integers might be stored in an array, boolean values in a compact bit vector, while sparse data representations could be applied in situations of low data density. Furthermore, table schemas can be modified with little overhead, by simply adding or removing column instances from the table.
Tables can handle column access in multiple ways. Data columns can be referenced by number (using an array look-up) or by name (using an associative map). It is also useful to include a row manager, which tracks the currently occupied rows and, if needed, maintains a map between table row numbers and column row numbers. These numbers may differ if filtered tables are supported, a possibility discussed later (§2.3). A row manager can also be used to track added and deleted rows, supporting dynamic tables by reusing memory occupied by a deleted row.
The Data Column pattern has been applied in many visualization frameworks, including Advizor [11], the InfoVis Toolkit [12], prefuse [14], and SAS/JMP [7]. However, row-based frameworks such as Polaris [20] are more amenable to tasks such as sorting and

HEER ET AL.: SOFTWARE DESIGN PATTERNS FOR INFORMATION VISUALIZATION 855
grouping table rows. Creating sorted indices of column values relieves some of these issues. An additional solution is to apply the Proxy Tuple pattern (§2.5), creating an object-oriented interface to individual table rows. Finally, the Abstract Factory or Factory Method patterns [13] can be applied to instantiate polymorphic data columns based on criteria such as data type and sparsity.
1.3 Cascaded Table
Allow relational data tables to inherit data from parent tables, efficiently supporting derived tables.
In many cases, it is necessary to extend the contents of a table without modifying it. A motivating example is the creation of visual abstractions, which add visual attributes such as location, color, size, and shape to an existing data model. In accordance with the Reference Model pattern, a separation between the visual data and the original data should be maintained. Another example is the creation of small multiples displays, which might vary a single visual property such as color across each display. It is desirable to reuse the remaining properties across each of the display models and coordinate updates to these properties. The Cascaded Table pattern provides a solution to this problem for relational data by extending an existing table without modifying it.
Figure 4. The Cascaded Table Pattern. A cascaded table inherits values from a parent table instance. The cascaded table may manage its own set of data columns, potentially shadowing columns in the parent. Column references not found in the child table are resolved against the parent table.
The structure of the Cascaded Table pattern is shown in Figure 4. A CascadedTable subclasses Table and also maintains a reference to a parent table. If a requested column is not found in the child table, the request is forwarded to the parent. CascadedTables may contain data columns that shadow columns in the parent table. For example, adding a new column with the same name as a column in the parent will override access to the parent’s column. Update notifications are relayed from parent tables to child tables, achieving coordination between parent and child. By extending a parent table with additional data, the Cascaded Table pattern is an example of Gamma et al’s Decorator pattern [13].
Proper use of the Cascaded Table pattern requires a row manager that maps between the rows of the child table and its parent table. Such management is especially useful when cascaded tables provide filtered views of the parent. For example, clients might specify a filter predicate to limit the rows accessible from the cascaded table (see the Expression pattern in §2.6).
The Cascaded Table pattern is used extensively in the prefuse visualization toolkit [14] to form visual abstractions, decorating a data set with visual properties. The resulting cascaded table provides easy access to both visual properties and the underlying data through a unified interface. Cascaded tables are also applied to create derived visual tables that override only a subset of visual properties, enabling reuse both within a display (e.g., adding labels onto shapes) or across displays (e.g., the small multiples example discussed earlier).
1.4 Relational Graph
Use relational data tables to represent network structures, facilitating data reuse and efficient data processing.
Second to relational data tables, network structures such as graphs and trees are amongst the most common data structures used in information visualization. These structures are typically implemented in an object-oriented fashion, with node objects storing adjacency lists of connected node and edge objects. However, in
visualization frameworks this representation creates an incongruity between network and table data structures and sacrifices benefits of relational data management (e.g., optimized query processing). The Relational Graph pattern addresses the issue by implementing network structures using relational data tables. Relational graph structures allow the machinery of relational tables to be used on network data and enable a level of data reuse unsupported by the typical object-oriented model.
Figure 5. The Relational Graph Pattern. Network structures are implemented using relational data tables to represent node and edge data. Edge tables maintain foreign keys which reference incident nodes.
The structure of the Relational Graph pattern is shown in Figure 5. A network structure such as a graph or tree can be represented using a set of tables. Each table stores the various data attributes of nodes and edges in the network. The edge table additionally maintains columns referencing source and target nodes, storing identifiers (called foreign keys) that map into a node table. This model mirrors the standard format of linked structures within relational database management systems. Tree structures can enforce topological constraints on the edge table, allowing only parent/child relationships. Multiple node tables may be used to model partite graphs or nodes with varying data schemas. In these cases, separate edge tables can be used to record edges between each pair of node tables.
There are numerous advantages to the relational graph approach. First, it helps unify data representation issues in multi-purpose visualization frameworks. Machinery for processing and manipulating relational tables can be applied to network data, including query processing, creation of derived columns (e.g., using the Expression pattern, §2.6), and the use of Cascaded Tables (§2.3). The same node table can be reused across multiple graphs, while edge data can be swapped in and out of an existing graph. Furthermore, this representation directly maps to the format commonly used in database systems, facilitating visualization of network data stored in a relational format.
However, use of the Relational Graph pattern does introduce new obstacles. First is the issue of performance, particularly for traversing the graph structure. Properly indexing the key fields can significantly improve performance, but involves an overhead that may be unacceptable for large graphs. As a result, Relational Graph implementations often include acceleration structures. The InfoVis Toolkit [12] adds auxiliary columns to node and edge tables, maintaining references to “next” and “previous” edges and storing statistics such as node degrees. Prefuse [14] maintains a separate table within the Graph object, storing adjacency lists and node statistics. Finally, the Relational Graph pattern replaces the familiar object-oriented model of graphs, potentially complicating programming tasks. This deficit can be addressed using the Proxy Tuple pattern (§2.5).
1.5 Proxy Tuple
Use an object-relational mapping for accessing table and network data sets, improving usability and data interoperability.
While patterns such as Data Column (§2.2) and Relational Graph (§2.4) improve efficiency, reusability, and extensibility, they introduce abstractions more complicated than the naïve approaches they replace. In particular, working with table relations and graph nodes and edges as simple objects is a more familiar and intuitive model to many programmers. The Proxy Tuple pattern provides a solution to this problem, using an object-relational mapping to

856 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 12, NO. 5, SEPTEMBER/OCTOBER 2006
 provide a simple interface for accessing, manipulating, grouping, and reordering table and graph data.
The structure of the Proxy Tuple pattern is shown in Figure 6. Instead of accessing table or graph data directly, Tuple instances constitute a proxy object that allows programmers to work with rows, nodes, and edges as simple objects. A Proxy Tuple is a lightweight object storing its row number and references to backing data sets—a table and, if appropriate, a graph. Tuples also provide methods for accessing and manipulating underlying data. Tuple instances can be sorted irrespective of their ordering in the backing table, and can be aggregated and grouped as desired, even combining Tuples from different tables or graphs. Tuples can be created by and stored within a tuple manager associated with the backing table or graph. The manager initializes Tuples as needed and invalidates them when records are deleted.
Figure 6. The Proxy Tuple Pattern. Tuples provide an object-oriented proxy for accessing a row of table data. The Node and Edge subclasses play a similar role for graphs, also enabling traversal of the network structure.
Proxy Tuples can also be used to improve data safety. Consider the case of using an integer value to refer to a table row. If that row is deleted and then later repopulated by a newly inserted row, access using the stored row number will not result in an exception and operations may carry unexpected consequences. A Proxy Tuple, however, can be invalidated immediately upon deletion of its backing row, preventing future access through that object and thus promoting data consistency.
The Proxy Tuple pattern is used throughout the prefuse visualization toolkit [14] to manage both abstract data elements and visual items displayed in a visualization. By providing a simplified interface to a complex subsystem, Proxy Tuple applies the Facade pattern of [13]. Tuple managers associated with a table or graph can employ the Factory Method pattern [13] to control the concrete types of instantiated tuples.
1.6 Expression
Provide an expression language for data processing tasks such as specifying queries and computing derived values.
Many visualization frameworks require a basic level of database functionality. Either programmers or end-users may need to specify queries or calculate derived measures from existing data fields. By including an Expression language, a visualization framework can support such functionality in a general fashion that can be reconfigured at run-time.
The structure of the Expression pattern is shown in Figure 7. The pattern is a direct application of Gamma et al’s Interpreter pattern [13], customized for data processing needs. Language statements are represented as a tree of Expression objects that perform calculations over data elements. Common expression objects include literal values, arithmetic or comparison operations, and data field references. A Predicate is a special type of Expression that returns values of type boolean.
Figure 7. The Expression Pattern. Expression language statements are constructed as a tree of processing objects. Expressions perform calculations over data elements and return the result. Predicate expressions returning values of type boolean can be used to specify queries.
Invocation of an expression causes recursive invocation of sub- expressions, with resulting values propagating up the expression tree to calculate the final result. Expressions may be used to specify selection queries (as Predicate expressions), create derived data fields (e.g., using a concrete Data Column (§2.2) instance that refers to an Expression instance), or perform other data processing tasks. The structural diagram depicts Expressions that take a single tuple as input. Single tuple expressions are sufficient for a range of useful calculations, though expanded contexts may be desired, for example to compute joins across tables.
Operations upon the expression tree itself can also be of great use. For example, traversals of the tree can be used to identify referenced data fields, compute optimized query plans for a Predicate, or print a text representation of the expression (also useful for generating queries to external databases). Such operations could be encapsulated within objects in accordance with Gamma et al’s Visitor pattern [13].
There are multiple means of constructing an expression tree. Individual objects can be manually instantiated and assembled into the expression tree. An ExpressionParser can be provided to compile text strings written in a textual expression language into the tree of objects. This approach is used in both Polaris (now Tableau) [20] and prefuse [14]. Another possibility, used in the Improvise framework [23], is to graphically represent the expression tree and allow users to edit expressions in a direct manipulation fashion.
1.7 Scheduler
Provide schedulable activities for implementing time-sensitive, potentially recurring operations.
Dynamic visualizations often make use of time-sensitive, recurring operations. The most common example is animation, which requires updating visual properties and redrawing the display at regular time intervals, usually over a specified duration. Other examples include time-sensitive responses to user input and computations bounded to a specified time duration (e.g., an iterative graph layout). The Scheduler pattern provides a solution that enables time sensitive operations and supports extensibility.
Figure 8. The Scheduler Pattern. Activity instances are registered with a centralized scheduler that runs the activities over a specified time interval, repeatedly running the activity at requested time steps.

HEER ET AL.: SOFTWARE DESIGN PATTERNS FOR INFORMATION VISUALIZATION 857
The structure of the scheduler pattern is shown in Figure 8. Custom operations are created by subclassing the abstract Activity class and implementing the “run” method (an example of the Template Method pattern [13]). An Activity has a specified start time, a duration, and a step time defining the desired length of time to wait between repeated invocations. The run method takes as a single argument a fractional value between zero and one that indicates the progress of the Activity within its duration. This value can increase linearly as time elapses or can be modified through the use of a pacing function. Slow-in, slow-out animation can be achieved using a pacing function with a sigmoidal shape.
The Scheduler pattern has been used extensively in visualization and user interface frameworks. The Information Visualizer [5] included a “governor” that oversaw animation and adjusted the level of detail when activities took longer than desired. Hudson and Stasko [15] used the Scheduler pattern to provide animation within the Artkit user interface toolkit, introducing the use of pacing functions. More recent frameworks, including Piccolo [3] and prefuse [14], use the pattern.
An important issue in implementing the Scheduler pattern is choosing how to handle concurrency. A common approach is for the Scheduler’s dispatch loop to run in a dedicated thread. The Piccolo toolkit takes a different approach. Its scheduler runs within the user interface event loop, resulting in a single-threaded model that frees programmers from explicitly handling concurrency issues. However, this approach carries the drawback that greedy activities with long- running “run” methods will leave all user interface components unresponsive, regardless of their relation to the visualization. Prefuse attempts a compromise between these models, maintaining a separate scheduling thread but performing automatic locking in both input controllers and schedulable visualization operators to help shelter programmers from concurrency issues.
1.8 Operator
Decompose visual data processing into a series of composable operators, enabling flexible and reconfigurable visual mappings.
When designing object-oriented visualization software, developers must decide upon the appropriate level of granularity for the visualization components of their system. Visualization frameworks such as Advizor [11] and the InfoVis Toolkit [12] adopt a model similar to traditional user interface toolkits, encapsulating different visualization designs into monolithic “widgets”. The result is a library of interactive components such as scatterplots, time-series charts, or treemaps that can be directly instantiated and then added to the user interface. Creating new visualizations requires subclassing existing widgets or writing entirely new components. An alternative approach is to target a finer level of granularity using an Operator pattern. The idea is to deconstruct visualization tasks into composable operators whose configuration can be modified at run- time. Example operators include layout algorithms, visual encodings, and distortion techniques. New visualizations can be constructed by composing existing operators and/or introducing new operators, facilitating reuse at the level of individual visualization techniques. The prefuse toolkit [14], for example, includes a demonstration of composing layout operators to create custom hybrid layout schemes. Operators also simplify many customization tasks, allowing clients to directly modify the set of operators that constitute the visualization.
The structure of the Operator pattern is shown in Figure 9. The basic structure is intentionally quite simple. An operator has a single required method that performs the operation (an instance of the Template Method pattern [13]), simplifying the creation of new, general-purpose operators. An operator performs processing on a Visualization instance. Operators may maintain a reference to the Visualization (as pictured), or have the Visualization passed as a parameter to the “operate” method (if stateless operators are desired). An individual operator may have any number of additional parameters, depending on its function. For example, a color encoder
might include a color palette used to determine color values of items retrieved from the visualization. Operators can be aggregated into composites, enabling sequential batch execution or conditional evaluation of operators.
Figure 9. The Operator Pattern. Operators are modules that perform a specific processing action, updating the contents of a visualization in accordance with a data state model [8]. Possible operators include visual encodings (for size, shape, color, etc), spatial layout algorithms, visibility filters, and animated interpolation. Composite operators aggregate individual operators to enable sequential or conditional execution.
Operators may be implemented as Activity instances in the Scheduler pattern (§2.8), enabling time-sensitive or recurrent invocation. Once schedulable, operators can additionally be used to implement animated transitions, as done within the prefuse toolkit [14]. Operators can also employ Expressions (§2.6). For example, a Predicate could be used to select a group of visual items for the operator to process. It is possible to implement operators within the Expression language itself, an approach partially supported in Improvise [23].
As described here, the use of operators fits squarely within the data state model defined by Chi [8]. The operators act upon the visualization, whose state updates as a result. Another variant of the operator pattern is the data flow model. Data flow operators have specific input and output types and are chained together in directed acyclic graphs to define processing flows. This approach is used in a number of 3D visualization frameworks, including the Visualization Toolkit [19]. Chi [9] has demonstrated that applications written in one model can be equivalently formulated in the other. We focus on the data state model due to its greater prevalence in information (as opposed to scientific) visualization.
1.9 Renderer
Separate visual components from their rendering methods, allowing dynamic determination of visual appearances.
Standard user interface toolkits use a component or “widget” model, in which interactive components are represented as individual objects responsible for drawing themselves and handling user input. Typically, such components include a “paint” method that issues the drawing commands for rendering the object onscreen. Changing the appearance of a component often requires subclassing that component and overriding the paint method. This approach is common in many visualization frameworks. For example, each visual item in the Piccolo toolkit [3] has its own paint method, and each visualization provided by the InfoVis Toolkit [12] is implemented as a stand-alone widget. This approach has the advantages of low overhead, familiarity, and simplicity. However, it also limits dynamic changes in appearance. For example, implementing a semantic zoom in the standard model requires either (1) creating entirely new sets of objects and swapping them based on the current zoom level or (2) creating a subclass with a custom paint method that explicitly handles each zoom level. The Renderer pattern solves this problem by decoupling the representation of a visual item from its rendering, enabling dynamic, run-time changes in visual appearance.
The structure of the Renderer pattern is shown in Figure 10. Rendering of visual items is performed by dedicated, reusable modules responsible for mapping the item’s visual attributes into actual pixels. Renderers perform the view transformation step of Card et al’s infovis reference model [6]. In addition to a drawing

858 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 12, NO. 5, SEPTEMBER/OCTOBER 2006
routine, a Renderer should also provide access to the pixel-level geometry, testing if a given point is contained within the item (useful for identifying when an item is underneath the mouse pointer), and calculating bounding boxes. Visual items might contain a direct reference to their current Renderer, or, as illustrated in Figure 10, a RendererFactory can be used. The RendererFactory dynamically assigns Renderers to items based on current conditions. The Renderer pattern supports extensibility through the introduction of new Renderers.
Figure 10. The Renderer Pattern. The mapping between items and their visual appearance is determined using Renderer modules, responsible for drawing, interior point testing, and bounds calculation. A RendererFactory can be used to assign Renderers to items based on current conditions, such as data attribute values or the zoom level.
Modular renderers have been applied in various frameworks to allow clients to extend or change rendering behaviours of provided widgets. For example, the Java Swing user interface toolkit uses a Renderer pattern to allow clients to specify the rendering behavior of individual items within Swing JTable and JTree widgets. The prefuse toolkit [14] uses the Renderer pattern for drawing visual items, using a RendererFactory to dynamically assign Renderers to visual items. The Renderer pattern can also be used in conjunction with other patterns. RendererFactory instances can apply the Production Rule pattern (§2.10) to provide customizable rule sets for Renderer assignment. By pooling and reusing individual Renderers for use with any number of items, a RendererFactory applies the Flyweight pattern [13].
1.10 Production Rule
Use a chain of if-then-else rules to dynamically determine visual properties using rule-based assignment or delegation.
In many design scenarios, a designer may specify numerous default settings and then override these settings to deal with special cases. For example, Cascading Style Sheets (CSS) are a widely-used technology enabling web designers to specify document properties such as typeface, color, and alignment and then override these settings as needed. Special cases could include custom settings for a particular class of document elements or events such as a mouse over. Similar needs arise in visualization software. For example, a particular color encoding may be desired by default, but overridden in special cases such as user interaction or inclusion in search results. Writing a custom color encoding to handle these cases would be an inefficient use of time and unnecessarily bloat the software with more class definitions. The Production Rule pattern provides a flexible solution to this need.
Figure 11. The Production Rule Pattern. A series of nested rules can be used to return individual values that meet desired conditions.
The structure of the Production Rule pattern is shown in Figure 11. Given a data item (e.g., a Tuple instance) as input, a rule set tests the data item against the conditions and returns a matching value.
Rule sets can begin with a default value; new conditions and values can then be added to handle special cases. The result is a dispatching structure employing Gamma et al’s Chain of Responsibility pattern [13]. Production rules can be modified at run-time, allowing the rule set to change while an application is running, possibly as a result of user interaction. As formulated here, the implementation of Production Rule uses predicate objects to encapsulate the rule conditions. When used in conjunction with the Expression pattern (§2.6), not only the predicates, but the entire rule structure may be implemented using expression constructs (i.e., using “if” statements).
Other visualization software patterns can benefit from the use of production rules. An implementation of the Renderer pattern (§2.9) may use production rules to assign renderers to visual items. The Operator pattern (§2.8) can use production rules for visual encoding operators. For example, a color encoding might use a production rule that evaluates to a specific color value in one condition (e.g., a color indicating a mouse-over event) but to a delegate color encoding by default (e.g., a nominal color encoding based upon a data attribute). Both of these approaches are used within the prefuse toolkit [14].
1.11 Camera
Provide a transformable camera view onto a visualization, supporting multiple views and spatial navigation of data displays.
Spatial navigation operations, such as panning and zooming, are often used to explore large data sets. Such techniques are often applied in conjunction with multiple views, for example to create overview+detail displays. A common approach to implementing such functionality is to treat the data display as a camera within the geometric space of the visualization. Moving, zooming, or rotating the camera accordingly changes the viewpoint seen by users. Multiple cameras can be used with a single visualization, enabling multiple views with unique perspectives.
Figure 12. The Camera Pattern. A view component maintains an affine transformation matrix that is applied to visual items when rendering. The affine transform matrix can be used to specify translation, rotation, scale, and shearing transformations on the geometry of the view.
The structure of the Camera pattern is shown in Figure 12. A view instance (a user interface component providing a graphics canvas) maintains an affine transformation matrix describing the current position, scale, and orientation of the camera view. When the display draws itself, all graphics operations are subject to this transform. As most modern 2D and 3D graphics libraries allow transformation matrices to be applied directly to a graphics context, this can be implemented in a straightforward way. Multi-view displays are created by instantiating any number of view components and setting their transforms.
The camera pattern has a long history of use in graphics libraries such as OpenGL and is found in nearly any framework that supports geometric zooming, such as Piccolo [3] and prefuse [14]. One limitation of the pattern is that it only supports affine geometric transformations. Non-geometric techniques, such as semantic zooming, can instead be implemented by changing the rendering behavior of visual items in response to the transform settings. Dynamic rendering behavior can be implemented in a modular and extensible manner using the Renderer pattern (§2.9). Animated view transitions can be achieved using the Scheduler pattern (§2.7); activity instances can incrementally update the view transformation and trigger repaints.

HEER ET AL.: SOFTWARE DESIGN PATTERNS FOR INFORMATION VISUALIZATION 859
1.12 Dynamic Query Binding
Allow data selection and filtering criteria to be specified dynamically using direct manipulation interface components.
Dynamic queries are a central technique in information visualization, allowing users to refine a data view through direct manipulation [1]. Any number of user interface widgets, such as sliders, range sliders, checkboxes, and text boxes may be used to input query conditions. The Dynamic Query Binding pattern describes the mechanisms by which one can easily create one or more widgets bound to a general-purpose query predicate, automatically updating both the predicate and any other bound components when interaction occurs.
The structure of the Dynamic Query Binding pattern is shown in Figure 13. As pictured, a particular data set and a field of interest are used as input to a dynamic query binding that maintains both a selection predicate and a data model. The data model participates in a Model-View-Controller pattern [16] and provides coordinated state for any number of dynamic query widgets. The concrete implementation of the model might store a bounded range of values, a list of selected items, or a textual search query. By providing Factory Methods [13], the dynamic query binding can instantiate and configure user interface widgets at the client’s request. For example, a range query binding might provide methods for creating appropriately configured range sliders. As users interact with these widgets, changes to the data model result in notifications to the dynamic query binding, which then updates the query predicate. Other observers can in turn be notified of changes to the predicate and take action accordingly. By explicitly representing query criteria as a predicate (possibly applying the Expression pattern, §2.6), dynamic queries can be used in a very general fashion. The predicate could be used to control item visibility, as is typically done, or as criteria for any number of alternative visual encoding or data processing operations.
Figure 13. The Dynamic Query Binding Pattern. Given a data set and data field as input, a dynamic query binds a selection predicate to a data model. The data model can be used as the backing state for any number of user interface widgets. The binding also serves as a factory for creating and configuring an appropriate set of dynamic query widgets.
Although most infovis frameworks provide support for dynamic queries, not all implement them in full generality. Some frameworks, such as the InfoVis Toolkit [12], do not provide generalized query predicates and so instead must update data structures directly, reducing the applicability of the pattern. An implementation might also forego using the Factory Method pattern, instead requiring clients to instantiate and configure the user interface widgets on their own. Alternatively, one might maintain a single instantiated widget instead of a backing model. By maintaining a backing data model, the general form of the pattern supports any number of coordinated components. Various widgets can be generated from a single binding and incorporated into different parts of an interface. All affect the same query predicate and simultaneously update in response to changes originating at any of the other bound widgets.
2 CONCLUSION
While many of the patterns presented can be fruitfully applied in isolation, it is often in the relationships among patterns that their greatest value is realized. For example, combining the Scheduler (§2.7) and Operator (§2.8) patterns enables the creation of reusable animation operators, while combining the Relational Graph (§2.4) and Proxy Tuple (§2.5) patterns provides a programmer-friendly interface to a powerful and flexible data representation. Figure 14 provides a partial illustration of the relationships between the patterns discussed in this paper, including both the proposed visualization patterns and related patterns from Gamma et al. [13]. The figure provides a roadmap to applying patterns in a holistic fashion.
Given the limited format of this paper, a great deal of discussion and a rich space of examples had to be curtailed. Each pattern warrants a longer discussion than we have provided here. In addition, some candidate patterns have been left out. For example, the Scenegraph abstraction used in 3D toolkits such as VTK [19] and 2D toolkits such as Piccolo [3], can be usefully described in a pattern format. We would also note that observing patterns within real world source code plays an important part in understanding the value of pattern-infused design. Interested readers may wish to explore one or more of the visualization frameworks freely available online, including Improvise [23], the InfoVis Toolkit [12], Piccolo [3], prefuse [14], and the Visualization Toolkit [19].
Finally, we reiterate that patterns are not static entities, but evolving descriptions of best practices. We make no claims as to the finality or completeness of the patterns presented here, only to their observed effectiveness in visualization contexts. We look forward to these and other patterns being proposed, challenged, refined, and applied.
REFERENCES
[1] Ahlberg, C., C. Williamson, and B. Shneiderman. Dynamic Queries for Information Exploration: An Implementation and Evaluation. ACM Human Factors in Computing Systems (CHI), 1992.
[2] Alexander, C., S. Ishikawa, M. Silverstein, M. Jacobson, I. Fiksdahl- King, S. Angel. A Pattern Language: Towns, Buildings, Construction. Oxford University Press. 1977.
[3] Bederson, B. B., J. Grosjean, J. Meyer. Toolkit Design for Interactive Structured Graphics. IEEE Transactions on Software Engineering, 30(8): 535-546. 2004.
[4] Bertin, J. Semiology of Graphics: Diagrams, Networks, Maps. translated by W. J. Berg. University of Wisconsin Press. 1983.
[5] Card, S. K., J. D. Mackinlay, G. G. Robertson, The Information Visualizer: An Information Workspace. ACM Human Factors in Computing Systems (CHI), 1991.
[6] Card, S. K., J. D. Mackinlay, B. Schneiderman (eds.). Readings in Information Visualization: Using Vision To Think. Morgan-Kaufman, 1999.
[7] Chen, H. Towards Design Patterns for Dynamic Analytical Data Visualization. Proceedings Of SPIE Visualization and Data Analysis, 2004.
[8] Chi, E. H., J. T. Riedl. An Operator Interaction Framework for Visualization Systems. IEEE Symposium on Information Visualization (InfoVis), 1998.
[9] Chi, E. H. Expressiveness of the Data Flow and Data State Models in Visualization Systems. Advanced Visual Interfaces (AVI), 2002.
[10] Cleveland, W. S., R. McGill. Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods. Journal of the American Statistical Association, 79(387): 531- 554. September 1984.
[11] Eick, S. G. Visual Discovery and Analysis. IEEE Transactions on Visualization and Computer Graphics, 6(10). January 2000.
[12] Fekete, J.-D. The InfoVis Toolkit. IEEE Symposium on Information Visualization (InfoVis), 2004.

860 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 12, NO. 5, SEPTEMBER/OCTOBER 2006
  Figure 14. Design Pattern Relationships. The network depicts interactions between design patterns, intending a more holistic pattern of how the various
 patterns apply or mutually reinforce each other. Patterns with italicized text (e.g., Flyweight) are taken from Gamma et al. [13]; those with a standard typeface
 were introduced in this paper. To simplify the diagram, patterns used extensively by the visualization patterns (e.g., Observer) have been omitted.
[13] Gamma, E., R. Helm, R. Johnson, and J. Vlissides. Design Patterns: Elements of Reusable Object-Oriented Software. Addison-Wesley, 1994.
[14] Heer, J., S. K. Card, J. A. Landay. prefuse: A Toolkit for Interactive Information Visualization. ACM Human Factors in Computing Systems (CHI), 2005.
[15] Hudson, S. E., J. T. Stasko. Animation Support in a User Interface Toolkit: Flexible, Robust, and Reusale Abstractions. ACM Symposium on User Interface and Software Technologies (UIST), 1992.
[16] Krasner, G. E., S. T. Pope. A Cookbook for Using the Model-View- Controller User Interface Paradigm in Smalltalk-80. Journal of Object- Oriented Programming, 1(3):26-49, August 1988.
[17] Mackinlay, J. D. Automating the Design of Graphical Presentations of Relational Information. ACM Transactions on Graphics, 5(2): 110-141. 1986.
[18] Schmidt, D. C. Using Design Patterns to Develop Re-usable Object- Oriented Communication Software. Communications of the ACM, 38(10), October 1995.
[19] Schroeder, W. J., K. M. Martin, W. E. Lorensern. The Visualization Toolkit: An Objecct-Oriented Approach to 3D Graphics. Prentice Hall, 1996.
[20] Stolte, C., D. Tang, and P. Hanrahan. Polaris: A System for Query, Analysis and Visualization of Multi-dimensional Relational Databases. IEEE Transactions on Visualization and Computer Graphics, 8(1), January 2002.
[21] Stolte, C., D. Tang, and P. Hanrahan. Multiscale Visualization Using Data Cubes. IEEE Symposium on Information Visualization (InfoVis), 2002.
[22] Tang, D., C. Stolte, and P. Hanrahan. Design Choices when Architecting Visualizations. IEEE Symposium on Information Visualization (InfoVis), 2003.
[23] Weaver, C. Building Highly-Coordinated Visualizations In Improvise. IEEE Symposium on Information Visualization (InfoVis), 2004.
Spatio-Temporal Detection of Divided Attention in Reading Applications Using EEG and Eye Tracking
Mathieu Rodrigue,1 Jungah Son,2 Barry Giesbrecht,3 Matthew Turk,1,2 Tobias Ho ̈llerer1,2 1Computer Science, University of California, Santa Barbara
2Media Arts and Technology, University of California, Santa Barbara 3Psychological and Brain Sciences, University of California, Santa Barbara
{mathieu, mturk, holl}@cs.ucsb.edu, jungah@umail.ucsb.edu, barry.giesbrecht@psych.ucsb.edu
   ABSTRACT
Reading is central to learning and communicating, however, divided attention in the form of distraction may be present in learning environments, resulting in a limited understanding of the reading material. This paper presents a novel system that can spatio-temporally detect divided attention in users during two different reading applications: typical document reading and speed reading. Eye tracking and electroencephalogra- phy (EEG) monitor the user during reading and provide a classifier with data to decide the user’s attention state. The multimodal data informs the system where the user was dis- tracted spatially in the user interface and when the user was distracted. Classification was evaluated with two exploratory experiments. The first experiment was designed to divide the user’s attention with a multitasking scenario. The second ex- periment was designed to divide the users attention by sim- ulating a real-world scenario where the reader is interrupted by unpredictable audio distractions. Results from both exper- iments show that divided attention may be detected spatio- temporally well above chance on a single-trial basis.
Author Keywords
Attention classification; EEG; eye tracking; learning tools;
ACM Classification Keywords
H.1.2 Information Systems: User/Machine Systems
INTRODUCTION
It is often easy to become distracted while reading, leaving the reader to not fully understand or acquire knowledge from portions of text. Typically, this happens when distracting sounds are heard or when dividing attention while attempting to multitask. When learning, divided attention creates inter- ference during the encoding process of the brain, thus affect- ing memory and retrieval of the information [7]. This could
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
IUI’15, March 29–April 1, 2015, Atlanta, GA, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM. Copyright 2015 ACM 978-1-4503-3306-1/15/03$15.00. http://dx.doi.org/10.1145/2678025.2701382
Figure 1. Users focus on the sequential red letters during speed reading.
have a large, negative impact in fields where sustained atten- tion and understanding is necessary. For example, in an ed- ucational or workplace environment, one might be distracted by surrounding peers, causing a decrease in task performance.
Interest in reading applications is larger than ever before, given mobile devices with high resolution screens and digital documents such as PDFs and e-books. Digital reading mate- rials are increasingly popular these days due to the fact that they are instantaneously and easily accessible through the in- ternet, and typically cheaper than their physical counterparts. The change from physical to digital reading modalities makes it possible to more easily employ perceptual devices [14] that monitor one’s attention state. Here, we discuss two types of reading applications and introduce a novel system to detect divided attention during reading.
Ordinary reading applications are the most common as they are analogous to reading a physical book. In this style of reading application, the user reads from left to right, and top to bottom. Pages are turned by button press or touch ges- ture. Speed reading has gained recent interest with mobile applications such as Spritz [3]. A speed reading application shows individual, consecutive words to a user (Figure 1), at a rate set by the user. By eliminating saccades and other potentially-distracting words in the periphery, one can read at much higher speeds. Speed reading applications are par- ticularly useful on mobile devices, with screens too small to provide a comfortable reading experience of full-sized docu- ments (e.g. mobile phones, smart watches). By allowing the device to rapidly display consecutive words, the user must be fully attentive at all times. Once a word, sentence, or para- graph is missed, the user may not be able to easily navigate back, or may not be aware of the precise point where attention was lost. This presents a problem to users who are, for ex- ample, multitasking or easily distracted by audio within their vicinity.
  The
  quick
  brown
   Time
fox
jumps

Novel Attention Monitoring
In this paper, we present a novel system, employing consumer-grade EEG and eye tracking devices, to detect spa- tially (in the reading application) and temporally if a user is either attending to a reading application or dividing their at- tention. The contributions of this paper are two-fold: 1) an algorithm for accurately detecting divided attention in read- ing applications, and 2) the coupling of consumer-grade EEG and eye tracking modalities to improve detection.
To explore the feasibility of classifying between attention and divided attention, two experiments were conducted for each reading task. The first experiment explores to what degree divided attention can be detected in a multitasking scenario. The second experiment explores to what degree divided at- tention can be detected in the presence of real-world unpre- dictable distractions.
RELATED WORK
There have been a handful of attempts to accurately measure states of attention using a wide variety of EEG devices. Berka et al. developed one of the first commercial systems, known as B-Alert [6], to detect states of attention (high-engagement, low-engagement, relaxed wakefulness, and sleepy) in real time by a medical grade EEG device, but were not config- ured to detect divided attention. Hamadicharef et al. were the first to apply the filter bank common spatial pattern (FBCSP) [8] algorithm to an attention task with EEG data. They found that the FBCSP method classified up to 89.4% between states of attention and relaxation using a 15-channel medical grade EEG device. Liu et al. [10] used a consumer-grade EEG de- vice with one electrode (Fp1) to create an attention classifier from pooled subject data. The participants listened to English phrases then answered related questions on a quiz under at- tentive and distracted conditions. Classification was reported to be on average 76% accurate (55%-60% for the inattention class, 87%-90% for attention, depending on the cost func- tion used). Mart ́ınez-Go ́mez et al. [11] found characteristic features from eye tracking data that describe a subject’s level of understanding and English language skill. Putze et al. [13] used the combination of EEG and eye tracking to facilitate video surveillance event selection. They demonstrated accu- rate event detection in an abstract event selection task, using a set of five electrodes.
The majority of the above attempts were performed uni- modally, with devices far too expensive for the average con- sumer. Similarly, the previously stated work has not at- tempted classifying divided attention from attention. Hence, we explore the possibility of combining low-cost EEG and eye tracking to detect states of attention from divided atten- tion in popular reading applications.
BRAIN-COMPUTER INTERFACE
The Emotiv Epoc [1], a consumer-grade EEG device, was used for signal acquisition. The device samples at 128hz and is equipped with 14 electrodes (AF3, AF4, F3, F4, F7, F8, FC5, FC6, O1, O2, P7, P8, T7, T8) which are po- sitioned according to the international 10-20 system. The brain-computer interface developed for this system detects changes in power from the frequency bands α (8-13hz), β (14-30hz), and θ (4-7hz) between divided and non-divided attention states.
EEG Preprocessing
EEG data was first low-pass filtered with a cutoff frequency of 50hz and high-pass filtered with a cutoff frequency of 0.16hz, both using a third-order butterworth filter. The data was then segmented into 1.5-second epochs, overlapping each previous epoch by 50%.
Epochs that contained eye blinks were detected using an eye tracker, and filtered according to [15]. An artifact removal algorithm was implemented based on Adaptive SWT-based Denoising (ASWTD). For the detected EEG segments, the wavelet coefficients at level 6 were obtained using the sta- tionary wavelet transform (SWT) and thresholded with the adaptive thresholding mechanism using a hard thresholding function. Denoised signals without artifacts were obtained by computing an inverse SWT on the thresholded wavelet coefficients. Since we were removing ocular artefacts, only the wavelet coefficients from levels 3 to 6 were thresholded. Epochs with other types of artifacts above a predetermined threshold were discarded.
Filter-Bank Common Spatial Pattern
EEG data was spatially filtered according to an adapted FBCSP algorithm [5], which involved IIR filtering the data into frequency bins, then performing the common spatial pat- tern algorithm on each frequency bin to find the source loca- tion of the signal on the scalp. CSP spatially filters the EEG channels by determining a matrix W such that W Σ1 W ⊤ = D1 and W Σ2W ⊤ = D2 where Σ1 and Σ2 are the estimated class covariance matrices, and D1 and D2 are diagonal ma- tricessuchthatD1 +D2 =I.Thefirstandlastn(n=6,in practice) columns of W contain spatial filters that maximally discriminate between the two classes.
Each preprocessed EEG epoch was first band pass filtered, using a third-order butterworth filter, in the α, β, and θ fre- quency bands. Literature [9] suggests that changes in the cho- sen frequency bands are correlated with attention. A dedi- cated portion of the EEG data was deployed to train a corre- sponding spatial filter Wα, Wβ, and Wθ using the common spatial pattern algorithm (CSP). The remaining band-pass fil- tered epochs were then spatially filtered by their correspond- ing spatial filter, using the n = 6 most discriminative spatial filters as reported by the CSP algorithm.
EEG Classification
Offline EEG data was determined to be representative of at- tention or divided attention by extracting features from the CSP source signals, and classifying test examples with a trained classifier. Log-variance features were extracted from the spatially filtered data fα = log (var (Wα X )), fβ = log (var (Wβ X )), fθ = log (var (Wθ X )). 10x10-fold cross validation with a support vector machine (SVM) classifier was employed using the MATLAB implementation with an RBF kernel, to decide if an EEG epoch was in the attention or divided attention state. A grid search was performed to op- timize σ for all participants, the remaining parameters were left as default.
EYE TRACKING
The Eye Tribe [4], a consumer-grade eye tracker was used for this work. Raw data was recorded at 60hz and divided into four-second epochs. Each EEG epoch had a corresponding,
Table 1. Average classification accuracy and per-class classification accuracy for EEG, eye tracking and combined modalities.
EEG
Average Att.
95.93% 97.45% 97.68% 97.22% 87.99% 88.21%
98.16% 99.59% 99.97% 99.95% 92.41% 93.02%
84.52% 85.71% 74.82% 73.76% 69.07% 65.56%
85.71% 88.10% 77.14% 80.0% 75.93% 74.63%
Eye Tracking
EEG + Eye Tracking
   Average Att Div. Att.
   Exp. ISR Exp. IR
Exp. IISR Exp. IIR
Subject 1 Subject 2 Subject 3
Subject 1 Subject 2 Subject 3
Subject 1 Subject 2 Subject 3
Subject 1 Subject 2 Subject 3
Div. Att.
94.40% 98.13% 87.77%
96.73% 100% 91.81%
83.33% 75.89% 73.33%
83.33% 74.29% 77.22%
Average Att.
96.54% 98.43% 97.79% 96.92% 90.06% 90.97%
97.8% 99.28% 99.19% 99.82% 91.25% 93.51%
83.51% 82.35% 79.43% 75.18% 82.41% 80.0%
90.48% 95.24% 84.29% 81.43% 80.93% 79.26%
Div. Att.
93.74% 98.66% 89.15%
95.09% 98.56% 88.99%
84.60% 83.69% 84.81%
85.71% 87.14% 82.59 %
63.04% 71.43% 65.99% 54.7% 67.81% 65.88%
54.05% 77.28% 69.75%
         72.26% 69.53% 74.01% 71.6% 63.14% 51.98%
74.09% 76.42% 74.31%
       66.31% 68.77% 72.34% 75.18% 73.61% 71.48%
64.02% 69.5% 75.74%
         78.57% 73.81% 69.29% 57.14% 71.39% 70.37%
83.33% 81.43% 72.41%
     four-second eye tracking epoch, aligned at the last sample. The Eye Tribe eye tracking algorithms are currently in active development. We used version 0.9.41 of the SDK, and a chin rest was used to minimize noise.
Fixation and Saccade Detection
An offline saccade and fixation detection algorithm adapted from [12] was employed. Saccades were found by finding the distance between the mean of two consecutive sliding win- dows,
attention conditions. The first task involved speed reading a passage of text. Each session lasted approximately two min- utes. An open source speed reading application, Spray [2], was used for this task. The second task required the subject to read a normal passage of text, one page at a time. Each session lasted the necessary amount of time for the subject to finish two pages, which was approximately two minutes. Par- ticipants were not allowed to go back to a previous page, but were encouraged to read as they normally would. Both tasks were split up into two conditions: attention and divided at- tention, and required the subjects to wear headphones. After each task, the participant reported a summary of what they had read to ensure the subject was participating in the task. EEG electrodes were checked after each task to ensure proper impedance levels, and eye tracking calibration was checked between experiments, but no recalibration was ever needed.
Experiment I
The first experiment was designed to simulate internal di- vided attention, such as multitasking. Three participants (one female, two males, ages 25-27) volunteered for the experi- ment. All had normal or corrected to normal vision. A total of eight sessions were performed, within subjects, for each task. The names of nine different colors were spoken through the headphones in a random order at one-second intervals. The volume of the headphones were set to be as low as possi- ble while still allowing the subject to hear the colors clearly. During the attention condition, the participant was asked to ignore the colors and completely focus on the speed reading task. During the divided attention condition, the participant was told to focus on the reading material but also count the number of times a target color was spoken during the session. The participant reported the number of colors heard after each session, which was compared to ground truth data to ensure the user was dividing attention between both stimuli.
Experiment II
The second experiment was designed to simulate an environ- ment where a reader is distracted by external stimuli. Three participants (three male, ages 22-34) volunteered for the ex- periment. All had normal or corrected to normal vision. A total of four sessions were performed, within subjects, for each task. The headphones were set to a moderate volume as determined by the participant. During the reading task, the participant heard multiple, unique sound clips that lasted for 30 seconds each and would play randomly, but never overlap.
mbefore(n) =
 1r 1r
  sx(n − k),   sy(n − k) rr
k=1 k=1  rr
  mafter(n)= 1 sx(n+k),1 sy(n+k) rr
  k=1 k=1
where n is the sample of interest, and r is the window size.
The distance d is calculated at every sample in eye tracking  ⊤
data by d = (mafter − mbefore) · (mafter − mbefore)
which forms a sequence of peaks, where the max of each peak represents a saccade. Once saccades are detected in the epoch, the median of the samples between saccades are marked as fixations. This algorithm may be applied to real- time system where data is processed in epochs.
Eye Tracking Classification
Four of the most discriminative eye tracking features from [11] were used for in our classifier: number of fixations, median saccade length, mean saccade velocity x, and mean saccade velocity y. Among other tested features, the cho- sen features were found to best characterize between states of attention in this experiment. Median was chosed instead of mean saccade length so that outliers would not affect classifi- cation. Additionally, log-variance features for saccade length and mean pupil size were employed after observing discrim- inative differences in the data. The features were added to their corresponding EEG-epoch feature vector and then clas- sified with the SVM classifier to determine attention states.
EXPERIMENTS
In this section, we’ll discuss two experiments to study the effects of distraction from multitasking, and external stimuli on classification. Both experiments required the subject to complete two tasks, both split up into attention and divided

 The sound clips contained distracting noises one might typ- ically hear in their environment while reading, such as ener- getic music, television shows, movies, dogs barking, or con- versations from other people. After each session, the partici- pant reported how distracted they were during audio playback (1-9 Likert scale, 9 being most distracted). Distractions rated below 4 were not used for classification. Three detachable EEG electrodes were found to be damaged and thus excluded from data acquisition: (F3, FC6, P7), (AF3, F3, FC5), and (F7, P7, T7), for subjects one, two and three, respectively.
RESULTS AND DISCUSSION
The objective of this work was to develop algorithms which allow EEG and eye tracking consumer devices to accurately detect divided attention from attention during reading. Ad- ditionally, we wanted to provide a proof of concept for map- ping human attention into a user interface, so software could intelligently react to and provide useful information about a reader’s experience. A user’s eye fixations are seen in (Fig- ure 3) which shows a clear discrimination between attention and divided attention conditions.
To avoid any bias in the data, a 10x10-fold cross-validation was used on the feature data for both experiments. Table 1 shows that classification accuracy of EEG and combined modalities in experiment I exceeds those for experiment II for both tasks. Figure 2 shows the higher dimensional EEG features from the highest classification accuracy obtained in experiment I. The feature data was projected onto the two largest principal components in order to visualize the higher dimensional feature space. It is clear from the visualization that the two classes are highly separable, resulting in accurate classification. The first experiment simulated a multitasking scenario, where the reader may be engaging in other activi- ties while they are reading, e.g. listening to a breaking news story on the television while reading. This type of task was designed to keep the subject’s attention divided among two sets of stimuli. The EEG classification accuracy is high, most likely due to the fact that the multitasking distraction effect was stronger, resulting in different power in the frequency bands between conditions. This experiment ensured that clas- sification was based on true divided attention and not driven by auditory perception signals in the brain, given that an iden- tical auditory signal (a spoken set of colors in pseudo-random order) was present across both conditions.
Figure 3. Classified user fixation data visualized from experiment I. Red circles represent divided attention, blue circles represent attention.
Whereas experimenent I was highly controlled to rule out the possibility that auditory brain signals are driving classifica- tion results, experiment II was designed to test a more real- istic scenario in which a user’s attention is divided by unex- epected and intrusive auditory distractions. Table 1 shows a lower classification percentage using both modalities for ex- periment II, but still well above chance. One possibility for lower classification accuracy was the exclusion of three EEG electrodes for each participant. Also, the second experiment did not guarantee that a subject would be distracted during the distraction condition. Given that some people may not have an issue with background noise as they read, e.g. people who commonly read with a running television in the room, this experiment may not have distracted all of the participants equally. This was also observed from the post-session ques- tionnaire, which confirmed that the external stimuli were not fully distracting to all participants. Mean scores for distrac- tion level during the two tasks were (6, 6.25), (7.25,4.75), and (4, 4.5), with their respective standard errors (0.629, 0.722), (0.913, 0.479), (0.408,0.654) for the three participants. Ad- ditionally, EEG classification from this experiment may have been partially influenced by the user’s perception of hearing different, intermittent audio clips.
CONCLUSION
We proposed a novel approach to detecting attention and di- vided attention during reading applications, using EEG and eye tracking consumer devices. Results from two experi- ments show that EEG classification is highly accurate during controlled multitasking scenarios, and still well above chance during unpredictable distraction scenarios. On the other hand, eye tracking data while not as accurate as EEG data, may be effective as a single mode of input for certain types of appli- cations. Additionally, by combining EEG and eye tracking features, classification generally performs better than either modality on its own. Future work will focus on classifying attention from divided attention in real-time. For example, the system could make note of when the user was distracted during speed reading, providing an easy way to navigate to areas the user missed, or warn the user, to regain focus. Simi- larly, during ordinary reading, the system could make note of spatio-temporal data received from the EEG and eye tracker, and remind the user to go back and review those areas. Other future work involves detection and multiclass classification among other states of attention (e.g. mind wandering), and attention detection in other evironment scenarios.
ACKNOWLEDGEMENTS
This work was partially supported by ONR grants N00014- 14-1-0133 and N00014-13-1-0872, as well as NSF grant IIS- 0747520.
0.15
0.1
0.05
0
−0.05
−0.1
−0.06 −0.04
−0.02 0
0.02 0.04
Attention
Divided Attention
0.06 0.08 0.1
Principal component 2
Principal component 1
Figure 2. First two principal components of EEG features from subject two during the ordinary reading condition for experiment I.
REFERENCES
1. Emotiv EPOC. http://www.emotiv.com/.
2. Spray Open Source Speed Reader.
https://github.com/chaimpeck/spray/.
3. Spritz. http://www.spritzinc.com/.
4. The Eye Tribe Eye Tracker. http://www.theeyetribe.com/.
5. Ang, K. K., Chin, Z. Y., Zhang, H., and Guan, C. Filter bank common spatial pattern (FBCSP) in brain-computer interface. In IEEE International Joint Conference on Neural Networks, 2008. IJCNN 2008. (IEEE World Congress on Computational Intelligence) (June 2008), 2390–2397.
6. Berka, C., Levendowski, D. J., Cvetinovic, M. M., Petrovic, M. M., Davis, G., Lumicao, M. N., Zivkovic, V. T., Popovic, M. V., and Olmstead, R. Real-time analysis of EEG indexes of alertness, cognition, and memory acquired with a wireless EEG headset. International Journal of Human-Computer Interaction 17, 2 (June 2004), 151–170.
7. Fernandes, M. A., and Moscovitch, M. Divided attention and memory: evidence of substantial interference effects at retrieval and encoding. Journal of Experimental Psychology. General 129, 2 (June 2000), 155–176.
8. Hamadicharef, B., Zhang, H., Guan, C., Wang, C., Phua, K. S., Tee, K. P., and Ang, K. K. Learning EEG-based spectral-spatial patterns for attention level measurement. In IEEE International Symposium on Circuits and Systems, 2009. ISCAS 2009 (May 2009), 1465–1468.
9. Klimesch, W. EEG alpha and theta oscillations reflect cognitive and memory performance: a review and analysis. Brain Research Reviews 29, 2–3 (Apr. 1999), 169–195.
10. Liu, N.-H., Chiang, C.-Y., and Chu, H.-C. Recognizing the degree of human attention using EEG signals from mobile. Sensors 13, 8 (Aug. 2013), 10273–10286.
11. Mart ́ınez-Go ́mez, P., and Aizawa, A. Recognition of understanding level and language skill using measurements of reading behavior. In Proceedings of the 19th International Conference on Intelligent User Interfaces, IUI ’14, ACM (New York, NY, USA, 2014), 95–104.
12. Olsson, P. Real-time and offline filters for eye tracking. Master Thesis (2007).
13. Putze, F., Hild, J., Ka ̈rgel, R., Herff, C., Redmann, A., Beyerer, J., and Schultz, T. Locating user attention using eye tracking and EEG for spatio-temporal event selection. In Proceedings of the 2013 International Conference on Intelligent User Interfaces, IUI ’13, ACM (New York, NY, USA, 2013), 129–136.
14. Turk, M., and Robertson, G. Perceptual user interfaces (introduction). Commun. ACM 43, 3 (Mar. 2000), 32–34.
15. Yong, X., Fatourechi, M., Ward, R. K., and Birch, G. E. Automatic artefact removal in a self-paced hybrid brain- computer interface system. Journal of NeuroEngineering and Rehabilitation 9, 1 (July 2012), 50.
  See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/228761562 Theoretical origins of concept maps, how to
construct them, and uses in education
Article · January 2007
CITATIONS READS
51 1,914
2 authors, including: Joseph D. Novak
Florida Institute for Human and Machine Cognition
149 PUBLICATIONS 10,200 CITATIONS SEE PROFILE
     All content following this page was uploaded by Alberto J. Cañas on 20 February 2017.
The user has requested enhancement of the downloaded file. All in-text references underlined in blue are added to the original document and are linked to publications on ResearchGate, letting you access and read them immediately.
 Vol. 3, No.1, November 2007 pp. 29-42
Theoretical Origins of Concept Maps, How to Construct Them, and Uses in Education 1
Joseph D. Novak & Alberto J. Cañas
Florida Institute for Human and Machine Cognition (IHMC)
ABSTRACT
Concept maps, as we define them, are graphical tools for organizing and representing relationships between concepts indicated by a connecting line linking two concepts. Words on the line, referred to as linking words or linking phrases, specify the relationship between the two concepts. Concepts and propositions are usually organized hierarchically, from most general, most inclusive to most specific. It is best to construct concept maps with reference to some particular question we seek to answer, which we have called a focus question. The concept map may pertain to some situation or event that we are trying to understand through the organization of knowledge in the form of a concept map, thus providing the context for the concept map.
In this paper we briefly present the origins and theoretical foundations of concept maps, explain how concept maps are constructed, and then show how the integration of concept maps with technology in software such as CmapTools facilitates the implementation of concept map-based learning environments that support our New Model for Education. Last, examples from three domains are used to describe how concept maps can be used to organize content based on the knowledge of domain experts, creating an environment that is easy to navigate for learners.
ORIGIN OF CONCEPT MAPS
Concept maps were developed in 1972 in the course of Novak’s research program at Cornell University where he sought to follow and understand changes in children’s knowledge of science (Novak & Musonda, 1991). During the course of this study the researchers interviewed many children, and they found it difficult to identify specific changes in the children’s understanding of science concepts by examination of interview transcripts. This program was based on the learning psychology of David Ausubel (1963; 1968; Ausubel et al., 1978). The fundamental idea in Ausubel’s cognitive psychology is that learning takes place by the assimilation of new concepts and propositions into existing concept and propositional frameworks held by the learner. This knowledge structure as held by a learner is also referred to as the individual’s cognitive structure. Out of the necessity to find a better way to represent children’s conceptual understanding emerged the idea of representing children’s knowledge in the form of a concept map. Thus was born a new tool not only for use in research, but also for many other uses. Figure 1 shows a concept map that illustrates the key features of concept map.
1Modified and abridged from Novak and Cañas (2006).
http://reflectingeducation.net
    © WLE Centre, Institute of Education, London ISSN 1746-9082
Novak, J.D. and Cañas, A.J. Theoretical Origins of Concept Maps, How to Construct Them and Uses in Education
  Figure 1: A concept map showing the key features of concept maps PSYCHOLOGICAL FOUNDATIONS OF CONCEPT MAPS
The question sometimes arises as to the origin of our first concepts. These are acquired by children during the ages of birth to three years, when they recognize regularities in the world around them and begin to identify language labels or symbols for these regularities (Macnamara, 1982). This early learning of concepts is primarily a discovery learning process, where the individual discerns patterns or regularities in events or objects and recognizes these as the same regularities labelled by older persons with words or symbols. This is a phenomenal ability that is part of the evolutionary heritage of all normal human beings. After age 3, new concept and propositional learning is mediated heavily by language, and takes place primarily by a reception learning process where new meanings are obtained by asking questions and getting clarification of relationships between old concepts and propositions and new concepts and propositions. This acquisition is mediated in a very important way when concrete experiences or props are available; hence the importance of “hands-on” activity for science learning with young children, but this is also true with learners of any age and in any subject matter domain.
In addition to the distinction between the discovery learning process, where the attributes of concepts are identified autonomously by the learner, and the reception learning process, where the attributes of concepts are described using language and transmitted to the learner, Ausubel made the very important distinction between rote learning and meaningful learning. Meaningful learning requires three conditions: (1) The material to be learned must be conceptually clear and presented with language and examples relatable to the learner’s prior knowledge. (2) The learner must possess relevant prior knowledge. (3) The learner must choose to learn meaningfully. The one condition over which the teacher or mentor has only indirect control is the motivation of students to choose to learn by
  Reflecting Education 30
Novak, J.D. and Cañas, A.J. Theoretical Origins of Concept Maps, How to Construct Them and Uses in Education
 attempting to incorporate new meanings into their prior knowledge, rather than simply memorizing concept definitions or propositional statements or computational procedures.
The nature of the indirect control which instructors have to encourage meaningful learning lies primarily in the instructional strategies used and the evaluation strategies used. Instructional strategies that emphasize relating new knowledge to the learner’s existing knowledge foster meaningful learning, as do evaluation strategies that encourage learners to use ideas they possess for solution of novel problems. Typical objective tests seldom require more than rote learning (Holden, 1992). In fact, the worst forms of objective tests, or short-answers tests, require verbatim recall of statements and this may be impeded by meaningful learning where new knowledge is assimilated into existing frameworks, making it difficult to recall specific, verbatim definitions or descriptions. This kind of problem was recognized years ago in Hoffman’s (1962) The Tyranny of Testing.
People often confuse rote learning and meaningful learning with teaching approaches that can vary on a continuum from direct presentation of information (which may be conceptually obscure or conceptually explicit) to autonomous discovery approaches where the learner perceives the regularities and constructs her/his own concepts. Both direct presentation and discovery teaching methods can lead to highly rote or highly meaningful learning by the learner, depending on the disposition of the learner and the organization of the instructional materials. There is the mistaken notion that “inquiry” studies will assure meaningful learning. The reality is that unless students possess at least a rudimentary conceptual understanding of the phenomenon they are investigating, the activity may lead to little or no gain in their relevant knowledge and may be little more than busy work (Mayer, 2004).
One of the powerful uses of concept maps is not only as a learning tool but also as an evaluation tool, thus encouraging students to use meaningful-mode learning patterns (Mintzes et al., 2000; Novak, 1990; Novak & Gowin, 1984). Concept maps are also effective in identifying both valid and invalid ideas held by students. They can be as effective as more time-consuming clinical interviews for identifying the relevant knowledge a learner possesses before or after instruction (Edwards & Fraser, 1983). Moreover, when learners work with concept maps, they become more proficient at meaningful learning and they can overcome misconceptions they held initially (Novak, 2002).
One of the challenges teachers face is how to organize the curriculum optimally to facilitate meaningful learning. When a teacher develops her/his own concept maps for a domain of study, the teacher gains a clearer understanding of the key concepts to be learned, and the concept map also provides guidance for the learning sequence. Working through the concepts on the map, from the more general, more inclusive concepts at the top of the map to the more specific, concepts lower in the map provides for a psychologically sound sequencing of instruction. The early acquisition of more general concepts provides anchorage or scaffolding for subsequent learning for more detailed, more specific concepts and principles. It is also possible to use concept maps to see how a given state or local curriculum matches or deviates from an optimal leaning sequence, and/or includes or omits necessary concepts. Heinze-Frey & Ludwig (2006) have done a good illustration of this for
        Reflecting Education 31
Novak, J.D. and Cañas, A.J. Theoretical Origins of Concept Maps, How to Construct Them and Uses in Education
 a section of the curriculum dealing with the environment for Lexington Public schools, as shown in Figure 2. Their concept map also illustrates how an instructional sequence that proceeds from the top down the map could optimize meaningful learning.
        Figure 2: A concept map showing similarities and differences in the State and local curriculum dealing with the area of environment
Many learners and teachers are surprised to see how this simple mapping tool facilitates meaningful learning and the creation of powerful knowledge frameworks that not only permit utilization of the knowledge in new contexts, but also the retention of the knowledge for long periods of time (Novak, 1990; Novak & Wandersee, 1991). There is still relatively little known about memory processes and how knowledge finally gets incorporated into our brain, but it seems evident from diverse sources of research that our brain works to organize knowledge into hierarchical frameworks and that learning approaches that facilitate this process significantly enhance the learning capability of all learners (Bransford et al., 1999).
While it is true that some students (and some teachers) have difficulty building concept maps and using these, at least early in their experience, this appears to result primarily from years of rote-mode learning practice in school settings rather than as a result of brain structure differences per se. So-called “learning style” differences are, to a large extent, derivative from differences in the patterns of learning that students have employed varying from high commitment to continuous rote-mode learning to almost exclusive commitment
   Reflecting Education 32
Novak, J.D. and Cañas, A.J. Theoretical Origins of Concept Maps, How to Construct Them and Uses in Education
 to meaningful mode learning. It is not easy to help students in the former condition move to patterns of learning of the latter type (Kinchin, 2001). While concept maps can help, students also need to be taught something about brain mechanisms and knowledge organization, and this instruction should accompany the use of concept maps. The information in the above paragraphs should become part on the instructional program for skilful use of concept maps. The information provided in this paper could be part of this instruction.
EPISTEMOLOGICAL FOUNDATIONS OF CONCEPT MAPS
As shown in Figure 1, we defined concept as a perceived regularity (or pattern) in events or objects, or records of events or objects, designated by label. It is coming to be generally recognized now that the meaningful learning processes described above are the same processes used by scientists and mathematicians, or experts in any discipline, to construct new knowledge. In fact, Novak has argued that new knowledge creation is nothing more than a relatively high level of meaningful learning accomplished by individuals who have a well organized knowledge structure in the particular area of knowledge, and also a strong emotional commitment to persist in finding new meanings (Novak, 1993; 1998). Epistemology is that branch of philosophy that deals with the nature of knowledge and new knowledge creation. There is an important relationship between the psychology of learning, as we understand it today, and the growing consensus among philosophers and epistemologists that new knowledge creation is a constructive process involving both our knowledge and our emotions or the drive to create new meanings and new ways to represent these meanings. Learners struggling to create good concept maps are themselves engaged in a creative process, and this can be challenging, especially to learners who have spent most of their life learning by rote. Rote learning contributes very little at best to our knowledge structures, and, therefore, cannot foster creative thinking or novel problem- solving.
We hear much today about constructivism and constructivist teaching. The fundamental idea behind constructivism is that each person must build her/his own understanding; the teacher cannot install knowledge into the learner’s head. The latter idea is basically a psychological idea, and there is also an epistemological aspect of constructivism that is less often emphasized. In contrast to the dominant positivist epistemology of the first half of the
20th Century, constructivist epistemology sees knowledge not as discovered absolute truths but rather knowledge is seen as a human construction that evolves as new ideas and new ways of looking at the world evolve. Here again we see the parallel between how people learn and how they construct knowledge. We shall see in the New Model for Education discussed below how concept maps can facilitate both new learning and new knowledge creation.
   Reflecting Education 33
Novak, J.D. and Cañas, A.J. Theoretical Origins of Concept Maps, How to Construct Them and Uses in Education
 CONSTRUCTING GOOD CONCEPT MAPS
In learning to construct a concept map, it is important to begin with a domain of knowledge that is very familiar to the person constructing the map. It is also important to define the area of knowledge to be mapped, and this is done best by preparing an appropriate focus question, or a question that will be answered by the knowledge that is mapped. Usually focus questions that require explanation, rather than simple description or classification, lead to better concept maps. Recall that concepts are constructed to code regularities in events or in objects. Generally speaking, focus questions that call for more event explanations require deeper, more meaningful thinking than those that describe object characteristics (Derbentseva et al, 2006). Examples of better and poorer focus questions are given below:
Good focus questions:
Why do we have seasons?
How can we help teachers become more effective?
How would you explain what Poe was trying to illustrate in this short story? Less Useful Focus Questions:
What are the key features of a flower?
What are common traits of effective teachers?
Given a selected domain and a defined question or problem in this domain, the next step is to identify the key concepts that apply to this domain. Usually 15 to 25 concepts will suffice. These concepts could be listed, and then from this list a rank ordered list should be established from the most general, most inclusive concept, for this particular problem or situation, to the most specific, least general concept. Although this rank order may be only approximate, it helps to begin the process of map construction. We refer to the list of concepts as a parking lot, since we will move these concepts into the concept map as we determine where they fit in. Some concepts may remain in the parking lot as the map is completed if the mapmaker sees no good connection for these with other concepts in the map.
The next step is to construct a preliminary concept map. This can be done on a sheet of paper or by writing all of the concepts on Post-its, or preferably by using the IHMC CmapTools (Cañas et al., 2004) computer software program described below. Post-its allow a group to work on a whiteboard or butcher paper and to move concepts around easily. This is necessary as one begins to struggle with the process of building a good hierarchical organization. Computer software programs are even better in that they allow moving of concepts together with linking statements and the moving of groups of concepts and links to restructure the map. When CmapTools is used in conjunction with a computer projector, two or more individuals can easily collaborate in building a concept map and see changes as they progress in their work. CmapTools also allows for collaboration between
    Reflecting Education 34
Novak, J.D. and Cañas, A.J. Theoretical Origins of Concept Maps, How to Construct Them and Uses in Education
 individuals in the same room or anywhere in the world, and the maps can be built synchronously or asynchronously, depending on the mapmakers’ schedules.
It is important to recognize that a concept map is never finished. After a preliminary map is constructed, it is always necessary to revise this map. Other concepts can be added. Good maps usually result from three to many revisions. This is one reason why using computer software is helpful.
Students often comment that it is hard to add linking words onto the “lines” of their concept map. This is because they poorly understand the relationship between the concepts, or the meanings of the concepts, and it is the linking words that specify this relationship. Once students begin to focus-in on good linking words, and on the identification of good cross- links, they can see that every concept could be related to every other concept. This also produces some frustration, and they must choose to identify the most prominent and most useful cross-links. This process involves what Bloom (1956) identified as high levels of cognitive performance, namely evaluation and synthesis of knowledge. Concept mapping is an easy way to encourage very high levels of cognitive performance, when the process is done well. This is one reason concept mapping can also be a very powerful evaluation tool (Edmondson, 2000).
CmapTools provides for the linking of any kind of digital resource (e.g. images, photos, videos, URLs, PDFs, other concept maps, etc.) to a concept or linking phrase to create in effect a knowledge portfolio or a knowledge model. To link a resource to a concept map, one only needs to drag that resource and drop it onto a target concept. The digital resource is now linked to the concept map and can be reached through the icon under the target concept.
Finally, the map should be revised, concepts re-positioned in ways that lend to clarity and better over-all structure, and a “final” map prepared. When computer software is used, one can go back, change the size and font style, and add colours to “dress up” the concept map.
Through the storing of concept maps in CmapServers, CmapTools encourages collaboration among users constructing the maps. When maps are stored in a server on the Internet, users with appropriate permissions (Cañas et al., 2003) can edit shared concept maps at the same time (synchronously) or at their convenience (asynchronously). “Discussion threads” and “Annotations” in the form of electronic “Post-It” notes can be used to make anecdotal comments on concept maps or during map construction. The high degree of explicitness of concept maps makes them an ideal vehicle for exchange of ideas or for the collaborative construction of new knowledge. We have also found that the obstacles deriving from personal insecurities and fear of embarrassment are largely circumvented, since critical comments are directed at the concept map, not at the person(s) building the map. Having learners comment on each other’s concept maps, whether they are in the same classroom or in different schools, is an effective form of peer-review and collaboration.
  Reflecting Education 35
Novak, J.D. and Cañas, A.J. Theoretical Origins of Concept Maps, How to Construct Them and Uses in Education
 A NEW MODEL FOR EDUCATION
A Concept Map-Centred Learning Environment
CmapTools provides a variety of features that make it possible for teachers to use concept maps for a variety of the tasks that students perform (Cañas & Novak, 2005). In addition to a network environment that fosters collaboration and the possibility of constructing knowledge models, the software allows users, among other features, to (a) search for information based on a concept map (Carvalho et al., 2001), by which a student can use the Cmap to research information to learn more about the topic, leading to an improved map with linked resources, and iteratively proceed on another search; (b) record the process of constructing a Cmap for later playback, providing support to the teacher in what is considered to be a key aspect of concept mapping: the process of constructing a map; (c) piece-wise display a concept map and associated resources full-screen for oral presentations; (d) graphically compare two Cmaps, allowing the teacher to compare the student’s map to his/hers for an initial evaluation. The concept map can thus become an artefact around which the various activities of the learning process can be centred, as shown in Figure 3.
Figure 3: A concept map can serve as the scaffold for building a knowledge portfolio combining all the various kinds of learning activity that can be added in digital form
   Reflecting Education
36
Novak, J.D. and Cañas, A.J. Theoretical Origins of Concept Maps, How to Construct Them and Uses in Education
 Based on the features provided by CmapTools described above, the student can use the concept map prepared as a pre-test as an initial step towards learning the pieces of knowledge that he/she needs to better understand, as the basis on which to perform the research that leads to this understanding, as a way to organize the various sources from which the student will construct this understanding, as the artefact with which to collaborate with peers, and as the means to present his/her findings at the end of the unit. Furthermore, the concept maps constructed by the student can become the foundation for a portfolio evaluation (see Vitale & Romance, 2000) of his/her performance.
CONCEPT MAP-BASED TRAINING AND PERFORMANCE SUPPORT
Systems that embody the knowledge and reasoning capabilities of experts in the performance of skilled tasks hold the promise of providing much greater utility than current training programs provide. The goal, as Wehrenberg (1989, p 38) aptly expressed it is "to put the right person in the right place at the right time with the right skills ..." Creating systems that embody the knowledge of experts require the capturing, representing and sharing of the experts’ knowledge in a form that can be taking advantage of by learners. Concept maps have been used successfully for many years as a knowledge elicitation technique (Ford et al., 1991, Coffey et al., 2002), and at IHMC we have used them to construct training and performance support systems for a variety of domains. In this section we briefly describe three of these systems: NUCES, El-Tech and Mars 2001 as examples of how the knowledge of experts can be captured and shared with learners through concept maps.
Nuces: Nuclear Cardiology Expert System
Nuces (Nuclear Cardiology Expert System) is a prototype diagnostic expert system based on first pass functional imaging of the heart (Ford et al., 1996). Concept maps were used to elicit the expert’s knowledge, and at the same maps later became the explanation component of the system as shown in Figure 4. The concept maps are linked to all types of resources (including other concept maps, videos of the expert, images, documents, research papers, WWW pages, etc.) that can be reached through the icons underneath the various concepts. By navigating through the concept maps, each learner can choose a personal path to follow that depends on the information being sought. Using the expert’s knowledge as a means to organize content leads to an easier navigation and searching of information (Carnot et al., 2001).
El-Tech: Electronic Technician
Nuces showed that the knowledge of a medical doctor could be captured through concept maps and integrated into a knowledge model that allowed individual learners to navigate through a large collection of resources. El-Tech (Electronic Technician) (Coffey et al., 2003), developed in a joint research effort with the Chief of Naval Education and Training of the US Navy, demonstrated that the same mechanism is possible at the technical level: it
  Reflecting Education 37
Novak, J.D. and Cañas, A.J. Theoretical Origins of Concept Maps, How to Construct Them and Uses in Education
 captures the expertise of an expert electronics technician on the RD-379A(V)/UNH, a fault- tolerant magnetic recorder/reproducer manufactured by Magnasync/Moviola corporation.
Mars 2001
Briggs et al. (2004) report on the use of concept mapping to create Mars 2001, a collection of over 100 concept maps created at the Center for Mars Exploration at NASA Ames, that is used to navigate over a huge collection of resources on the WWW2. Students of all ages navigate through the concept maps as a way to both learn from the expert’s knowledge and to reach the diverse media that are linked to the concept maps. Mars 2001 is an excellent example of how concept maps can be used to organize content in a non-linear fashion, using the expert’s knowledge as the organizational structure, truly standing in the shoulders of giants in the creation of these knowledge models. Of particular interest is the fact that in the case of Mars 2001, the expert (Dr. Geoff Briggs) constructed the maps himself (and with some help from colleagues) and there were no knowledge elicitation sessions and maps constructed by knowledge engineers, as was the case of Nuces and El-Tech.
SUMMARY
We have introduced concept maps as a tool to represent and share knowledge, explaining briefly their theoretical foundations and how to construct concept maps. We then presented how to take advantage of the integration of concept maps with technology, as exemplified by CmapTools, as a means to provide a concept map-centred learning environment that supports a New Model of Education. Last, we briefly presented three different domains where concept maps have been used to construct a training/learning environment, whereby the concept maps facilitate the construction of a non-linear navigation mechanism through which learners easily reach the information they are seeking.
REFERENCES
Ausubel, D. P. (1963) The psychology of meaningful verbal learning. New York: Grune and Stratton.
Bloom, B. S. (1956) Taxonomy of educational objectives; the classification of educational goals (1st edn.). New York: Longmans Green.
Bransford, J., Brown, A. L., & Cocking, R. R. (Eds.) (1999) How people learn: Brain, mind, experience, and school. Washington, D.C.: National Academy Press.
Briggs, G., D. A. Shamma, et al. (2004) ‘Concept Maps Applied to Mars Exploration Public Outreach’. In Cañas, A.J., Novak J.D., & González F.M. (Eds.) Concept Maps:
2 The Mars concept maps can be reached at http://cmex.ihmc.us.
        Reflecting Education 38
Novak, J.D. and Cañas, A.J. Theoretical Origins of Concept Maps, How to Construct Them and Uses in Education
 Theory, Methodology, Technology. Proceedings of the First International Conference on Concept Mapping (Vol. I: pp109-116). Pamplona, Spain, Universidad Pública de Navarra.
Cañas, A. J., Ford, K. M., Novak, J. D., Hayes, P., Reichherzer, T., & Suri, N. (2001) ‘Online concept maps: Enhancing collaborative learning by using technology with concept maps’. In The Science Teacher, 68(4), pp49-51.
Cañas, A. J., Hill, G., Carff, R., Suri, N., Lott, J., Eskridge, T., et al. (2004). ‘CmapTools: A knowledge modeling and sharing environment’. In Cañas, A.J., Novak J.D., & González F.M. (Eds.) Concept maps: Theory, methodology, technology. Proceedings of the first international conference on concept mapping (Vol. I, pp.125-133).Pamplona, Spain: Universidad Pública de Navarra.
Cañas, A. J., Hill, G., Lott, J., & Suri, N. (2003) Permissions and access control in CmapTools (Technical Report No. IHMC CmapTools 2003-03). Pensacola, FL: Institute for Human and Machine Cognition.
Carnot, M. J., B. Dunn, et al. (2001). ‘Concept Maps vs. Web Pages for Information Searching and Browsing’. Available at http://www.ihmc.us/users/acanas/Publications/CMapsVSWebPagesExp1/CMapsVSWebPa gesExp1.htm
Carvalho, M. R., Hewett, R., & Cañas, A. J. (2001) ‘Enhancing web searches from concept map-based knowledge models’. In Callaos, N., Tinetti, F.G., Champarnaud J.M & Lee J.K. (Eds.) Proceedings of SCI 2001: Fifth multiconference on systems, cybernetics and informatics (pp. 69-73). Orlando, FL: International Institute of Informatics and Systemics.
Cañas, A. J. & J. D. Novak (2005) ‘A Concept Map-Centered Learning Environment’.
Symposium at the 11th Biennial Conference of the European Association for Research in Learning and Instruction (EARLI), Cyprus.
Coffey, J. W., R. R. Hoffman, et al. (2002) ‘A Concept-Map Based Knowledge Modeling Approach to Expert Knowledge Sharing’. Proceedings of IKS 2002 - The IASTED International Conference on Information and Knowledge Sharing. M. Boumedine. Calgary, Canada, Acta Press: 212-217.
Coffey, J. W., A. J. Cañas, et al. (2003) ‘Knowledge Modeling and the Creation of El-Tech: A Performance Support System for Electronic Technicians’. In Expert Systems with Applications 25(4), pp483-492.
Derbentseva, N., Safayeni, F. & Cañas, A.J. (2006) ‘Concept Maps: Experiments on Dynamic Thinking’. Journal of Research in Science Teaching, 44(3).
Edmondson, K. (2000) ‘Assessing science understanding through concept maps’. In Mintzes, J., Wandersee, J. & Novak J. (Eds.) Assessing science understanding (pp. 19-40). San Diego: Academic Press.
            Reflecting Education 39
Novak, J.D. and Cañas, A.J. Theoretical Origins of Concept Maps, How to Construct Them and Uses in Education
 Edwards, J. & Fraser, K. (1983) ‘Concept maps as reflections of conceptual understanding’. In Research in Science Education, 13, 19-26.
Ford, K. M., Cañas, A.J. et al. (1991) ‘ICONKAT: An integrated constructivist knowledge acquisition tool’. In Knowledge Acquisition 3, pp215-236.
Ford, K. M., Coffey, J.W. et al. (1996) ‘Diagnosis and Explanation by a Nuclear Cardiology Expert System’. In International Journal of Expert Systems 9, pp499-506.
Heinze-Frey, J. & Ludwig, F. (2006) ‘CmapTools facilitates alignment of local curriculum with State Standards: A case study’. In Cañas, A.J. & Novak, J.D. (Eds.) Concept Maps: Theory, Methodology, Technology. Proceedings of the Second International Conference on Concept Mapping. San Jose, Costa Rica: Universidad de Costa Rica.
Holden, C. (1992) ‘Study flunks science and math tests’. In Science Education, 26, p541. Kinchin, I. (2001) ‘If concept mapping is so helpful to learning biology, why aren't we all
doing it?’ In International Journal of Science Education, 23(12), pp1257-1269. Macnamara, J. (1982) Names for things: A study of human learning. Cambridge, MA:
M.I.T. Press.
Mayer, R.E. (2004) ‘Should there be a three-strikes rule against discovery learning?’ In
American Psychologist, 59(1), pp14-19.
Mintzes, J. J., Wandersee, J. H., & Novak, J. D. (2000) Assessing science understanding: A
human constructivist view. San Diego: Academic Press.
Novak, J. D. (1990). ‘Concept maps and vee diagrams: Two metacognitive tools for science
and mathematics education’. In Instructional Science, 19, pp29-52.
Novak, J. D. (1991) ‘Clarify with concept maps: A tool for students and teachers alike’. In
The Science Teacher, 58, pp45-49.
Novak, J. D. (1993) ‘Human constructivism: A unification of psychological and epistemological phenomena in meaning making’. In International Journal of Personal Construct Psychology, 6, pp167-193.
Novak, J. D. (1998) Learning, creating, and using knowledge: Concept maps as facilitative tools in schools and corporations. Mahwah, NJ: Lawrence Erlbaum Associates.
Novak, J. D. (2002) ‘Meaningful learning: The essential factor for conceptual change in limited or appropriate propositional hierarchies (liphs) leading to empowerment of learners’. In Science Education, 86(4), pp548-571.
Novak, J. & Cañas, A.J. (2006) The theory underlying concept maps and how to construct them. Technical Report IHMC CmapTools 2006-1. Florida Institute for Human and Machine Cognition, Pensacola.
                     Reflecting Education 40
Novak, J.D. and Cañas, A.J. Theoretical Origins of Concept Maps, How to Construct Them and Uses in Education
 Novak, J. D., & Gowin, D. B. (1984) Learning how to learn. New York, NY: Cambridge University Press.
Novak, J. D., Meister, M., Knox, W.W., and Sullivan, D.W. (1966) The World of Science Series. Books One through Six. Indianapolis, IN: Bobbs-Merrill.
Novak, J. D., & Musonda, D. (1991) ‘A twelve-year longitudinal study of science concept learning’. In American Educational Research Journal, 28(1), pp117-153.
Novak, J. D., & Wandersee, J. (1991) ‘Coeditors, special issue on concept mapping’. In Journal of Research in Science Teaching, 28(10).
Valittuti, G. (2004). Personal communication.
Vitale, M. R., & Romance, N. R. (2000) ‘Portfolios in science assessment: A knowledge- based model for classroom practice’. In Mintzes, J.J., Wandersee, J.H. & Novak J.D. (Eds.), Assessing science understanding: A human constructivist view. San Diego, CA: Academic Press.
Wehrenberg, S. B. (1989) ‘The Future Just-In-Time Work Force’. In Personnel Journal, pp. 36-44. February.
Joseph D Novak
Completing graduate studies at the University of Minnesota in 1958, Dr. Novak taught biology at Kansas State at Emporia, and Purdue University. From 1967 to 1995, he was Professor of Education and Biological sciences at Cornell University where his research focused on human learning, educational studies and knowledge creation. He is currently Professor Emeritus, Cornell University and Senior Research Scientist at the Institute for Human and Machine Cognition, Univ. of West Florida. He is author or co-author of 27 books and more than 130 book chapters and papers in professional books and journals. He has consulted with more than 400 schools, universities and corporations, including recent
         Reflecting Education 41
Novak, J.D. and Cañas, A.J. Theoretical Origins of Concept Maps, How to Construct Them and Uses in Education
 work with Procter and Gamble, and NASA. His recent book, ‘Learning, creating, and using knowledge: concept maps as facilitative tools in schools and corporations’, (LEA., 1998) is currently being translated into five foreign languages. Dr. Novak is listed in Who’s Who in America, and other lists, and has received a number of awards and honours including Honorary Doctorates from The University of Comahue in 1998 in Nuquen, Argentina, and The Public University of Navarra in 2002 in Pamplona, Spain and the University of Urbino in Urbino, Italy in 2006. He received the first award for contributions to science education from the Council of Scientific Society Presidents. His current research work includes studies on students’ ideas on learning and epistemology, and methods of applying educational ideas and tools (such as concept mapping for knowledge archiving and utilization) in corporate settings, schools, universities and distance learning. He is married with three children and two grandchildren.
Correspondence: Joseph Novak, jnovak@ihmc.us Alberto J Cañas
For many years, Dr. Cañas has been involved in the use of computers in education, with particular interest in understanding the pedagogical aspects of using technology, and leveraging on his Computer Science background to come up with innovative solutions. He is interested not only in the theoretical aspects, but also in the implementation details and scalability of computers in education efforts. He has been a consultant to Presidents of Costa Rica and Panama in the large scale introduction of computers into the public school systems, resulting in the creation of the Omar Dengo Foundation in Costa Rica and the Conéctate al Conocimiento Project in Panama. He directed the Quorum Project while at the Univ. of West Florida, a joint effort with IBM Latin America that led to the creation of a computer network that allowed thousand of students in schools throughout seven countries in the Americas to have their own email address and work on collaborative projects before Internet arrived in those countries. At IHMC, with the support of NASA and the US DOD, he has led the development of CmapTools, a software suite to represent and share knowledge models that is used by students and professionals in over 150 countries.
Correspondence: Alberto Cañas, acanas@ihmc.us
     View publication stats
Reflecting Education 42
SCIENTIFIC STUDIES OF READING, 1(4), 317-339 Copyright © 1997, Lawrence Erlbaura Associates, Inc.
Understanding Eye Movements in Reading
Keith Rayner
Department of Psychology University of Massachusetts
The primary characteristics of eye movements during reading are reviewed and 4 areas are summarized: (a) the span of effective vision, (b) integration of information across eye movements, (c) the decision of where to fixation next, and (d) the decision of when to move the eyes. In addition, some current controversies about eye movements and reading are discussed.
In his article, McConkie (this issue) describes some historical and personal reflec- tions on our initial work using eye movements to study the reading process. In this article, I review the major findings that have emerged over the past 25 years of research on eye movements in reading. My own bias is that understanding eye movements during reading is vitally important for understanding the reading process. The results of many studies using eye movement data have placed severe constraints on the direction a theory of reading should go. In addition, eye move- ment data have proved to be very useful in adjudicating between alternative theoretical accounts of how different processes operate during skilled reading. I shall return at the end of the article to further discuss why understanding eye movement behavior is important in understanding reading.
Research on eye movements during reading over the past 25 years can be divided into two types of efforts: (a) studies that deal with aspects of eye movements per se and (b) studies that use eye movements as a tool or method of investigation for language processing per se. For the most part, in this article, I focus on the former type of study (see Rayner & Sereno, 1994, for a review of the latter types of work). As I hope to document in this article, considerable advances have been made in understanding eye movements during reading. My goal is to review these findings
Requests for reprints should be sent to Keith Rayner, Department of Psychology, University of Massachusetts, Amherst, MA 01003. E-mail: rayner@psych.umass.edu

318 RA YNER
and point out where there seems to be some agreement. However, after reviewing some general findings, I discuss some current controversies.
Four major issues are reviewed: (a) the span of effective vision, (b) integration of information across eye movements, (c) the decision about where to fixate next, and (d) the decision about when to move the eyes. Prior to discussing these issues, I first provide a brief overview of some basic facts about eye movements and reading. Then, I discuss issues related to the most appropriate measure of processing time related to eye fixations.
BASIC FACTS ABOUT EYE MOVEMENTS AND READING
During reading, we make a series of eye movements (saccades) in which the eyes move very rapidly. The saccades are separated by periods of time when the eyes are relatively still (fixations). The typical saccade is about six to nine letter spaces; this value is not affected by the size of the print as long as it is not too small or too large (Morrison & Rayner, 1981). Thus, the appropriate metric to use when discussing eye movements in reading is letter spaces, and not visual angle (gener- ally, 3 to 4 letter spaces is equivalent to 1° of visual angle). Because of the high velocity of the saccade, no useful information is acquired while the eyes are moving; readers only acquire information from the text during the fixations (Wolverton & Zola, 1983). The average fixation duration in reading is on the order of 200 to 250 msec. The other primary characteristic of eye movements is that about 10% to 15% of the time readers move their eyes back in the text (regressions) to look at material that has already been read.
As text difficulty increases, fixation durations increase, saccade lengths de- crease, and regression frequency increases. More important, the values presented for fixation duration, saccade length, and regression frequency are averages and there is considerable variability in all of the measures. Thus, although the average fixation duration might be 225 msec and the average saccade length might be 8 letter spaces for a given reader, for others these values might be somewhat higher or lower. This between reader variability (which also exists for regression fre- quency) is perhaps not as important as the fact that there is considerable within reader variability. In other words, although a reader's average fixation duration is 225 msec, the range can be from under 100 msec to over 500 msec within a passage of text. Likewise, the variability in saccade length can range from 1 letter space to over 15 letter spaces (though such long saccades typically follow regressions).
Eye movements during reading are necessary because of acuity limitations in the visual system. A line of text extending around the fixation point can be divided into three regions: foveal, parafoveal, and peripheral. In the foveal region (extend- ing 1° of visual angle to the left and right of fixation), acuity is sharpest and the
letters can be easily resolved. In the parafoveal region (extending to 5° of visual angle on either side of fixation) and the peripheral region (everything on the line beyond the parafoveal region), acuity drops off markedly so that our ability to identify letters is not very good even in the near parafovea. Thus, the purpose of eye movements is to place the foveal region on that part of the text to be processed next.
MEASURES OF PROCESSING TIME
One great virtue of eye movement data is that they enable researchers to study moment-to-moment processing activities of readers. One of the hopes that McConkie and I had initially was that eye movements would provide such infor- mation, and my belief is that the past 25 years of research has validated this hope.
As indicated earlier, there is quite a bit of variability in how long individual readers fixate and how far they move their eyes. What causes this variability? A great deal of research (discussed later) has demonstrated that much of the variability in fixation time and saccade length is related to cognitive processes associated with comprehension. Indeed, fixation times vary as a function of the ease or difficulty associated with comprehending the words in the text. Thus, it has become important to identify processing time measures for eye movement data in relation to individual words (see Blanchard, 1985). If readers made only one fixation on each word in the text, there would be little problem. Unfortunately, things are not that simple because at least 20% to 30% of the words in text are skipped altogether (i.e., do not receive a fixation) and some words are fixated more than once before the reader moves on to another word.
Because of the skipping and multiple fixation problems, a number of different measures of processing time associated with individual words have been proposed. One measure, gaze duration (Just & Carpenter, 1980), is the sum of the total fixation time on a word when it is encountered for the first time. Specifically, all of the fixations on a word, before the reader moves to another word, are summed; regressions back to the word are therefore not included in the gaze duration measure. Likewise, if a reader made one fixation on word n, then regressed back to an earlier word (n - 2), and then came back to word n, only the first fixation on the word would contribute to the gaze duration. Gaze duration is probably the most frequently used measure of processing time for a word. A second measure, first fixation duration (Inhoff, 1984), represents the duration of the first and/or only fixation on a word on the first pass, again conditional on the word being fixated. A third measure is the single fixation duration (Rayner, Sereno, & Raney, 1996), which is the duration of fixations on words that are fixated exacdy once on the first pass through the text. Obviously, measures of mean second and third fixations on a word can also be obtained. However, because most words are fixated only once, these
EYE MOVEMENTS IN READING 319
320 RAYNER
measures are not commonly examined. Finally, a fourth measure is the total time spent on a word. This value includes not only the first pass fixation time included in the gaze duration, but also any additional time spent on the word when regressing back to it.
The aforementioned measures record how long a reader fixates a word given that he or she fixated it. To make the record complete, measures of the probability that a word was fixated or skipped are also usually taken, as well as the probability that a word was skipped initially and later regressed to. My general belief is that as much information as possible should be examined in inferring cognitive activities associated with word processing. Examination of all of the measures discussed earlier provides researchers with a great deal of information to be used to construct a coherent explanation of how words are processed. With these preliminary points
1
What is the size of the perceptual span or the effective visual field (the area from which readers acquire useful information) during an eye fixation in reading? This basic question has inspired a great deal of research. To investigate this question, George McConkie and I developed what has become known as the eye-contingent display change paradigm. As noted by McConkie (this issue), around the same time that we developed the technique, Steve Reder and Kevin O'Regan were also working on developing the technique. In this paradigm, a reader's eye movements are monitored (generally every millisecond) by a highly accurate eye-tracking system. The eyetracker is interfaced with a computer that controls the display monitor from which the respondent reads. The monitor has a rapidly decaying phosphor and changes in the text are made contingent on the location of the reader's eyes. Generally, the display changes are made during saccades and the reader is not consciously aware of the changes.
There are three primary types of eye-contingent paradigms: the moving window, foveal mask, and boundary techniques. With the moving window technique (McConkie & Rayner, 1975), on each fixation a portion of the text around the reader's fixation is available to the reader. However, outside of this window area, the text is replaced by other letters, or by Xs (see Figure 1). When the reader moves his or her eyes, the window moves with the eyes. Thus, wherever the reader looks, there is readable text within the window and altered text outside the window. The rationale with the technique is that when the window is as large as the region from which a reader can normally obtain information, reading will not differ from when
behind us, I now turn to a review of some central issues in eye movement research.
THE SPAN OF EFFECTIVE VISION
'The summaries presented in the next four sections are adapted (and updated) from Rayner (1995).
move smoothly across the page of text *
XXXX XXXothly across the XXXX XX ХХХХ *
ХХХХ XXXXXXXX xxxxss the page of teXX
move smoothlXXXXXXXs the page of text *
move smoothly acrosXXXXXXXage of text
move smoothly across the date of text * *
move smoothly across the page of text
*
Normal
Moving Window
FIGURE 1 Examples ofthe moving window, foveal mask, and boundary paradigms. The first line shows a normal line of text with the fixation location marked by an asterisk. The next two lines show an example of two successive fixations with a window of 17 letter spaces and the other letters replaced with Xs (and spaces between words preserved). The next two lines show an example oftwo successive fixations with a 7-letter foveal mask. The bottom two lines show an example of the boundary paradigm The first line shows a line of text prior to a display change with fixation locations marked by asterisks. When the reader's eye movement crosses an invisible boundary (the letter e in the), an initially displayed word (date) is replaced by the target word (page). The change occurs during the saccade so that the reader does not see the change.
Foveal Mask
Boundary
321
322 RA YNER
there is no window present. The foveal mask technique (Rayner & Bertera, 1979) is very similar to the moving window paradigm except that the text and replaced letters are reversed. Thus, wherever the reader looks, the letters around the fixation are replaced by Xs whereas outside of the mask area the text remains normal (see Figure 1). Finally, in the boundary technique (Rayner, 1975), an invisible boundary location is specified in the text and when the reader's eye movement crosses the boundary, an originally displayed word or letter string is replaced by a target word (see Figure 1). The amount of time that the reader looks at the target word is computed both as a function of the relation between the initially displayed stimulus and the target word and as a function of the distance that the reader was from the target word prior to launching a saccade that crossed the boundary.
Research using these techniques has been used to determine the size of the perceptual span or area of effective vision during reading. The major findings from the research are as follows:
1. The perceptual span extends 14 to 15 character spaces to the right of fixation (DenBuurman, Boersema, & Gerrisen, 1981; McConkie & Rayner, 1975; Rayner, 1986; Rayner & Bertera, 1979; Rayner, Inhoff, Morrison, Slowiaczek, & Bertera, 1981; Rayner, Well, Pollatsek, & Bertera, 1982).
2. The span is asymmetric and extends further to the right of fixation than to the left for readers of English (and other left-to-right orthographies). To the left of fixation, the span extends to the beginning of the currendy fixated word, or 3 to 4 letter spaces (McConkie & Rayner, 1976; Rayner, Well, & Pollatsek, 1980). For readers of languages printed from right-to-left (such as Hebrew), the span is asymmetric but in the opposite direction from English so that it is larger left of fixation than right (Pollatsek, Bolozky, Well, & Rayner, 1981).
3. No useful information is acquired below the line of text (Inhoff & Briihl, 1991; Inhoff & Topolski, 1992; Pollatsek, Raney, LaGasse, & Rayner, 1993).
4. The word identification span (or area from which words can be identified on a given fixation) is smaller than the total span of effective vision (Rayner et al., 1982; Underwood & McConkie, 1985). The word identification span generally does not exceed 7 to 8 letter spaces to the right of fixation.
5. The size of the span of effective vision and the word identification span is not fixed, but can be modulated by word length. For example, if three short words occur in succession, the reader may be able to identify all of them. If the upcoming word is constrained by the context, readers acquire more information from that word (Balota, Pollatsek, & Rayner, 1985) and if the fixated word is difficult to process, readers obtain less information from the upcoming word (Henderson & Ferreira, 1990; Inhoff, Pollatsek, Posner, & Rayner, 1989; Kennison & Clifton, 1995; Rayner, 1986).
6. Orthography influences the size of the span. Specifically, experiments with Hebrew readers (Pollatsek et al., 1981) suggest that their span is smaller than that
of English readers and experiments with Japanese (Ikeda & Saida, 1978; Osaka, 1992) and Chinese readers (Inhoff & Liu, in press) suggest that their span is even smaller. Hebrew is a more densely packed language than English, and Japanese and
2
Chinese are more densely packed than Hebrew.
7. Reading skill influences the size of the span. Beginning readers (at the end of
second grade) have a smaller span than skilled readers (Rayner, 1986) and adult dyslexic readers have smaller spans than skilled readers (Rayner, Murphy, Hender- son, & Pollatsek, 1989; Rayner, Pollatsek, & Bilsky, 1995). However, it is most likely the case that a smaller perceptual span in dyslexic readers is due to their difficulty processing fixated words. Thus, the smaller span does not cause their reading problems (Rayner et al., 1995; Underwood & Zola, 1986).
INTEGRATION OF INFORMATION ACROSS SACCADES
What kind of information is integrated across saccades in reading? Experiments using both the moving window and the boundary technique have demonstrated a preview benefit from the word to the right of fixation; information obtained about the parafoveal word on fixation n is combined with information on fixation n + 1 to speed identification of the word when it is subsequendy fixated (Blanchard, Pollatsek, & Rayner, 1989; Rayner et al., 1982).
A number of experiments using the boundary paradigm have varied the ortho- graphic, phonological, morphological, and semantic similarity between an initially displayed stimulus and a target word in attempts to determine the basis of the preview effect. The major findings from these studies are as follows:
1. There is facilitation due to orthographic similarity (Balota et al., 1985; Balota & Rayner, 1983; Rayner, 1975; Rayner, McConkie, & Ehrlich, 1978; Rayner et al., 1982; Rayner, McConkie, & Zola, 1980) so that chest facilitates the processing of chart. However, the facilitation is not strictly due to visual similarity because changing the case of letters from fixation to fixation (so that CAArTbecomes cHaRt on the next) has little effect on reading behavior (McConkie & Zola, 1979; O'Regan & Levy-Schoen, 1983; Rayner, McConkie, & Zola, 1980).
2. The facilitation is in part due to abstract letter codes associated with the first few letters of an unidentified parafoveal word (Rayner et al., 1980; Rayner et al., 1982), though there may be some facilitation from other parts of the word to the right of fixation besides the beginning letters (see Inhoff, 1989a; Inhoff & Tousman, 1990). However, the bulk of the preview effect is due to the beginning letters (Briihl & Inhoff, 1995; Rayner et al., 1982). Inhoff s research shows that the effect is not
Densely packed refers to the fact that it takes more characters per sentence in English than Hebrew, for example.
EYE MOVEMENTS IN READING 323
324 RA YNER
simply due to spatial proximity because there is farilitation from the beginning letters of words when readers are asked to read sentences from right to left, but with letters within words printed from left to right (Inhoff et al., 1989).
3. There is facilitation due to phonological similarity (Henderson, Dixon, Petersen, Twilley, & Ferreira, 1995; Pollatsek, Lesch, Morris, & Rayner, 1992). Thus, beech facilitates beach and shoot facilitates chute, with less facilitation in the latter case.
4. Although morphological factors can influence fixation time on a word (Beauvillain, 1996; Lima, 1987), they don't appear to be the source of the preview benefit (Inhoff, 1987,1989b; Lima, 1987; Lima & Inhoff, 1985).
5. There is no facilitation due to semantic similarity. Thus, song as the initial stimulus does not facilitate the processing of tune, even though such words yield semantic priming effects under typical priming conditions (Rayner, Balota, & Pollatsek, 1986).
THE DECISION ABOUT WHERE TO FIXATE NEXT
There are two components to the issue of how eye movements are controlled during reading: (a) where to fixate next and (b) when to move the eyes. It appears that there are separate mechanisms involved in these decisions (Rayner & McConkie, 1976; Rayner & Pollatsek, 1981), and they will accordingly be discussed separately. The primary findings concerning where to fixate next are as follows:
1. Word length seems to be the primary determinant of where to fixate next when moving forward through the text (see Point 8 for regressions). When word length information about the upcoming word is not available (because the spaces are either removed or filled with other letters or letter-like characters), readers move their eyes a shorter distance than when such information is available (McConkie & Rayner, 1975; Morris, Rayner, & Pollatsek, 1990; Pollatsek & Rayner, 1982; Rayner & Bertera, 1979; Rayner & Pollatsek, 1996; Spragins, Lefton, & Fisher, 1976). Also, the length of the word to the right of fixation strongly influences the size of the saccade (O'Regan, 1979,1980; Rayner, 1979).
2. There is a landing position effect such that readers tend to fixate about halfway between the beginning and the middle of words (Dunn-Rankin, 1978; McConkie, Kerr, Reddix, & Zola, 1988; O'Regan, 1981; O'Regan, Levy-Schoen, Pynte, & Bragaillere, 1984; Rayner, 1979; Rayner et al., 1996). Rayner (1979) originally termed this prototypical location as the preferred viewing location. Subsequently, O'Regan and Levy-Schoen (1987) distinguished between the preferred viewing location and what O'Regan and colleagues now refer to as the optimal viewing position. The optimal viewing location is the location in a word at which recognition time is minimized and it is a bit to the right of the preferred viewing location, closer
to the center of the word. Extensive research efforts have examined the conse- quences of being fixated at locations other than this optimal viewing location (McConkie, Kerr, Reddix, Zola, & Jacobs, 1989; Vitu, 1991; Vitu, O'Regan, & Mittau, 1990) and it has been found that the consequences are more serious when words are presented in isolation than when they are in text. This result suggests either that contextual information overrides low-level visual-processing constraints or that readers are somewhat flexible about where they can acquire information around fixation.
3. There is a launch site effect such that where readers land in a word is strongly influenced by where the saccade came from (McConkie et al., 1988; Rayner et al., 1996). Thus, whereas the most frequent landing position may be near the middle of the word, if the prior saccade was launched some distance (8 to 10 letters) from the target word then the landing position will be shifted to the left of center. Likewise, if the prior saccade was launched close (2 to 3 characters) to the beginning of the target word, the landing position will be shifted to the right of center.
4. Given the two preceding findings, the optimal strategy would be to fixate near the middle of each successive word. However, because short words can often be identified when they are to the right of the currently fixated word, they are often skipped (Blanchard et al., 1989; Rayner, 1979). Factors such as this result in the landing position distribution being spread somewhat due to the launch site effect.
5. Although it has been suggested that semantic information within an as yet-unfixated parafoveal word can influence the landing position in that word (see Everatt & Underwood, 1992; Hyona, Niemi, & Underwood, 1989; Underwood, Bloomfield, & Clews, 1988; Underwood, Clews, & Everatt, 1990), neither Rayner and Morris (1992) nor Hyona (1995) were able to replicate the effect. At this point, it seems safest to conclude that there is no semantic preprocessing effect in which an unidentified parafoveal word influences where the eyes land.
6. On the other hand, if a parafoveal word is identified on the current fixation, the word will typically be skipped and the duration of the fixation prior to the skip is inflated (Hogoboam, 1983; Pollatsek, Rayner, & Balota, 1986). Factors such as word length, word frequency, and predictability influence if a word will be skipped (Rayner et al., 1996).
7. The orthographic regularity of the initial letter clusters in a parafoveal word influence how far into the word the initial saccade goes. Beauvillain, Dore, and Baudouin (1996) and Hyona (1995) found that an irregular letter sequence at the beginning of the parafoveal word results in landing position closer to the beginning of the word than when the sequence is regular.
8. There has not been as much investigation of regressions as there has of forward saccades. Intraword regressions may be due to the eye initially landing in a bad location (O'Regan, 1990), but lexical processes are also involved (Rayner & Pollatsek, 1987). Larger interword regressions back to earlier words or sentences are generally assumed to be due to comprehension failures (Ehrlich & Rayner,
EYE MOVEMENTS IN READING 325
326 RA YNER
1983). It is interesting that skilled readers are very accurate in regressing to regions of text that were the source of the comprehension problem (Frazier & Rayner, 1982; Murray & Kennedy, 1988).
THE DECISION ABOUT WHEN TO MOVE THE EYES
A great deal of research indicates that the amount of time a reader fixates on a word or segment of text reveals something about the cognitive processes associated with comprehending that word or segment (although there is some controversy on this point that is discussed in the next section). Some relevant findings are as follows:
1. During reading, information gets into the processing system very early in a fixation (thus leaving a lot of time for processes associated with word recognition and other necessary processes). Experiments using the foveal mask paradigm in which the onset of the mask is delayed following a saccade have demonstrated that if the reader has 50 msec to process the text prior to the onset of the mask then reading proceeds quite normally (Ishida & Dceda, 1989; Rayner et al., 1981; Slowiaczek & Rayner, 1987). If the mask occurs earlier, reading is disrupted. Although readers may typically acquire the visual information needed for reading during the first 50 msec of a fixation, they can extract information at other times during a fixation as needed (Blanchard, McConkie, Zola, & Wolverton, 1984).
2. Although word length strongly effects gaze duration (Kliegl, Olson, & Davidson, 1982; Rayner et al., 1996), it is also influenced by a number of lexical, syntactic, and discourse variables. Furthermore, single fixation duration and first fixation duration have been shown to be influenced by such variables (particularly word frequency). In particular, there are demonstrations that the following variables influence fixation time: (a) word frequency (Henderson & Ferreira, 1993; Hyona & Olson, 1995; Inhoff & Rayner, 1986; Just & Carpenter, 1980; Raney & Rayner, 1995; Rayner, 1977; Rayner & Duffy, 1986; Rayner & Fischer, 1996; Rayner & Raney, 1996; Rayner, Sereno, Morris, Schmauder, & Clifton, 1989; Rayner et al., 1996; Schmauder, 1991); (b) contextual constraint (Altarriba, Kroll, Sholl, & Rayner, 1996; Balota et al., 1985; Ehrlich & Rayner, 1981; Rayner & Well, 1996; Schustack, Ehrlich, & Rayner, 1987; Zola, 1984); (c) semantic relations between words in a sentence (Carroll & Slowiazek, 1986; Morris, 1994; Sereno & Rayner, 1992); (d) anaphora and coreference (Duffy & Rayner, 1990; Ehrlich & Rayner, 1983; Garrod, Freudenthal, & Boyle, 1994; Garrod, O'Brien, Morris, & Rayner, 1990; O'Brien, Shank, Myers, & Rayner, 1988); (e) lexical ambiguity (Binder & Morris, 1995; Doplrins, Morris, & Rayner, 1992; Duffy, Morris, & Rayner, 1988; Folk & Morris, 1995; Rayner & Duffy, 1986; Rayner & Frazier, 1989; Rayner, Pacht, & Duffy, 1994; Sereno, 1995; Sereno, Pacht, & Rayner, 1992); and (f) syntactic disambiguation (Altmann, Garnham, & Dennis, 1992; Altmann, Garn-
ham, & Henstra, 1994; Britt, Perfetti, Garrod, & Rayner, 1992; Clifton, 1993; Ferreira & Clifton, 1986; Ferreira & Henderson, 1990; Frazier & Rayner, 1982; Rayner, Carlson, & Frazier, 1983; Rayner & Frazier, 1987; Rayner, Garrod, & Perfetti, 1992; Trueswell, Tanenhaus, & Kello, 1993).
3. Although the frequency of the fixated word influences fixation time on the word, the frequency of word n + 1 does not influence fixation time on word n (Henderson & Ferreira, 1993). Thus, it appears that it is primarily characteristics of the fixated word that influence processing time on the word. However, there is also a spillover effect. Thus, for example, when readers fixate on a low frequency word, the duration of fixation n is longer than when a high frequency word is fixated, but the duration of fixation n + 1 is also inflated: The difficulty in processing the low frequency word spills over to the next fixation (Rayner & Duffy, 1986).
CONTROVERSIES ABOUT EYE MOVEMENTS
For the most part, I have implied that there is quite a bit of consistency in the findings of eye movement research. However, there are also some controversies. In this section, I describe four such controversies. They relate to whether or not (a) eye movements are controlled on a moment-to-moment basis or are due to preexisting oculomotor strategies, (b) the spaces between words are a useful cue in planning and executing eye movements, (c) semantic preprocessing influences where to fixate next, and (d) the eye-contingent display change paradigm yields effects that are due to the display change per se. Each of these controversies are now discussed. But, first a general distinction between different classes of models of eye movement control needs to be made.
There are now quite a few proposals for how eye movements are controlled in reading. For the sake of simplicity, I lump the various proposals into two general categories: (a) processing models (Henderson & Ferreira, 1990; Just & Carpenter, 1980; Morrison, 1984; Pollatsek & Rayner, 1990; Rayner & McConkie, 1976; Rayner & Pollatsek, 1989) in which moment-to-moment comprehension processes (like lexical access of the fixated word) influence the movement of the eyes and (b) oculomotor models (Kowler & Anton, 1987; O'Regan, 1990, 1992; O'Regan & Levy-Schoen, 1987) in which the movement of the eyes is not directly related to ongoing language processing, but is primarily due to oculomotor factors.
According to the processing models, the decision about when to move the eyes is primarily affected by linguistic variables so that fixation times on words reflect moment-to-moment processing complexities of the text. For example, the fre- quency of the currently fixated word affects how easy the word is to identify, and thus determines the time the eyes spend on the word. Somewhat independendy from this, the decision about Where to move the eyes is affected by perceptual aspects of the forthcoming word, such as its length and distance from the current fixation. The where decision is, however, not directly affected by lexical factors of the forthcoming
EYE MOVEMENTS IN READING 327
328 RAYNER
parafoveal word unless it is identified on the current fixation. Thus, for example, the word frequency (Henderson & Ferreira, 1993) or the informativeness distribu- tion (Rayner & Morris, 1992) of the parafoveal word (word n +1) does not influence fixation time on word n unless it (word n + 1) is identified on fixation n. If the parafoveal word (word n + 1) is identified on fixation n, then it will generally be skipped by the ensuing saccade. Word length, word frequency, and predictability all can influence whether or not a parafoveal word is identified on fixation n.
According to oculomotor models, the location in a word at which the eyes are initially fixated largely determines how long the eyes remain fixated. Perceptual considerations, such as the strong loss of visual resolution from fovea! to parafoveal vision, have led oculomotor theorists to tightly link the processing of a word to the location at which the word is being fixated. Thus, the decision about when to move the eyes depends on the outcome of the previous decision about where to move the eyes. If the reader fixates at a nonoptimal position in a word (the optimal position is the center of the word), another fixation will need to be made on the word. Lexical factors can have an influence if a single fixation on a word is very long, and they can influence the second of two fixations on a word. Thus, oculomotor factors determine how long readers look at words and fixation times are only rather indirectly influenced by the lexical properties of words.
Thus, there is controversy over the extent to which lexical properties of words influence fixation time on the word. In actuality, there has been a long-standing debate over this issue. Because the reaction time of the eyes in simple oculomotor tasks is known to be at least 175 msec (Rayner, Slowiaczek, Clifton, & Bertera, 1983), it has frequently been argued that the duration of a fixation is too short to permit the reader to process the foveal and parafoveal words, make a decision on the basis of that information where to send the eyes, and then set up the motor program to move the eyes (see Bouma & deVoogd, 1974). However, some of the processing models assume that some of these activities go on in parallel or are independent of each other. Furthermore, it is possible that, for various reasons, the response time to move the eyes in reading is shorter than the estimates obtained from simple oculomotor tasks (see McConkie, Underwood, Zola, & Wolverton, 1985, for one such argument).
In the current controversy, there seems to be acceptance of the fact that characteristics of the fixated word can influence fixation time on that word. What is in dispute seems to be the source of the effect. In support of the idea that oculomotor factors primarily determine when to move the eyes, Vitu, O'Regan, Inhoff, and Topolski (1995) recently reported a study in which respondents either read normal text or they "read" text in which all of the letters had been replaced by zs. Vitu et al. reported that eye movement behavior was quite similar in the two situations; they found that both global characteristics (e.g., the length of saccades, durations of fixations, and the frequency distribution of fixation durations and saccade lengths) and local characteristics (e.g., skipping rates, landing position, and
refixation probability) of eye movements were quite similar in the two situations. From this they argued that the similarity in eye movement characteristics in the two situations suggested that predetermined oculomotor strategies are an important element in determining oculomotor behavior during reading.
Rayner and Fischer (1996) extended Vitu et al.'s (1995) study by examining eye behavior with respect to specific target words of high or low frequency when respondents read normal text or z-strings. Globally, they found that the reading condition led to shorter fixations, longer saccades, and less frequent skipping of target strings than did scanning the transformed text. Locally, the manipulation of word frequency affected fixation durations on the target word during reading, but not during scanning. They also found that when readers were asked to search through normal text for a target word, word frequency did not affect fixation time (see also Rayner & Raney, 1996). Rayner and Fischer also found that there were more refixations on target words in reading than in scanning or visual search. Contrary to Vitu et al., we concluded that although there are some surface similarities in eye movements when reading and scanning, that eye movements during reading are strongly influenced by immediate processing demands. It should also be noted that Rayner et al. (1996) showed that single fixations on words are strongly influenced by word frequency, as is the duration of the first of two fixations on a word. Both of these findings are inconsistent with the basic tenets of the oculomotor model. Pynte, Kennedy, and Murray (1991) and Sereno (1992) likewise observed that the duration of the first of two fixations is influenced by the properties of the fixated word. Thus, although oculomotor factors undoubtedly have some influence on eye movement control, the bulk of the evidence is consistent with the processing model.
The second current controversy relates to the usefulness of spaces between words. As noted earlier, a fair amount of research has demonstrated that when space information between words is not available, reading is slowed considerably (by as much as 50%). However, Epelboim, Booth, and Steinman (1994) recently reported a study in which respondents read unspaced text as their eye movements were monitored. Because some of their readers could read unspaced text relatively well and their eye movement patterns were quite similar when reading normal and unspaced text, they argued that unspaced text is relatively easy to read. From this they concluded that the spaces between words are not important in guiding eye movements and that words, not spaces, are the important cues in deciding where to look next. While agreeing that word recognition plays an important role in eye movement control, Rayner and Pollatsek (1996) challenged the conclusion that spaces are not important in reading. They showed that only a couple of Epelboim et al.'s respondents could read unspaced text reasonably well and that the majority of their respondents, like those in other studies, were slowed significantly by the absence of space information. In some recent studies in my lab, we have also found marked differences in local eye movement characteristics when reading spaced and unspaced text. More critically, Kohsom and Gobet (in press) recently demonstrated
EYE MOVEMENTS IN READING 329
330 RA YNER
that when native Thai readers read Thai text with spaces inserted between the words (Thai is normally printed without spaces) that their reading performance is actually facilitated even though they have had no previous experience reading Thai with spaces between the words. Kohsom and Gobet concluded, as did Rayner and Pollatsek (1996), that when spaces are present, they are used to guide eye move- ments. The bulk of the evidence is thus consistent with the idea that the spaces between words are a useful cue in deciding where to look next.
The third controversy was mentioned earlier in this article. Specifically, Under- wood et al. (1990) reported some results that suggest that some type of semantic preprocessing of unidentified (and as yet-unfixated) parafoveal words influences where readers fixate in words. Rayner and Morris (1992) pointed out some theoreti- cal and methodological problems with the research. Furthermore, we were unable to find evidence consistent with such preprocessing effects. Recent studies by Hyona (1995) and Beauvillain (1996) are also consistent with the findings of Rayner and Morris (1992). With respect to this issue, my view is that the weight of the evidence is inconsistent with the semantic preprocessing view.
Finally, there is some controversy with respect to studies that have utilized the eye-contingent display change paradigm. Specifically, it has been suggested that effects found may be due to the display changes per se or to respondents seeing, either consciously or unconsciously, the change (see O'Regan, 1990). Although one always has to be careful in eye-contingent experiments to ascertain that the display changes are taking place at the appropriate time and that the findings are not artifactual in some way, I know of no evidence to suggest that findings from such experiments are due to display changes per se. Indeed, Briihl and Inhoff (1995) recently conducted some analyses examining whether the point at which the display change occurred (provided that it was within a reasonable time window) or the magnitude of the change affected fixation times following the change; they found no evidence that would support the position that display changes per se camouflage effects manifest in fixation times.
TOWARD A MODEL OF EYE MOVEMENT CONTROL
Hopefully, it is evident from this article that a great deal has been learned about eye movements during reading. Given that we have gained so much knowledge about the characteristics of eye movements during reading, I wondered if it might be possible to predict (a) how long readers would fixate on words and (b) when readers skip words in reading. Thus, over the past couple of years, my colleagues (Erik Reichle, Sandy Pollatsek, and Don Fisher) and I have implemented a simulation model of eye movement control in reading . The model (which is called the E-Z Reader model) does a very good job of predicting fixation time on words and skipping rates for words. Furthermore, it is psychologically plausible and does the job with only a few free parameters. Space limitations preclude any extended
discussion of the model (see Reichle, Pollatsek, Fisher, & Rayner, in press), but it does account for a large number of the findings that have been reviewed in this article. At the moment, it does not account for landing position effects (which we think will be relatively easy to implement in the model) nor does it account for higher order effects (which we think will be difficult to implement) such as syntactic or discourse effects. It also does not account for long regressions (short, within word regressions are accounted for by the model). But, it does account for effects of frequency, predictability, the preview effect, spillover effects, and so on.
One important benefit from our modeling work is that the model predicted effects that had not previously been observed. For example, it had previously been reported that when readers skip a word that the duration of the fixation prior to the skip is inflated (Hogaboam, 1983; Pollatsek et al., 1986), and the model accounted for this result. However, it had not previously been reported that when a word is skipped, the duration of the fixation after the skip is also inflated. The model predicted this effect and we found that it was present in the corpus of data we used to compare actual reading performance with the model.
Given the large amount of data that have been collected regarding eye move- ments during reading, my view is that the time is ripe for the development of formal models that account for the characteristics of eye movements in reading. My guess is that a fair amount of our effort over the next few years will be to refine the model.
UNDERSTANDING EYE MOVEMENT BEHAVIOR DURING READING
At the beginning of this article, I asserted that understanding eye movements during reading was important for understanding skilled reading. As I indicated there, eye movement data have been very useful in discriminating between different theoreti- cal accounts of reading-related processes (see Rayner & Sereno, 1994). The aim of the research and simulation work in my laboratory has been to give a reasonable account of how cognitive and lexical processing influences the eye movements of skilled readers. This is a necessary and important enterprise for two reasons.
First, as noted earlier in this article, aspects of eye behavior, such as the durations of eye fixations on words or on regions of text, are often used to infer cognitive processes in reading (Just & Carpenter, 1987; Rayner & Pollatsek, 1989). Second, reading is perhaps the most important skill that people acquire for which they may not have been biologically programmed. If we can understand the skill that has been acquired in reading, it might shed light on skill acquisition in general. I have long believed that if we can understand what skilled readers do, it will be advantageous in teaching children the skill in the first place and in providing remediation for those who do not learn it well.
I tend to believe that the facts that we have learned about eye movements in reading and about reading in general from studying eye movements have placed
EYE MOVEMENTS IN READING 331
332 RA YNER
severe constraints on a theory of reading. When George McConkie and I began our research on reading 25 years ago, the view of the skilled reader was one in which reading was only incidentally visual and in which the reader spent most of his or her time generating predictions of upcoming words. Our research, and that of others, has shown that readers are not unsystematically scanning the text looking for the clues to meaning, but rather that they are systematically moving their eyes from left to right across the text fixating on most of the content words (while skipping some function words). We have shown that the region from which readers obtain meaning is rather limited, but that the processing associated with each word is very rapid and that the link between the eyes and the mind is very tight.
PERSONAL REFLECTIONS
I would like to conclude this article with two points. First, I greatly appreciate the award that was bestowed on George McConkie and myself from the Society for the Scientific Study of Reading. My first professional appointment was in the School of Education at the University of Rochester (with a joint appointment in Psychology). I left Rochester for a position in the Psychology Department at the University of Massachusetts in part because I felt that the work that I was doing was not really appreciated by those interested in reading in the field of education; the work that I was doing seemed to be much better appreciated within the field of cognitive psychology. The fact that McConkie and I were selected as the first recipients of the award by an organization that has typically met in association with the American Educational Research Association suggests that at least some people within the educational field appreciate the nature of the work we have done and I genuinely appreciate this fact.
Second, I want to confirm that setting up an active eye movement laboratory is no small feat and that it takes people who are dedicated to the task. Although I was able to obtain the basic equipment for my lab while still at Rochester, it wasn't until I moved to University of Massachusetts (UMass) that the development of the lab came to fruition. I could never have accomplished the task without the many colleagues at the UMass who have been involved in the lab development. I would specifically like to acknowledge the efforts of Sandy Pollatsek Chuck Clifton, and Jim Bertera in the original hardware and software development in the eye tracking lab at the UMass. And, finally, I would like to thank the many excellent graduate students, post docs, and colleagues that I have collaborated with over the years that I have been at UMass. They have made it exciting and a lot of fun!
As this article hopefully makes clear, I believe that we have learned a lot about reading from the study of eye movements. My best guess is that the bulk of future research will utilize eye movements as a tool to study reading and that not as much of the work will focus on understanding eye movements per se. But, either way, our understanding of reading will benefit from the work.
ACKNOWLEDGMENTS
Preparation of this article was supported by National Institute of Health Grant HD26765, National Science Foundation Grant SBR-9121375, and by a Research Scientist Award from the National Institute of Mental Health (MH01255).
This article is based on a talk given at the meeting of the Society for Scientific Studies in Reading, New York City, May 1996.
REFERENCES
Altarriba, J., Kroll, J. F., Sholl, A., & Rayner, K. (1996). The influence of lexical and conceptual constraints on reading mixed-language sentences: Evidence from eye fixations and naming times. Memory & Cognition, 24, 477-492.
Altmann, G. Т. M., Gamham, A., & Dennis, Y. I. L. (1992). Avoiding the garden path: Eye movements in context. Journal of Memory and Language, 31, 685-712.
Altmann, G. Т. M., Garnharrij A., & Henstra, J. A. (1994). Effects of syntax in human sentence parsing: Evidence against a structure-based parsing mechanism. Journal of Experimental Psychology: Learning, Memory, and Cognition, 20, 209-216.
Balota, D. A., Pollatsek, A;, & Rayner, K. (1985). The interaction of contextual constraints and parafoveal visual information in reading. Cognitive Psychology, 17, 364-390.
Balota, D. A., & Rayner, K. (1983). Parafoveal visual information and semantic contextual constraints. Journal of Experimental Psychology: Human Perception and Performance, 9, 726-738.
Beauvillain, C. (1996). The integration of morphological and whole-word form information during eye fixations on prefixed and; suffixed words. Journal of Memory and Language, 35, 801-820.
Beauvillain, C., Dore, K., & Baudouin, V. (1996). The "center of gravity" of words: Evidence for an effect of word-initial letters. Vision Research, 36, 589-603.
Binder, K. S., & Morris, R. K. (1995). Eye movements and lexical ambiguity resolution: Effects of prior encounter and discourse topic. Journal of Experimental Psychology: Learning, Memory, and Cognition, 21, 1186-1196.
Blanchard, H. E. (1985). A comparison of some processing time measures based on eye movements. Acta Psychologica, 58, 1-15.
Blanchard, H. E., McConkie, G. W., Zola, D., & Wolverton, G. S. (1984). Time course of visual information utilization during fixations in reading. Journal of Experimental Psychology: Human Perception and Performance, 10, 7 5 - 8 9 .
Blanchard, H. E., Pollatsek, A., & Rayner, K. (1989). Parafoveal processing during eye fixations in reading. Perception & Psychophysics, 46, 85-94.
Bouma, H., & deVoogd, A. H. (1974). On the control of eye saccades in reading. Vision Research, 14, 273-284.
Briihl, D., & Inhoff, A. W. <1995). Integrating information across fixations in reading: The use of orthographic bodies and of exterior letters. Journal of Experimental Psychology: Learning, Memory, and Cognition, 21, 55-67.
Britt, M. A., Perfetti, C. A., Garrod, S. C., & Rayner, K. (1992). Parsing in context: Context effects and their limits. Journal of Memory and Language, 31, 293-314.
Carroll, P., & Slowiaczek, M. L. (1986). Constraints on semantic priming in reading: A fixation time analysis. Memory & Cognition, 14, 509-522.
Clifton, C. (1993). The role of thematic roles in sentence parsing. Canadian Journal of Experimental Psychology, 47, 222-246.
EYE MOVEMENTS IN READING 333
334 RA YNER
DenBuurman, R., Boersema, Т., & Gerrisen, J. F. (1981). Eye movements and the perceptual span in reading. Reading Research Quarterly, 16, 227-235.
Dopkins, S., Morris, R. K., & Rayner, K. (1992). Lexical ambiguity and eye fixations in reading: A test of competing models of lexical ambiguity resolution. Journal of Memory and Language, 31, 461-476.
Duffy, S. A., Morris, R. K., & Rayner, K. (1988). Lexical ambiguity and fixation times in reading. Journal of Memory and Language, 27, 429-446.
Duffy, S. A., & Rayner, K. (1990). Eye movements and anaphor resolution: Effects of antecedent typicality and distance. Language and Speech 33, 103-119.
Dunn-Rankin, P. (1978). The visual characteristics of words. Scientific American, 238, 122-130. Ehrlich, K., & Rayner, K. (1983). Pronoun assignment and semantic integration during reading: Eye movements and immediacy of processing. Journal of Verbal Learning and Verbal Behavior, 22,75-87. Ehrlich, S. F., & Rayner, K. (1981). Contextual effects on word perception and eye movements during
reading. Journal of Verbal Learning and Verbal Behavior, 20, 641-655.
Epelboim, J., Booth, J. R., & Steinman, R. M. (1994). Reading unspaced text: Implications for theories
of reading eye movements. Vision Research, 34, 1735-1766.
Everatt, J., & Underwood, G. (1992). Parafoveal guidance and priming effects during reading: A special
case of the mind being ahead of the eyes. Consciousness and Cognition, 1, 186-197.
Ferreira, F., & Clifton, C. (1986). The independence of syntactic processing. Journal of Memory and
Language, 25, 348-368.
Ferreira, F., & Henderson, J. M. (1990). The use of verb information in syntactic parsing: Evidence
from eye movements and word-by-word self-paced reading. Journal of Experimental Psychology:
Learning, Memory, and Cognition, 16, 555-568.
Folk, J. R., & Morris, R. K. (1995). Multiple lexical codes in reading: Evidence from eye movements,
naming time, and oral reading. Journal of Experimental Psychology: Learning, Memory, and
Cognition, 21, 1412-1429.
Frazier, L., & Rayner, K. (1982). Making and correcting errors during sentence comprehension: Eye
movements in the analysis of structurally ambiguous sentences. Cognitive Psychology, 14,178-210. Garrod, S., Freudenthal, D., & Boyle, E. (1994). The role of different types of anaphor in the on-line
resolution of sentences in a discourse. Journal of Memory and Language, 33, 39-68.
Garrod, S., O'Brien, E. J., Morris, R. K., & Rayner, K. (1990). Elaborative inferencing as an active or passive process. Journal of Experimental Psychology: Learning, Memory, and Cognition, 16,
250-257.
Henderson, J. M., Dixon, P., Petersen, A., Twilley, L. C, & Ferreira, F. (1995). Evidence for the use
of phonological representations during transaccadic word recognition. Journal of Experimental
Psychology: Human Perception and Performance, 21, 82-97.
Henderson, J. M., & Ferreira, F. (1990). Effects of fovea! processing difficulty on the perceptual span
in reading: Implications for eye movement control. Journal of Experimental Psychology: Learning,
Memory, and Cognition, 16, 417-429.
Henderson, J. M., & Ferreira, F. (1993). Eye movement control during reading: Fixation measures reflect
foveal but not parafoveal processing difficulty. Canadian Journal of Experimental Psychology, 47,
201-221.
Hogaboam, T. (1983). Reading patterns in eye movement data. In K. Rayner (Ed.), Eye movements in
reading: Perceptual and language processes (pp. 309-332). New York: Academic.
Hyona, J. (1995). Do irregular letter combinations attract readers' attention? Evidence from fixation locations in words. Journal of Experimental Psychology: Human Perception and Performance, 21,
68-81.
Hyona, J., Niemi, P., & Underwood, G. (1989). Reading long words embedded in sentences: Informa-
tiveness of word parts affects eye movements. Journal of Experimental Psychology: Human Perception and Performance, 15, 142-152.
Hyona, J., & Olson, R. K. (1995). Eye fixation patterns among dyslexic and normal readers: Effects of word length and word frequency. Journal of Experimental Psychology: Learning, Memory, and Cognition, 21, 1430-1440.
Ikeda, M . & Saida, S. (1978). Span of recognition in reading. Vision Research, 18, 83-88.
Inhoff, A. W. (1984). Two stages of word processing during eye fixations in the reading of prose. Journal
of Verbal Learning and Verbal Behavior, 23, 612-624.
Inhoff, A. W. (1987). Lexical access during eye fixations in sentence reading: Effects of word structure.
In M. Coltheart (Ed.), Attention and performance 12, (pp. 403-418). Hillsdale, NJ: Lawrence
Erlbaum A ssociates, Inc.
Inhoff, A. W. (1989a). Lexical access during eye fixations in reading: Are word codes used to integrate
lexical information across interword fixations? Journal ofMemory and Language, 28, 444-461. Inhoff, A. W . (1989b). Parafoveal processing of words and saccade computation during eye fixations mieadmg.JourrudofExperimenMPsychology:HummPercepttonandPeformance, 15,544-555. Inhoff, A. W., & Briihl, D. (1991). Semantic processing of unattended text during selective reading:
How the eyes see it. Perception & Psychophysics, 49, 289-294.
Inhoff, A. W., & Liu, W. (in press). The perceptual span and oculomotor activity during the reading of
Chinese sentences. Journal ofExperimental Psychology: Human Perception and Performance. Inhoff, A. W., Pollatsek, A., Posner, M. I., & Rayner, K. (1989). Covert attention and eye movements
during reading. Quarterly Journal of Experimental Psychology, 41 A, 63-89.
Inhoff, A. W., & Rayner, K. (1986). Parafoveal word processing during eye fixations in reading: Effects
of word frequency. Perception & Psychophysics, 40, 431-439.
Inhoff, A. W., & Topolski, R. (1992). Lack of semantic activation from unattended text during passage
reading. Bulletin of the Psychonomic Society, 30, 365-366.
Inhoff, A. W., & Tousman, S. (1990). Lexical priming from partial-word previews. Journal of
Experimental Psychology: Human Perception and Performance, 16, 825-836.
Ishida, Т., & Ikeda, M. (1989). Temporal properties of information extraction in reading by a text-mask
replacement technique. Journal of the Optical Society of America A, 6, 1624-1632.
Just, M. A., & Carpenter, P. A. (1980). A theory of reading: From eye fixations to comprehension.
Psychological Review, 87, 3 2 9 - 3 5 4 .
Just, M. A., & Carpenter, P. A. (1987). The psychology of reading and language comprehension.
Newton, MA: Allyn and Bacon.
Kennison, S. M., & Clifton, C. (1995). Determinants of parafoveal preview benefit in high and low
working memory capacity readers: Implications foreye movement control. Journal of Experimental
Psychology: Learning, Memory, and Cognition, 21, 68-81.
Kliegl, R., Olson, R. K., & Davidson, B. J. (1982). Regression analyses as a tool for studying reading
processes: Comments on Just and Carpenter's eye fixation theory. Memory & Cognition, 10,
287-296.
Kohsom, C, & Gobet, F. (in press). Adding spaces to Thai and English: Effects on reading. In
Proceedings of the 19th Annual Meeting of the Cognitive Science Society.
Kowler, E., & Anton, S. (1987). Reading twisted text: Implications for the role of saccades. Virion Research, 27, 45-60.
Lima, S. D. (1987). Morphological analysis in sentence reading. Journal of Memory and Language, 26, 84-99.
Lima, S. D., & Inhoff, A. W. (1985). Lexical access during eye fixations in reading: Effects of word-initial letter sequence. Journal of Experimental Psychology: Human Perception and Performance, 11, 272-285.
McConkie, G. W., Kerr, P. W., Reddix, M. D., & Zola, D. (1988). Eye movement control during reading: I. The location of initial fixations on words. Vision Research, 28, 1107-1118.
McConkie, G. W., Kerr, P. W., Reddix, M. D., Zola, D., & Jacobs, A. M. (1989). Eye movement control during reading: П. Frequency of refixating a word. Perception & Psychophysics, 46, 245-253.
EYE MOVEMENTS IN READING 335
336 RAYNER
McConkie, G. W., & Rayner, K. (1975). The span of the effective stimulus during a fixation in reading. Perception & Psychophysics, 17, 578-586.
McConkie, G. W., & Rayner, K. (1976). Asymmetry of the perceptual span in reading. Bulletin of the Psychonomic Society, 8, 365-368.
McConkie, G. W., Underwood, N. R., Zola, D., & Wolverton, G. S. (1985). Some temporal charac- teristics of processing during reading. Journal of Experimental Psychology: Human Perception and Performance, 11, 168-186.
McConkie, G. W., & Zola, D. (1979). Is visual information integrated across successive fixations in reading? Perception & Psychophysics, 25, 221-224.
Morris, R. K. (1994). Lexical and message-level sentence context effects on fixation times in reading. Journal of Experimental Psychology: Learning, Memory, and Cognition, 20, 9 2 - 1 0 3 .
Morris, R. K., Rayner, K., & Pollatsek, A. (1990). Eye movement guidance in reading: The role of parafoveal letter and space information. Journal of Experimental Psychology: Human Perception and Performance, 16, 268-281.
Morrison, R. E. (1984). Manipulation of stimulus onset delay in reading: Evidence for parallel programming of saccades. Journal ofExperimental Psychology: Human Perception and Performance, 10, 667-682.
Morrison, R. E., & Rayner, K. (1981). Saccade size in reading depends upon character spaces and not visual angle. Perception & Psychophysics, 30, 3 9 5 - 3 9 6 .
Murray, W. S., & Kennedy, A. (1988). Spatial coding in the processing of anaphor by good and poor readers: Evidence from eye movement analyses. Quarterly Journal ofExperimental Psychology, 40A, 693-718.
O'Brien, E. J., Shank, D. M., Myers, J. L., & Rayner, K. (1988). Elaborarive inferences during reading: Do they occur on-line? Journal ofExperimental Psychology: Learning, Memory, and Cognition, 12, 346-352.
O'Regan, J. K. (1979). Eye guidance in reading: Evidence for the linguistic control hypothesis. Perception & Psychophysics, 25, 5 0 1 - 5 0 9 .
O'Regan, J. K. (1980). The control of saccade size and fixation duration in reading: The limits of linguistic control. Perception & Psychophysics, 28, 112-117.
O'Regan, J. F. (1981). The convenient viewing hypothesis. In D. F. Fisher, R. A. Monty, & J. W. Senders (Eds.), Eye movements: Cognition and visual perception (pp. 289-298). Hillsdale, NJ: Lawrence Erlbaum Associates, Inc.
O'Regan, J. K. (1990). Eye movements and reading. In E. Kowler (Ed.), Eye movements and their role in visual and cognitive processes (pp. 395-453). Amsterdam: Elsevier.
O'Regan, J. K. (1992). Optimal viewing position in words and the strategy-tactics theory of eye movements in reading. In K. Rayner (Ed.), Eye movements and visual cognition: Scene perception and reading (pp. 333-354). New York: Springer-Verlag.
O'Regan, J. K., & Levy-Schoen, A. (1983). Integrating visual information from successive fixations: Does transaccadic fusion exist? Vision Research, 23, 765-768.
O'Regan, J. K., & Levy-Schoen, A. (1987). Eye movement strategy and tactics in word recognition and reading. In M. Coltheart (Ed.), Attention and performance 12 (pp. 363-383). Hillsdale, NJ: Lawrence Erlbaum Associates, Inc.
O'Regan, J. K., Levy-Schoen, A., Pynte, J., & Brugaillere, B. (1984). Convenient fixation location within isolated words of different length and structure. Journal ofExperimental Psychology: Human Perception and Performance, 10, 2 5 0 - 2 5 7 .
Osaka, N. (1992). Size of saccade and fixation duration of eye movements during reading: Psycho- physics of Japanese text processing. Journal of the Optical Society of America A, 9, 5-13.
Pollatsek, A., Bolozky, S., Well, A. D., & Rayner, K. (1981). Asymmetries in the perceptual span for Israeli readers. Brain and Language, 14, 174-180.
Pollatsek, A., Lesch, M., Morris, R. K., & Rayner, K. (1992). Phonological codes are used in integrating information across saccades in word identification and reading. Journal ofExperimental Psychology: Human Perception and Performance, 18, 1 4 8 - 1 6 2 .
Pollatsek, A., Raney, G. E., LaGasse, L., & Rayner, K. (1993). The use of information below fixation in reading and in visual search. Canadian Journal ofExperimental Psychology, 47, 179-200.
Pollatsek, A., & Rayner, K. (1982). Eye movement control in reading: The role of word boundaries. Journal ofExperimental Psychology: Human Perception and Performance, 8, 817-833.
Pollatsek, A., & Rayner, K. (1990). Eye movements and lexical access in reading. In D. A. Balota, G. B. Flores d'Arcais, & K. Rayner (Eds.), Comprehension processes in reading (pp. 143-163). Hillsdale, NJ: Lawrence Erlbaum Associates, Inc.
Pollatsek, A., Rayner, K., & Balota, D. A. (1986). Inferences about eye movement control from the perceptual span in reading. Perception & Psychophysics, 40, 123-130.
Pynte, J., Kennedy, A., & Murray, W. S. (1991). Within-word inspection strategies in continuous reading: Time course of perceptual, lexical, and contextual processes. Journal of Experimental Psychology: Human Perception and Performance, 17, 458-470.
Raney, G. E., & Rayner, K. (1995). Word frequency effects during two readings of a text. Canadian Journal of Experimental Psychology, 49, 151-172.
Rayner, K. (1975). The perceptual span and peripheral cues in reading. Cognitive Psychology, 7, 65-81. Rayner, K. (1977). Visual attention in reading: Eye movements reflect cognitive processes. Memory &
Cognition, 4, 443-448.
Rayner, K. (1979). Eye guidance in reading: Fixation locations in words. Perception, 8, 21-30. Rayner, K. (1986). Eye movements and the perceptual span in beginning and skilled readers. Journal
ofExperimental Child Psychology, 41, 2 1 1 - 2 3 6 .
Rayner, K. (1995). Eye movements and cognitive processes in reading, visual search, and scene
perception. In J.M. Findlay, R. Walker, & R. W. Kentridge (Eds.), Eye movement research:
Mechanisms, processes and applications (pp. 3-22). Amsterdam: North-Holland.
Rayner, K., Balota, D. A., & Pollatsek, A. (1986). Against parafoveal semantic preprocessing during
eye fixations in reading. Canadian Journal of Psychology, 40, 473-483.
Rayner, K., & Bertera, J. H. (1979). Reading without a fovea. Science, 206, 468-469.
Rayner, K., Carlson, M., & Frazier, L. (1983). The interaction of syntax and semantics during sentence
processing: Eye movements in the analysis of semantically biased sentences. Journal of Verbal
Learning and Verbal Behavior, 22, 358-374.
Rayner, K., & Duffy, S. A. (1986). Lexical complexity and fixation times in reading: Effects of word
frequency, verb complexity, and lexical ambiguity. Memory & Cognition, 14, 191-201.
Rayner, K., & Fischer, M. H. (1996). Mindless reading revisited: Eye movements during reading and
scanning are different. Perception & Psychophysics, 58, 734-747.
Rayner, K., & Frazier, L. (1987). Parsing temporarily ambiguous complements. Quarterly Journal of
Experimental Psychology, 39A, 6 5 7 - 6 7 3 .
Rayner, K., & Frazier, L. (1989). Selection mechanisms in reading lexically ambiguous words. Journal
ofExperimental Psychology: Learning, Memory, and Cognition, 15, 779—790.
Rayner, K., Garrod, S., & Perfetti, C. A. (1992). Discourse influences during parsing are delayed.
Cognition, 45, 109-139.
Rayner, K., Inhoff, A. W., Morrison, R. E., Slowiaczek, M. L., & Bertera, J. H. (1981). Masking of
foveal and parafoveal vision during eye fixations in reading. Journal ofExperimental Psychology:
Human Perception and Performance, 7, 167-179.
Rayner, K., & McConkie, G. W. (1976). What guides a reader's eye movements? Vision Research 16,
829-837.
Rayner, K., McConkie, G. W., & Ehrlich, S. F. (1978). Eye movements and integrating information
across fixations. Journal of Experimental Psychology: Human Perception and Performance, 4, 529-544.
EYE MOVEMENTS IN READING 337
338 RAYNER
Rayner, К., McConkie, G. W., & Zola, D. (1980). Integrating information across eye movements. Cognitive Psychology, 12, 206-226.
Rayner, K., & Morris, R. K. (1992). Eye movement control in reading: Evidence against semantic preprocessing. Journal of Experimental Psychology: Human Perception and Performance, 18, 163-172.
Rayner, K., Murphy, L. A., Henderson, J. M., & Pollatsek, A. (1989). Selective attentional dyslexia Cognitive Neuropsychology, 6, 357-378.
Rayner, K., Pacht, J. M., & Duffy, S. A. (1994). Effects of prior encounter and global discourse bias on the processing of lexically ambiguous words: Evidence from eye fixations. Journal of Memory and Language, 33, 527-544.
Rayner, K., & Pollatsek, A. (1981). Eye movement control during reading: Evidence for direct control. Quarterly Journal of Experimental Psychology, 33A, 3 5 1 - 3 7 3 .
Rayner, K., & Pollatsek, A. (1987). Eye movements in reading: A tutorial review. In M. Coltheart (Ed.), Attention and performance 12 (pp. 327-362). Hillsdale, NJ: Lawrence Erlbaum Associates, Inc.
Rayner, K., & Pollatsek, A. (1989). The psychology of reading. Englewood Cliffs, NJ: Prentice Hall. Rayner, K., & Pollatsek, A. (1996). Reading unspaced text is not easy: Comments on the implications ofEpelboim et al.'s (1994) study for models ofeye movement control in reading. Vision Research,
36,461-465.
Rayner, K., Pollatsek, A., & Bilsky, A. B. (1995). Can a temporal processing deficit account for
dyslexia7 Psychonomic Bulletin & Review, 2, 501-507.
Rayner, K., & Raney, G. E. (1996). Eye movement control in reading and visual search: Effects of word
frequency. Psychonomic Bulletin & Review, 3, 245-248.
Rayner, K., Sc. Sereno, S. C. (1994). Eye movements in reading: Psycholonguistic studies. In M. A.
Gernsbacher (Ed.), Handbook of psycholinguistics (pp. 57-82). San Diego, CA: Academic. Rayner, K., Sereno, S. C, Morris, R. K., Schmauder, A. R., & Clifton, C. (1989). Eye movements and on-line language comprehension processes [Special issue]. Language and Cognitive Processes, 4,
21-50.
Rayner, K., Sereno, S. C, & Raney, G. E. (1996). Eye movement control in reading: A comparison of
two types of models. Journal ofExperimental Psychology: Human Perception and Performance,
22, 1188-1200.
Rayner, K., Slowiaczek, M. L., Clifton, C, & Bertera, J. H. (1983). Latency of sequential eye
movements: Implications for reading. Journal ofExperimental Psychology: Human Perception and
Performance, 9, 912-922.
Rayner, K., & Well, A. D. (1996). Effects of contextual constraint on eye movements in reading: A
further examination. Psychonomic Bulletin & Review, 3, 504-509.
Rayner, K., W ell, A. D., & Pollatsek, A. (1980). Asymmetry of the effective visual field in reading.
Perception & Psychophysics, 27, 537-544.
Rayner, K., Well, A. D., Pollatsek, A., & Bertera, J. H. (1982). The availability ofuseful information
to the right of fixation in reading. Perception & Psychophysics, 31, 537-550.
Reichle, E. D., Pollatsek, A., Fisher, D. L., & Rayner, K. (in press). Towards a model of eye movement
control in reading. Psychological Review.
Schmauder, A. R. (1991). Argument structure frames: A lexical complexity metric? Journal of
Experimental Psychology: Learning, Memory, and Cognition, 17, 49-65.
Schustack, M. W., Ehrlich, S. F., & Rayner, K. (1987). The complexity of contextual facilitation in
reading: Local and global influences. Journal of Memory and Language, 26, 322-340.
Sereno, S. C. (1992). Early lexical effects when fixating a word in reading. In K. Rayner (Ed.), Eye
movements and visual cognition (pp. 304-316). New York: Springer-Verlag.
Sereno, S. C. (1995). Resolution of lexical ambiguity: Evidence from an eye movement priming
paradigm. Journal of Experimental Psychology: Learning, Memory, and Cognition, 21, 582-595.
Sereno, S. C., Pacht, J. M., & Rayner, K. (1992). The effect of meaning frequency on processing lexically ambiguous words: Evidence from eye fixations. Psychological Science, 3, 296-300.
Sereno, S. C., & Rayner, K. (1992). Fast priming during eye fixations in reading. Journal of Experi­ mental Psychology: Human Perception and Performance, 18, 173-184.
Slowiaczek, M. L., & Rayner, K. (1987). Sequential masking during eye fixations in reading. Bulletin ofthe Psychonomic Society, 25, 175-178.
Spragins, А. В., Lefton, L. A., & Fisher, D. F. (1976). Eye movements while reading and searching spatially transformed text: A developmental examination. Memory & Cognition, 4, 36-42.
Trueswell, J. C., Tanenhaus, M. K., & Kello, С. (1993). Verb-specific constraints in sentence processing: Separating effects of lexical preference from garden-paths. Journal ofExperimental Psychology: Learning, Memory, and Cognition, 19, 528-553.
Underwood, G., Bloomfield, R., & Clews, S. (1988). Information influences the pattern of eye fixations during sentence comprehension. Perception, 17, 267-278.
Underwood, G, Clews, S., & Everatt, J. (1990). How do readers know where to look next? Local information distributions influence eye fixations. Quarterly Journal ofExperimental Psychology, 42A, 39-65.
Underwood, N. R., & McConkie, G. W. (1985). Perceptual span for letter distinctions during reading. Reading Research Quarterly, 20, 153-162.
Underwood, N. R., & Zola, D. (1986). The span of letter identification for good and poor readers. Reading Research Quarterly, 21, 6-19.
Vitu, F. (1991). The influence of parafoveal processing and linguistic context on the optimal landing position effect. Perception & Psychophysics, 50, 58-75.
Vitu, F., O'Regan, J. K., Inhoff, A. W., & Topolski, R. (1995). Mindless reading: Eye movement characteristics are similar in scanning strings and reading text. Perception & Psychophysics, 57, 352-364.
Vitu, F., O'Regan, J. K., & Mittau, M. (1990). Optimal landing position in reading isolated words and continuous text. Perception & Psychophysics, 47, 583-600.
Wolverton, G. S., & Zola, D. A. (1983). The temporal characteristics ofvisual information extraction during reading. In K. Rayner (Ed.), Eye movements in reading: Perceptual and language processes (pp. 41-51). New York: Academic.
Zola, D. (1984). Redundancy and word perception during reading. Perception & Psychophysics, 36, 277-284.
Manuscript received August 16, 1996 Accepted December 3, 1996
EYE MOVEMENTS IN READING 339
   Available online at www.sciencedirect.com
ScienceDirect
Procedia Manufacturing 3 (2015) 5427 – 5434
6th International Conference on Applied Human Factors and Ergonomics (AHFE 2015) and the Affiliated Conferences, AHFE 2015
User centered evaluation of interactive data visualization forms for document management systems
Antje Heinicke*, Chen Liao, Katrin Walbaum, Jennifer Bützler and Christopher M. Schlick
Chair and Institute of Industrial Engineering and Ergonomics of RWTH Aachen University, Bergdriesch 27, 52062 Aachen, Germany
Abstract
In order to manage the overload of digital information in the SMEs document management is becoming increasingly important. With a DMS documents can be searched, checked, edited and forwarded, which simplifies the handling of documents for the employees. Compared to the inciting and joyful designed user interfaces used for private matters, the interfaces of the up-to-date DMS are lagging far behind regarding usability since DMS are usually designed according to functional aspects. When solving the tasks, positive user experiences and joy of use are rather rare although this can help to encourage the acceptance and positive attitude towards software. In order to improve the usability aspects of DMS, in a first step, interactive visualizations were developed for the DMS data analysis and were tested for usability and attractiveness. Results of the study show that zoomable tree map is the most appropriate visualization type for DMS data and thus is recommended for interactive presentation of data structures.
Keywords: Document Management, Joy of use, Data visualization, Usability
* Corresponding author. Tel.: +49 241 80 99 456; fax: +49 241 80 92 131. E-mail address: a.heinicke@iaw.rwth-aachen.de
           © 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license
© 2015 The Authors. Published by Elsevier B.V.
(http://creativecommons.org/licenses/by-nc-nd/4.0/).
Peer-review under responsibility of AHFE Conference.
Peer-review under responsibility of AHFE Conference
  2351-9789 © 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
Peer-review under responsibility of AHFE Conference
doi:10.1016/j.promfg.2015.07.669
5428 Antje Heinicke et al. / Procedia Manufacturing 3 (2015) 5427 – 5434
1. Introduction
As the overload of digital information in small and medium enterprises (SMEs) requires an adequate management, DMSs increasingly gain importance [1]. Due to its cross-process functionality DMS are applied in SMEs across all departments and disciplines. Compared to user interfaces that are designed for private matters, interfaces of the up-to-date DMS are lagging far behind regarding usability [2]. Poor usability reduces the effectiveness and efficiency in accomplishing specific tasks, and lowers the system’s overall acceptance. Woywode et al. [3] showed that increased usability is an important differentiating characteristic in the software market which contributes to an immediate increase in competitiveness.
The main goal of data visualization is to communicate information clearly and effectively. Both aesthetic form and functionality need to go hand in hand. The tree view chart is often used for data visualization [4] since this node link diagram with nodes expanding from left to right allows users to see the data in a traditional way. Another frequently used diagram is sunburst a circular design with nodes expanding from the inside to the outside. Here the size of the nodes’ area can be determined by their metadata (e.g. amount, volume, importance). Thus the proportion between the metadata can be detected visually. As a traditional tool in visualization, zoom is quite indispensable when large graph structures are explored [5]. It helps to focus on specific areas by reducing the visual complexity.
One possible way to increase the usability of DMS is to include interactive data visualizations into the data analysis section. This can help users to get a general view of the overall data (sum of invoices), and find the required figures (amount of paid or outstanding invoices) quickly. In a DMS various data about creditors and debtors can be evaluated and presented. This involves amongst others the representation of sales, payments made and outstanding items. Most of these visualizations to display the metadata are in a static form but interactive can lead to higher usability as users are allowed to make direct manipulation or navigation when using the charts.
A comparison of different data visualizations shows, that the sunburst chart came out on top regarding aesthetics and was among the best rated visualizations in terms of efficiency and effectiveness [6]. Here, the used dataset comprises of a file directory structure containing subfolders and files. Research indicate that the tree view possessed a faster response time for correct answers, and people tended to spend more time on this visualization before they gave up on a difficult tasks. In addition users were quicker in solving data-retrieval tasks with the tree view chart. However, this online study examined the visualizations only in a static form. Accordingly, it has to be examined whether this result also applies to interactive visualizations of DMS data analysis structures.
Therefore, in this study four different interactive visualization techniques for DMS creditor data were developed based on sunburst and tree view charts combined with zoom techniques. The visualization forms were implemented interactively using Web technologies and tested in an empirical study. The aim was to examine how efficiently and effectively information can be detected in the different visualization forms and which one will be rated by the participants as most joyful. Based on the research mentioned above it is to expect that there are no major differences between tree-based and sunburst-based visualizations regarding execution times and error rates. It is assumed that the zoomable visualizations are seen as more attractive than the other visualizations.
2. Method
2.1. Participants
21 subjects aged between 20 and 31 years (M=25.57, SD=2.69) participated in the study. All participants had experience in document management; five participants used particular document management software to manage their documents. The participants rated their software skills as ‘good’.
2.2. Design
The experimental analysis was based on a full factorial design with one within-subject factor. The type of interactive visualization form was the within factor. It had four levels: 1) Collapsible tree, 2) sequence sunburst, 3) zoomable tree map and 4) zoomable sunburst are displayed in Table 1.
Table 1 Visualization types used
Antje Heinicke et al. / Procedia Manufacturing 3 (2015) 5427 – 5434 5429
       State
   Visualization
  Collapsible tree
   Sequence sunburst
  Zoomable tree map
   Zoomable sunburst
     Initial state
                     Ex- panded / high- lighted/ zoomed state
              Collapsible tree is a visualization type with expandable nodes and links that represent the relationship between parent and child data nodes. A single click on a node expands or folds the node so that child nodes could or could not be seen. Users read from left to right and click the nodes along the path to find a specific file. As initial state, only three levels were shown. For the collapsible tree continuous edges were used since their use leads to good results regarding execution times and error rates [9].
Sequence sunburst is a circular design that constantly shows all nodes in the system. Higher levels are positioned outside the circle. The size of the area that a node occupies is related to its metadata and therefore its children’s metadata (here total amount of invoice/s). When the cursor is positioned on a node, its parent nodes are highlighted in a path, so that the hierarchical structure became more obvious.
Zoomable tree map is a resizable design consisting of rectangle nodes. The size of the node area is related to its metadata and therefore its children’s metadata (here total amount of invoice/s). As initial state, only three levels were shown. All nodes are clickable in this visualization. When clicking on a node it became the start node with the next two children levels shown. The levels above the clicked node disappear, only his parent node is still visible to its left as a button to go back in the structure.
Zoomable sunburst is a resizable circular design. As in sequence sunburst and partition layout the size of the node area is related to its metadata. As initial state, only three levels were shown. All nodes are clickable in this visualization. When clicking on a node it became the start node with his next three children levels shown around him. The levels above the clicked node disappear, only his parent node is still visible in the center circle to go back in the structure.
Effectiveness, efficiency, attractiveness and mental effort were investigated regarding the type of visualization. In order to evaluate the effectiveness, the relative frequency of not successfully processed tasks was recorded. For the efficiency task execution time and eye-tracking data (fixations and length of scan path) were analyzed.
Regarding attractiveness the perceived usability and joy of use were measured with the questionnaire AttrakDiff [7]. The AttrakDiff questionnaire measures the pragmatic and hedonic quality of a system. A product has pragmatic quality if it supports the task completion effectively and efficiently. The aspects related to the hedonic
5430 Antje Heinicke et al. / Procedia Manufacturing 3 (2015) 5427 – 5434
quality should bring joy and fun to the user, for example, when a product works particularly stimulating. It consists of 23 seven-level items whose endpoints are each formed by a contrasting adjective (e.g. "confusing" - "clear", "superior" - "usual", "good" - "bad"). In each case, several items are combined into a scale. The mean value of the items, constitute the scale value (from 3 very good to -3 very bad) for pragmatic quality (PQ), hedonic quality (HQ) and attractiveness (ATT).
The Rating Scale Mental Effort (RSME), developed by Zijlstra [8], was used for the measurement of mental effort. The RSME consists of a scale with a range of 0–150, with nine descriptive indicators ranging from 3 (not effortful) to 114 (awfully effortful). Participants are asked to mark a point on the scale which reflects their amount of mental workload invested in the task performance.
The data structure and the actual data which underlies the visualizations were varied to avoid learning effects; one structure was sorted according to the years (year creditor paid|outstanding invoice order|invoice|delivery note, the other one according to creditors (creditor year paid|outstanding invoice order|invoice|delivery note. The documents related to the invoices (order, invoice, delivery note) were openable as pdf with a double click.
The participants had to conduct four types of tasks with the data visualizations: Determination of creditors which charged the highest total amount of invoices in a given year, determination of the year in which the highest total amount of invoices was charged by a given creditor, determination of the total amount of invoices which was charged in a given year by a given creditor and a navigation task, where a specific invoice document had to be opened.
2.3. Apparatus
The experiment was conducted using a 22” TFT-monitor with a resolution of 1680x1050px, a standard keyboard and a standard mouse. Eye movements were measured using a SMI RED500 Eye Tracking system. The viewing distance was set to 50cm, with the aid of marks on the ground (chair position). The corresponding online questionnaire was filled in using a second 22” TFT-monitor, a standard keyboard and a standard mouse.
The visualizations were developed as a combination of D3.js and CSS embedded in a HTML document. D3.js is a JavaScript library with extensive visualization components and a data-driven approach to DOM manipulation [10]. It was selected as the basis visualization tool since it provides numerous kinds of chart models and many options regarding customization and interaction.
The experiment’s interface consisted of four parts, a breadcrumb trail at the top left containing a metadata text field at the top center and the visualization graph in the center with the instruction on top. The breadcrumb trail was designed to show users where they were located in the data structure (See Fig. 1), since in some types of visualization the root node became invisible after zooming into the chart. The metadata text field displayed numbers that were asked in the tasks. The instruction was displayed in the upper screen area so that no head movement took place during the task.
Fig. 1. Breadcrumb for the year based data structure with all possible instances
2.4. Procedure
In the beginning the participants had to fill in a questionnaire regarding demographic data and software knowledge. Before the experiment began, as introduction a background scenario was presented to the participants. After the scenario, the first run of the main experiment started. During the main test participants were asked to solve four different tasks according to four visualizations. The sequence of the visualizations was permutated. For each

Antje Heinicke et al. / Procedia Manufacturing 3 (2015) 5427 – 5434 5431
visualization type, participants had to do a training task, in which they could get familiar with the visualization and with the interaction. They were free to ask any questions. During the actual test, they were asked to find the answer as soon as possible, and were not allowed to ask any questions. Before each main task started, the eye-tracking system was calibrated. After finishing the tasks for one visualization type, the participants filled in the AttrakDiff questionnaire and rated their mental effort using the RMSE scale for this visualization type. At the end of all four runs, there was a final questionnaire which allowed the participants to select their favorite visualization and give some feedback.
2.5. Data analysis
The statistical analysis in this work was calculated using the statistical software package SPSS Version 21.0. Data for mental effort, attractiveness, task execution time, fixation count and scan path length were not normally distributed. The Friedman-Test was used to analyse the differences between the variables’ means. The chosen level of significance for each analysis with the Friedman-Test was α=0.05. Post hoc analysis was done with Wilcoxon signed-rank tests using a Bonferroni correction, resulting in a significance level set at α=0.0125.
3. Results
3.1. Effectivity
The analysis showed a significant effect of the factor visualization type on the number of errors (χ2(3) = 15.84, p = 0.001). The zoomable sunburst visualization resulted descriptively in the lowest error rate (see Table 2), whereas statistically only the difference between zoomable tree map and zoomable sunburst was significant (Z = - 3.162, p = 0.002).
Table 2 Error rate depending on visualization type
Error rate for task solving
3.2. Effectiveness
Collapsible tree 4,8%
Sequence sunburst 4,8%
Zoomable tree map
3,6%
Zoomable Sunburst
15,5%
       There was a significant difference in task execution time depending on which type of visualization was used (χ2(3) = 10.686, p = 0.014). As depicted in Figure 2 the zoomable sunburst lead to highest execution times but the post-hoc tests only showed a statistical significant difference between zoomable tree map and zoomable sunburst (Z = -2.778, p = 0.005). Descriptively the zoomable tree map showed the lowest execution times.
 30 25 20 15 10
5 0
               Collapsible tree
Sequence sunburst Zoomable tree map Zoomable sunburst
Fig. 2. Mean task execution time
Mean task excecution time [s]
 5432 Antje Heinicke et al. / Procedia Manufacturing 3 (2015) 5427 – 5434
Regarding the eye tracking data, fixations on the visualization area, fixations on the breadcrumb area and the scan
path length were analyzed (see Fig. 3).
60 8 8
77 66 55
50
40
30 4 4
20 10
33 22 11
000
Fig. 3. Eye tracking data: Mean fixation count visualization (left), mean fixation count breadcrumb (middle), mean scan path length regarding visualization area (right)
There was a statistical significant difference in fixation counts on the visualization area depending on which type of visualization was used, χ2(3) = 13.958, p = 0.003. Here the differences between zoomable sunburst and collapsible tree (Z = -4.281, p = 0.00002), zoomable sunburst and sequence sunburst (Z = -3.105, p = 0.002) as well as zoomable sunburst and zoomable tree map (Z = -3.832, p = 0.00013) are significant. Regarding fixation counts on the breadcrumb area, there are significant differences depending on the visualization type, χ2(3) = 11.025, p = 0.012. Here the pair zoomable tree map and collapsible tree shows significant differences, Z = -2.904, p = 0.004. The data for the length of scan path regarding visualization area shows also significant differences depending on type of visualization χ2(3) = 8.629, p = 0.035. The scan path length for zoomable sunburst is significantly higher to the scan path length of collapsible tree (Z = -3.559, p = 0.0004), to the scan path length of sequence sunburst (Z = -3.175, p = 0.0015) as well as to the scan path length of zoomable tree map (Z = -3.528, p = 0.0004).
3.3. Subjective evaluation
Fig. 4 shows the mean values of the AttrakDiff qualities. The hedonic quality is divided into the dimensions identity (e.g. valuable) and stimulation (e.g. innovative) according to [7].
Mean fixation count visualization area
Mean fixation count breadcrumb area
Mean scan path length [tsd px]
Pragmatic quality
Hedonic quality-identity Hedonic quality- Attractivness 3 stimulation 3
3 2222 1111 0000
3
-1 -1 -1 -1 -2 -2 -2 -2 -3 -3 -3 -3
Fig. 4. Comparison of the four visualizations’ mean values for the four qualities of the AttrakDiff questionnaire
Antje Heinicke et al. / Procedia Manufacturing 3 (2015) 5427 – 5434 5433
Significant difference exists regarding pragmatic quality (χ2(3) = 11.668, p = 0.009) for the pairs sequence sunburst and zoomable tree map (Z = -3.131, p = 0.002) as well as zoomable sunburst and zoomable tree map (Z = - 2.820, p = 0.005). Also for the hedonic quality-stimulation (χ2(3) = 18.778, p = 0,0003) regarding the pairs zoomable sunburst and collapsible tree (Z = -2.597, p = 0.009), sequence sunburst and zoomable tree map (Z = - 2.676, p = 0.007) as well as zoomable sunburst and zoomable tree map (Z = -3.027, p = 0.002).
Table 3 shows the results for the in the final questionnaires made subjective evaluations. For the pair-by-pair comparison the participants should select out of two visualizations the one they liked better. In this way each visualization was compared to all the others. Collapsible tree won the most pair-by-pair comparisons followed by zoomable tree map. Finally, the participants were able to specify, which visualization they liked best of all; zoomable tree map was named by the most participants.
Table 3 Subjective evaluation final questionnaire
Wins pair-by-pair comparisons Mentions best visualization
3.4. Mental effort
Collapsible tree
3 6
Sequence sunburst
0 3
Zoomable tree map
2 9
Zoomable sunburst
1 3
     Fig. 5 shows the results for the rating scale mental effort.
 50 45 40 35 30 25 20 15 10
5 0
                        Collapsible tree
Sequence sunburst
Zoomable tree map
Zoomable sunburst
Fig. 5. Mean RSME Values for the four visualizations
Zoomable tree map has the lowest value with ‘little effort’ (regarding mean value). Sequence sunburst has the highest value with ‘some effort’ (regarding mean value). There was a statistically significant difference in RSME depending on which type of visualization was used (χ2(3) = 13.781, p = 0.003). Here, significant differences exist for the pair sequence sunburst and zoomable tree map (Z = -2.848, p = 0.004) as well as for the pair zoomable tree map and zoomable sunburst (Z = -2.994, p = 0.003).
4. Discussion
Contrary to expectations, it cannot be confirmed that tree-view based and sunburst based visualizations are equally good regarding efficiency and effectiveness. When using zoomable sunburst the number of fixations and the scan path length were significantly higher respectively longer compared to the other visualizations. Regarding relative frequency of not successfully processed tasks, task execution time and RSME the measured values for zoomable sunburst are significant higher than those for zoomable tree map. In terms of AttrakDiff’s pragmatic quality the ratings for zoomable sunburst are significant lower than those for zoomable tree map. Only the values for AttrakDiff’s hedonic quality-stimulation and attractiveness for zoomable sunburst were significant higher than for
Mean RSME - Value
5434 Antje Heinicke et al. / Procedia Manufacturing 3 (2015) 5427 – 5434
collapsible tree and zoomable sunburst. Also zoomable sunburst was preferred to sequence sunburst in the pair-by- pair comparison. The other tested sunburst version, sequence sunburst, reached on average better results, however, gets only for AttrakDiff's hedonic quality-stimulation significant higher ratings than one other visualization (zoomable tree map). Zoomable sunburst has not won a single pair-by-pair comparison. In contrast to the results from another study [2] where sunburst was among the best rated visualizations in terms of efficiency and effectiveness it cannot be confirmed that sunburst is a good method to present the DMS creditor data in an interactive form. Hence, sunburst can be used as gimmick but is not useable for everyday work with large data sets.
The usual visualization type for data structures, collapsible tree, ranks in the middle regarding efficiency, effectiveness and attractiveness. It won the pair-by-pair comparison against all other visualizations and is in principle well suited to present the used data.
Even though the differences to all other visualization are not always significant, when using zoomable tree map the best results regarding the following values were achieved: relative frequency of not successfully processed tasks, task execution time, fixation count visualization, fixation count breadcrumb, RSME and AttrakDiff’s pragmatic quality. Also it was most often mentioned as the best visualization and won the pair-by-pair comparisons against zoomable sunburst and sequence sunburst. Only for AttrakDiff’s hedonic quality-stimulation zoomable tree map get the lowest ratings from all visualizations (significant in comparison with the two sunburst diagrams). According to the study’s results zoomable tree map is the most appropriate visualization type for the used DMS data and thus can be recommended for interactive presentation of data structures.
Acknowledgements
The joint research project uSelect DMS (01MU12018A) is supported by the Federal Ministry of Economics and Technology within the framework of the research program "SME-Digital" initiative "Simply intuitive - usability for SMEs" program.
References
[1] Sontow K, Treutlein P, Sontow R (2012), ERP in der Praxis - Anwenderzufriedenheit, Nutzen & Perspektiven. Trovarit AG, 2012.
[2] Heinicke, A.; Bröhl, C.; Bützler, J.; Schlick, C.: Usability of Document Management Systems Considering Users' Level of Experience: A Survey, In: Proceedings of the 5th International Conference on Applied Human Factors and Ergonomics 2014 (AHFE), Hrsg.: Ahram, T.;
Karwowski, W.; Marek, T., The Printing House, Inc., Stoughton, FL, USA 2014, ISBN 978-1-4951-1572-1, S. 359-367
[3] Woywode et al. 2012, Woywode, M., Mädche, A., Wallach, D., Plach, M., „Gebrauchstauglichkeit von Anwendungssoftware als Wettbewerbsfaktor für kleine und mittlere Unternehmen (KMU)“, http://www.usability-in-germany.de/ergebnis, letzter Aufruf 08.04.2015,
2012
[4] Plaisant, C.; Grosjean, J.; Bederson, B.B.: SpaceTree: Supporting Exploration in Large Node Link Tree, Design Evolution and Empirical
Evaluation. In: IEEE Symposium on Information Visualization, 2002. INFOVIS 2002, 2002, S. 57–64
[5] Herman, I.; Melancon, G.; Marshall, M.S.: Graph Visualization and Navigation in Information Visualization: A Survey. In: IEEE
Transactions on Visualization and Computer Graphics Bd. 6 (2000), Nr. 1, S. 24–43
[6] Cawthon, N. and Vande Moere, A..: The effect of aesthetic on the usability of data visualization, In IEEE Int. Conf. Info Vis (IV’07), Zurich,
Switzerland, pages 637–648, 2007.
[7] Hassenzahl, M.; Burmester, M.; and Koller, F. 2003. AttrakDiff: Ein Fragebogen zur Messung wahrgenommener hedonischer und
pragmatischer Qualität. In: Mensch& Computer 2003. Interaktion in Bewegung, Hrsg.: J. Ziegler & G. Szwillus, S. 187-196.
[8] Zijlstra, F. R. H. 1993. Efficiency in work behavior: A design approach for modern tools. Delft: Delft University Press.
[9] Bützler, J.; Bröhl, S.;Schlick, C. M. Altersrobuste kognitionsergonomische Gestaltung von Netzplänen in Projektmanagement Software. In
VerANTWORTung für die Arbeit der Zukunft, 61. Kongress der Gesellschaft für Arbeitswissenschaft. GfA-Press, 2015.
[10] Bostock, M.; Ogievetsky, V. ; Heer, J.: D3: Data-Driven Documents. In: IEEE Transactions on Visualization and Computer Graphics Bd. 17
Visual Analysis of Perceptual and Cognitive Processes
Michael Raschke1, Tanja Blascheck1, Marianne Richter2, Tanja Agapkin1, Thomas Ertl1 1Institute for Visualization and Interactive Systems, University of Stuttgart, Universita ̈tsstrasse 38, 70569 Stuttgart, Germany
Keywords: Abstract:
2EXC Simulation Technology, University of Stuttgart, Pfaffenwaldring 5a, 70569 Stuttgart, Germany michael.raschke@vis.uni-stuttgart.de
visual analysis, perception processes, cognition processes, visualization
The success of visualization techniques depends on their support of perceptual and cognitive processes to perceive the graphically represented information. Apart from measuring accuracy rates of correctly given answers and completion times in user studies, eye tracking experiments provide an additional technique to analyze perceptual and cognitive processes of visual tasks. This paper presents an interdisciplinary approach for studying structures of scan paths by visual means. We propose to annotate graphical elements with semantic information. This annotation allows us to analyze the fixation sequences on these annotated graphical elements with respect to reading processes, visual search strategies, and visual reasoning.
1 INTRODUCTION
A key factor for the success of a visualization technique is the efficiency of how users perceive information using this visualization technique. Apart from measuring completion times and recording accuracy rates of correctly given answers during task performances, eye tracking experiments provide an additional technique to analyze how the attention of a visualization observer is changing on a presented stim- ulus. Therefore, gazes on the stimulus are recorded and afterwards aggregated to fixations and saccades for measuring which areas on the stimulus have been focused on. These scan paths show fixations dur- ing a moment-by-moment processing of a visual task (Koerner, 2011), (Rayner, 1998) and therefore are most suitable to identify or differentiate between comprehension processes (Grant and Spivey, 2002).
To better study cognitive and perceptual processes during a visual task, we propose to annotate task relevant areas (areas of interest) on the stimulus with information about the semantic meaning of graphi- cal elements inside these areas. As an appropriate visualization technique to study both declarative and procedural cognitive processes we are using the parallel scan path visualization technique (Raschke et al., 2012). By mapping semantic information to the area of interest axes in the parallel scan path visualization our approach allows us to study mental processes such as visual search, visual reasoning as well as cross-checking behavior.
2 RELATED WORK
Eye tracking is used in a wide field of user experi- ments in psychology, cognitive science, marketing, vi- sualization, human-computer, and human-machine in- teraction. This paper will focus on the analysis of eye tracking experiments in the visualization research do- main. One example of an eye tracking study in visu- alization research is the comparison of different types of graph layouts such as radial, orthogonal and tradi- tional conducted by Burch et al. (Burch et al., 2011). We will later use this eye tracking experiment for a demonstration of our approach. Another example is the eye tracking experiment by Huang et al. Their re- sults show that graphs are read following a geodesic- path tendency. As a result, links which go towards the target node are more likely to be searched first (Huang et al., 2009). Kim et al. investigate the influence of peripheral vision during the perception of visualiza- tions (Kim et al., 2012).
Different approaches have been developed to analyze eye tracking data. One approach is to an- alyze eye tracking results with statistical methods to find correlations between different dimensions in the recorded eye tracking data (Holmqvist et al., 2011). These correlations can later be identified with metrics such as cognitive workload. The statistical analysis can be supported by techniques from visual analytics as discussed by Andrienko et al. (Andrienko et al., 2012). If the definition of areas of interest is possible, string editing algorithms can be used to find
patterns in eye movements between these areas of interest (Duchowski et al., 2010).
Classical visualization approaches for eye tracking data are heat map (Wooding, 2002) and scan path visu- alizations (Spakov and Miniotas, 2007). Different ap- proaches of eye tracking data visualizations have been developed to satisfy various kinds of experimental set- ups and research questions. One implementation of an eye tracking data visualization is eSeeTrack by Tsang et al. that combines a time line and a tree-structured visual representation to extend current eye-tracking vi- sualizations by extracting patterns of sequential gaze orderings (Tsang et al., 2010). A method, which can be classified in-between visual and statistical analysis techniques, is presented by Ponsoda et al. The authors use transition matrices for analyzing eye movement recordings during free viewing (Ponsoda et al., 1995). Opportunities and challenges for developing new vi- sualization techniques for eye tracking data analysis have been discussed by Ramloll et al. (Ramloll et al., 2004).
Although many approaches have been developed to analyze eye movements, the interpretation of one or more scan paths with respect to focused areas on the stimulus and relating cognitive and perceptual processes is complicated and there is no common way of interpretation. To fill this gap between statistical results or visualizations of eye-tracking data and their interpretation, Conversy et al. present a descriptive model of visual scanning (Conversy et al., 2011). The authors have developed a method to describe an idealized scanning of visual representations. Where the assumption of an idealized scan path is useful for designing visualizations or simulation models of cognition processes, typical scan paths from eye tracking experiments do not show this ideal behavior. Therefore, common patterns of eye movements have to be extracted in order to formulate ideal scan paths. Additionally, these findings can also be used to set up models for simulating perceptual and cognitive processes of visual understanding (Pinker, 1990; Lohse, 1993; Shah, 1997). In our work we propose to annotate areas on a visualization with semantic information to better analyze the scan paths on the stimulus with respect to procedural and declarative knowledge processes. Besides using only simple word lists of identifiers for graphical elements our approach goes further and allows us to annotate areas on visualizations with more complex information from ontological models about graphical knowledge as presented by Pinker et al. in his visual search modeling algorithms.
Finally, an adequate visualization technique is missing, that both graphically shows fixation se-
quences on annotated areas on a stimulus and allows to study cognitive and perceptual processes. In this work we will use the parallel scan path visualization technique that maps gaze durations and fixations inside areas of interest to vertical axes in one dia- gram (Raschke et al., 2012). Areas of interest are defined on the stimulus and are mapped to vertical coordinate axes in the parallel scan path plot. The leftmost axis indicates time, starting from the bottom of the diagram with the start time of the eye tracking measurement. The orientation of the parallel scan path visualization is arbitrary. In the following we use a vertical time axis from bottom (start of the scan path recording) to top (end of the scan path recording) as introduced in the original work. The horizontal axis displays all selected areas of interest as independent values. Saccades between areas of interest are indicated with dashed lines. Ascending lines indicate fixations outside given areas of interest. Using the parallel scan path visualization for our approach, areas of interest axes in the parallel scan path visualization will be identified with semantically annotated areas on the stimulus.
3
MOTIVATION AND CONCEPT
Our concept is based on three assumptions accord- ing to findings during eye tracking user studies and to the literature:
1. To perform a given task using a visualization, task relevant areas on the stimulus are focused on in serial order. We assume that the sequence of fixations on these areas represent perceptual and cognitive processes (Koerner, 2011), (Rayner, 1998), (Grant and Spivey, 2002).
2. For the analysis of perception and knowledge processes we are dividing up knowledge into declarative knowledge and procedural knowledge according to standard classification of psychology and cognitive sciences (Anderson et al., 2004a).
3. Basedonthisdefinitionweclassifyaninformation seeking method on a stimulus using two kinds of visual tasks: 1) Visual search and reading processes without drawing a conclusion - the semantic meaning of graphical elements and declarative knowledge is of interest - (cf. Fig- ure 2a+d) and 2) tasks where the observer of a visualization has to draw a conclusion - the syntactical relations of graphical elements are of additional interest. Declarative knowledge as well as procedural knowledge play an important role (cf. Figure 2b+c,e+f).
 Figure 1: Graphical elements are annotated with semantic information either from an ontology or by using identifiers from word lists. The fixation sequence is analyzed with the parallel scan path visualization technique.
Based on these three assumptions this paper con- tributes an analysis concept with three steps (cf. Fig- ure 1):
1. Taskrelevantareasonthestimulushavetobeiden- tified.
2. Theseareashavetobesemanticallyannotatedwith respect to the meaning of graphical elements inside these areas. This annotation can either be done classically with identifiers from word lists or with links to elements from an ontology that describes graphical elements and their relations for a given visualization concept.
3. Finally, saccades between these areas as well as fixation durations and summarized fixation dura- tions outside these areas are visually analyzed.
In the following we will describe these three steps
in more detail.
3.1 Identification of Task Relevant Areas
In most cases, areas with a high density of fixations of one or more participants indicate graphical elements which are relevant to perform a given task. These ar- eas can simply be identified with heat maps (cf. Fig- ure 1, middle). After relevant graphical elements of the stimulus are identified, areas of interest are defined which contain these graphical elements.
3.2 Annotation of Task Relevant Areas
Areas of interest are annotated with information about their semantic meaning (cf. Figure 1, top). The se- mantic information could be the type of the graphical element, its name, or a reference to another resource that contains further information about this graphical element depending on the research question. An adequate naming of the graphical elements is crucial for the syntactic and semantic analysis of the scan paths (Perini, 2005), (Pinker, 1990). If an ontology or a model of visualization knowledge is available, additional information such as the relationship of focused graphical elements can be used during the analysis.
3.3 Visual Analysis
Figures 2a-c show three exemplary parallel scan path visualizations for a) visual search and reading processes, b) visual reasoning, and c) mixed pro- cesses representing different types of perceptual and cognitive processes.
Figure 2a shows an idealized scan path for reading the position of a given point in a Cartesian coordinate system (Figure 2d shows the stimulus). An idealized fixation sequence could be the following: focus on the data point (1), on the x-axis to read the corresponding x-value (2), on the data point again (3), followed by a reading of the vertical position on the y-axis (4). At this moment, all task relevant graphical elements have been focused on at least one time to answer the task. Figure 2b shows the idealized scan path for the task
“Why is the road wet?” (cf. Figure 2e) that could be to first focus on the road (1), then on the puddle (2), on the cloud (3), on the sun (4), and on the fire hydrant (5). Finally, the attention could move to the puddle again (6). This fixation sequence shows single steps of logical reasoning: The solution is to verify, if there is rain coming from the cloud (false), or if the fire hydrant is sprinkling (true), or if both is true (false). Finally, Figure 2c shows an ideal scan path for the task “Is the lowest temperature of all given temperatures in
     (a) (b) (c)
(d) (e) (f)
Figure2: Idealizedscanpathswhichshowsimpleexamplesofscanpathstoillustratethevisualanalysisapproachpresentedin this paper: visual search processes (a+d) and reading of plotted values in a diagram, an exemplary reasoning task (b+e) and an exemplary visual analysis of a simple mixed task with visual search, reading, and reasoning (c+f).
     southern France lower than the given temperature in northern France?” (cf. Figure 2f). The idealized scan path shows a first fixation on the −10◦C label in the south east of France (1), then on the −4◦C label in the south (2). The next step is to move the attention to the northern part of France to read the 0◦C label (3). By memorizing the lower temperature from the south and comparing it with the one in the north the answer to the question can be given. This fixation sequence indicates that declarative knowledge has to be memorized and processed. Times for reading the temperature values and memorizing them as well as restoring them again from memory can be directly seen in the visualization.
study design. At the beginning of every scenario, participants were given a short tutorial with instruc- tions to the following task. Afterwards the actual experiment was started. Before every measurement in a scenario, we calibrated the eye tracker system Tobii T60 XL with the calibration algorithm im- plemented in Tobii Studio 2.7. To further confirm this calibration of all stimuli during a run of a task, we showed a black cross in the center of the screen between every stimulus. Subjects had to focus on this marker. To start a task in a scenario, participants had to press a key on the keyboard. Then, the stimulus was presented. Subjects had to press a key on the keyboard again, if they were sure to answer the given question. The stimulus disappeared and the black cross was presented again. Then, the participants had to give their answer. The fourth case study is based on a an experiment conducted by Burch et al. Due to space limitation we will refer to the original work (Burch et al., 2011) for a detailed description of the experiment.
4.1 Fixation Loops during a Visual Task
Twenty-four different bar charts were used in the first task block and have synthetically been generated with randomized values for each category. To study different visual search strategies we have used four types of labels of the x-axis: alphabetically, non- alphabetically, with increasing numbers, and with
4
CASE STUDIES
In this section we will demonstrate how the pre- sented approach can be applied to four case studies using data from real eye tracking experiments. Due to our work aiming at demonstrating the presented approach, we will concentrate on showing fixation sequences of some exemplary stimuli and participants. The first three case studies are based on a pilot study with ten participants. With this pilot study we tested the experimental design and set-up for a future eye tracking experiment with forty participants to study perception and cognition processes in different vi- sualization scenarios. For every scenario, we have designed a separate experiment with a within-subject
randomized numbers. Participants had to perform two classes of tasks: 1) read the value of a bar for a given category and 2) compare the values of two bars of two given categories with each other and state the value of the higher one. In this sub section, we will show results of an analysis of one stimulus and recorded fixations of one participant who has performed a task of the first task class. A demonstration of an analysis for a result from the second task class will be given in the next sub section. Figure 3a shows a stimulus presenting a bar chart. In this example we have used a numerical labeling of the bar categories with continuous, increasing, natural numbers. The participant had to read the value of the bar with category “7”. Figure 3b shows a heat map over all ten participants for this task. The annotated areas on the stimulus are presented in c). We have defined four areas of interest: one for the label indicating bar category “7”, one at the top point of the bar and one at the corresponding value label on the y-axis. A fourth area of interest was used to check, if the participant had focused on the black cross at the beginning of the task. Figure 3d shows the parallel scan path visualization of the fixation sequence for one participant. The completion time for this task and this participant was approximately 4.5 seconds. First, the participant focused on the black cross, then searched for the bar label. Afterwards, his attention alternately moved from the bar top to the bar value and back, followed by a short check of the correct category. At this moment, he has focused on all task relevant graphical elements on the stimulus and thus could have answered the task. However, he did not state the answer, but performed another reading of the bar value to cross-check his answer. This second loop of fixations can clearly be seen in the fixation plot starting at 3 seconds.
4.2 Visual Search
Figure 4a shows one stimulus for the second class of tasks of the first task block with a heat map of all par- ticipants (b), the areas of interest with the annotations (c), and the parallel scan path plot of one participant from the experiment (d). The task was to compare the values of the bars of category “H” and category “B” and to give the value of the higher one. The visualiza- tion shows, that the participant at first searched for the labels of category “H” and “B” starting from the label
“I”. This process is shown by an ascending plot line from left to right in the parallel scan path plot starting from the bottom with two fixations on label “H”. The completion time for this visual search process is ap- proximately 4.5 seconds. Once, the location of the bar
for the categories “H” and “B” were found, the par- ticipant compared the height of the top points of the bar. This is graphically represented by an alternating attention between “top point bar B” and “top point bar H”. Finally, the participant read the bar value for bar “H” by moving his attention to the area of interest on the y-axis with the bar values.
4.3 Visual Analysis of Reasoning Processes
In a second task block we studied reasoning processes using Venn diagrams. We presented ten Venn dia- grams with different numbers of elements to study different levels of logical complexity. One stimulus is shown in Figure 5a. In this task block partici- pants had to answer logical questions such as “How many people have a high school graduation (in Ger- man “Abitur”) or got their dream job (in German
“Traumjob”) solely by absolving an apprenticeship (in German “Ausbildung”)?”, see Figure 5. These ques- tions represent logical reasoning processes based on logical AND, OR, NAND, NOR, and XOR relations. A heat map over all participants’ fixations is shown in Figure 5b, annotated areas of interest on the stimulus are presented in c), and the parallel scan path of one participant in d). In total, we have defined seven areas of interest. Three areas of interest represent the three labels for the plotted categories on the right side of the stimulus, and four areas of interest show task relevant information such as the number of people with high school graduation, dream jobs, and apprenticeship (numbers inside ellipses).
The parallel scan path visualization shows, that in the first two seconds of the experiment, the participant focused on the labels on the right side of the diagram. Once these three categories were identified and men- tally memorized the participant alternatively moved his attention between the areas of interest “Number of people with high school graduation and dream job”,
“Number of people with a dream job”, and “Number of people with a dream job and apprenticeships”. This second phase represents the logical reasoning process. In the last third of the experiment the participant checked the category labels again and finally stated the answer.
4.4 Visual Analysis of Cross-Checking in a Complex Visualization Scenario
Burch et al. evaluated traditional, orthogonal, and radial tree diagrams and compared accuracy, com- pletion times, and exploration behavior to test six hypotheses for the task: “Find the least common
       (a) (b) (c) (d)
Figure 3: We have studied how participants read values of a bar of a given category in our pilot study. This example shows the task “Please, read the value of bar ’7’.”. The stimulus for this task is shown in a), a heat map of all participants in b), the annotated areas of interest on the stimulus in c), and finally, the parallel scan path visualization of one participant in d). The analysis shows, that this participant performed one “fixation loop” in the second half of the experiment before stating the answer.
(a) (b) (c) (d)
Figure 4: This example shows results of the task “Compare the value of category ’H’ with the value of category ’B’ and state the value of the higher one.”. In d) the ascending parallel scan path from left to right in the first half of the experiment shows the fixations of the visual search process for finding the two labels “H” and “B” followed by comparing the bar heights.
(a) (b) (c) (d)
Figure 5: In a second task block we have studied reasoning processes using Venn diagrams. These figures show results of the analysis of the task “How many people have a high school graduation (in German “Abitur”) or got their dream job (in German “Traumjob”) solely by absolving an apprenticeship (in German “Ausbildung”)?”. For a detail description of the analysis we
refer to the text in section “Case Studies”.
(a) (b) (c) (d)
Figure 6: These four figures show results of the fourth case study where we demonstrate how the presented approach can be used to analyze processes of cross-checking during performing a given task, as discussed by Burch et al.

ancestor from a set of marked leaf nodes.” (Burch et al., 2011). For a demonstration of our approach we will concentrate on the second part of their fifth hy- pothesis “[...] cross-checking is used more frequently in the radial layouts.”. In the case of their experiment “cross-checking” means, that participants find a so- lution for the task and verify this solution several times. In a follow up work Burch et al. proved this hypothesis using visual analytics techniques (Burch et al., 2013). We can verify their results using our approach. Figure 6a shows a radial tree diagram stimulus from the experiment together with the heat map of all participants (Figure 6b). Relevant graphical elements are the solutions (on the left, below the blue point which indicates the root node of the tree) given by the participants and three marked leaf nodes 1, 2, and 3 (marked by three red arrows in the original experiment), see Figure 6c. The parallel scan path visualization is presented in Figure 6d for the time interval between 0s and 24s. It shows three selected participants (blue, red, and green) from the exper- iment. The fixation sequence of the green subject shows, that he at first focused on leaf node 3, then on the solution, and then on leaf node 3 again. Next, his attention moved to the other two leaf nodes and the solution. He did not perform a cross-checking and answered the question after approximately 11s. At this moment he has focused on all relevant semantic graphical elements for this task. This eye movement pattern can be interpreted as an ideal scan path. The blue subject performed cross-checking after having focused on all relevant graphical elements during approximately the first 9s of the measurement. The red participant first shifted his attention between leaf node 3 and 1, then between leaf node 2 and 1.
think, that this question cannot be answered with a definite “yes” or “no”. It depends on the complexity of the visualization, the visual task and the required mental processes. But we believe, that the presented approach can be helpful for discussing this question.
The second step of our analysis approach is to an- notate graphical elements with information about their semantic meaning. In this step an adequate naming of the graphical elements is extremely important for a goal oriented analysis and correct interpretation of the fixation sequences. The question how an adequate naming can be found is an open question and has to be investigated in future work. In our work, we have proposed to either use identifier from word lists according to standard approaches in eye tracking research or to link areas of interest to elements of an ontology. This ontology can then be used during the eye tracking analysis and for the set-up of a cognitive model about declarative knowledge of graphical elements. Another issue about the annotation concept is, that the parallel scan path visualization technique only allows non-overlapping areas of interest. Thus, we cannot annotate graphical elements which are overlapping. This overlapping of several semantic annotation areas additionally leads to further questions about knowledge modeling as in our case it is not clear which semantic meaning of an area of interest on the stimulus is relevant.
During the analysis of the pilot study results we found limitations of the parallel scan path visualiza- tion technique. The fourth case study shows, that the higher the number of visualized participants is, the more difficult it becomes to find common eye movement patterns due to visual clutter of the parallel scan path visualization technique. To reduce this visual clutter, we propose to implement filtering, aggregation, and contextual focusing algorithms into the visualization tool. Additional pattern recognition algorithms such as Levenshtein distance can be used to pre-process the eye tracking data. Thus, only fixation sequences will be shown which are relevant for a certain research question. If two participants strongly vary in their completion times, their comple- tion times could be scaled in order to plot them on the same axis length. This would make it possible to better compare different fixation time stamps with each other and thus, compare different visual search strategies. Another limitation is caused by the limited number of vertical axes which can be presented in the visualization. Additionally, an optimal ordering of these axes can positively influence the analysis. An automatic ordering could be done using information from transition matrices.
5
LIMITATIONS AND IMPROVEMENTS FOR FUTURE WORK
A crucial question for analyzing eye tracking results is whether the recorded eye movements reflect mental processes? There has been a long discussion going on about this question and it is not yet clearly answered. Some works argue that mental processes and eye movements are not correlating (e.g. (Ander- son et al., 2004b)), others argue that eye movements reflect mental activities which they call the “Eye-Mind Hypothesis” (e.g. (Just and Carpenter, 1980) and (Rayner, 1998)). Results of our eye tracking experi- ments show that fixations and sequences of fixations on a stimulus correlate with cognitive processes. We
6 CONCLUSION
This paper contributes an approach to visually ana- lyze eye tracking data with respect to perceptual and cognitive processes based on a semantic annotation of graphical elements inside the visualization. To be able to use the concept in practice we have implemented a prototype and have presented four case studies cover- ing reading processes, visual search strategies, visual reasoning and cross-checking. Finally, we have dis- cussed limitations of this approach and presented pos- sible improvements for future work. Besides tackling limitations in future work, we are planning to use the analysis concept during the development of cognitive models for visual search and visual reasoning.
REFERENCES
Anderson, J. R., Bothell, D., Byrne, M. D., Douglass, S., Lebiere, C., and Qin, Y. (2004a). An integrated theory of the mind. Psychological Review, 111:1036–1060.
Anderson, J. R., Bothell, D., and Douglass, S. (2004b). Eye movements do not reflect retrieval processes. Psycho- logical Science.
Andrienko, G. L., Andrienko, N. V., Burch, M., and Weiskopf, D. (2012). Visual analytics methodology for eye movement studies. IEEE Transactions Visualiza- tion Computer Graphics, 18(12):2889–2898.
Burch, M., Andrienko, G. L., Andrienko, N. V., Ho ̈ferlin, M., Raschke, M., and Weiskopf, D. (2013). Visual task solution strategies in tree diagrams. In Proc. IEEE PacificVIS 2013, pages 169–176.
Burch, M., Konevtsova, N., Heinrich, J., Hoeferlin, M., and Weiskopf, D. (2011). Evaluation of traditional, orthog- onal, and radial tree diagrams by an eye tracking study. IEEE Transactions Visualization Computer Graphics, 17(12):2440 –2448.
Conversy, S., Chatty, S., and Hurter, C. (2011). Visual scan- ning as a reference framework for interactive represen- tation design. Information Visualization, 10(3):196– 211.
Duchowski, A. T., Driver, J., Jolaoso, S., Tan, W., Ramey, B. N., and Robbins, A. (2010). Scanpath compari- son revisited. In Proceedings of the 2010 Symposium on Eye-Tracking Research and Applications (ETRA), pages 219–226, New York, NY, USA. ACM.
Grant, E. R. and Spivey, M. J. (2002). Guiding attention produces inferences in diagram-based problem solving. In Proceedings of the Second International Confer- ence on Diagrammatic Representation and Inference, Diagrams 2002, pages 236–248, London, UK, UK. Springer-Verlag.
Holmqvist, K., Nystrom, M., Andersson, R., Dewhurst, R., Jarodzka, H., and van de Weijer, J. (2011). Eye Track- ing. A comprehensive guide to methods and measures. Oxford University Press.
Huang, W., Eades, P., and Hong, S.-H. (2009). A graph reading behavior: Geodesic-path tendency. In Proc. IEEE PacificVIS 2013, PACIFICVIS ’09, pages 137– 144, Washington, DC, USA. IEEE Computer Society.
Just, M. A. and Carpenter, P. A. (1980). A theory of reading: from eye fixations to comprehension. Psychological review, 87(4):329.
Kim, S.-H., Dong, Z., Xian, H., Upatising, B., and Yi, J. S. (2012). Does an eye tracker tell the truth about visual- izations?: Findings while investigating visualizations for decision making. IEEE Transactions Visualization Computer Graphics, 18(12):2421–2430.
Koerner, C. (2011). Eye movements reveal distinct search and reasoning processes in comprehension of complex graphs. Applied Cognitive Psychology, 25(6):893–905.
Lohse, G. L. (1993). A cognitive model for understanding graphical perception. Human-Computer Interaction, 8(4):353–388.
Perini, L. (2005). The truth in pictures. Philosophy of Science, 72(1):262–285.
Pinker, S. (1990). Artificial Intelligence and the Future of Testing, chapter A Theory of Graph Comprehension, pages 73–126. Lawrence Erlbaum Assoc Inc.
Ponsoda, V., Scott, D., and Findlay, J. (1995). A probability vector and transition matrix analysis of eye movements during visual search. Acta Psychologica, 88(2):167 – 185.
Ramloll, R., Trepagnier, C., Sebrechts, M., and Beedasy, J. (2004). Gaze data visualization tools: Opportunities and challenges. In Proceedings of the Information Vi- sualization, Eighth International Conference, pages 173–180, Washington, DC, USA. IEEE Computer So- ciety.
Raschke, M., Chen, X., and Ertl, T. (2012). Parallel scan- path visualization. In Proceedings of the Symposium on Eye Tracking Research and Applications, ETRA ’12, pages 165–168, New York, NY, USA. ACM.
Rayner, K. (1998). Eye movements in reading and informa- tion processing: 20 years of research. Psychological bulletin, 124(3):372–422.
Shah, P. (1997). A model of the cognitive and perceptual processes in graphical display comprehension. In Proc. American Association for Artificial Intelligence Spring Symposium, AAAI Technical Report FS-97-03. Stan- ford University.
Spakov, O. and Miniotas, D. (2007). Visualization of eye gaze data using heat maps. Electronics and Electrical Engineering / Elektronika ir Elektrotechnika, pages 55–58.
Tsang, H. Y., Tory, M., and Swindells, C. (2010). eSeeTrack– visualizing sequential fixation patterns. IEEE Trans- actions on Visualization and Computer Graphics, 16(6):953–62.
Wooding, D. S. (2002). Fixation maps: quantifying eye- movement traces. In Proceedings of the 2002 sympo- sium on Eye tracking research & applications, ETRA ’02, pages 31–36, New York, NY, USA. ACM.
Visual Analytics: Combining Automated Discovery with Interactive Visualizations
Daniel A. Keim, Florian Mansmann, Daniela Oelke, and Hartmut Ziegler
University of Konstanz, Germany
first.lastname@uni-konstanz.de http://infovis.uni-konstanz.de
Abstract. In numerous application areas fast growing data sets develop with ever higher complexity and dynamics. A central challenge is to fil- ter the substantial information and to communicate it to humans in an appropriate way. Approaches, which work either on a purely analytical or on a purely visual level, do not sufficiently help due to the dynamics and complexity of the underlying processes or due to a situation with intelligent opponents. Only a combination of data analysis and visualiza- tion techniques make an effective access to the otherwise unmanageably complex data sets possible.
Visual analysis techniques extend the perceptual and cognitive abili- ties of humans with automatic data analysis techniques, and help to gain insights for optimizing and steering complicated processes. In the paper, we introduce the basic idea of Visual Analytics, explain how automated discovery and visual analysis methods can be combined, discuss the main challenges of Visual Analytics, and show that combining automatic and visual analysis is the only chance to capture the complex, changing char- acteristics of the data. To further explain the Visual Analytics process, we provide examples from the area of document analysis.
1 Introduction
The information overload is a well-known phenomenon of the information age, since our ability to collect and store data is increasing at a faster rate than our ability to analyze it. In numerous application areas fast growing data sets de- velop with ever higher complexity and dynamics. The analysis of these massive volumes of data is crucial in many application domains. For decision makers it is an essential task to rapidly extract relevant information from the immense volumes of data. Software tools help analysts to organize their information, gen- erate overviews and explore the information in order to extract potentially useful information. Most of these data analysis systems still rely on visualization and interaction metaphors which have been developed more than a decade ago and it is questionable whether they are able to meet the demands of the ever-increasing masses of information. In fact, huge investments in time and money are often lost, because we lack the possibilities to make proper use of the available data. The basic idea of Visual Analytics is to visually represent the information, allowing

 3
the human to directly interact with the data to gain insight, draw conclusions, and ultimately make better decisions. The visual representation of the informa- tion reduces complex cognitive work needed to perform certain tasks. “People use visual analytics tools and techniques to synthesize information and derive in- sight from massive, dynamic, ambiguous, and often conflicting data ... to provide timely, defensible, and understandable assessments” [1].
The goal of Visual Analytics research is to turn the information overload into an opportunity. Decision-makers should be enabled to examine this massive in- formation stream to take effective actions in real-time situations. For informed decisions, it is indispensable to include humans in the data analysis process and combine their flexibility, creativity, and background knowledge with the enormous storage capacity and the computational power of today’s computers. The specific advantage of Visual Analytics is that decision makers may focus their full cognitive and perceptual attention on the decision, while allowing them to apply advanced computational methods to make the discovery process more effective.
The rest of this paper is structured as follows: Section 2 defines Visual Analyt- ics, discusses related research areas, and presents a model of the Visual Analytics Process. In Section 3, we discuss the major technical challenges of the field. To foster a deeper understanding of Visual Analytics, Section 4 details examples of how visual and automatic methods can be used for an advanced interactive document analysis. Finally, Section 5 summarizes the key aspects of our paper.
2 Visual Analytics
In this section we will discuss Visual Analytics by defining it, by listing related research areas, and by presenting a model of the Visual Analytics Process.
2.1 Definition
According to [1], Visual Analytics is the science of analytical reasoning supported by interactive visual interfaces. Today, data is produced at an incredible rate and the ability to collect and store the data is increasing at a faster rate than the ability to analyze it. Over the last decades, a large number of automatic data analysis methods have been developed. However, the complex nature of many problems makes it indispensable to include human intelligence at an early stage in the data analysis process. Visual Analytics methods allow decision makers to combine their human flexibility, creativity, and background knowledge with the enormous storage and processing capacities of today’s computers to gain insight into complex problems. Using advanced visual interfaces, humans may directly interact with the data analysis capabilities of today’s computer, allowing them to make well-informed decisions in complex situations.
2.2 Related Research Areas
Visual Analytics can be seen as an integral approach combining visualization, human factors, and data analysis. Figure 1 illustrates the research areas related
 4
to Visual Analytics. Besides visualization and data analysis, especially human factors, including the areas of cognition and perception, play an important role in the communication between the human and the computer, as well as in the decision-making process. With respect to visualization, Visual Analytics relates to the areas of Information Visualization and Computer Graphics, and with respect to data analysis, it profits from methodologies developed in the fields of information retrieval, data management & knowledge representation as well as data mining.
2.3 The Visual Analytics Process
The Visual Analytics Process combines automatic and visual analysis methods with a tight coupling through human interaction in order to gain knowledge from data. Figure 2 shows an abstract overview of the different stages (represented through ovals) and their transitions (arrows) in the Visual Analytics Process.
In many application scenarios, heterogeneous data sources need to be inte- grated before visual or automatic analysis methods can be applied. Therefore, the first step is often to preprocess and transform the data to derive different rep- resentations for further exploration (as indicated by the Transformation arrow in Figure 2). Other typical preprocessing tasks include data cleaning, normal- ization, grouping, or integration of heterogeneous data sources.
After the transformation, the analyst may choose between applying visual or automatic analysis methods. If an automated analysis is used first, data mining methods are applied to generate models of the original data. Once a model is created the analyst has to evaluate and refine the models, which can best be done by interacting with the data. Visualizations allow the analysts to interact
    .%9(,# $0(%-?+*,2+3$8 @$*$-?+*,2+3$8 @$*$-A 2%2%1
0%,%12*%&3#"#
.%9(,# $0(%-:25"$82;$0(% </2+%0=/-:25"$82;$0(% ’(# )"*+,->,$)72/5
+*,-.%/()*
!"#$%&"’%()*
                                 !"# $%&’(# )"*+,-.%*+,$/0(% ’(1%203+-456/7(8(16 4+,/+)0(%
Fig. 1. Research Areas Related to Visual Analytics
the automatic analysis. In summary, in the Visual Analytics Process knowledge can be gained from visualization, automatic analysis, as well as the preceding interactions between visualizations, models, and the human analysts.
The Visual Analytics Process aims at tightly coupling automated analysis methods and interactive visual representations. The classic way of visually ex- ploring data as defined by the Information Seeking Mantra (“Overview first, Zoom/Filter, Details on demand”) [2] therefore needs to be extended to the Visual Analytics Mantra [3]:
“Analyze First -
Show the Important -
Zoom, Filter, and Analyze Further - Details on Demand”
With massive data sets at hand all three steps of the Information Seeking Mantra are difficult to implement. An overview visualization without losing in- teresting patterns is difficult to create, since the amount of pixels of the display does not keep pace with the increasing flood of data. In Visual Analytics, it
 5
 Visual Data Exploration
 Mapping Transformation
User Interaction
Visualization
   Data
Model
Building Data
Model Visualization
Parameter refinement
Knowledge
Mining
Models
Automated Data Analysis
Feedback loop
Fig. 2. The Visual Analytics Process is characterized through interaction between data, visualizations, models about the data, and the users in order to discover knowledge
with the automatic methods by modifying parameters or selecting other analysis algorithms. Model visualization can then be used to evaluate the findings of the generated models. Alternating between visual and automatic methods is char- acteristic for the Visual Analytics process and leads to a continuous refinement and verification of preliminary results. Misleading results in an intermediate step can thus be discovered at an early stage, leading to better results and a higher confidence. If a visual data exploration is performed first, the user has to confirm the generated hypotheses by an automated analysis. User interaction with the visualization is needed to reveal insightful information, for instance by zooming in on different data areas or by considering different visual views on the data. Findings in the visualizations can be used to steer model building in
 6
is therefore not sufficient to just retrieve and display the data using a visual metaphor; it is rather necessary to analyze the data according to its value of in- terest, show the most relevant aspects of the data, and at the same time provide interaction models, which allow the user to get details of the data on demand.
3 Challenges of Visual Discovery
With information technology becoming a standard in most areas in the past years, more and more digital information is generated and collected. As the amount of data is continuously growing and the amount of pixels on the display remains rather constant, the huge amount of data to be visualized exceeds the limited amount of pixels of a display by several orders of magnitude. One key challenge of Visual Analytics is therefore scalability as it determines the ability to process large datasets in terms of computational overhead. In particular, since we are deal- ing with visualization techniques, the visual scalability of the techniques has to be considered, which is defined as the capability of visualization tools to effectively display large data sets in terms of either the number or the dimension of individ- ual data elements [4]. While relying on increased hardware performance to cope with larger and larger problems, researchers need to design more effective Visual Analytics algorithms to bring this data onto the screen in an appropriate way.
Tremendous streams of time related or real time data are generated by dy- namic processes, arising in business, networks, or telecommunications. Examples are sensor logs, web statistics, network traffic logs, or atmospheric and meteoro- logical records. Analyzing these data streams is an important challenge, since the sheer amount of data does often not allow to record all data at full detail. This results in the need for effective compression and feature extraction to manage and access the data. Furthermore, real-time requirements put an additional bur- den upon the application developers. To enable quick identification of important information and timely reaction to critical process states or alarming incidents, analysis techniques and metaphors need to be developed, which render the user capable of analyzing real time data streams by presenting the results instantly in a meaningful and intuitive way.
To be capable of accessing information from a number of different sources, real-world applications require scalable methods for the synthesis of heteroge- neous types of data. The heterogeneous data sources may include collections of vector data, strings, text documents, graphs, or multimedia objects. Inte- grating these data sources touches a number of fundamental problems in de- cision theory, information theory, statistics, and machine learning, evidently posing a challenge for Visual Analytics, too. The focus on scalable and ro- bust methods for fusing complex heterogeneous data sources is thus key to a more effective analysis process. Computational biology is one such application domain where the human genome, for example, is accompanied by real-valued gene expression data, functional annotation of genes, genotyping information, a graph of interacting proteins, equations describing the dynamics of a system,
 7
localization of proteins in a cell, and natural language documents in the form of papers describing experiments or partial models.
Visual Analytics can also help to close the Semantic Gap. Since humans are the ultimate instance for defining semantics, Visual Analytics may signif- icantly improve the way semantic definitions are obtained and refined. In partic- ular, methods from semantics research may capture associations and complex relationships within the data sets to support decision-centered visualization. While ontology-driven techniques and systems have already started to enable new semantic applications in a wide span of areas, further research is nec- essary to increase our capabilities for creating and maintaining large domain ontologies and automatic extraction of semantic meta data, since the integra- tion of different ontologies to link various datasets is hardly automated yet. Research challenges arise from the size of ontologies, content heterogeneity, and link analysis over ontology instances or meta data. New Visual Analytics meth- ods to resolve semantic heterogeneity and discover complex relationships are thus needed.
Finally, evaluation as a systematic determination of merit, worth, and signif- icance of a technique or system is essential to the success of Visual Analytics. Different aspects need to be considered when evaluating a system, such as func- tional testing, performance benchmarks, measurement of the effectiveness of the display, economic success, user studies, assessment of its impact on decision- making, etc. Note that not all of these aspects are orthogonal nor can they always be applied. Since Visual Analytics deals with unprecedented data sizes, many developed applications contain novel features to support a previously un- solvable analysis task. In such a case, the lack of a competing system turns a meaningful evaluation into a challenge in itself.
4 Example Application: Visual Document Analysis
Document Analysis is an area in which the need for visual analysis techniques is quite obvious. Large amounts of information are only available in textual form (e.g. books, newspapers, patents, service reports, etc.). But often these valuable resources are not used, because reading and analyzing the documents would take too much effort. Take for example a company’s need to know the public opinion about one of its products and especially about rumors regarding that product. Knowing about such rumors is important to be able to quickly react to undesired developments and to effectively influence the public opinion in a favorable way. The Internet is a great place for understanding the public opinion since nowadays a significant percentage of the population participates in writing blogs, commenting on products at merchant sites, stating their opinions in forums, etc. And people read other people’s comments to get information and form their opinion. With current search engines, however, it is not easy to find the relevant information related to the public opinion about a company’s product, since search engines usually return millions of hits with only a small percentage being relevant to the task.
 8
The example shows that it is impossible for a human to find and analyze all the relevant documents. On the other hand, an automatic semantic analysis of the documents is still infeasible today due to a) the impressive flexibility and complexity of natural language as well as b) the need to semantically interpret the content. The challenge that researchers try to tackle with Visual Analysis techniques is how to allow the human and computer to effectively work together to bridge the Semantic Gap.
Text can be analyzed on different abstraction levels:
• statistical level (e.g. frequencies of (specific) words, average sentence length, number of tokens or types, etc.)
• structural level (structural components of a document, such as header, footer, title, abstract, etc.)
• syntactical level (principles and rules for constructing sentences)
• semantic level (linguistic meaning)
• pragmatic level (meaning in context; consequence for actions)
The higher the abstraction level the more difficult it is for the computer to appropriately analyze a text. Counting words and characters as done at the sta- tistical level is a simple task which can easily be performed by the computer. The identification of the structure of a document and the analysis of the syntax is already more challenging but can still be computationally approached (see e.g. the techniques presented in [5] [6]). Analyses on the semantic and pragmatic level are much more challenging. The idea of Visual Analytics is to let the human and the computer cooperate in solving the task. Humans contribute background knowledge, interpretation, and semantic analysis of the text whereas the com- puter supports the human analysts in the best possible way to enable them to deal with large data sets, e.g. by performing the time-consuming preprocessing and filtering steps.
4.1 Quasi-semantic Document Properties
The vision for automatic document analysis is to teach the computer to under- stand a document in a way similar to humans including its semantic and prag- matic aspects. Since this goal seems to be too ambitious at the current state of research, we start by teaching the computer to analyze a document with respect to one semantic aspect at a time. This task is relevant in many real application scenarios. Often large amounts of documents have to be analyzed with respect to a certain analysis question. Examples for such document analysis questions include:
• What is the public opinion regarding a product / a politician / a “hot” news topic, etc. that is expressed in news articles, blogs, discussion groups, etc. on the Internet?
• How trustworthy are the statements?
 9
• How much emotion content is contained in the documents (e.g. hate in ter- rorist webpages)?
We call the document property that is central to the considered document analysis question a quasi-semantic property. We define quasi-semantic properties as higher-level document properties that capture one semantic aspect of a docu- ment (e.g. positive / negative statements with respect to a given product name). Most quasi-semantic properties cannot be measured directly. Nevertheless, com- binations of low-level features (i.e. statistical, structural and syntactical features) can be used to approximate quasi-semantic properties of the documents, which help to bridge the semantic gap. The challenge is how to determine the best combination of low-level features to approximate such higher-level document properties.
4.2 Opinion Analysis
Figure 3 shows a set of documents that have been analyzed with respect to a quasi-semantic property that tries to assess the positive or negative opinion expressed in the documents. To automatically assess the polarity of a sentence we counted the number of opinion signal words. The signal words were given in form of two lists that contain adjectives, verbs and nouns (such as “bad”, “problem”, “wonderful”, etc.) that hint at a subjective statement and its polar- ity. The algorithm can easily be improved by taking context dependent signal words or negation into account (cf. [7] and [8]). For illustrative purposes, we use the title pages of the November 2007 issues of The Telegraph as text corpus. The figure shows that there are some articles that are completely negative (e.g. the article in the lower right corner) and others that are mostly positive (such as the articles about the Queen in the 1st column, 3rd row). Interestingly, there are also articles with quite mixed opinions or with a sudden change in polarity (for example, the first article in the last column, 1st row or the lower left article in the 4th column, 1st row). The example demonstrates that by combining au- tomatic and visual methods it becomes possible to quickly analyze a document corpus with respect to a quasi-semantic property without reading it.
4.3 Authorship Attribution
Our second case study shows how Visual Analytics techniques can be used to analyze the discrimination power of low-level features that are commonly used in authorship attribution. Given some documents with known authorship the task of authorship attribution is to assign a document with unknown authorship to the author that has written it. Thus, in this case the quasi-semantic property that we would like to measure is the writing style of an author.
In previous work we focused on the development of techniques that support the analysis of low-level features and thus can be used to find (combinations of) low-level features that are able to approximate a desired quasi-semantic prop- erty. In fully automatic document analysis often just a single feature value is
 10
Fig. 3. Title pages of The Telegraph in November 2007. The text has been analyzed with respect to a quasi-semantic property ‘that tries to assess the positive or negative opinion expressed in the documents. Sentences with positive statements are highlighted in green, the ones with negative statements in red, respectively. The degree of positive- ness or negativeness is denoted by the intensity of the color. (courtesy of The Telegraph)
calculated per document. With the use of visualization techniques, it is possible to extract a sequence of feature values and present it to the user as a character- istic fingerprint for each document. By doing this it is possible to analyze the development of the values across the document in detail. Figure 4 shows our Literature Fingerprinting technique which was first presented in [9]. In Figure 4,

 11
  (a) Average sentence length
(c) Function words (First Dimen- sion after PCA)
(e) Hapax Legomena
(b) Simpson’s Index
(d) Function words (Second Dimen- sion after PCA)
(f) Hapax Dislegomena
    Fig. 4. Literature Fingerprinting Technique (see [9]). Instead of calculating a single fea- ture value per document, a sequence of feature values is extracted and presented to the user as a characteristic fingerprint for each document. In the example above, the tech- nique is used to analyze the discrimination power of text features for authorship attri- bution. Each pixel represents the feature value for one text block and the grouped pixels belong to one book. The different feature values are mapped to color. If a feature is able to discriminate between the two authors, the books in the first row (that have been writ- ten by J. London) are visually different from the remaining books (written by M. Twain). Each subfigure shows the visualization of the values of one specific low-level feature that is commonly used for authorship attribution. It can easily be seen that not all features are able to discriminate between the two authors. Furthermore, it is interesting to ob- serve that the book Huckleberry Finn (middle book in the middle column of the books of M. Twain) sticks out in a number of features as if it was not written by Mark Twain.
 12
the technique is applied to several books of Jack London (first row of each subfig- ure) and Mark Twain (last three rows of each subfigure). Each pixel represents a text block of 10,000 words and the pixels are arranged from left to right and top to bottom as they appear in the sequence of the book. Neighboring blocks have an overlap of 9,000 words to obtain a continuous and split-point independent representation. Color is mapped to the feature value, ranging from blue for high feature values to red for low feature values. In this example the values of five different features have been calculated:
• the average sentence length
• the frequencies of specific function words; the resulting high-dimensional
feature vectors are projected into low-dimensional space using a Principal Component Analysis and the first and second dimension are visualized in Figures 4(c) and 4(d).
• three vocabulary measures, namely Hapax Legomena Index, Hapax Disle- gomena Index and Simpson’s Index which are calculated as follows:
Hapax Legomena Index (R):
R= 100logN 1−V1/V
Hapax Dislegomena Index (D):
D = V2
  V
of lexical units that occur exactly r times
Please refer to [10] for an overview of the different features that are used for authorship attribution.
Each subfigure shows visualizations of all documents for one specific low-level feature. If the feature is able to discriminate between the two authors, the books in the first row (books by Jack London) have to be different from the ones in the last three rows (books by Mark Twain). It can easily be seen that there are some low-level features for which this is largely true, e.g. average sentence length in Figure 4(a) but also Simpson’s Index in Figure 4(b). Others do not seem to have any discrimination power with respect to the two authors at all (e.g. Hapax Dislegomena which is depicted in Figure 4(f)). Interestingly, there is one book of Mark Twain that sticks out in many visualization, namely The Adventures of Huckleberry Finn (middle book in the middle row of the books by Mark Twain). The writing style of this book seems to be totally different from all the other books of Mark Twain.
This case study shows a small example of how Visual Analytics may help in better solving complex analysis tasks. The visual analysis enables the ana- lyst to detect problems with the low-level feature used and adapt the similarity
Simpson’s Index (S):
S =
where N = the number of tokens V = the number of types Vr = the number
 ∞r=1 r(r − 1)Vr N(N − 1)

 13
measures to make the authorship attribution more effective. While the example clearly shows the advantage of Visual Analytics it is only a first step toward a Visual Document Analysis system which tightly integrates automated document analysis and interactive document exploration capabilities.
5 Conclusions
Since data volumes are increasing at a faster pace than our ability to analyze them, there is a pressing need for automatic analysis methods. However, most automatic analysis methods require a well-defined problem and often return large and complex models. Visual Analytics turns the information overload problem into an opportunity by integrating interactive data exploration with advanced knowledge discovery algorithms.
In this paper, we motivate and define Visual Analytics, present a model of the Visual Analytics Process for a deeper understanding of how methods from visual data exploration and information mining can be combined to gain insights into large and complex datasets. The paper sketches the main challenges of Visual Analytics and describes why these challenges are difficult to solve. In particular, we give a demonstrative example of how Visual Analytics methods can help to gain insights in document analysis with an application to the authorship attribution problem.
Acknowledgement
We thank Jo ̈rn Schneidewind for for helpful comments on the manuscript.
References
1. Thomas, J., Cook, K.: Illuminating the Path: Research and Development Agenda for Visual Analytics. IEEE Press, Los Alamitos (2005)
2. Shneiderman, B.: The eyes have it: A task by data type taxonomy for information visualizations. In: IEEE Symposium on Visual Languages, pp. 336–343 (1996)
3. Keim, D.A., Mansmann, F., Schneidewind, J., Ziegler, H.: Challenges in visual data
analysis. In: Information Visualization (IV 2006), London, United Kingdom, July
5-7. IEEE, Los Alamitos (2006)
4. Eick, S.G.: Visual scalability. Journal of Computational & Graphical Statistics
(March 2002)
5. Klein, D., Manning, C.D.: Accurate unlexicalized parsing. In: ACL 2003:
Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pp. 423–430. Association for Computational Linguistics (2003), http://nlp.stanford.edu/software/lex-parser.shtml
6. Hadjar, K., Rigamonti, M., Lalanne, D., Ingold, R.: Xed: a new tool for extract- ing hidden structures from electronic documents. In: International Workshop on Document Image Analysis for Libraries, pp. 212–224 (2004)
 14
7. Oelke, D., Bak, P., Keim, D., Last, M., Danon, G.: Visual evaluation of text fea- tures for document summarization and analysis. In: IEEE Symposium on Visual Analytics and Technology (VAST 2008) (to appear, 2008)
8. Ding, X., Liu, B., Yu, P.S.: A holistic lexicon-based approach to opinion mining. In: WSDM 2008: Proceedings of the international conference on Web search and web data mining, pp. 231–240. ACM, New York (2008)
9. Keim, D., Oelke, D.: Literature fingerprinting: A new method for visual literary analysis. In: IEEE Symposium on Visual Analytics and Technology (VAST 2007) (2007)
10. Holmes, D.I.: Authorship Attribution. Computers and the Humanities 28, 87–106 (1994)
 2014 IEEE Workshop on Electronics, Computer and Applications
Fan Bao
Dalian Maritime University Dalian, China 310311901@qq.com
Abstract-The concepts of Data Warehouse, Cloud Computing and Big Data have been proposed during the era of data  ood. By reviewing current progresses in data warehouse studies, this paper introduces a framework to achieve better visualization for Big Data. This framework can reduce the cost of building Big Data warehouses by divide data into sub dataset and visualize them respectively. Meanwhile, basing on the powerful visualization tool of D3.js and directed by the principle of Whole-Parts, current data can be presented to users from different dimensions by different rich statistics graphics.
Keywords-component; D3.js; Visua zation; Data Warehouse
I. INTRODUCTION
Data Warehouse, Cloud Computing, and Big Data have proposed within latest two decades. In the background of the Big Data, the data warehouse has become a hot topic in the research area. A typical data warehouse uses different data source to transform the data into a data warehouse through the process of ETL. And it will use OLAP services to achieve the data conversion and analyze at the last [1]. However, using this way to move and consolidate data is expensive. The high-demand of hardware cannot be ignored when we build a data warehouse. The main purpose of the data warehouse is to perform data analysis and requirements analysis. The data warehouse stores all kinds of raw data. The data will be calculated, loaded and then (if necessary) come the analysis. The calculating velocity of data warehouse is limited. It will consume with the number of resources. And calculating di erent data sources would be repeated if needed analysis again. If this calculation can be broken down and computed respectively in the data source's server, it will help to reduce the overhead of double counting huge data by using a data access interface. This article introduces a data's visual  amework for Big Data and uses the Web visual presentation tool- D3 s to achieve statistics graphical. Maybe this way can reduce the total cost of constructing a large data warehouse. Based on this idea­ "First give a right size, properly screened summary, and then display the details needed", interactive visualization graphics will be shown  om a different dimension analysis. In the paper, interactive visualization graphics will be demonstrated by the book category dimension, time dimension and other dimensions.
Jia Chen
Dalian Maritime University Dalian, China chenj ia8080@sina.com
II. BASIC CONCEPTS
Visual framework for big data in d3.js
 A. D3 s
D3 s is a JavaScript library for manipulating documents based on data [2]. It achieves visualization show by data loading, data binding, analytic transformation elements and excessive element. It is different with Excel, d3 to provide users with a customized mapping rule. According to the needs, users can determine the mapping values to the graphic, such as display color, size. D3 does not support older versions of the browser, so it can make the d3 code clean. D3 is displayed graphical by using CSS3, Hyper Text Markup Language and Scalable Vector Graphics on the Web. D3 is currently the latest version 3.4.4.
D3 is a good at dealing with SVG, which is the World Wide Web Consortium speci cation speci ed network vector graphics standard [3]. SVG strict observes XML syntax and use text format's description language to describe the image content.   is a resolution-independent vector and image graphics format [4]. Using SVG to achieve the visual is becoming a new trend.
B. Data Warehouse
A data warehouse is a subject-oriented, integrated, time­ characterized and stable point of data collection to support management decision-making in the process [5]. The amount of data in data warehouse is very large, People can be from different dimensions (i.e., the angle of observation of the transaction) to view the data. Data warehouse is using multidimensional data model to storage data. Multidimensional data model includes dimension tables, fact tables and so on. Dimensional table is used to guide the selection of data  om different angles in the fact table. In the multidimensional data model, the user can  nd the required data in the fact table  om different dimensions. Visualization tools will help people organize and display data.
  Data visualization
The basic idea of data visualization technology is the database for each data item as a single pixel element represents, then a large number of data sets constitute image of data [6]. Meanwhile using multi-dimensional data to represent each attribute value of data, the data can be observed  om different dimensions and used more in-depth observation and analysis. The main purpose of data
    978-1-4799-4565-8/14/$31.00©2014 IEEE
47
 2014 IEEE Workshop on Electronics, Computer and Applications
visualization is to convey information by using graphical tools and communicating clearly and e ectively. Due to the di erent degrees of data, Data visualization must implement the zoom feature. At the same time, users can browse or speci c knowledge about the data set using the dynamic response graphics[7]. Standard Web technologies for data visualization allow the user who views the statistics on different operating systems by using the newer versions of browsers.
Currently, the visualization techniques have JavaScript, Flash, Java Applet, etc. in the Web  ont-end. According to different demand, people can select the appropriate demand development tools. For example, to generate a simple chart, Data Wrapper is not a bad choice. Map's mapping can be used Karto aph. Because JavaScript does not require any plug-in and the user experience is better, so I chose JavaScript.
III. THE F MEWO  OF VISUAL ANALYSIS
In the background of large data, I proposed request method of a data visualization data, as shown below by research and analysis of data warehouse costs and construction. As shown in Figurel.
dimension. The results will help managers  nd book sales law and adapt the present market demand.
By using D3 s, data visualization will be well achieved and use intuitive vector to replace the traditional form. This article is based on book sales data, by selecting different statistical dimensions, using a pie chart shows the overall pro le, with a bar graph showing the speci c sales under each dimension.
A. Scheme Design
During data storage design, the data are divided into book-categories dimensions and time dimensions. Book sales table contains sales encoding, book-categories, books-sub­ class, book name, price, and sales time six  elds. We designed classi cation code tables for book categories and book sub-classes. It is an important idea for D3.js is not hiding the original data. When the page is loaded, it will push all the data required for book sales to the client, and screening and integrating data base on user's choice. There is no need to sending the request every time you select, this can reduce the burden on the server. The structure of Sales data table is shown in Table 3
   s a  analysis
Figure 1.
 laR�1
> WI�I�:U R nsand summ
D••
visual framework
Field name
order no sub no book name sales
time
Type Length
50
Field Description
int OrderNumber
int CategoriesNumber varchar Book name
 0at Selling price datetime Sales of the time
TABLE I THE TABLE OF SALES DATA
   By "sub_no "we can make the connectIOn between the code table and the fact table to determine the categories of books. Speci c processes shown in Figure2:
  Each data source can be a small data warehouse in Fig. 1. Their situation of theme settings is the same. Each data source can be visualized and statistics according to demands. When making the total statistics, people will propose demands in the  rst. Then each data source calculates statistical data according to the demands and provide access interface. At the last, data will be returned and summarized and displayed in the browser and stored in the data warehouse. This way can reduce the cost of setting up a large data warehouse. Through the interface, the original data values can also be hidden. So between the data and the user is translucent, not like d3 that is transparent. Based on this idea, a data source for data calculation and visualization can be achieved.
IV. THE IMPLEMENT OF DATA VISUALIZATION IN D3.JS
Currently, there are a lot of various industry sales data. Data are huge and complex. Single, static tradition "chart­ oriented" visual display has been unable to meet the needs of business decision-makers. Through the thought of the data warehouse dimension, interaction diagram of Book sales data will be shown  om the books category dimension and time
Figure 2.
visual processing
 48
 2014 IEEE Workshop on Electronics, Computer and Applications
B. Implementation and Image display
When data is transmitted to the  ont-end, we create a .ashx  les in C #, using List (T) class to access the sales data, and through the method by JavaScriptSerializer transforming data into a JSON, then through Ajax request to obtain all the sales table facts data.
First, create a BookSales class, including Book_catg, Sub_catg, Book_name, Sales and S time  eld. A er obtaining the fact that book sales data  om the database, we create a listList<BookSales> list = new List<BookSales> (SalesJength);
We can call ToJson (list)  nction to convert the variable list to JSON format, and pass it on to the  ont pages.
public string ToJson (object 0) {
JavaScriptSerializer servializer new JavaScriptSerializer 0;
return servializer.Serialize (0); }
By the following code you can get book sales information in the  ont-end:
$(document).ready ( nction 0 {
$.post ("request.ashx", function (data) {
}); });
On the  ont-end, before using D3 s , we should add a reference like this: <script src="/js/D3.js"></script>0
When we do the analysis in the book category and year, we use a pie chart to display the percentage of total sales in major categories.
First, we need to sort the data according to categories and organize the data as "hierarchical model" structure, with categories for the roots, and "end" for the end.
Then add a listener to each pie incident, the page will display the detail of selected category through a bar chart based on users' demand. When the organized data transmit to sequence s for visual display, it should also transmit the detail data that is in order to achieve seeing details of the pie.
In this article BookCatg and Years are both using a way of pie chart to bar chart to display.
For example, when clicking the book category to view, the code of implementation and the effect is shown below, and wherein classi 1 c1assi 2 can be multiplexed to di erent categories.
aryjs = 0;
var m = "Book_catg"; jsonsort(m);f
for (var i = 0; i < 1; i++) {
f += parseInt(Sjson. Data[i].Sales); if (i + 1 < I) {
sl = Sjson. data[i].Book_catg;
s2 = Sjson.data[i + l].Book_catg; c1assi l(f, sl, s2);
} else if (i+1==1) {
s1 = Sjson.data[i].Book_catg;
s2 = Sjson.data[i - l].Book_catg;
c1assi 2(f, i, sI, s2); }
else{ json_txt += '{ +Sjson.data[i].Book_catg +'-'+'end'+ '" , "B":'" + f + '" }]}';}
} Ilfor
Results shown in Figure3:
Figure 3.
Result of BookCatg
ot
1   j   "
In the Fig.l, when selecting BookCatg class and putting the mouse on each of the different colors, we can see the book of "Business & Money" accounted for 46.8% of total sales in the upper le  corner .Clicking the color, you will get a bar graph shown below. It records the speci c sales of per books under the category of "Business & Money" .When the mouse over the bar, you'll see that it belongs Economics that is under the "Business & Money" category. Results shown in Figure 4:
Figure 4.
When we select Month dimensional analysis performed,  ont-end data will be arranged according to the year, and then every year's monthly data for statistical analysis to scatter plot a different way to the annual sales of the month with different color display, as shown in Figure 5:
�

 2014 IEEE Workshop on Electronics, Computer and Applications
  lll    S0 1
displaying of graphics, sales trends, it can help managers to make sales management optimization and increase sales.
V. SUMMARY
Statistical methods of data are diverse. Users can choose their own visual display method case by case. The reason why the proposed visual  amework is chosen mainly based on the following reasons:
1. D3.js is a library of JavaScript. When using D3.js to visualize, he can directly upgrade to a newer browser version instead of installing plug-ins in the  rst place.
2. D3 s is used to generate interpreted data. Users can convey important information data by some rules extracted  om the data view.
3. The advantage of graphic is to display the statistics  om the whole to the details according to demand. Thus this graphic will be interactive visualized.
4. Using SVG graphics will not allow graphic detail view enlarged and distorted.
5. While the original data will not be hidden by D3.js, the approach, using the text of the proposed  amework to achieve the translucent data, can not only make it easier for users to statistic data but also reduce the resources and costs to build large-scale data warehouse.
This  amework for data visualization, data mining and statistical methods is not comprehensive and may not meet all the needs of users. However, based on the idea in this article, the user can build a  amework according to his demand and then develop visual constraints, choose different data dimensions to statistic data.
REFERENCES
[I] M. Schneider, "A general model for the design of data warehouses," Int. J. Production Economics, Vol. 112, pp.309-325, 2008. (Pubitemid 351179609)
[2] http//d3js.org/, retrieved on May 13th, 2014.
[3] Scott Murray, Interactive Data Visualization for the Web, United States of America, 2010 [11].
[4] Dailey D,Frost J,Strazzullo D.Building Web Applications with SVG[MlMicroso  Press,2012.
[5] Prat N, Comyn-Wattiau I, Akoka J. Combining objects with rules to represent aggregation knowledge in data warehouse and OLAP systems. Data & Knowledge Engineering, 2011, 70(8) 732-752.
[6] Keim, Daniel A. "Information visualization and visual data mining." Visualization and Computer Graphics, IEEE Transactions on 8.1 (2002) 1-8
[7] Zhao Vue, Chen Zhiwei,Cai Shuhui,et al.Dynamic Web
 As shown above, different years will be described with different colors. Users can visually compare the month's di erent sales, and we can see the  uctuations in sales annually. Such as through the map2014[2,443], you can know the February 2014 sales is 443 dollar.
Finally, View the Overall categories to detail can be viewed separately  om each of the proportion of total sales, as shown in Figure 6:
   0.460%
 0.460%
Dr vi il$ in with  k kq   � vr pAg
 Figure 6.
View of overall
 When we see  om the  gure in the navigation bar, this book is named "Proof of Heaven" sales accounted for 0.460% of the total.
By analyzing di erent dimensions, users can view book sales and income  om different angles. Based on the
Visualization of Large Scientific Computing Data. 2012(5)3-6
Modern Computer,
1 PROJECT DESCRIPTION
Eye trackers are increasingly present in academic research and commercial applications [1]. Researchers in human-computer in- teraction use eye tracking to study the usability of computer inter- faces, as it can provide data that describe user behaviour and reveal usability problems that traditional methods might have missed [8]. It has also been used in various medical exams, e.g., for detecting schizophrenia [5], and in developmental psychology to study the perceptual and cognitive abilities of children [4].
Analyzing eye tracking data involves examining a set of quan- titative metrics, including fixation count, saccade amplitude, pupil size, etc. While statistical analysis is suitable for generating these metrics, visualization techniques can reveal spatial-temporal pat- terns and trends in the data that can then be verified through statis- tical testing [1]. Thus, the tasks performed on eye tracking visual- izations include understanding overall distribution of fixations and identifying sequential patterns of user’s eye movement. To draw conclusions about the behaviours of a group of users requires com- paring the patterns and finding common ones among users.
Eye tracking datasets are often large in volume: the size of a dataset from an HCI experiment is often on the order of hundreds of thousands of fixations [1]. For spatial-temporal analyses, the necessary attributes in the input data are the coordinates of the fix- ations and timestamps. To display in-context visualizations, the vi- sual content provided to the users during the experiment is needed. Regions of the visual content can be defined as areas of interest (AOI), and they usually follow the semantic information of the con- tent.
2 PERSONAL EXPERTISE
I have been working with eye-tracking data in my research project on adaptive visualization for a year. The goal of the project is to infer a user’s cognitive abilities and the difficulties of the presented visualization tasks from user’s gaze behaviour. I had attempted to achieve this goal using machine learning approaches with AOI- based gaze sequence features, but the resulting classification accu- racies were not as desired.
3 PROPOSED SOLUTION
The eye tracking dataset has the properties of spatial, node-link network, and time-series data. Each fixation is situated at a spatial location and contains the attribute of fixation duration. Two fixa- tions are linked together by a saccade, which is a fast movement of the eye. When fixations are aggregated into AOIs, AOI transi- tions can form a type of network that allows directional links and multiple links between two AOIs.
Reduction is a suitable strategy for dealing with these large vol- ume datasets, as the visual clutter resulted from displaying all fix- ations and saccades would impede the analysis. The two subdivi- sions of reduction strategies are filtering and aggregation [7].
Various algorithms for aggregating fixations and scan paths have been developed (e.g., [2]). Considering the scope of this project,
∗e-mail: mikewu@cs.ubc.ca
the aggregation used will be in the form of grouping fixations by predefined AOI or dynamically defined regions.
The dataset can be reduced through filtering by the following criteria:
• filter by individual user, task, and/or trial in the experiment so that only the fixations and scan paths of the selected user/task/trial are displayed;
• filter by time intervals within the trials, e.g., only show the fixations occurred during the first 3 seconds of each trial;
• highlightfixationsintheselectedregion,wheretheregioncan be a predefined AOI or a dynamically defined region;
• highlight sequences of fixation following a defined pattern, e.g., all transitions between AOI 1 to AOI 2.
To coordinate the spatial and temporal information, the system contains multiple views.
• The timeline view acts as a control for filtering by time in- terval, and temporal information, such as AOI duration, can be overlaid on the timeline. If multiple trials are selected, a list of timelines are arranged vertically and aligned on the left (i.e., the start time).
• The trial view shows the list of trials grouped by either user or task and acts as a filter control with multiple selection.
• The eye-gaze view displays the spatial information of fixa- tions and scan paths on top of the visual content presented to the users in the experiment. This view allows selection of regions to facilitate filtering fixations by region and filtering sequences by a series of regions.
Once a filtering is initiated in one view, the other views are updated. For example, when the user selects a region in the eye-gaze view, fixations outside the region are faded out, the timeline highlights the timestamps at which these fixations occur, and the trial view shows the frequencies of the occurrence of the selected fixations in each trial.
The filtering in the eye-gaze view is visualized through dynamic layers. The bottom layer in the view is the visual content presented to the experiment participants, and this layer allows for in-context analysis.
4 SCENARIO OF USE
Jim is a HCI researcher who just conducted an experiment and col- lected eye tracking data of the participants.
1. First,heexportsthedatafromtheeyetrackerandtransformed it to the format supported by the system.
2. Then, once he loads the data into the system, an overview of all the fixation is displayed, the trial view is populated with trials that are grouped by users and by tasks, and if AOIs are defined, the timeline view is color-coded by AOI durations.
Visualizing Eye Gaze Sequences
Project Proposal
Mike Wu∗ University of British Columbia

 Figure 1: Mockup design of selecting a sequence of regions. The timeline component is based on [6].
3. Next, Jim selects individual trials one-by-one to examine the data in details; the eye-gaze view and the timeline view update accordingly on demand.
4. Then, Jim wants to focus on the first 10 seconds into each trial, so he adjusted the slider on the timeline to filter out fixations that occurred after 10 seconds.
5. Once he observes a region in which many fixations concen- trates, he selects it, and fixations that fall outside the region are faded out.
6. Now, he is presented with the option of showing the neigh- bours of the highlighted fixations, and he chooses to show neighbours that are one saccades away. He quickly finds a common trend of a series of three steps, i.e., there is a com- mon neighbourhood that leads into the selected region and a common neighbourhood that follows from the selected region.
7. Jimwantstoseehowoftenthisspecificsequenceoccursinthe rest of the dataset, so he builds a custom sequence by selecting three regions consecutively (Figure 1). As a result, the trial view shows the frequencies of this sequence, and the timeline views show the time intervals at which this sequence occur.
8. In the trial view, he notices that this particular sequence oc- cur frequently for half of the users, rarely occur for the other half, so he decides to look more closely at this sequence and investigate the statistical significance of this difference.
PROPOSED IMPLEMENTATION
6 MILESTONES
M 1
2 3
4 5
6
Target Nov. 7
Nov. 14
Nov. 21
Nov. 28 Dec. 5
Dec. 12
Tasks
Finalize detailed design
Build the eye-gaze view showing fixations and saccades Build the trial view
Support filter eye-gaze view by trials
Build timeline view
Support filter eye-gaze view by time intervals
Build selection by region
Build highlighting neighbours
Build selection by sequence of regions
Prepare final presentation and reports
 5
Traditional visualization approaches via scan path and heat map suffer from visual clutter and the lack of temporal information, re- spectively [9]. The temporal information, in terms of the sequence of users eye movement, can often reveal insights such as the effi- ciency of the arrangement of elements in the interface and user’s strategies when processing the visual information [8].
There has been an increasing number of eye-tracking visualiza- tions developed in recent years [1], and some components of the proposed visualization system use designs implemented in previ- ous systems. For example, the timeline view is similar to the de- sign of the “scarf plots” in the ISeeCube system [6], and the style of the scan path follows the coding strategy designed by Goldberg and Helfman [3]. Blascheck et al. surveyed the field and found that many of the systems lack the support for interactive analysis [1]. They classified these systems according to a set of categories; the proposed system is both point- and AOI-based and facilitates in- teractive spatial-temporal analysis with multiple users. There is no existing system that falls into this set of categories.
I plan to implement this system as a web application and build mainly with the D3.js library. I will also reply on the Crossfilter JavaScript library to support the coordinated views. The supported input data format will be JSON initially.
7
PREVIOUS WORK
REFERENCES
[1] T. Blascheck, K. Kurzhals, M. Raschke, M. Burch, D. Weiskopf, and T. Ertl. State-of-the-art of visualization for eye tracking data. In Pro- ceedings of EuroVis, volume 2014, 2014.
[2] J.H.GoldbergandJ.I.Helfman.Scanpathclusteringandaggregation. In Proceedings of the 2010 Symposium on Eye-Tracking Research & Applications, pages 227–234. ACM, 2010.
[3] J. H. Goldberg and J. I. Helfman. Visual scanpath representation. In
Proceedings of the 2010 Symposium on Eye-Tracking Research & Ap-
plications, pages 203–210. ACM, 2010.
[4] G.Gredeba ̈ck,S.Johnson,andC.vonHofsten.Eyetrackingininfancy
research. Developmental neuropsychology, 35(1):1–19, 2009.
[5] P.S.Holzman,L.R.Proctor,andD.W.Hughes.Eye-trackingpatterns
in schizophrenia. Science, 181(4095):179–181, 1973.
[6] K.Kurzhals,F.Heimerl,andD.Weiskopf.Iseecube:visualanalysisof gaze data for video. In Proceedings of the Symposium on Eye Tracking
Research and Applications, pages 351–358. ACM, 2014.
[7] T.Munzer.VisualizationAnalysisandDesign.CRCPress,1stedition,
2014.
[8] A. Poole and L. J. Ball. Eye tracking in hci and usability research.
Encyclopedia of human computer interaction, 1:211–219, 2006.
[9] M.Raschke,T.Blascheck,andM.Burch.Visualanalysisofeyetrack- ing data. In Handbook of Human Centric Visualization, pages 391–409.
Springer, 2014.
 Liberal Arts and Engineering Studies California Polytechnic State University, San Luis Obispo
Visualizing Relationships between Related Variables:
Improving Physics Education through D3.js Network Visualizations
A Senior Project presented to the Faculty of Liberal Arts and Engineering Studies California Polytechnic State University, San Luis Obispo
In Partial Fulfillment of the Requirements for the degree Bachelor of Arts in Liberal Arts and Engineering Studies by Stephanie Friend March 2015
© 2015 Stephanie Friend
 Table of Contents
Deliverable Background
Physics Education Taxonomies
Data Visualization Taxonomies System Architecture
Visualizations
Figure 1. Visualization of Problem Set Variable Distribution Figure 3. Relevant Equations to a Chapter
Figure 4. Relevant Equations to a Variable
Analysis and verification Evaluation Method
Overall Rating of Visualizations
Figure 5. Professor­oriented Visualizations
Figure 6. Student­oriented Visualizations
Figure 7. Undirected Graph Boxplots
Table 2. Quantitative Analysis of Physics Majors/Minors
Suggestions for Improvement
Completion of Goals Societal impacts
Beyond Cal Poly Personal Next Steps Conclusion
References
1

 Introduction
The use of data in web and mobile applications is exploding. Mass amounts of data are analyzed using complex algorithms and visualized using a variety of techniques and graphs. This information is often portrayed as a simple and understandable data visualization to help companies analyze information such as financial metrics or web traffic. Data visualizations have a similar potential to analyze and simplify information when used in the educational realm. phiMap is a web application that was conceptualized by two Cal Poly professors, Dr. Eric White and Dr. Sandrine Fischer­White. It began development this summer with the help of a physics student, George Jing. phiMap analyzes and visualizes physics problem sets and equations by allowing one to select specific exercises, chapters and more from an engineering physics book. phiMap then has the ability to audit these problem sets using physics education taxonomies and display relationships between concepts, equations, and exercises through useful visualizations. In my research, I have created Javascript visualizations for phiMap that serve to simplify the processes of both teaching and learning physics. These visualizations aim to present relationships between physics variables in an easy to understand manner, and they could eventually have a huge impact on physics education. phiMap could prove to be an amazing resource, first at Cal Poly and potentially across the United States.
Deliverable
As my deliverable for my senior project, I have created four distinct network visualizations that augment phiMap. Each visualization uses phiMap’s database of chapters from Randall Knight’s Physics for Scientists and Engineers: A Strategic Approach, Standard Edition (3rd Edition, 2012) t o portray information about the various topics to readers. This book was chosen due to its use in Cal Poly’s series engineering physics classes. Two of the visualizations are meant as tools for the professor when assigning problem sets, and two of the visualizations are meant as tools for the student when
2
 learning physics. The first set of visualizations is accessible within the phiMap web application and pulls directly from a database of problem sets that the Cal Poly physics department has chosen for their students. The second set of two visualizations is compatible with phiMap but work as independent web pages, as an examination into the possibilities that phiMap holds for benefits to students. The four types of visualizations are explained in more detail in the Implementation section of this paper.
Background
Similar Applications
Most web applications related to physics education focus on presenting problem sets and exercises to students. For example, Problem Roulette is a project that allows students to view random but topic­related past­exam problems (Evrard). Mastering Physics is an online program by Pearson that allows students to view homework assignments and tutorials (Antonenko). The  Problem Solving Learning Portal by Iowa State University captures the way that students solve problems by requiring them to fill in categories such as “relevant concepts” and “qualitative analysis”.  phiMap fills a void by focusing on educational research and the professor, applying known taxonomies to problem sets and allowing professors to see the analysis.
phiMap’s Approach
phiMap uses the Taxonomy of Introductory Physics Problems (TIPP) to analyze problem sets, which focuses on the cognitive processes that students used and knowledge domains that they have to access. The taxonomy views the student as completing 4 cognitive steps when processing knowledge: retrieval, comprehension, analysis, and knowledge utilization (Teodorescu et al, 2013). phiMap provides a variety of resources to analyze the problem sets using this taxonomy. Many of the visualizations that I have provided for use within phiMap use basic concepts from this taxonomy to analyze whether problem sets will be meaningful for students.
3
 Data Visualization Taxonomies
Existing taxonomies for data visualizations aid in designing the most effective visualization by dividing data visualizations into categories and determining an appropriate type of graph. There are 5 main categories that data visualizations are typically separated into (Introduction). Geospatial and temporal data visualizations are irrelevant to this project as they deal with geography and time. The three other main categories are multidimensional, hierarchical, and network. Network visualizations were chosen because in each instance, variables are connected to each other through a relationship. The specific type of network visualization used in this project is an undirected graph, or a node­edge where the edge between two nodes does not have a direction. While undirected graph is the conventional name, especially within Computer Science, these types of visualizations are referred to as force graphs in D3, due to their relationship to physical properties to optimize the layout by repulsive nodes.
System Architecture
phiMap is a web application, meaning that it utilizes the user’s web browser as the client, allowing the user to interact with a database. Once published, a web application can be visible to anyone with a web browser and an internet connection and is accessible on any device that can access the internet. The server side of the web application is built through the Django framework, Python, and a SQLlite database where the data is stored. Python is used to query the database and return information that can be represented in an HTML template or D3 data visualization, then returning the results in a format that can be used by template HTML files. Django makes a lot of things easier, such as interacting with a database in order to manipulate the data. The client­facing interface of the web application is built with Twitter Bootstrap, HTML, CSS, Javascript, and the visualizations are created using the Javascript language, the JQuery library, and the D3 Javascript library. The D3.js Data Driven Documents Javascript library is intended for easy data manipulation and visualizations. It was chosen over other visualization libraries because of the vast documentation and examples that it has. D3.js is widely used
4
 in industry, and knowledge of the library is a highly applicable skill as well. I implemented each visualization using D3 examples, and tutorials as a basis.
Implementation
Timeline
Visualizations were developed using an iterative process as well as regular input from the professors involved with phiMap. Research into creating the visualizations began in September and main development and incorporation of the visualizations into the phiMap database began in December. Weekly meetings with the phiMap team occurred October through February.
2014
September: Topic refinement, Research (Relevant technologies), Experimentation October ­ November:  Research (Visualization design), Experimentation, Paper December: First force graph visualization created, More research and experimentation
2015
January: Development
February:  Main development completed, User evaluation created March: User testing, Analysis, Final development, Paper, Presentation
5
 Visualizations
Four types of visualizations were created as part of my deliverable. These visualizations may be applied to each chapter in the phiMap database. Each visualization is outlined below.
Professor-oriented Visualizations
Problem Set Variable Distribution
Figure 1. Visualization of Problem Set Variable Distribution
6

 This visualization is meant to aid the professor in assigning problem sets to students. The visualization will show the professor if relevant variables to the chapter are distributed equally throughout the problem set. Equal distribution is represented not only by the number of times that the variable occurs, but also by whether the variable is meant to be solved for (the t arget variable),  given a s a specific value, or  hidden (neither of the above). Once a problem set is selected to analyze, variables are connected in a node­edge graph to other variables of the same type. A graph with a disproportionate number of one type of variable will appear unbalanced. A graph with a more even distribution of types, where variables occur as multiple types, will be more connected. A node also becomes larger as it occurs more times in a problem set, showing if a variable is over or underrepresented. Finally, a graph with a disproportionate number of one type of variable will appear unbalanced, while a graph with a more even distribution of types, where variables occur as multiple types, will be more connected. Problem Set Difficulty
7
 Figure 2. Visualization of Problem Set Difficulty
 One is able to select a group of problems. These three problems are then represented as node­edge graphs, and they are shown relative in difficulty to the other problems. Nodes represent variables and edges represent equations. Variables are closer in distance to each other if they occur together multiple times in the problem set, hypothetically making the problem easier. This visualization was not tested in the user evaluation.
8
 Student-oriented Visualizations
9
 Figure 3. Relevant Equations to a Chapter
 Figure 4. Relevant Equations to a Variable
 These two visualizations are meant to help the beginning physics student with their problem solving skills. Nodes represent variables and edges represent equations. In the first visualization, one variable (in this case, mass) is centralized, and all related equations and variables are connected. In the second visualization, all relevant equations to Chapter 14 are represented as a set of lines and variables. Each equation is connected to other equations by co­occurring variables. Both visualizations have the potential to help students when trying to solve a problem for a specific variable. Students can visually see how different variables are connected to each other and better connect the dots for which equations they can use to solve a problem.
Analysis and verification
Evaluation Method
A user evaluation was created to analyze the success of my visualizations and obtain suggestions on how to improve their usability. This evaluation can be viewed  here. The evaluation contains demographic questions about the participants as well as quantitative and qualitative questions about the visualizations. Following the demographic portion, the evaluation contains two main sections to evaluate the success of the visualizations created for my senior project. One section focuses on a visualization meant for professors, while the other section focuses on two visualizations meant for students. Two versions of the evaluation were distributed, each with a different set of visualizations shown first. The evaluation concludes with general questions about how the visualizations could improved as well as possible applications within physics education. The evaluation was distributed to 2 physics classes and the physics department. 61 participants had taken the survey at the time of analysis. 3 were professors, 4 were physics majors or minors, and all other participants were currently in a physics class. 27% had experience with visualizations and 17% had experience with undirected graphs.
10

 Overall Rating of Visualizations
I asked 2 main questions about each set of visualizations. Each question could be answered from 1­10 on the Likert scale. This data can be seen below. Each mean was within 0.5 of 5, but the standard deviation for each data point was around 2.5, showing that the data had a wide spread. Even though my data was slightly lower than my goal, the large standard deviation shows that many people still found the visualizations easy to understand, as well as useful.
11
  Section 1: Visualizations for Professors
  Section 2: Visualizations for Students
     How easy were these visualizations to understand?
   How helpful would these visualizations be for professors?
   How easy were these visualizations to understand?
  How helpful would these visualizations be for students?
 4.7/10
    5.5/10
     4.8/10
    5.5/10
 Table 1. Quantitative Analysis of All Participants
One of my goals in user testing was obtaining specific information on the aspects of the visualizations that the participants found confusing. For both visualizations, 40% of users found the meaning of the lines confusing. 15 ­ 30% of participants found other aspects of the graphs as well. This data can be seen in the bar graphs below.
 Figure 5. Professor-oriented Visualizations
12
  Figure 6. Student-oriented Visualizations
 Demographic Influences on Data
Knowledge of directed and undirected groups raised the mean of how easy it was to understand the visualizations. A difference in the spread of the data can be seen in the boxplots below. This information predicts that if the intended user was first taught about undirected graphs, they would be able to better understand the meaning of the visualizations.
Figure 7. Undirected Graph Boxplots
It is difficult to compare the results of students compared to professors due to the low number of participants, and each professor had varied responses to each question. However, physics majors rated the application much higher than engineering physics students. This information can be seen below.
13

 14
  Section 1: Visualizations for Professors
  Section 2: Visualizations for Students
     How easy were these visualizations to understand?
   How helpful would these visualizations be for professors?
   How easy were these visualizations to understand?
  How helpful would these visualizations be for students?
 6/10
    7.5/10
     6.5/10
    7.5/10
 Table 2. Quantitative Analysis of Physics Majors/Minors
Suggestions for Improvement
In addition to the numerical data I collected, I also gained qualitative suggestions as to how to improve my visualizations. Many students asked for an improved key to explain the visualizations in detail. Participants also thought that an example problem or more labels and explanations alongside the visualization would be helpful as well. There seemed to be the most confusion over the lines in the visualization, with many suggestions for them to appear more spread out or for some of them to be removed.
Completion of Goals
I had a few personal goals as well while completing my senior project. I gained a thorough understanding of the capabilities of the D3 Javascript library. I also gained familiarity with Python and Django, connecting front­end code with data and and basic knowledge of best visualization design practices.
Societal impacts
The societal impacts of phiMap are immense. Once complete, it will not only have the ability to impact engineering physics classes at Cal Poly but also the potential to impact physics education across the
 United States. Through the use of different teaching methodologies for physics, phiMap will lead to new and innovative ways to teach physics. It also serves as a platform for future physics, computer science, design, and possibly education students to enhance their skills and improve their resumes. The work on phiMap will never be complete, as new visualizations, relationships, and methodologies can always be created and studied. Through the use of my visualizations, teachers will be able to analyze problem sets and better serve their students, improving the success of students in physics courses at Cal Poly and beyond.
Beyond Cal Poly
As phiMap gains more capabilities, it would also be optimal to expand its scope past Cal Poly. This is difficult due to the need for a database of problems; however, many schools use the same textbook. phiMap could also provide the ability for users to upload their own textbooks or problem sets for analysis as a solution to this problem. Furthermore, it would be interesting to look into how quickly physics textbooks become out of date and how this would affect the viability of phiMap. However, my visualizations are independent of the current data and therefore could be used with any textbook.
Personal Next Steps
Individually, I plan to continue growing my knowledge of the D3 library, Javascript, and visualization design. The technology industry is continuing to become more data­driven, and with a deep technical knowledge of the API, I can become a highly valuable software engineer and user interface designer, using my expertise for a variety of circumstances.
Conclusion
As technology continues to grow, data visualizations will become an important educational resource for all fields of study. The possibilities for influencing physics education through phiMap are immense due
15
 to the variety of important relationships that could be portrayed. My research and work on the data visualizations will be instrumental in making sure phiMap is as effective as possible. Through the proper use of physics education and data visualization taxonomies, phiMap could become an amazing and easy to understand resource for professors and students alike.
References
Antonenko, Pavlo D., Craig A. Ogilvie, Dale S. Niederhauser, John Jackman, Piyamart Kumsaikaew, Rahul R. Marathe, and Sarah M. Ryan. "Understanding Student Pathways in Context­rich Problems."Education and Information Technologies 16.4 (2011): 323­42. Web.
Evrard, August E et al. "Problem Roulette: Studying Introductory Physics in the Cloud." American Journal of Physics (2013): n. pag. Web.
Handbook of Data Visualization. Berlin: Springer, 2008.
Heer, Jeffrey, and Maneesh Agrawala. "Software Design Patterns for Information Visualization." IEEE Transactions on Visualization and Computer Graphics, 12.5 (2006): 853­860.
Iliinsky, Noah P. N., and Julie Steele. Designing Data Visualizations. Sebastopol, CA: O'Reilly, 2011. Print.
"Introduction to Data Visualization Tags: Data, Visualization ."Visualization Types. N.p., n.d. Web. 30 Nov. 2014.
16
 Kirk, Andy. Data Visualization: A Successful Design Process; a Structured Design Approach to Equip You with the Knowledge of How to Successfully Accomplish Any Data Visualization Challenge Efficiently and Effectively. Birmingham, UK: Packt Pub, 2012.
Kosara, R, F Drury, L.E Holmquist, and D.H Laidlaw. "Visualization Criticism." IEEE Computer Graphics and Applications, 28.3 (2008): 13­15.
Krusberg, Zosia. "Emerging Technologies in Physics Education." Journal of Science Education and Technology, 16.5 (2007): 401­411.
Teodorescu, Raluca E., Cornelius Bennhold, Gerald Feldman, and Larry Medsker. "New Approach to Analyzing Physics Problems: A Taxonomy of Introductory Physics Problems." Physical Review Special Topics ­ Physics Education Research 9.1 (2013): n. pag. Web.




