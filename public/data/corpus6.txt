Web-based Information Visualization Using JavaScript
Selin Guldamlasioglu
University of Tampere
School of Information Sciences Interactive Technology
M.Sc. thesis
Supervisor: Harri Siirtola
June 2015
University of Tampere
School of Information Sciences
Interactive Technology
Selin Guldamlasioglu: Web-based Information Visualization Using JavaScript M.Sc. thesis, 57 pages
June 2015
Since the available digital information grows rapidly, the utilization of the information visualization becomes of utmost importance to perform complex data analysis tasks. With the help of the visualizations, hidden facts about the data such as patterns, outliers and tendencies can be revealed. Strategic decisions can be established with the insights gained during the interactive data exploration.
The latest developments in browser technology have enabled developers to build interactive web-based visualizations to target broader audience. This thesis is dedicated to investigate usage of JavaScript in web applications. After describing the main theories and frameworks in the information visualization literature, a number of visualization tools are introduced. D3.js is selected as the visualization framework to visualize Tampere Unit for Computer-Human Interaction Research Center’s publication data.
According to design considerations and experiment gained from visualization of the web-based publication data, it is possible to create powerful and reusable interactive visualizations with JavaScript frameworks. Theories and frameworks mentioned in the literature review worked as a guideline while visualizing the web-based data.
Key words: information visualization, web-based visualization, JavaScript and information visualization
i

Contents
1. Introduction ............................................................................................................... 1
2. Information Visualization using JavaScript .............................................................. 4
2.1. Information Visualization................................................................................ 4 2.1.1. Data Types and Information-Seeking Mantra ..................................... 5 2.1.2. Goals of the Information Visualization ............................................... 8 2.1.3. Gaining Insight .................................................................................. 10 2.1.4. Activity of Information Visualization................................................ 12 2.1.5. Reference Model for Visualization.................................................... 13
2.2. Web-based Information Visualization........................................................... 15
2.2.1. Visualization Tools ............................................................................ 16
2.2.2. Web-based Visualization Tools ......................................................... 18
2.2.2.1 SVG ........................................................................................... 18
2.2.2.2 HTML5 Canvas ......................................................................... 18
2.2.2.3 JavaScript................................................................................... 19
2.3. JavaScript and a web browser as a visualization platform ............................ 20
2.3.1. Reference Model and JavaScript ....................................................... 20
2.3.2. Interaction and Animation ................................................................. 21
2.3.3. Compatibility and Accessibility......................................................... 22
2.3.4. Client-sideFrameworks.....................................................................22
3. D3.js: A Data-driven Visualization Framework ..................................................... 24
3.1. Major Goals ................................................................................................... 24
3.2. Creating a Design .......................................................................................... 25
3.2.1. Selection.............................................................................................26
3.2.2. DataOperations.................................................................................26
3.2.3. Interaction and Animation ................................................................. 26
3.2.4. Modules.............................................................................................27
3.3. Example Applications.................................................................................... 27 3.3.1. Example Study 1: Flight Analysis ..................................................... 27 3.3.2. Example Study 2: Real-time Analysis of Twitter .............................. 30 3.3.3. Example Study 3: Stock market visualization ................................... 32
4. Visualization of the Publication Data ..................................................................... 35
4.1. User Requirements......................................................................................... 35
4.2. Related Work ................................................................................................. 36
4.3. Implementation .............................................................................................. 38 4.3.1. External JavaScript Libraries............................................................. 38 4.3.2. DOMManipulation............................................................................40 4.3.3. DataAbstractions...............................................................................41 4.3.4. Interaction..........................................................................................42
ii
iii
5. Implementation Results: Description, Analysis and Synthesis............................... 44
5.1. Results ........................................................................................................... 44
5.2. Discussion...................................................................................................... 49
6. Conclusions ............................................................................................................. 52
References ...................................................................................................................... 53
1. Introduction
Information visualization is an emerging discipline that utilizes visual representations of abstract data to help people to understand the underlying meanings of the information. Visual data observation makes more sense on human beings than looking at the raw information. In today’s big-data world, meta-information, i.e., information about the information, has become more important. While dealing with the data, information visualization not only appeals to the eyes, but also has a potential to reveal the hidden information.
When William Playfair [1801] invented the pie chart, he already discovered that alluring the eye makes people read the information easily. He chose to display series of circles arranged side by side with their centres aligned where some of the circles were divided into sectors (See Figure 1.1). Although the effectiveness of the chosen method is questionable by some researchers (e.g., Stephen Few [2007]), pie charts can still be regarded as one of the most popular ways to display information.
Figure 1.1 The first pie chart ever from William Playfair’s “Statistical Breviary” [1801]
1

After Playfair’s invention, historical examples vary with Napoleon’s Russian campaign of 1812 [Tufte, 2001] and John Snow’s map that represents the cholera clusters of the epidemic data [Johnson, 2006]. In addition to the historical examples demonstrating the earlier usage of visualization tools, The Joy of Stats by Hans Rosling [2010] revealed how information visualization can be applied creatively to demonstrate big quantities of public data. Rosling visualized the development story of the world using augmented reality animation. He illustrated how modern technology can be used to visualize mass amounts of data.
Although Rosling’s visualization was a good example to demonstrate big amounts of data, it was lacking to provide a platform where people could interact with the data. In today’s Internet of Things world, devices are connected with each other within the Internet infrastructure. Web browsers are widely being used to retrieve, present and traverse information on the World Wide Web.
One of the most common applications of information visualization can be regarded as visualizing web-based information due to the increasing power of web browsers. Today’s interactive and mobile-friendly visualizations require responding to the user actions such as clicking, hovering, scrolling and touch gestures. Users want to navigate through data and filter the information according to their needs. Nowadays web browsers are responsible for data processing instead of servers because of the increased demand for real-time interactivity.
Since JavaScript is the language of web browser, it is commonly used while visualizing the data. Client-side JavaScript, i.e., JavaScript interpreter embedded in a browser, combines scripting capabilities of JavaScript together with the document object model (DOM) presented by the browser.
Client-side JavaScript enables users to control document appearance and content, interact with HTML forms and other users. This approach has both advantages and disadvantages. Since JavaScript code is executed on the user’s computer, processing is fast as there is no need to process data in server. JavaScript also provides an extended functionality for web pages so that developers can customize the visualization and behaviour of the websites. Although advantages of using JS in web browsers are varied,
2
there are some related drawbacks. JavaScript frameworks, which are being used to extend functionality, might raise security issues since they execute on client side immediately. Moreover, inconsistencies between different layout engines might lead to functionality and interface variations. This disadvantage doesn’t afflict the web-based visualization using JavaScript since most of the modern browsers support the latest versions of JavaScript.
This thesis mainly focuses on the usage of JavaScript in web applications. The main purpose of this work is to examine JavaScript visualization frameworks. After examining the important theories and frameworks in the process of information visualization, example web-based applications will be examined with regard to these theories and frameworks. Lastly, Tampere Unit for Computer-Human Interaction’s (TAUCHI) publication data will be visualized with selected JavaScript framework to demonstrate the strength of JavaScript in web-based applications.
This thesis consists of six chapters. Chapter 2 introduces the main principles and concepts of information visualization. This is followed by a detailed discussion on the web-based information visualization and usage of visualization tools. Chapter 3 focuses on the selected visualization framework, D3.js, and demonstrates the application of this framework to the recent web based visualization examples. Chapter 4 explains the implementation details of the TAUCHI’s publication data and implementation results can be seen in Chapter 5. In the conclusion chapter, visualization of the TAUCHI’s publication data will be discussed regarding the theories and frameworks mentioned during the literature review.
3
2. Information Visualization using JavaScript
This chapter discusses the main principles of information visualization and describes web-based information visualization regarding the current visualization tools. Lastly, usage of JavaScript in the field of information visualization is introduced in detail.
2.1. Information Visualization
Information visualization is a research area that aims to allow people to gain insight, understand and analyse the data. According to a dictionary, ‘visualize’ means “to form a mental image, to make something visible”. Vast quantities and different types of information are being generated these days, emerging the need to extract information from data overload.
Card et al. [1999] describes information visualization as the use of computer-supported, interactive, visual representations of abstract data to amplify cognition. One of the main goals of information visualization can be regarded as augmenting human cognition in a way that viewers gain knowledge about the underlying meaning of the data.
Information visualization influences the lives of humankind directly. To exemplify, people need to search through various information such as metro lines, e-mails and schedules on a daily basis. Finding the valuable information hidden in large amounts of data can be a difficult task. Therefore, adequate exploration of data enables people to make use of the data.
As information visualization is an evolving discipline, historical examples indicate the significance of the different visualization techniques. In 1931, Harry Beck, engineering draftsman, altered and improved London Underground Tube map [Degani, 2013]. He eliminated all of the surface details except the Thames River that brought simplicity to his drawing. The result was so simple and comprehensible that society embraced his map and it became an essential guide to London. Beck’s revolutionary design with modifications and additions survives to the present day.
4
Although Harry Beck prepared striking Tube map to display available information, lack of proper visualization techniques caused some disasters in the history. In 1986, Space Shuttle Challenger exploded 73 seconds after its launch causing the death of seven US astronauts. The reason for the explosion was the failure of an O-ring seal on a booster rocket due to the cold weather. Edward Tufte, noted statistician in the field of data visualization, indicates that inaccurate assessment of the facts lead to the explosions. Although design engineers prepared 13 charts supporting the launch, they couldn`t display the relationship between O-ring failure and temperature. Tufte even claims that this tragic accident might have been avoided with a clear representation. [Robinson et al., 2002]
Impact of information visualization in today`s world is increasing as generated digital content is also expanding. One of the important aspects of information visualization can be regarded as interactivity. Visual representation of the data allows viewers to gain insight of the data and reach conclusions by directly interacting with the data. Viewers can interact with the data intuitively without requiring understanding of the underlying structures. It is crucial to understand the information visualization process while creating interactive web-visualizations. User friendly, highly interactive and reusable charts can be created if designers convey to the main principles of information visualization.
2.1.1. Data Types and Information-Seeking Mantra
The terms data, information and knowledge are being extensively used in the field of information visualization. It is important to differentiate these terms to be able to understand data types better.
Russell Ackoff, organizational theorist, classifies the content of the human mind into five categories: data, information, knowledge, understanding and wisdom [Ackoff, 1989]. He defines data as symbols. Data can exist in any form independent of its usability. He describes information as the data that is processed to be useful. Information provides answers to “who”, “what”, “where” and “when” questions. He further characterizes knowledge by stating that knowledge provides answers to “how” questions, as knowledge is the application of data and information. Ackoff [1989]
5
continues his definitions by stating that understanding is the appreciation of “why” whereas wisdom is the evaluated understanding.
Ackoff [1989] makes distinction between data and information in perceptual and cognitive space. It is also important to make the distinction in computational space since data and information can be stored in a computer. Data is defined as computerized representations, whereas information is the data that represents the results of a computational process [Chen et al., 2009].
Although one can categorize data in many ways (e.g., nominal, ordinal, interval), Shneiderman [1996] discusses about the data type taxonomy with seven data types (one, two, three dimensional data, temporal data, multi-dimensional data, tree and network data) and seven tasks (overview, zoom, filter, details on demand, relate, history and extract). Shneiderman`s Visual Information-Seeking Mantra [Shneiderman, 1996] provides a framework for designing information visualization applications.
According to Shneiderman [1996], data types can be described as follows:
• One-dimensional data: Consists of linear data types (e.g., textual documents which are sequentially organized).
• Two-dimensional data: Includes planar or map data where each item in the collection corresponds to some part of the area. Every item includes task- domain attributes.
• Three-dimensional data: Includes real world objects. Understanding the position and orientation of the object is one of the challenging tasks of the viewers.
• Temporal data: Consists of items which has a start and finish time where timelines are needed (e.g., medical records, project management).
• Multi-dimensional data: Items with n-attributes relate with points in an n- dimensional space. Common user tasks include, for instance, finding patterns, correlations and outliers.
• Tree data: In a tree hierarchy, each item has a link to the parent item except from the root. Finding the number of levels in a tree can be one of the major user tasks.
6
• Network data: Includes items linked to an arbitrary number of other items where these items cannot fit into a tree conveniently (e.g., visualizing the social networks).
After defining different data types, Shneiderman [1996] states his mantra to achieve powerful visualizations: overview first, zoom and filter, then details on demand. This mantra provides designers a framework and explains the essential elements of interacting with different types of data. Details about his mantra can be seen in below [Shneiderman, 1996]:
• Overview: Provides a general context for understanding the whole dataset. Patterns and themes can be observed from this viewpoint.
• Zoom and Filter: Once the data is presented to the user with overview, viewers can focus on the particular area depending on their interest. Zooming allows users to adjust the size and the position of the data on screen where filtering removes the irrelevant information.
• Details on demand: Viewers should be able to observe the raw data if they demand. Visualization application should not enforce the users to change the view to be able to see the details.
• Relate: Users can observe the relationships between the items. For instance, users of HomeFinder [Williamson and Shneiderman, 1992] could select an attribute garage, to find the houses with a garage in a price range and area relating the houses only with a garage.
• History: Viewers should be supported with undo and replay options to provide historical information to the users.
• Extract: Once the user has found the related information by overviewing first, zooming and filtering, and then details on demand, it would be useful to enable users to save the relevant information by extraction.
Despite the fact that there are different data types, designers can cope with these different types of data by applying the Visual Information-Seeking Mantra [Shneiderman, 1996]. Shneiderman`s Mantra provides an efficient guideline to achieve successful visualizations. Frequent use of the Mantra could be an indication that
7
overview first, zoom and filter, then details on demand approach provides strong visualizations under different scenarios with different data types.
2.1.2. Goals of the Information Visualization
The main goals of information visualization mainly focus on making viewers gain insight by forming a mental model. As information visualization includes human interaction with the data, main aim of information visualization is to acquire the viewer with the knowledge. It has been proven that information visualization works as a learning tool to foster knowledge acquisition. Experiment that investigates the understanding of the relations in a data set leads to the conclusion that cognitive processes in knowledge acquisition can be enhanced by applying information visualization techniques. [Keller et al., 2006]
The main goals of information visualization can be listed as discovery, decision-making and explanation. Each of these goals, explained as follows, plays a significant role in gaining insight:
• Discovery: Information visualization provides metadata, i.e., data about the data. Therefore, it is possible to create hypotheses without investigating all bits of the data. Exploratory data analysis is a data analysis approach which utilizes mostly graphical techniques such as box plots, histograms and scatter plots. Tukey [1977] defines exploratory data analysis in 1962 as: “Procedures of analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate ...”. Viewers can make discoveries by finding outliers, tendencies and trends with the help of the exploratory analysis. Viewers can also formulate hypotheses and verify or refute the hypotheses.
• Decision-making: After deriving hypotheses about the data, information visualization aims to provide confirmative analysis. For example, Dr. John Snow`s main hypothesis was that cholera spreads through water [Johnson,
8
2006]. In order to prove his hypothesis, he mapped the number of deaths, which lead to the confirmation of his hypotheses (See Figure 2.1).
Figure 2.1 Original map by John Snow demonstrating the cholera clusters in London epidemic cases of 1854 [Johnson, 2006]
In contrast to the exploratory data analysis, confirmatory analysis begins by stating a hypothesis. Examination of the hypothesis is achieved in a goal- oriented manner. Visualization either confirms or rejects the hypothesis as a result.
• Explanation: Another goal of information visualization is to present the fact which results in high-quality visualization of the data. Presentation serves the purpose of delivering the results of an analysis efficiently and effectively. User selects the appropriate presentation technique, determining the facts to be presented beforehand.
Observers gain insight by discovery, decision-making and explanation. Information visualization aims to acquire the observers with knowledge. Viewers can make new discoveries simply by finding outliers, detecting trends and tendencies. They can also refute or accept hypothesizes by observing or interacting with the data. Large quantities of data can be explained to the viewers so that they can understand the underlying meanings of the data.
9

10
2.1.3. Gaining Insight
Human vision contains millions of neurons where each individual neuron works in parallel to extract the features of visualization. This rapid parallel processing and pattern recognition happen regardless of human attention [Ware, 2004]. Visualization means the formation of a mental model or cognitive map of the data [Spence, 2014]. Visualization is perceived in mind and results in mental model or internal model, i.e., cognitive maps.
Figure 2.2 Training dataset [Siirtola, 2013] shown in spreadsheet, geographical and boxplot forms using R software

11
For instance, Figure 2.2 demonstrates the mapping of the training data which includes latitude, longitude, altitude, date, and time information. The training terrain, altitude differences, average heart rate and highest speed can be inferred from the visualizations. It is not possible to gain these insights only by observing the raw dataset. Although these insights aren`t explicitly stored in the data, they are gained through pattern recognition. Therefore, visual representation plays an important role in gaining insight. Visualizations can hide the meanings or mislead the viewers unless data is visualized accurately.
Yi et al. [2008] claim that there are certain procedures leading to insight. These procedures are as follows:
1. Provide overview: Viewers can discover the overall picture of the data with this process. Further inquiries needs to be done on the dataset can be determined during this step by grasping knowledge about the data comprehensively.
2. Adjust: Observers can change the level of abstraction and selections in a flexible manner. This way, large quantities of data can be explored using filtering. Additionally, uninteresting parts of the data can be omitted.
3. Detect pattern: Specific distributions, outliers and relationships in the dataset can be found by detecting patterns. New knowledge discoveries can be made by recognizing patterns within the dataset.
4. Match mental model: Cognitive load in the process of knowledge acquisition can be reduced by linking the data with the viewer`s mental model. Metaphors can provide more effective mapping while matching the data with the mental model.
Information visualization is a strong tool for the knowledge discovery. Additional to simple insights such as minimum, maximum or average values, complex insights as patterns, clusters, paths can be derived from information insight. Viewer`s internal model can be refined, clarified and extended by gaining insight.
12
2.1.4. Activity of Information Visualization
Considering the fact that information visualization enhances the ability to comprehend huge amounts of data, there are various differences in visualizations such as interaction paradigms and different data types. Although there are divergences in different visualizations, it is conceivable to analyse visualizations systematically.
Ware [2004] separates the process of data visualization into four: collection and storage of data, pre-processing and transporting the data into human-readable format, displaying the image on screen, and finally the perceiver (see Figure 2.3).
Figure 2.3 A schematic diagram of the visualization process [Ware, 2004]
Data is gathered from both physical and social environments. After data is collected, it needs to be pre-processed and transformed into organized canonical data format. At this stage, data entities need to be associated with attribute values. Graphics engine is responsible for mapping the dataset into visual form. Once the visual forms are created, views need to be provided to present transformations (e.g., navigation). Views are interpreted by users through human visual system, visual and cognitive processes.
Although Ware [2004] explains the process of information visualization, it lacks the ability of providing a platform to compare and contrast different information visualization systems. Different reference models were developed to simplify the discussion of different visualization systems by making correlations.

13
2.1.5. Reference Model for Visualization
Card et al. [1999] established a reference model by defining the milestones in the process of information visualization (see Figure 2.4). Reference model aims to provide a framework for developers who create new visualizations.
Figure 2.4 Reference model for visualization [Card et al., 1999]
According to the Card et al.`s model, raw data needs to be transformed into structured data using data tables. Data tables contain the metadata. Further transformations can be performed on metadata to derive additional characteristics of the data. Visual mappings constitute to the core of the reference model by converting data tables into visual structures. Visual structures are abstract, representing the derived characteristics of the data, which undergoes to the visual mappings. At this stage human vision can process the visual structures, whereas data tables were representing the mathematical relations. Once the visual structures are created and graphical views are acquired by view transformations, human interaction can alter the views (e.g., colour, size and orientation).
According to the Card et al.`s model (Figure 2.4), human interaction can take part at any stage of the visualization process. User selects the relative raw-data in the data transformations stage. During the visual mappings, user determines the mapping between the data variables and abstract structures (e.g., axes). User can interactively control the presentation of data, for instance, by distorting at the view transformations stage.

14
Although Card et al.`s [1999] reference model provides a step-by-step description on how to visualize raw data and how to include human interaction during the visualization process, this model lacks to provide detailed information for the developers. This lack of information is being criticized by some researchers. Chi [2000] criticizes the previous reference models by stating “Researchers have attempted to construct taxonomies of information visualization techniques by examining the data domains that are compatible with these techniques. However, these taxonomies do not help the implementers understand how to apply and implement these techniques”. After this criticism, Chi proposes a new way to taxonomize information visualization techniques by providing an overview (See Figure 2.5) to his earlier Data State Model [Chi and Riedl, 1998].
Figure 2.5 Information Visualization Data State Reference Model [Chi and Riedl, 1998]
There are four data stages in Chi`s Data State Model [Chi, 2000]: value, analytical abstraction, visual abstraction and view. Value corresponds to the raw data whereas analytical abstraction represents the metadata. Visualization abstraction means the information that can be visualized on screen using visualization techniques. User interacts with the view as it is the end-product. Data transformation, visualization transformation and visual mapping transformation operators have similar tasks as in the Card et al.`s Reference Model [Card et al., 1999].

15
Chi uses this Data State Reference Model to taxonomize different visualization techniques. For instance, one of the visualization techniques called GraphViz [AT&T, 2014] can take a graph as a raw data and remodel the graph by extracting edges and nodes during the data transformation stage. Complex graph layout algorithm places the nodes on a 2D plane during the visual mapping transformation stage. Chi [2000] gives many examples like GraphViz to provide sequential order of operations so that developers can create modular visualizations.
Although both of the Card et al.`s and Chi`s reference model and taxonomies share similar techniques to allow reuse, Chi puts more emphasize on the implementers so that they can understand how to apply these techniques broadly. Both of the Reference Model and Data State Model are important while analysing the information visualization activities. These models can be utilized to present further categorization to identify similarities between different visualization techniques.
The most important aspect of using reference models can be reuse. If designers can understand how to apply and implement information visualization techniques, they can reuse different parts of the system to construct new information visualizations. Reuse leads to the rapid development that brings broad benefits. Implementers are more likely to construct new visualizations if they understand the interactions between the data and operations clearly. Therefore, usage of reference models leads to rapid development.
2.2. Web-based Information Visualization
Utilization of information visualization becomes crucial when dealing with complex data sets. Data, information, users and applications work interactively in a distributed environment with the latest developments in browser and internet technologies. Web- based information visualization has a broad target group as there is no necessity for installing software to operate on the data. Users can interact with the up-to-date data without relying on the platform. Collaborative data analysis can be possible as most of the modern web browsers support the implementation of visualizations without requiring any other software installation.
16
There are different tools and technologies for developing interactive web-based visualizations. Before the improvement of browser technologies, small applets were the major visualization tools. These applets required third-party installations like Java Applets or Adobe Flash. Although there are still some tools requiring software installation, web-based visualizations tools tends to work in browser natively. Different visualization tools are explained in detail in the following chapters.
2.2.1. Visualization Tools
Bostock and Heer [2009] divide data visualization tools into two to make better comparison between the visualization tools as follows:
1. Graphical Systems: In this kind of systems, cognitive mapping between the representation and results accomplished directly since designers manipulate the graphical visualization using the selected software tool (e.g., vector-based Adobe Illustrator). Although higher-level tools like Adobe Flash provides illustration and animation, constructing even simple visualizations with these systems requires advanced design and programming skills.
2. Visualization Systems: Bostock and Heer [2009] describe these systems as tools that are designed for explicit data visualizations. These tools support data- management, layout algorithms, interaction and animation.
a. Consumer Software: This category includes the most commonly used visualization tools such as Microsoft Excel and Google Spreadsheets. Some of the main tasks include the selection of relative data cells and determining the desired type of the chart. Although this kind of software is considered as successful in terms of ease of use and immediate visualization, there are some short comes mentioned by Wilkinson [2005]. He argues that chart metaphor is restrictive, disabling the user from understanding what to do with the data after constructing the visualization. He continues his arguments that charts give the impression of data exploration rather than the experience.
17
b. Analytical and Exploratory Tools: These types of tools are designed for providing flexible options for data explorations. Statistical programming language R can be regarded as an example for such tools. R has specific data visualization packages to replace base graphics with scales and layers. Although analytical and exploratory tools utilize the metadata by choosing appropriate visual encodings, it is not possible for designers to customize all visual aspects. Additionally, graphical output is mostly being used for research purposes rather than presentation since control over graphical output is limited [Bostock and Heer, 2009].
c. Programming Toolkits: According to Bostock and Heer [2009], programming toolkits are popular for presenting live data and allowing user interaction. Some of the toolkits (e.g., Google Chart API) support limited number of chart types, presenting similar characteristics with consumer software. More expressive visualization toolkits (e.g., InfoViz [Li, 2012]) allow extensions by modifying existing components or creating new components from scratch. On the other hand, Prefuse and Flare [Chi and Riedl, 1998] conveys the Chi`s Data State Model [2000], i.e., designers can specify properties of configurable operators to perform actions such as layout and color encoding. However, these kinds of toolkits transform easy tasks into complex ones [Bostock and Heer, 2009].
According to Bostock and Heer [2009], there is no strict distinction between graphical systems and visualization systems. Graphical systems use low-level graphical components whereas visualization systems utilize high-level abstractions designed for data visualization. Protovis, Bostock and Heer`s [2009] solution, adapts properties of both graphical and visualization systems. Protovis runs natively in the browser, but also allows declarative specification so that other rendering engines (e.g., Java2D, Flash) can be integrated in the future. With this approach they aim to optimize the Data State Model [Chi, 1998] through lazy evaluation of visual properties of large datasets.
18
2.2.2. Web-based Visualization Tools
In this section selected tools for web-based information visualization will be described considering the recent developments in web tools.
2.2.2.1 SVG
Scalable Vector Graphics (SVG) is an XML-based image format with support for interactivity and animation. World Wide Web Consortium (W3C) explicitly recommends the usage of SVG in web browsers [W3C, 2011]. It is possible to integrate SVG with other W3C standards like Document Object Model (DOM).
SVG is being used to define vector-based 2D web graphics in XML format. Since it is vector based, image size and scale can be changed without quality loss. As the images are scalable, they can also be zoomed without deterioration. Since SVG objects are available in DOM, JavaScript event handlers for the objects can be created, for instance, when the user clicks on a certain circle.
Every drawn shape is considered as a SVG object. When the user changes the attributes of an object, web browser re-enders the shape automatically. Therefore, usage of SVG is suited for the applications for large rendering areas like Google Maps.
Although SVG brings benefits like scalability, it is not widely supported. Intensive research needs to be carried out after creating a SVG image to ensure the image will appear and function as intended through different kinds of browsers. Additionally, SVG is not suitable for graphic-intensive applications as complex SVG images render slowly in the browser.
2.2.2.2 HTML5 Canvas
The Canvas Element is part of the HTML5 and enables dynamic rendering of 2D shapes. Canvas element with height and width attributes creates a container for the shapes. JavaScript code can access the specific canvas area while dynamically generating the graphics.
19
Canvas API is a low level pixel-oriented model. Shapes cannot be changed without overwriting the pixels. Unlike an SVG-object, once the shape is drawn, system forgets the fact that shape was drawn. As there is no automatic rendering with the Canvas element, entire shape would need to be redrawn if its shape or position is changed. Additionally, unlike SVG, it is not possible to create event handlers for Canvas elements, e.g., coordinates need to be matched manually to determine mouse click events. Canvas is suited better for graphic-intensive applications as it is more memory- efficient compared to SVG.
2.2.2.3 JavaScript
JavaScript (JS) is considered as the programming language of the web as most of the modern HTML pages are using JavaScript. Main usage of JS is the client-side JavaScript where users control and alter the document content in a web browser.
JS program can manipulate the content through containing Document and Element objects. Presentation of the content can be changed by scripting CSS styles. Additionally, event handlers can change the behaviour of documents. Combination of scriptable content, presentation, and behaviour is called Dynamic HTML (DHTML) [Flanagan, 2011]. Scripts embedded in HTML pages interact with the Document Object Model (DOM) directly to alter the page content dynamically.
JavaScript can be utilized to enhance the browsing experience, for example, by creating animations, playing audio/video and validating input values. Additional to content, presentation and behaviour manipulation, JavaScript APIs allow web applications to work asynchronously, sending and retrieving data without interfering with the current user actions. Quick and responsive applications can be built with JavaScript since JS code runs on user`s browser locally.
20
2.3. JavaScript and a web browser as a visualization platform
JavaScript is a dynamic language, which is mainly used to modify the displayed document content in web browsers. JavaScript can change HTML DOM (the Document Object Model) elements, attributes and styles. JavaScript includes an eval function that can execute statements at run-time [Richards et al., 2010].
JavaScript has a powerful object literal notation (JSON) since objects can be created easily by listing their components. As JSON object is in text format, it can be easily read. Additionally, it is language independent [Crockford, 2008], allowing data to be read and used by different programming languages.
In addition to dynamic objects and expressive object literal notation, JavaScript functions are objects with lexical scoping, i.e., scope of inner function contains the scope of parent function. Lexical scoping enhances the functionality of the language since all variable definitions are accessible [Gentleman and Ihaka, 2000].
As it can be complex to support all different web browser requirements, there are many JavaScript libraries available for easier development. Although JavaScript frameworks have common characteristics like DOM manipulation, every framework has different main functionality. For instance, jQuery, one of the most commonly used libraries in websites [SimilarTech, 2014], uses CSS selectors to manipulate HTML elements, whereas D3 is a JavaScript library that manipulates documents based on data by providing powerful visualization components.
2.3.1. Reference Model and JavaScript
Card et al.’s [1999] Data Reference Model is important for understanding and applying JavaScript properties for visualizing the data. Figure 2.6 represents the modified version of the Reference Model for information visualization with JavaScript.
21
   Figure 2.6 Reference model adapted for JS web visualization [Card et al., 1999].
JSON corresponds to the data table as it lists the attributes of the metadata. Visual structures, such as a pie chart, are the visualization means that are interpreted directly by the human. Canvas element of HTML5 is being used to draw elements on webpage. View transformations interactively modify visual structures to create interactive visualizations (e.g., zooming and fish eye view). Views are created with HTML elements that are formatted with CSS.
Reference model describes the approximate steps in the information visualization process. Information visualization reference model can be applied easily to the visualizations with JavaScript. JavaScript includes dedicated structures, such as JSON, SVG, Canvas, and HTML, which fit into data tables, visual structures and views of the model.
2.3.2. Interaction and Animation
Spence [2007] defines the interaction between human and computer as the heart of modern information visualization. Usually core of the data is so large that one single view is unlikely to lead to gaining insight. Interactive exploration is required so that the view triggers an `a ha! ́ moment.
JavaScript provides an excellent interaction support in web browsers. Client-side JS frameworks can be utilized to enhance dynamic behaviours such as interaction and
22
animation. For instance, interactive SVG bar charts with smooth transitions and user interaction can be generated with the various JavaScript frameworks (e.g., D3.js).
2.3.3. Compatibility and Accessibility
Web visualizations are being viewed in browsers from different ages and from different vendors such as Google and Opera. It can be a challenging task to create JS visualizations that behaves as intended in different kinds of browsers. Flanagan [2011] suggests the usage of additional libraries to deal with the incompatibility problems.
In addition to compatibility issues, it is important to provide accessible visualizations to users. Device-independent events should be supported rather than device-dependent events (e.g., mouse over) to be able to include the users regardless of which device they are using (e.g., keyboard, mouse).
2.3.4. Client-side Frameworks
Client-side frameworks build a higher-level API on top of the standard APIs offered by web browsers. JavaScript code needs to be written to use the APIs defined by the selected framework [Flanagan, 2011]. A well-written framework addresses the compatibility and accessibility issues described in the previous section. One of the main benefits of using a framework is extended functionality with less coding.
There are many JavaScript frameworks for information visualization. Some of the most widely used open-source frameworks include:
• Processing.js: This framework is JS port of the Processing programming language designed for visualizing the web. It converts the written Processing code into JavaScript and executes. This module makes use of the HTML5 canvas element to render 2D and 3D visualizations. The Processing API includes various methods such as canvas and color manipulation, shapes and image drawing, and math functions. [Fry and Reas, 2014]
23
• Raphaël.js: This library provides an API for SVG manipulation. This is used mostly for drawing vector graphics on web. Additionally, this API tries to solve the compatibility issues with old-versions of Internet Explorer using VML (Vector Markup Language). Websites like Washington Post and the Times Online makes use of this framework because of the compatibility advantages. [Dmitry, 2014]
• Protovis: Bostock and Heer [2009] created Protovis JS library to generate SVG graphics. In Protovis, visualizations are designed as hierarchy of marks such as bars and dots. Inheritance, scales and layouts are allowed in Protovis. However, it has limitations to provide animation and transitions. Therefore, Heer and Bostock developed a successor to Protovis, D3.js, to provide an improved support for animation and interaction.
• D3.js: Data-driven Documents (D3) is a JS library for direct manipulation of a native representation [Bostock et al., 2011]. Main purpose of D3 is to create highly interactive and responsive data visualizations by allowing animation. D3 JavaScript library makes use of the SVG, HTML5 and CSS standards to create visualizations for modern web browsers.
There are different JavaScript libraries for making the development of JavaScript-based applications easier. Every JavaScript framework has specified functionality such as DOM manipulation, GUI development, graphics and visualization development and web application related. JavaScript frameworks are beneficial in terms of dealing with the inconsistencies between runtime environments. Devoted visualization frameworks (e.g., Processing.js, D3.js) become advantageous while creating web-based applications since developers can concentrate on the visualization components.
24
3. D3.js: A Data-driven Visualization Framework
As described in the previous chapter, D3.js is designed to propose a novel visualization approach for web. With D3.js, designers can generate and modify web content by direct manipulation of the document object model. D3.js is a very powerful visualization library since it makes use of JavaScript, SVG and CSS.
3.1. Major Goals
According to Bostock et al. [2011], interactive visualizations should combine various web technologies such as HTML, CSS, JavaScript and SVG. They claim that visualization toolkits, which lack the ability of direct manipulation of the existing DOM, cannot make use of the different web technologies [Bostock et al., 2011].
Some of the earlier visualization tools, such as Protovis [Bostock and Heer, 2009], present accessibility and expressiveness drawbacks. It is difficult to learn the representation since the DOM is encapsulated in tool-specific formats. Additionally, tools that provide structured solutions do not enhance the development of new graphic solutions.
D3.js developers categorize their goals as follows, aiming to overcome the drawbacks related with the earlier visualization toolkits [Bostock et al., 2011]:
• Compatibility: D3.js aims to provide reusable components to improve accessibility and efficiency. Moreover, additional JS libraries can be utilized to assure backwards compatibility with older browsers.
• Debugging: D3.js simplifies debugging due to the JS console in developer tools. Developers can interactively debug by running JS in the browser.
• Performance: D3.js emphasizes transformation rather than representation. Since there is no specific format for certain visualizations (e.g., drawing a circle), developers can make use of this characteristic to improve performance, for instance, by writing pure HTML code.
25
Additional to the developer’s goals, there are certain requirements for a web-based visualization framework. Some of these applied requirements are mentioned below [Aufreiter, 2011]:
• Declarative language design: D3.js adapts the properties of declarative programming. This approach has potential to improve performance with the help of higher-level abstractions. D3 selections allow this declarative approach.
• Data Representation and Transformation: In D3 there are no predefined vocabularies for graphical representations. Instead, alternative forms of graphics are built on top of CSS3, HTML5 and SVG.
• Interaction: Arbitrary data can be bound to DOM with the data-driven approach. Data is available to event handlers, enhancing the interaction.
• Animation: Instead of mapping the data into a static representation, D3 emphasizes manipulation. There is a transition operator devoted to animated transitions. CSS transitions can be also utilized for animation purposes.
• Extensibility: There are various external modules to allow extensibility. For instance, geo module can be used to transform geographic data into SVG paths without disrupting the core library.
D3.js allows great control over the final visual result with compatibility, debugging and performance benefits. Since D3.js fulfils the requirements of a web-based visualization framework mentioned by Aufreiter [2011], it can be regarded as a powerful library while creating dynamic and interactive web visualizations. It is common to encounter various web visualizations developed with D3.js in different areas such as data analytics and newspapers (e.g., The New York Times [Ashkenas et al., 2012]).
3.2. Creating a Design
Atomic function of D3.js can be regarded as the selection operation. Elements can be retrieved from the current document using the selection method. After relevant elements are selected, operators can be applied to modify content (e.g., attributes, styles, HTML). Event handlers can be used to create interaction whereas attributes can be
26
changed smoothly over time to allow animation. Various modules (e.g., layouts) can be used to achieve common visualization tasks. [Bostock, 2011]
3.2.1. Selection
D3 makes use of CSS3 to select elements. There are two selection functions: select and selectAll where the former selects only the first matching element whereas the latter selects all of the matching elements. Elements can be filtered and selected according to their tags, class names, identifiers and attributes. The selector format, defined by the W3C Selectors API, is compatible with modern browsers. [Bostock et al., 2011] After selecting the elements, operators can be applied to manipulate attributes, styles, properties, HTML and text content of the selected element.
3.2.2. Data Operations
In D3.js, data is bound to DOM elements. D3 allows reuse by enabling users to work with different datasets without requiring additional work. D3 accepts various data types such as arrays, strings and objects. Additionally, it can handle large datasets in JSON formats or CSV files. The data operator is being used to bind the input data to the selected DOM nodes. Additionally, there are dedicated enter and exit selections for data binding to enable adding new nodes and removing unwanted nodes. [Bostock, 2011]
3.2.3. Interaction and Animation
D3.js focuses on manipulation rather than the static representation of the data. Therefore, this framework includes support for smooth transitions. Event listeners can be added to selected elements to recognize events supported by the browser (e.g., click, mouseover, submit). Transition applies the operations smoothly over time to allow animation. [Bostock, 2011]
27
3.2.4. Modules
D3.js is extensible with available optional modules. Selection and transition operations are the main required operations for a module. These prototypes can be extended by adding new methods. Additional modules can always be added to extend the functionality. For instance, csv module supports reading and writing comma-separated values whereas layout module provides various reusable visualization layouts such as tree-maps.
3.3. Example Applications
In this section, three example applications, which include D3.js during the application development, will be examined.
3.3.1. Example Study 1: Flight Analysis
Example study NormSTAD Flight Analysis: Visualizing Air Traffic Patterns over the United States [Ayhan et al., 2014] addresses the issue of the optimization of the air traffic. Ayhan et al. claim that air carries would benefit if air traffic is visualized. Flights can be scheduled more efficiently with the help of the interactive visualization of the air traffic patterns. Not only fuel consumption, landings and takeoffs can be optimized but also general picture of the air traffic can be monitored.
NormSTAD is an interactive web-based visualization application which empowers users to analyze flight data and make observations on time, distance, altitude and speed of the flights. This study intends to optimize air traffic by detecting trends and anomalies and performing strategic planning. NormSTAD uses data provided by the Aircraft Situation Display to Industry.
Normalized flight information is displayed on customizable line chart (see Figure 3.1). Users can customize the line chart according to airline, date and flight number. Figure 3.1 depicts the example application of NormSTAD. Outlier in red can be spotted once bottom graph is used to select the range. It can be observed from Figure 3.1 that, although red flight is closer to the destination compared to the other flights, it doesn`t
28
arrive earlier [Ayhan et al., 2014]. Further analysis on the outlier flight can be performed to find the causes of the problem and this flight could be optimized.
   Figure 3.1 NormSTAD`s Line Chart displaying the “Distance vs. Actual flight time”. [Ayhan et al., 2014]
Researchers preferred to implement web-based interface of NormSTAD with D3.js. First of all, data is imported and normalized values are stored locally using JavaScript. After importing the data, D3.js is employed to bind the imported data with the selected airlines and flights. This way visualization could be updated interactively when the selections are changed. Line charts are created with D3.js. In addition to D3.js, Google Maps API was used to provide map view of the flight routes.
Although researchers have not explicitly stated that they have conveyed the Information Seeking Mantra [Shneiderman, 1996] and Reference Model [Card et al., 1999] during the development of the visualization software, NormSTAD can be explored further regarding these frameworks.
Figure 3.2 demonstrates the application of Reference Model for the NormSTAD. Raw data is the data retrieved from the Aircraft Situation Display to Industry. After retrieving the data, researchers transformed the data into a tabular form. Line chart and map chart constitute to the major visual structures which are conceived with SVG. After applying view transformations, views are created with HTML and formatted with CSS.
29
   Figure 3.2 Reference model applied to the NormSTAD flight analysis tool.
Web-based information visualization tool NormSTAD can be examined further by taking Shneiderman`s Mantra into account as follows:
• Overview: NormSTAD provides an overview with the possible airlines and flights. General idea about the dataset can be derived from the overview.
• Zoom and Filter: Although zooming isn`t encouraged with NormSTAD, filtering is being extensively used. Users can limit the results appearing on the line chart by using the filters panel. Users can filter airlines, flight numbers and flight dates.
• Details on demand: Researchers have created separate panel for supporting details on demand. When user requires, additional information pertaining to selected flight(s) is being displayed in a tabular form.
NormSTAD is a proposed solution for optimizing the flight landings and takeoffs. Air traffic controllers can investigate flights interactively to discover flight patterns (holding, slow down etc.). Researchers applied JavaScript and D3.js to the development of the web-based visualization application. D3.js facilitated interactive line charts. Researchers separated data models, visual models, views and interaction as in the Reference Model. NormSTAD can be regarded as a successful visualization application since it conveys the principles of the Information-Seeking Mantra and allows interaction.
30
3.3.2. Example Study 2: Real-time Analysis of Twitter
A study called RApID: A System for Real-time Analysis of Information Diffusion in Twitter [Taxidou and Fischer, 2014] examines how Twitter users influence each other in real-time basis. Taxidou and Fischer developed a system called RApID to visualize the information being propagated from user to user.
Twitter maintains a social graph of followers. Information diffusion from user to user is available explicitly with the concept of retweeting. Tracing, understanding and predicting how information spreads in social media can be a challenging task. User roles like opinion leaders or spammers could be identified with the help of the understanding of the information diffusion.
RApID includes a model of information cascades where nodes correspond to the users and edges represent “who has influenced by whom”. Real-time analysis of the information cascades is not a trivial task as visualization of the large and fast evolving graph is required. Updates are needed to be integrated into cascades instantaneously because of the dynamic nature of the cascading graphs. Therefore, researchers developed their own web-based visualization techniques to overcome the presentation of dynamic, large-scale cascade graphs. They have established new collapsing techniques to deal with the complex structure of the information cascades.
Taxidou and Fischer [2014] chose to visualize large and fast-evolving graph data using D3.js. User interface of RApID allows interactive visualizations of evolving cascades to provide information about how users influence each other (See Figure 3.3).
31
   Figure 3.3 Visualization of information cascade
As Figure 3.3 depicts, visualization of information cascade includes nodes and edges. There are four complementary aspects of the illustrated visualization [Taxidou and Fischer, 2014]:
1. Dynamic representation of the evolving information cascades
2. Real-time distributions (e.g., geographical spread)
3. Relevant user information when pointed with the mouse (e.g., number of
followers)
4. Information about the original tweet (e.g., number of retweets).
In order to fulfill the mentioned complementary aspects, Taxidou and Fischer [2014] developed their own web-based visualization technique of combining/collapsing leaf nodes with their parent nodes. D3.js is selected to visualize information cascades. With the help of D3.js, researchers could achieve dynamic visualizations which evolve over time with the dynamic collapsing algorithms.
Researchers kept visual Information-Seeking Mantra in mind during the design phase of RApID as they developed a system with overview first, zoom and filter, then details on demand principle. Details of how researchers complied with the mantra are described below:
32
• Overview: Detailed overview of the dataset is provided. It is possible to see the pins on the world map as tweets are being gathered. Information cascade can be played to observe the influences of users on each other.
• Zoom and Filter: Zooming is made available as users can select collapse threshold, link distance and sub-tree size. Additionally, users can filter the information cascades according to the collapsing techniques (e.g., full cascade, two level). With the help of these options, users can zoom and limit the displayed information through filtering.
• Details on demand: When the user points the mouse over a certain node, relevant user information is being displayed. User information includes, for instance, name, tweet and original creator. Users can retrieve detailed information with the interactive selection while still observing the overview.
RApID demonstrates how presentation problem of dynamic cascading graphs can be solved. Valuable insights can be gained with RApID by interacting with the dynamic and evolving cascades. RApID has proven that JavaScript and D3.js can visualize large, fast and evolving data sets with the application of Information Seeking Mantra [Shneiderman, 1996].
3.3.3. Example Study 3: Stock market visualization
Example study Stock Lamp: An Engagement-Versatile Visualization Design [Tanahashi and Ma, 2015] investigates the way users interact with the real-time visualizations. Although most of the design methodologies in the field of information visualization assume that users will be fully engaged in the visual exploration of the information, real-time applications do not always receive active interaction from the users. Therefore, Tanahashi and Ma [2015] have introduced new design concept called engagement-versatile design to serve the users with various engagement styles.
Tanahashi and Ma [2015] applied engagement-versatile design concept to the system called Stock Lamp to help users keep track of the stock market in real-time. First of all, they have identified different modes of engagement and derived design implications. Later on, they have applied these modes to the Stock Lamp`s visualization design. Tanahashi and Ma [2015] have defined three types of engagement modes as follows:
33
• Periphery - Passive mode: This mode suggests that visualization isn`t in the user`s sight directly and the user doesn`t pay much attention as s/he doesn`t actively interact with the system. This kind of user prefers to glance at the data quickly to check the updates.
• Focus - Passive mode: Users do not engage in data interaction in this mode, although visualization is in the user`s direct sight and user focuses on the system. Users prefer gazing at the visualization and passively interact to gain information.
• Focus - Active mode: In this type of engagement mode, users give full attention to the visualization by interacting to explore the data. Users who prefer this type of mode intends to explore data extensively for in-depth data analysis purposes.
Stock Lamp consists of two views, lamp-view and info-view, to help part-time investors in different situations regarding their attention and participation. Either one of the views is displayed based on the user`s engagement mode. In lamp-view users can view abstract information about the stocks without seeing detailed information whereas in info-view (See Figure 3.4) detailed information with descriptions is provided to the users. [Tanahashi and Ma, 2015]
   Figure 3.4 A screenshot of info-view [Tanahashi and Ma, 2015].
34
Researchers have used D3.js to implement the visualizations of info-view (Figure 3.4). They stated that simplicity of D3.js was the main reason for using it while developing the visualizations. With the help of D3.js, user interactions could be included and pilot users could test the interactions based on mouse and touchscreen.
Although developers of Stock Lamp mostly focused on providing different user engagement modes, it is still possible to observe the principles of the visual Information-Seeking Mantra [Shneiderman, 1996] in this manner:
• Overview: General information about the stocks is provided in the lamp-view. Users can understand the general picture of the stocks with the lamp-view and observe the updates on the stocks.
• Zoom and Filter: Users can zoom into the specific stock with the info-view. Detailed information about the selected stock is given in the overview of the info-view. It is possible to see history of the stock and related stocks. However, filtering isn`t supported in Stock Lamp.
• Details on demand: After selecting the specific stock in info-view, users can demand additional details such as news or Twitter feed about the stock.
Researchers of Stock Lamp present a new design concept, engagement-versatile design, to target the users with different engagement styles. According to Tanahashi and Ma, users do not always pay attention or actively interact with the visualizations in real life. First, Tanahashi and Ma present taxonomy of user engagement with traditional visualizations and determine the unique characteristics of each engagement style. Later, they use this knowledge in Stock Lamp, where investors with different engagement styles can keep track of the stock market in real time. D3.js is utilized for generating info-view as D3.js has broad built-in library for managing user interactions with visual elements. [Tanahashi and Ma, 2015] Visualizations comply with the visual Information-Seeking Mantra [Shneiderman, 1996] while serving the users with different attention and participation modes.
35
4. Visualization of the Publication Data
Although there are various JavaScript toolkits available for information visualization, it is still common to encounter traditional visualizations in web pages. In this thesis, visualization of the web-based publication data is developed with D3.js library in order to demonstrate the strength of JavaScript in web browsers.
4.1. User Requirements
Currently, publication data of the Tampere Unit for Computer-Human Interaction (TAUCHI) is listed as shown in Figure 4.1. Users can select from the publication years to overview the articles published during the selected year. Although users can reach to the articles directly with the provided URL links, it is not possible to gain insight or make comparisons about the overall publication information with this kind of visualization approach.
Figure 4.1 Current way of visualizing the publication data of TAUCHI Research Center.

36
Major user requirements when observing the visualization of the TAUCHI’s publication data can be listed as follows:
1. Users should be able to observe the number of publications made every year and make comparisons between the publication years.
2. Users should be able to observe the list of the researchers who publish articles in TAUCHI research center.
3. Users should be able to observe which researcher makes the most publications.
4. Users should be able to make comparisons between the researchers.
5. Users should be able to observe the number of publications every researcher published during the year span (e.g., 2011 to 2015).
6. Users should be able to observe which topics were the most popular in overall publications.
7. Users should be able to observe which topics were the most popular during the selected year.
8. Users should be able to observe which researcher is specialized in which area.
Once the mentioned requirements are fulfilled, users can gain insight about the publication data and different aspects of the web-based publication data could be unveiled.
4.2. Related Work
Keshif is a D3.js based visual data browser, which aims at data exploration with data filtering and discovery [Yalcin, 2014]. Users of Keshif can easily filter and preview the results with a mouse over interaction. Yalcin [2014] claims that Keshif provides scalable visualizations even for big data collections. Keshif is able to load the data in a flexible manner. Structured data can be easily loaded from Google spread sheets and CSV files without requiring to hassle with coding and importing the data to JavaScript. However, for JSON objects users need to write their own importers.
37
Yalcin [2014] visualized University of Maryland’s publication data as in Figure 4.2 in order to demonstrate the usage of Keshif. He manually entered the publication data into the Google spread sheet format. The spread sheet form has six tabs: publications, authors, keywords (e.g., education), venues (e.g., CHI, INTERACT), venue types (e.g., article, book, and thesis) and author types (e.g., student, researcher).
Figure 4.2 An example of Keshif with Google spread sheet based publication data [Yalcin, 2014].
Shneiderman`s Information-Seeking Mantra [Shneiderman, 1996] is applied to Keshif to create a powerful visualization tool as follows:
• Overview: Users can overview publications, topics, coauthors, publication types and date. It is possible to grasp knowledge about the mostly mentioned topics or during which year the most publications are made with the overview.
• Zoom and Filter: Users can focus on one particular area or filter information according to their interests. Users can filter publications according to topic,

38
coauthor, publication type, venue and date. All of the charts in Keshif allow filtering. Once a filter is selected, visualization animates smoothly.
• Details on demand: Users can retrieve detailed information about publications. External links are provided to reach the full publication content.
• Relate: Users can relate publications with topics, coauthors, publication types and date by selecting an attribute. For instance, when the user selects the topic HCI, all of the related researchers can be discovered.
Keshif provides successful visualization techniques for visualizing the publication data with the help of the application of Shneiderman`s Information-Seeking Mantra [1996]. Developers can enter data manually to Google Spread Sheets and create visualizations with Keshif. Although Yalcin [2014] presents an effortless visualization platform for the developers, this kind of visualization requires developers to structure data manually. Keshif brings a powerful approach to create quick and easy visualizations. However, additional coding is required for big data sets since it is not feasible to enter data to the spreadsheet form manually.
4.3. Implementation
Implementation of the development of the interactive charts aims to satisfy the user requirements mentioned in Section 4.1. Implementation part of this thesis consists of three major segments: getting the data from web, parsing the data and creating the visualizations. In this section, issues encountered during the achievement of the mentioned segments will be explained.
4.3.1. External JavaScript Libraries
In order to visualize the TAUCHI`s publication data, different JavaScript frameworks are used for different purposes. Pure JavaScript is used to parse the data. Additional to the JavaScript, jQuery is exploited to retrieve the data from the previous years. Visualizations such as bar chart and researchers list are derived with D3.js. In addition
39
to JavaScript and D3.js, an additional D3 layout cloud library enabled the creation of word cloud visualization. This way, the principle of separation of concerns is enhanced.
Usage areas of jQuery, D3.js and D3 layout cloud in the implementation process are explained in detail below:
• jQuery.js: jQuery is a cross-platform JavaScript library especially designed to simplify client-side programming assignments. Implementation tasks such as HTML/DOM manipulation and event handling can be easily accomplished with jQuery. Many lines of JavaScript code are wrapped in a single line of code in jQuery, empowering simplicity for the developers.
During the implementation part, jQuery is utilized to make Ajax, i.e., asynchronous JS and XML, calls. Web applications can send and retrieve data from the server without interfering with the behaviour of the current page with Ajax calls. With the help of the jQuery, publication data from the previous years could be retrieved.
• D3.js: D3 is used to visualize the publication data. Bar chart and researcher list are created with D3.js library. Bar chart represents the number of publications published per year whereas researcher list demonstrates how many publications every researcher has done during the year span. These visualizations are explained in detail in Chapter 5.
• D3.layout.cloud.js: World cloud generator is an additional JavaScript library built on top of D3 developed by Davies [2014]. This library allows words to be replaced in a word cloud randomly. Original algorithm places all of the given words into the word cloud with a random font size assignment.
The main intention for creating a word cloud is to give an overall idea about the topics mentioned in the publications. Users should be able to observe which keywords are the most frequently mentioned in overall publications. Additionally, it would be beneficial to grasp an idea about the area of interest of the researchers. Because of these reasons, original algorithm has been changed.
40
Rather than displaying all of the words mentioned in the titles, it is preferred to keep a counter to determine the most frequently mentioned words. This way, users could be enlightened with the necessary information.
JSON-Object words includes information about every word mentioned in titles and counter for the frequency of the word. Font size is enhanced according to this counter. Creation of a word cloud object can be seen in Code Segment 1.
var wcloud = d3.layout.cloud().size([500, 300]) .words(words.map(function(d){
return {text: d.Item, size:
fontSize(+d.Count)};}))
.padding(4)
.rotate(function(){return ~~(Math.random()*2)* 90;}) .font("Impact")
.fontSize(function(d) { return d.size; }) .on("end", draw)
.start();
Code Segment 1. Creation of a word cloud object
4.3.2. DOM Manipulation
HTML DOM defines HTML objects and contains HTML element properties, methods and events to access and alter the elements. JavaScript enables DOM manipulation, allowing dynamic creation of HTML pages.
DOM manipulation with JavaScript provided various flexibilities during the implementation part. Dynamic manipulation of an HTML document is achieved in the following ways:
• Adding new HTML elements and attributes: Button with “visualize” label is added at the end of the publication list. Place of the button is selected with D3.js. Button is created with JS DOM manipulation.

41
• Creating new HTML events: After clicking onto the visualize button, new page with the charts needs to be created. Therefore, button on-click event is created.
• Altering the CSS styles in the page: Once the new page is created, CSS styles need to be determined for the visualization styles. Additional ChartStyle.css file is created with the style details. This CSS file is added to the newly created page with the JS’s document write function.
4.3.3. Data Abstractions
Data can be abstracted with JavaScript for data representation and manipulation. JavaScript Object Notation (JSON) makes the data easy to read for humans. After parsing the data received from server, it is saved into JSON objects in desired formats for the visualizations. Bar chart, researcher list and word cloud make use of the following JSON objects:
• Bar chart: Displays the total number of articles published every year to facilitate comparisons (See Figure 5.3). Bar chart is created with D3.js. JSON data linked to the chart is shown in Code Segment 2.
var barchartJSON =[{"Year":"2011","Total":42}, {"Year":"2012","Total":49}, {"Year":"2013","Total":60}, {"Year":"2014","Total":64}, {"Year":"2015","Total":5}]
Code Segment 2. Data used for creating a bar chart
• Researcher list: Displays all of the researcher names who published articles in the TAUCHI Research Center and number of publications every researcher published (See Figure 5.4 and 5.5). This list is created with D3.js. JSON data used for conceiving this visualization can be seen in Code Segment 3.

42
 var listJSON = [{"Researcher":"Raisamo Roope", "Published": [[2011,8], [2012,13], [2013,14], [2014,22], [2015,3]], "Total":60}, {"Researcher": "Turunen Markku", "Published": [[2011,5], [2012,12], [2013,17], [2014,10]], "Total":44},...]
Code Segment 3. Part of the dataset used for creating researcher list
• Word cloud: Displays the word cloud as in Figure 5.6 using the article titles. Word cloud is created with D3.layout.cloud.js. JSON data attached to the word cloud is structured as shown in Code Segment 4.
var wordcloudJSON = [{"Item":"pervasive","Count":2}, {"Item":"eye","Count":4}, {"Item":"tracking","Count":2}, {"Item":"mobile","Count":9}, {"Item":"eye-based","Count":1}, {"Item":"interaction","Count":9},...]
Code Segment 4. Part of the dataset used while creating word cloud
4.3.4. Interaction
The key feature of modern visualizations can be regarded as user interaction. Therefore, implementation process has been aiming at creating interactive charts. D3.js’ capacity of creating highly interactive visualizations simplified the development process of the charts.
Bar chart and researcher list are made interactive as follows:
• Bar chart: When a user hovers, selection is highlighted as shown in Figure 5.3. Word cloud is updated with the selected year’s most frequently mentioned words.
• Researcher list: When a user points the mouse over a researcher name, number of articles published by the selected researcher is displayed as in Figure 5.5. Word cloud is updated according to the topics selected researcher wrote his/her articles on.

43
Event handling mechanism makes the interaction possible. Common HTML events include, for instance, onchange, onclick, onmouseover and onmouseout events. Event handling attributes can be linked to HTML elements with JavaScript. Event handlers for the bar chart and researcher list include mouseover and mouseout events to enable interaction patterns described above.
44
5. Implementation Results: Description, Analysis and Synthesis
Implementation task was aiming to visualize TAUCHI Research Center`s publication data so that users could gain insight about different aspects of the publications with interactive data exploration. JavaScript, D3.js and external JS library were utilized to create interactive visualizations in a web browser.
5.1. Results
In order to achieve the user requirements mentioned in Section 4.1, it was necessary to place Visualize option button to the current publication web page of the TAUCHI. Once the user selects the visualize option, new tab with the visualizations opens. This newly created page can be seen in Figure 5.1. Implementation part results in three visualizations: bar chart, researcher list and word cloud.
Figure 5.1 Newly created web page where bar chart, researcher list and word cloud are displayed.

45
Bar chart, illustrated in Figure 5.2, fulfills the first user requirement so that users can see number of articles published every year and make comparisons between the years. Exact number of publications made every year is printed on top of the every bar. Bar chart is interactive in this way: when user points the mouse over one of the bars, selected year is highlighted as depicted in Figure 5.2.
Figure 5.2 Bar chart representing the number of articles published every year.
Figure 5.3 demonstrates part of the researcher list. All the researchers who publish articles in TAUCHI Research Center are listed in this chart. Therefore, height of this chart is determined dynamically depending on the names of the researchers in the publication data.
Researchers are listed according to the total number of publications, i.e., researcher with the most publications is listed at the top. The year of the publication is marked with a circle. Circle sizes allow comparisons so that it is possible to acquire knowledge about when a researcher made most of his/her publications. Bigger circle size implies that most of the publications are made during the relative year. Circle size doesn`t allow comparisons between the researchers, instead it enables making yearly comparisons for a selected researcher.

46
   Figure 5.3 List of the researchers who publish articles in TAUCHI. (A part of the researchers is displayed in this figure).
Researcher list is also interactive. Users can observe the number of articles a researcher has published by pointing the mouse over the name of the researcher. For instance, when user points the mouse over “Raisamo Roope”, the number of articles he has published for every year is displayed as in Figure 5.4.
47
   Figure 5.4 Interactive researcher list depicting the mouse over action.
Word cloud (Figure 5.5) includes information about the frequency of the words mentioned in all of the titles of the publications. External D3.layout.cloud.js library has handled the representation of the words in a random order. In other words, word cloud has different content or layout of the words every time it loads.
When the page loads, mostly mentioned words in the overall publications are displayed as in Figure 5.5. Frequency of a word in overall publication tittles is counted. Font size of the word in the word cloud is directly proportional with the frequency of the word.
48
   Figure 5.5 Word cloud generated when the page loads. By default it represents the most mentioned words in overall years in a random order.
Although word cloud itself is not interactive, bar chart and researcher list can dynamically modify the word cloud content. As it can be seen in Figure 5.6, users can modify word cloud content by moving mouse over one of the years. In this case, word cloud is filled with the most frequently mentioned words during the selected year. When the user does not select a year, word cloud is updated with the most frequently mentioned words in overall years (Figure 5.5).
Figure 5.6 Modifying word cloud content with bar chart.
Figure 5.7 is derived as an example to demonstrate how researcher list updates word cloud. After pointing mouse over Harri Siirtola, the supervisor of this thesis, word cloud is filled with his most frequently mentioned publication titles. The main aim for this interaction is to let user get a glimpse about in which area the selected researcher is specialized.

49
   Figure 5.7 Updated word cloud content when the user points mouse over researcher Harri Siirtola.
With the help of the newly created visualizations, users can learn about different aspects of the data. JavaScript and additional JavaScript libraries helped the creation of highly interactive and modern visualizations. Instead of displaying the publication data as a list, interactive charts are preferred to enhance users’ cognitive learning processes and unveil the hidden aspects of the publication data.
5.2. Discussion
Visualization of the TAUCHI`s publication data provides an overview to the users so that users can learn more about the data and view certain aspects of the data without losing the overall context. Users can familiarize themselves with the dataset and gain knowledge with the help of the exploratory data analysis.
Shneiderman`s Information Seeking Mantra [1996] makes exploratory data analysis possible with overview, zoom and filter and details on demand functionalities. Resulting visualizations conveys to the Shneiderman`s Mantra as explained in below:
• Overview: Overview serves as an entry point to allow further inquiries to take place on the dataset. The resulting visualization in Figure 5.1, not only displays the number of articles published every year, but also lists the researchers who publish articles in the TAUCHI Research Center.
50
Additionally, it is also possible to learn about the most frequently mentioned
words in overall articles.
• Zoom and Filter: Users might request to focus on their area of interest. Some
portion of the dataset is displayed to the user with filtering. Zooming and filtering enhances data exploration by revealing additional information about the dataset. The resulting visualizations allow zooming and filtering since availability of these options is crucial for interactive data visualizations. As in Figures 5.4 and 5.6, users can zoom and filter with the help of the bar chart and researcher list.
• Details on demand: Users can discover about the details of the selected researcher. Although Shneiderman [1996] suggests providing details with a pop-up window, modern visualisations don`t use pop-up menus as often. Therefore, details about the selected researcher are provided with mouse over action and word cloud is updated with the details of the researcher, as shown in Figures 5.4 and 5.5.
Card et al.`s [1999] reference model for information visualization has worked as a guideline during the implementation part. Figure 5.8 represents the application of reference model to the implementation of the visualization process. Data models, visual models and interactive controls are separated from each other for creating extensible and reusable software architecture. Publication data which is available on the TAUCHI`s publications web page corresponds to the raw data in Card et al.`s model. Metadata is stored in data tables, which consist of JSON objects in this implementation case. Visual structures are created and elements are drawn with SVG on the web page. Additionally, views are created with HTML elements and styled with CSS.
51
   Figure 5.8 Application of the Reference Model [Card et al., 1999] to the implementation of the visualization process.
Currently TAUCHI`s publication data is being displayed textually. Although it is possible to see, for example, every article published during the certain year and article authors, modern visualization techniques in web browsers could be applied to the publication data. With the help of JavaScript and additional visualization libraries, different aspects of information could be revealed.
Resulting visualizations provide new insight about the dataset and brings new opportunities to users to explore data. User requirements mentioned in Section 4.1 could be fulfilled by applying modern visualization tools for web browsers. Unlike the previous publication data representation, users can inspect the number of articles published every year. Detailed list of researchers are also provided so that users can further investigate these researchers` publications and their area of interests. It is also possible to acquire knowledge about the most frequently mentioned words for selected year, researcher or in overall. Users can overview, zoom and filter and retrieve details on demand while exploring the publication data.
52
6. Conclusions
In the course of this thesis, issues for visualizing web-based applications using JavaScript have been addressed. TAUCHI Research Center`s publication data has visualized in order to demonstrate JavaScript`s strength for web-based visualizations.
Shneiderman [1996] proposed Information Visualization Mantra to achieve powerful visualizations with overview first, zoom and filter, then details on demand principle. His mantra has been frequently used and applied to the modern web-based visualization examples. Therefore, Shneiderman`s Mantra has administered to the visualization of the TAUCHI`s publication data.
As interaction between human and computer is the heart of modern visualizations [Spence, 2007], interactivity was of utmost importance while visualizing the publication data. Developed charts, namely bar chart and researcher list, enhance data exploration by allowing user interaction. Unlike the current way of displaying the publication data, users can observe different aspects of the data interactively. For instance, users can select a researcher and grasp an idea about the selected researcher`s area of interests interactively.
D3.js has been chosen as the framework while developing the interactive charts. Main reasons for selecting D3.js as the visualization framework include direct DOM manipulation, data abstractions and interaction. Visualization tasks were accomplished by employing the DOM interface directly. Additionally, data abstractions with JSON objects not only led to having reusable charts, but also enabled the application of the Reference Model [Card et al., 1999].
After introducing the main principles and concepts of information visualization, this thesis explains web-based tools for visualizing the data. Secondly, it introduces D3.js framework for web-based applications and demonstrates the example cases where D3.js is utilized for visualizing web-based datasets. Finally, it administers the mentioned characteristics of D3.js to the application of web-based data visualization. These results can be used as a guideline when selecting the appropriate tools and theories for visualizing information on World Wide Web.
53
References
[Ackoff, 1989] Ackoff, Russell. "From Data to Wisdom." Journal of Applied Systems Analysis 16 (1989): 3-9.
[Aufreiter, 2011] Aufreiter, Michael. “Web-based Information Visualization”. Thesis. Linz University, 2011.
[Ashkenas et al., 2012] Ashkenas, Jeremy, Matthew Bloch, Shan Carter, and Amanda Cox. "The Facebook Offering: How It Compares." The New York Times. 17 May 2012. <http://www.nytimes.com/interactive/2012/05/17/business/dealbook/how-the- facebook-offering-compares.html>.
[AT&T, 2014] "Graphviz - Graph Visualization Software." AT&T Labs Research, 2014. <http://www.graphviz.org/>.
[Ayhan et al., 2014] Samet Ayhan, Brendan Fruin, Fan Yang, and Michael O. Ball. “NormSTAD flight analysis: visualizing air traffic patterns over the United States.”Proceedings of the 7th ACM SIGSPATIAL International Workshop on Computational Transportation Science – IWCTS `14 (2014): 1-10. ACM.
[Bostock, 2011] Bostock, Mike. "Mbostock/d3." GitHub. 29 May 2011. <https://github.com/mbostock/d3/wiki/Selections>.
[Bostock and Heer, 2009] Bostock Michael and Jeffrey Heer. “Protovis: A Graphical Toolkit for Visualization.” IEEE Transactions on Visualization and Computer Graphics 15, (2009): 1121-1128.
[Bostock et al., 2011] Bostock, Michael, Vadim Ogievetsky, and Jeffrey Heer. "D3: Data-driven Documents." IEEE Transactions on Visualization and Computer Graphics 17 (2011): 2301-309.
54
[Card et al., 1999] Card, Stuart K., Jock D. Mackinlay, and Ben Shneiderman. Readings in Information Visualization: Using Vision to Think. San Francisco, CA: Morgan Kaufmann, 1999. 17-23.
[Chen et al., 2009] Chen, Min, David Ebert, Hans Hagen, Robert S. Laramee, Robert Van Liere, Kwan-Liu Ma, William Ribarsky, Gerik Scheuermann, and Deborah Silver. "Data, Information, and Knowledge in Visualization." IEEE Computer Graphics and Applications 29.1 (2009): 12-19.
[Chi and Riedl, 1998] Chi, Ed H., and John T. Riedl. "An Operator Interaction Framework for Visualization Systems." INFOVIS '98 Proceedings of the 1998 IEEE Symposium on Information Visualization. Research Triangle Park, North Carolina. 1998. 63-70.
[Chi, 2000] Chi, Ed H. "A Taxonomy of Visualization Techniques Using the Data State Reference Model." InfoVis 2000 Proceedings of the 2000 IEEE Symposium on Information Visualization. Salt Lake City, UT. 2000. 69-75.
[Crockford, 2008] Crockford, Douglas. JavaScript: The Good Parts. Beijing: O'Reilly, 2008.
[Davies, 2014] Davies, Jason. "Jasondavies/d3-cloud." GitHub. 10 Jan. 2014. Web. 28 Apr. 2015. <https://github.com/jasondavies/d3-cloud>.
[Degani, 2013] Degani, Asaf. "A Tale of Two Maps: Analysis of the London Underground "Diagram"" Ergonomics in Design: The Quarterly of Human Factors Applications 21.3 (2013): 7-16.
[Dmitry, 2014] Baranovskiy, Dmitry. "Raphaël-JavaScript Library." Raphaël- JavaScript Library. MIT, 2008. <http://www.raphaeljs.com/>.
[Few, 2007] Few, Stephen. "Save the Pies for Dessert." Perceptual Edge Visual Business Intelligence Newsletter (Aug. 2007).
55
[Flanagan, 2011] Flanagan, David.JavaScript: The Definitive Guide. 6th ed. Sebastopol, CA: O'Reilly, 2011.
[Fry and Reas, 2014] Fry, Ben, and Casey Reas. "Processing.js." Processing.js. 25 Mar. 2014. Web. 13 Nov. 2014. <http://processingjs.org/>.
[Gentleman and Ihaka, 2000] Gentleman, Robert, and Ross Ihaka. "Lexical Scope and Statistical Computing." Journal of Computational and Graphical Statistics 9,3 (2000): 491-508.
[Johnson, 2006] Johnson, Steven.The Ghost Map: The Story of London's Most Terrifying Epidemic--and How It Changed Science, Cities, and the Modern World. New York: Riverhead, 2006.
[Keller et al., 2006] Keller, Tanja, Peter Gerjets, Katharina Scheiter, and Bärbel Garsoffky. "Information Visualizations for Knowledge Acquisition: The Impact of Dimensionality and Color Coding." Computers in Human Behavior 22.1 (2006): 43-65.
[Li, 2012] Li, Zheng. "InfoViz - Open Source JavaScript Information Visualization." InfoViz. MIT, 2012. Web. 19 Dec. 2014. <http://infoviz.org/>.
[Playfair, 1801] William Playfair, The Statistical Breviary; Shewing the Resources of Every State and Kingdom in Europe, 1801.
[Richards et al., 2010] Richards, Gregor, Sylvain Lebresne, Brian Burg, and Jan Vitek. "An Analysis of the Dynamic Behavior of JavaScript Programs." Proceedings of the 2010 ACM SIGPLAN Conference on Programming Language Design and Implementation. New York: ACM, 2010. 1-12.
[Robinson et al., 2002] Robison, Wade, Roger Boisjoly, David Hoeker, and Stefan Young. "Representation and Misrepresentation: Tufte and the Morton Thiokol Engineers on the Challenger." Science and Engineering Ethics 8 (2002): 59-81.
[Rosling, 2010] Rosling, Hans. "The Joy of Stats." BBC News. BBC, 2010.
56
[Shneiderman, 1996] Shneiderman, Ben. "The Eyes Have It: A Task by Data Type Taxonomy for Information Visualizations." Proceedings of the IEEE Symposium on Visual Languages. Washington. IEEE Computer Society, 1996. 336-43.
[Siirtola, 2013] Siirtola, Harri. "Information Visualization." Exercise 3. UTA, Tampere. 2013. Lecture.
[SimilarTech, 2014] JavaScript Technologies Market Share and Web Usage Statistics. SimilarTech, Web. 06 Oct. 2014. <https://www.similartech.com/categories/javascript>.
[Spence, 2007] Spence, Robert. Information Visualization: Design for Interaction, 2nd ed.: Prentice-Hall, 2007. 136.
[Spence, 2014] Spence, Robert. Information Visualization: An Introduction. 3rd ed. London: Springer, 2014. 29.
[Tanahashi and Ma, 2015] Tanahashi, Yuzuru, and Kwan-Liu Ma. "Stock Lamp: An Engagement-Versatile Visualization Design." CHI '15 Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. 595-604. ACM.
[Taxidou and Fischer, 2014] Taxidou, Io, and Peter M. Fischer. "RApID: A System for Real-time Analysis of Information Diffusion in Twitter." CIKM '14 Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management. New York: ACM, 2014. 2060-2062.
[Tufte, 2001] Tufte, Edward R. The Visual Display of Quantitative Information. 2nd ed. Graphics, 2001.
[Tukey, 1962] Tukey, John W. "The Future of Data Analysis." The Annals of Mathematical Statistics 33.1 (1962): 1-67.
[Tukey, 1977] Tukey, John W. Exploratory Data Analysis. Addison-Wesley, 1977.
57
[Yalcin, 2014] Yalcin, Adil. "Keshif." GitHub. University of Maryland, 28 Apr. 2014. Web. 29 Apr. 2015. <https://github.com/adilyalcin/Keshif>.
[Yi et al., 2008] Yi, Ji Soo, Youn-ah Kang, John T. Stasko, and Julie A. Jacko. "Understanding and Characterizing Insights: How Do People Gain Insights Using Information Visualization?" BELIV '08 Proceedings of the 2008 Workshop on Beyond Time and Errors: Novel Evaluation Methods for Information Visualization. New York: ACM, 2008.
[W3C, 2011] "Scalable Vector Graphics (SVG) 1.1 (Second Edition)." W3C Recommendation. W3C, 16 Aug. 2011. Web. 11 Nov. 2014. <http://www.w3.org/TR/SVG/>.
[Ware, 2004] Ware, Colin. "A Model of Perceptual Processing." Information Visualization: Perception for Design. 2nd ed. San Francisco: Morgan Kaufmann, 2004. 20-23.
[Wilkinson, 2005] Wilkinson, Leland. "Graphics Versus Charts." The Grammar of Graphics. 2nd ed. New York: Springer, 2005. 2. Print.
[Williamson and Shneiderman, 1992] Williamson, Christopher, and Ben Shneiderman. "The Dynamic HomeFinder: Evaluating Dynamic Queries in a Real-estate Information Exploration System." SIGIR '92 Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. New York: ACM, 1992. 338-46. Web.
FixFix: Fixing the Fixations
Goran Topic ́∗
Akito Yamaya† Akiko Aizawa‡ Pascual Mart ́ınez-Go ́mez§
 Abstract
FixFix is a web-based tool for editing reading gaze fixation datasets. The purpose is to provide gaze researchers focusing on reading an easy-to-use interface that will facilitate manual interpretation, but even more so to create gold standard datasets for machine learning and data mining. It allows the users to identify fixations, then move them either singly or in groups, in order to correct both variable and systematic gaze sampling errors.
Keywords: eyetracking,gazedata,editor,reading
Concepts: •Human-centered computing → Graphical user in-
terfaces; Visualization systems and tools;
1 Introduction
There is much we can learn from gaze data as captured by an eye tracker. For example, psycholinguistics exploits the data about eye movements to infer reading patterns, which in turn informs about cognitive processes that accompany the reading activity. Often, re- searchers will want to apply machine learning methods to the gaze sequence data in order to discover these patterns. Most of these methods rely on the capability of collecting gaze statistics on areas of interest, such as fixation durations or counts on word bounding boxes or salient objects in images.
However, eye data collected with modern eye-trackers still suffer from variable and systematic errors (see [Chapanis 1951] for a char- acterization of errors). The equipment might not have sufficient precision to pinpoint the exact point where the reader is looking, or the readers’ head movement might distort the data. Furthermore, while heat maps might use the gaze samples directly, some appli- cations will be much more interested in eye fixations. And finally, it is impossible for a reader to create a “perfect” reading dataset, one where one’s eyes do not wander, where the gaze follows each text line perfectly and in a straight line, one never backtracks, is never distracted, never moves one’s head, and whose each fixation cleanly resolves to a word on the page. In short, whereas many ma- chine learning algorithms require a clean gold standard dataset, no unedited gaze dataset is fit to be a gold standard.
This was the motivation behind FixFix, a fixation editor (Fig. 1).
∗e-mail: goran topic@nii.ac.jp †e-mail: yamaya@nii.ac.jp ‡e-mail: aizawa@nii.ac.jp §e-mail: pascual@nii.ac.jp
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third- party components of this work must be honored. For all other uses, contact the owner/author(s). ⃝c 2016 Copyright held by the owner/author(s). ETRA ’16, March 14-17, 2016, Charleston, SC, USA
ISBN: 978-1-4503-4125-7/16/03
DOI: http://dx.doi.org/10.1145/2857491.2884060
Figure 1: FixFix screenshot displaying the file browser on the up- per left; adjustable parameters for fixation identification on the left; temporal window and selection sliders on the bottom; word bound- ing boxes and fixations for left (red) and right (blue) eyes.
2 Description
FixFix is a web application that allows the user to load a raw gaze dataset, find fixations (currently using the I-DT method [Salvucci and Goldberg 2000]), then allow the user to easily change the po- sition of single fixations or groups of fixations. Finally, the edited fixation data can be downloaded.
FixFix features a small file browser window that shows documents uploaded to the server. Upload is done simply by dragging and dropping the gaze dataset documents directly into the file browser window. Several data formats are accepted: a TSV format with either raw gaze data or fixation data, a TSV format with text re- gions and their bounding box coordinates, or an XML format that combines both. The TSV format in particular is easy to convert into, allowing existing corpora (such as Dundee Corpus [Kennedy 2003]) to be easily imported into FixFix.
When a file with bounding box data is chosen, the text regions are rendered on the larger part of FixFix window, showing where the text regions are in the document. If a file containing gaze data (raw or processed) is selected, the gaze data will be rendered. The two views are superimposed on each other.
  319
Correcting variable errors: fixation identification
The raw gaze data just shows up as a cloud of dots, one per valid sample. If the dataset has data for both left and right eyes, they will show up in red and blue. An average position between the two will be shown in black. To reduce the visual clutter, each of these can be individually toggled on or off. The I-DT method is available to perform fixation identification, where the dispersion and fixa- tion duration thresholds can be manually adjusted for each specific dataset.
Correcting systematic errors
If the data is processed, or if the loaded file contains the fixation data, then fixations are displayed instead. Fixations are, as with raw data, shown with red, blue or black dots; they are also con- nected by lines, which shows the temporal order of the fixations. The sequence of fixations is roughly broken up into lines, using a very na ̈ıve return sweep detection algorithm, to make editing more intuitive. Each dot is also either “frozen” or not, giving it an icy outline: the dots at the start and at the end of a line start out as frozen.
Linear systematic errors can be correct using translation and scaling operations. When carrying out a translation operation, the user is expected to mark the start and the end of a selection, right-click on a fixation and select “Move”, and then move one of the fixations to its desired new location. Such action will result in all fixations in the selection to move by the same constant shift. To perform a scaling operation, the user only needs to select a fixation as a scaling point of origin and drag any fixation in the selection to a new location. This action will move all fixations in the selection proportionally to their distance with respect to the origin.
The temporal window of selected fixations can be moved forward and backward with the arrow keys or by dragging a slider at the bot- tom of the UI. The temporal resolution of this movement can also be adjusted by resizing the slider. This can let the user clearly see the temporal progression of the samples. Another slider controls the temporal slice of data that is displayed, in case the user wants to restrict themselves to editing only a certain part (for example, if there is multiple rereading of the same text in the dataset) or in case the sheer amount of data is impacting the performance of the application.
Non-linear systematic errors can be corrected by dragging fixations individually or in small sequences. Dragging a red or blue dot will only change the left or the right eye data; dragging a black dot will affect both equally. There are two editing modes: Single mode and Multi mode. In Single mode, only the dragged fixation is affected. In Multi mode, all consecutive non-frozen fixations on both left and right of the dragged fixation will be affected; then the dragged fixa- tion itself will become frozen. This allows the user to quickly adjust whole lines by correcting the biggest “offender” first, then make smaller corrections with the already adjusted fixations securely im- mobile. Of course, a fixation can also be manually frozen or un- frozen.
Other operational considerations
FixFix features an undo functionality, allowing any editing action to be reverted.
The user can also select a gaze dataset as a reference — typically the original, unedited copy of the current dataset. The reference should have the fixations at same timestamps as the current dataset (i.e. one would load the original dataset as it was before it was edited as the reference). The reference is then shown as an overlay on the
currently edited document, showing which points were moved, and where.
Finally, the user can download the edited fixation data. The format is TSV, one of the formats that can be uploaded to FixFix, and also easy to import into any further data analysis tool.
3 Use cases
In [Yamaya et al. 2015], the authors utilised FixFix for creating a gold standard dataset. They proposed an automatic fixation remap- ping method that associates sequential fixations with a text line to reduce the vertical error in detected gaze location. In order to as- sess the accuracy, they compared the word offset position between the automatically remapped fixations and the fixations of the gold dataset.
4 Conclusion and future work
We designed FixFix with the hope to allow eye-tracking researchers to better visualise the reading data, and decrease their effort when manually mapping fixations to their areas of interest. This tool fea- tures an implementation of the I-DT method for fixation detection with adjustable parameters, and several operations on single fix- ations and fixation sequences to perform translation, scaling and general non-linear error corrections.
In future work, we plan to integrate automatic or semi-automatic mechanisms to map fixations to areas of interest, and automatic event recognizers such as regressions or return sweeps.
FixFix homepage can be found at https://kmcs.nii.ac.jp/fixfix/, and the source is available from GitHub: https://github.com/ KMCS- NII/fixfix.
Acknowledgements
This work was supported by JSPS KAKENHI Grant Number 24300062, and FY2015 NII collaborative research grant.
References
CHAPANIS, A. 1951. Theory and methods for analyzing errors in man-machine systems. Annals of the New York Academy of Sciences 51, 7, 1179–1203.
HORNOF, A., AND HALVERSON, T. 2002. Cleaning up systematic error in eye-tracking data by using required fix- ation locations. Behavior Research Methods 34, 592–604. 10.3758/BF03195487.
KENNEDY, A., 2003. The Dundee corpus. [CD-ROM]. School of Psychology, The University of Dundee.
SALVUCCI, D. D., AND GOLDBERG, J. H. 2000. Identifying fix- ations and saccades in eye-tracking protocols. In Proceedings of the 2000 Symposium on Eye Tracking Research & Applications, ACM, New York, NY, USA, ETRA ’00, 71–78.
YAMAYA, A., TOPIC ́ , G., MART ́INEZ-GO ́ MEZ, P., AND AIZAWA, A. 2015. Dynamic-programming–based method for fixation-to- word mapping. In Intelligent Decision Technologies, Springer, 649–659.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TLT.2016.2626292, IEEE Transactions on Learning Technologies
IEEE TRANSACTIONS ON JOURNAL NAME, MANUSCRIPT ID 1
A novel web-based approach for visualization
and inspection of reading difficulties on
university students
Carolina Mejia, Beatriz Florian, Ravi Vatrapu, Susan Bull, Sergio Gomez, and Ramon Fabregat
Abstract—Existing tools aim to detect university students with early diagnosis of dyslexia or reading difficulties, but there are not developed tools that let those students better understand some aspects regarding their difficulties. In this paper, a dashboard for visualizing and inspecting early detected reading difficulties and their characteristics, called PADA (acronym for the Spanish name Panel de Analíticas de Aprendizaje de Dislexia en Adultos), is presented. PADA is a web-based tool designed to facilitate the creation of descriptive visualizations required for a better understanding of students about their learner model. Through information visualization techniques, PADA shows students the knowledge in their learner models in order to help them to increase their awareness and to support reflection and self-regulation about their difficulties in reading. PADA provides different learning analytics on reading performance of students, so that they can self-identify their strengths and weaknesses and self-regulate their learning. This paper describes examples that cover a variety of visualizations (bar-charts, line-charts, and pie-charts) to show user model fragments as personal details, reading profiles, learning styles, and cognitive traits of the students. We tested PADA with 26 students (aged 21–53 years) of different academic programs and levels, dyslexic and non-dyslexic. The results show that PADA can assist students in creating awareness, and help them to understand their difficulties associated with the reading tasks, as well as facilitate reflection and self-regulation in the learning process. Implications for the design of learning analytics are discussed and directions for future work are outlined.
Index Terms— Dyslexia, university students, reading difficulties, open learner modeling, learning analytics solutions. ——————————   ——————————
1 INTRODUCTION
DYSLEXIA is a common learning disability which may persist into adulthood [1], [2]. University students with this type of disability may experience difficulties dur- ing their academic careers, since reading is the basis of most, if not all, formal educational processes and has sig- nificant importance in many learning domains. Although, dyslexia in the university context has not been studied in depth, a number of research studies have borne out that, in spite of having a learning disability, dyslexic students could develop compensatory learning strategies to help them to succeed in their studies [3], [4].
In Spain, university students are not questioned about their learning disabilities when they start their academic careers; therefore, the number of specific cases of students with dyslexia is unknown. For this reason, our research work begun focused on detecting Spanish-speaking uni- versity students who have a previous diagnosis of dyslexia or are affected with some reading difficulties (i.e. dyslexia symptoms). Then, we detect the learning style that these
          ————————————————
  C. Mejia is with the FEAV, Universidad EAN, Bogotá, Colombia, CO 110231. E-mail: cmejiaco@ ean.edu.co.
  B. Florian-Gaviria is with the EISC, Universidad del Valle, Cali, Colombia, CO 76001000. E-mail: beatriz.florian@correounivalle.edu.co.
  R. Vatrapu is with the cbsBDA, Copenhagen Business School, Denmark & Westerdals Oslo ACT, Norway. E-mail: rv.itm@cbs.dk.
  S. Bull is with the Electronic, Electrical and Computer Engineering, Bir- mingham University, UK. E-mail: s.bull@bham.ac.uk.
  S. Gomez is with the UMB Virtual, Universidad Manuela Beltran, Cajicá, Colombia, CO 250247. E-mail: sergio.gomez@umb.edu.co.
  R. Fabregat is with the IIA, University of Girona, Girona, Spain, CO 17071. E-mail: ramon.fabregat@udg.edu.
students have acquired to deal with their difficulties, i.e. their learning preferences. Finally, it became necessary to study the cognitive processes associated with reading that can be altered in these students, with the aim of determin- ing their cognitive deficits, and identifying whether or not the student has dyslexia [5], [6].
Learning disabilities of students can be studied by de- fining learner models based on demographics, characteris- tics, preferences and cognitive traits associated with the learning disability of the student. Thus, understanding the types and severity of dyslexia and reading difficulties can be achieved. In previous studies, the definition of the learner model was made and tools were developed to cap- ture personal details of the students, detect their reading difficulties, detect their learning style, and assess their cog- nitive processes [5]–[7].
As current step in our research work, it is essential to support and assist these students in overcoming their dif- ficulties during their higher education and beyond. Studies on university students with learning disabilities have re- vealed that (1) awareness of their weaknesses, and then some of their strengths, as well as (2) ability to make deci- sions and self-regulate their learning, are powerful predic- tors for their academic success [8]–[10]. Therefore, first of all, it is necessary to create awareness in students of their reading difficulties, learning styles and cognitive deficits. This awareness facilitates learning reflection by encourag- ing students to make decisions and self-regulate their learning. Secondly it is necessary to provide specialized
1939-1382 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more
xxxx-xxxx/0x/$xx.00 © 200x IEEE Published by the IEEE Computer Society
information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TLT.2016.2626292, IEEE Transactions on Learning Technologies
2
feedback and advice to support such self-regulation pro- cesses of the students.
Consequently, this study is directed towards the visual- ization and inspection of information from the learner model to create awareness in the affected students. To do so, opening the learner model to the students is proposed. Other research studies taking this approach have shown that it is a successful strategy to promote awareness-rais- ing, which leads to reflection on learning, and its facilitate self-regulation, thereby the learning process is supported [11], [12].
In this sense, one of the emerging visualization tech- niques and potential impact on enhanced learning are learning analytics [13]–[15]. Basically, these analytics are graphical representations of aggregated data about stu- dents, for purposes of understanding their activity and performance in a fairly intuitive format, thus achieving the optimization of learning.
According to aforementioned issues, in this paper we present a Dashboard of Learning Analytics of Dyslexia in Adults (namely PADA, acronym for the Spanish name Panel de Analíticas de Aprendizaje de Dislexia en Adultos). PADA is a web-based tool designed to help with the un- derstanding and inspecting of the student model, promote awareness and facilitate reflection on reading difficulties. It has an architecture based on the Activity-based Learner- models framework proposed in [16] to have a flexible and extendable dashboard to open more fragments of the learner model if they are later required.
The concern in our research aims to collect answers re- garding whether students would find PADA useful to de- tect their reading difficulties and their implications for learning and cognition. An important contribution of this paper presented in a case study, examined the usefulness of PADA in terms of assisting affected students to achieve understanding, inspection, awareness, reflection and en- courage self-regulation in their learning process, particu- larly in the reading process.
This paper is structured as follows. In the second sec- tion, we provide a short review of the work related to sup- porting adult students with dyslexia, the importance of opening the learner model to these students, and the ap- proach to the emerging area of learning analytics. The third section summarizes the research proposal in term of a re- view of related works. The fourth describes the tools of previous research works used to collect data to feed the student model. The fifth section describes PADA and its architecture. Also, it presents how learning analytics were built. In the sixth section questions that aim to achieve re- search objectives which were addressed by the case study are discussed, as well as description of the setup of the con- ducted case study with 26 students. The sample consists of dyslexic and non-dyslexic students and all of them re- ported to be affected with some reading difficulties. Sec- tion seven presents the results and findings of the case study. Finally, in the eighth section some conclusions are drawn and discussed.
IEEE TRANSACTIONS ON JOURNAL NAME, MANUSCRIPT ID
2 RELATED WORK
2.1 Support of Students with Dyslexia
Several studies have explored dyslexia in children: detect- ing population of children with dyslexia, assessing their cognitive processes to determine specific deficits and cre- ating assistance programs to improve their learning effi- ciency to read and write [17]–[19]. However, as we men- tioned before, research in learning disabilities has shown that the dyslexia problem can persist into adulthood. Moreover, dyslexia has not been deeply studied in univer- sity students. Therefore, we rely on the methodological ap- proaches used with children to define the steps in this re- search work. Thus, three phases are followed: 1) detection of affected students, 2) assessment of cognitive processes and 3) application of assistance programs. Some related works that have been focused on university students inde- pendently in some of these phases are presented:
  Detection of affected students. There are two parallel ways in which the detection could be made. One way is the detection of reading difficulties, i.e., the individual weaknesses of the affected students. In this sense, find- ings reported provided reasonable evidence in support of the self-report questionnaire as a highly predictive tool to detect or contact with students with learning dis- abilities [20], [21]. We highlight the work of [22] who designed a self-report questionnaire that is hand-filled by students at the University of Malaga (Spain), mak- ing it possible to detect students previously diagnosed with dyslexia or those with reading difficulties among this population. The other way is the detection of learn- ing styles, i.e., the strengths (or preferences) of these af- fected students. Although there is little empirical evi- dence on the positive effects that learning styles theory has on learning performance [23], other researchers re- marks that students with dyslexia succeed when teach- ing is multi-sensory and uses all channels [24], [25]. In this sense, as [24], [26] remarks it can be relevant detect- ing the learning style of these students to identify the most effective learning strategies they could use to learn. Likewise, many students with dyslexia have acknowledged that awareness of their learning style has helped them to understand the ways in which they learn, to understand their strengths, even their weak- nesses, and to develop appropriate strategies [25], [27]. There are many models to detect the learning style [24], [28], we highlight the revised version of the Felder-Sil- verman model [29] for a number of reasons: (1) it has aimed at university students, particularly it has been tested with dyslexic students [30], (2) it is easy to ad- minister and takes short time, (3) it is also easy to fill- in, which helps to avoid biased responses, (4) it has tested in electronic form, and (5) it has been validated, and shown to produce reliable results.
  Assessment of cognitive processes. After the detection phase it is necessary to assess the cognitive processes that can be altered in those students and determining whether or not they have dyslexia [31], [32]. Therefore,
1939-1382 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TLT.2016.2626292, IEEE Transactions on Learning Technologies
AUTHOR ET AL.: TITLE
several tools to identify dyslexia-related cognitive defi- cits were analyzed and reported in previous research work. The findings revealed that in the Spanish lan- guage, appropriate tools are not found for the assess- ment of these cognitive processes in adult dyslexic pop- ulation, but we highlight the work of [2] who con- ducted a research that consisted in the adaptation of an English assessment instrument to Spanish language of phonological and orthographic processes.
  Applicationofassistanceprograms.Detectionofread- ing difficulties and learning styles, as well as identifica- tion of cognitive deficits are necessary for generating of appropriate assistance to help students to overcome these shortcomings and support the cognitive perfor- mance of them [2], [33]. Previous works related with as- sistance in Spanish spoken universities and support for the treatment of these difficulties and cognitive deficits in university students has not been found.
Previous studies have shown that detection, assessment and assistance supported by technologies tend to increase affected students’ motivation and personalize their learn- ing process [33]–[35]. Technologies also help these stu- dents progress in skills development and enhance their learning performance [33], [36]. Additionally, the benefits of using assistive technology (e.g. speech recognition sys- tems, screen readers, and talking spell checkers) are con- sidered in compensatory strategies for these affected stu- dents [37], [38]. Finally, technology encourages a new chal- lenge: to promote student reflection on their learning (skills, difficulties, preferences, misconceptions, etc.) [39]– [41]. However, this challenge has not been studied in stu- dents with dyslexia or reading difficulties [9], [42].
2.2 Open Learner Modeling
A learner model is responsible for storing the student in- formation. Basically, this model represents knowledge, in- terests, preferences, goals, background, and individual traits of the students during their learning process, allow- ing for personalization and adaptation towards their cur- rent needs [43]. Traditionally the information in the learner model is closed to the students. However, benefits of open- ing the learner model to students to encourage awareness and reflection have been argued [11], [12], [44], [45].
For this reason, an open learner model approach is pro- posed, in which the learner model is accessible for viewing by the students in an understandable format. Moreover, considering the fact that university students with dyslexia may not have received adequate assistance during their learning process allowing them to know and deal with their difficulties; looks like open learner modeling is an op- portunity to promote autonomy in these students so that they can recognize their reading difficulties, learning styles and learn about their cognitive processes for themselves. Consequently, self-regulation is supported, so that stu- dents affected will be able to identify the appropriate focus of their efforts, to overcome their difficulties and meet their learning needs [41], [44]–[46].
Basically, if a student views the learner model, infor-
mation is provided about his/her competencies for each
topic inquired previously. A review of the literature shows
3
that an open learner model allows access to the learner model content in a variety of forms [12]. The most common of which are skill meters, textual descriptors and tables for each topic or concept to be accessed [11], [46], to more com- plex structured representations of understanding such as bayesian networks [47], hierarchical trees and concept maps [48]. Others include simulation [49] and Fuzzy Mod- els [50]. Recent work has also used treemaps to visualize the learner model [51].
Currently, an emerging area for the visualization of the learner model have been explored: learning analytics [13]– [15], [52]. Its primary goal is closely tied to, a series of other fields of study including business intelligence, web analyt- ics, academic analytics, educational data mining, and ac- tion analytics [53]. In recent years, however, there has been particular concern among researchers with using the learn- ing analytics to improve teaching and learning. Particu- larly works in this area are based on the aggregation and analysis of students’ data collections in their social con- texts, for purposes of understanding and optimizing learn- ing, teaching, and the environments in which it occurs. Learning analytics seeks to select, capture, aggregate, re- port, predict, use, refine, and share data during the learn- ing processes for teachers and students [53]. The aim of learning analytics is to provide useful support for under- standing and decision making during learning and teach- ing. Thus, learning analytics focus on the detection of key- activity and key-performance indicators which can be based on statistical and data mining techniques, so that for instance recommendations can be made for learning activ- ities, resources, training, people, etc. that are likely to be relevant. Alternatively, the data can be processed so that they can be further extended to support other educational roles in decision-making, as remarked in [14], [15], [54]. For instance, in this study the data are extended to teachers and experts (e.g., psychologist, pedagogical expert, or counselor) to support teaching strategies and assistance for students with dyslexia or reading difficulties.
Thus, open learner modeling and learning analytics are two areas tightly related to learner model visualization. Open learner modeling is more centered on personaliza- tion and learning contexts while learning analytics do more emphasis on semantic aggregation, statistical anal- yses, and results towards prediction and recommendation.
2.3 Approach for Learning Analytics Production
In order to open the learner model using learning analytics solutions for understanding the performance and activity in learning contexts and considering the fact such solutions can be extended to teachers and experts, we look for a framework to deliver this type of indicators from different perspectives: students, teachers and experts.
Accordingly, in previous researcher done by some of the authors [16] a technical framework was defined to build Activity-based Learner-models in order to monitor ac- tivities of students and deliver learning analytics solutions for different perspectives on social planes. Thus, a perspec- tive defines the set of available learning analytics functions for a role in the web-based software (e.g. student, teacher,
1939-1382 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TLT.2016.2626292, IEEE Transactions on Learning Technologies
4
expert, etc.) whereas a social plane determines the moni- tored and analyzed population related with an activity or outcome (e.g. a single student, a classroom, a group, etc.). For instance, an expert can request a particular learning an- alytic, such as the performance in a task, either for a single student or for the whole classroom.
The Activity-based Learner-models are based on foun- dations of different research areas. From the pedagogical area, Engeström’s Activity Theory is used to model activity dimensions [55]. From the computer supported collabora- tive learning (CSCL) area, the Dillenbourg and Jermann’s concept of social planes allows us to model activities taking into account social interactions [56]. From the personaliza- tion and context management area the actuator-indicator model gives a framework to implements the software ar- chitecture by dividing its construction in four well-defined functional layers (i.e. sensor, semantic, control, and indica- tor) [57]. A brief summary of the evolution of these three mentioned pillars and the union of them is presented be- low.
  Engeström ́s Activity Theory [55] is the pedagogical base. This model describes the structural relations be- tween the components of an activity (1. instruments, 2. a subject, 3. an object, 4. rules, 5. community, and 6. di- vision of labor) to leads an outcome. The activity ́s out- comes can trigger new activities and each element can be related to individual activities. Thus, complex pro- cess can be described recursively. The three first com- ponents, called the action part, are visible. The other components are constrains in the context part. This model has been used widely to identify potential im- provements of work settings for instance in [58], [59] among others. Recently in [60] its potential for person- alized clinical diagnosis systems has been explored.
  Engeström ́s Activity Theory was introduced in educa- tional technology by means of the concept of social planes [56]. Thus, the original element “community” is better expressed with the concept of “social planes”. It was found evidence for activating awareness and re- flection through visualization of information from dif- ferent social planes. The original Activity Theory had some other adaptations in the new area of research. The elements “teacher” and “learner” replace the “subject” and the “object” of the original model respectively. In addition, the element “division of labor” is understood in educational technology as “cooperative process”. Fi- nally, particular constraints of an educational software system (such as a Learning Management System or LMS) add extended relations between instruments, procedural rules (such as instructional rules), and co- operative process. Figure 1 shows a parallel view of the original Engeström Activity Theory and the extended version for educational technology [55].
  The Actuator-Indicator Model allows implementation of the extended Activity Theory in educational soft- ware. For instance, in [16] the implementation of Activ- ity-based Learner-models was related to the LMS Moo- dle, based on existing data in the Moodle log table and using the Moodle log function for the sensor layer. The
IEEE TRANSACTIONS ON JOURNAL NAME, MANUSCRIPT ID
Actuator-Indicator Model describes four technological layers to proceed from monitoring and assessment to suitable response to learners. The four layers are: 1) the sensor layer to collect data; 2) the semantic layer to ag- gregate the previous information with semantic mean- ing; 3) the control layer to process aggregated infor- mation using rules, data mining techniques or statisti- cal analysis and also to coordinate the result delivery to display functions; and 4) the indicator layer to display the results of previous analysis in the corresponding in- terface.
Fig. 1. Engeström’s Activity Theory and educational technology extension.
All in all, with Activity-based Learner-models a wider communal perspective on the learning process is available for learning analytics and visualization of the learner model.
3 PROPOSAL SUMARY
Our vision is that students with dyslexia or reading diffi- culties can learn at their own pace, knowing their strengths and weaknesses, and using their own strategies. To do so, the effectiveness and quality of their learning experience should be enhanced, by providing a better fit between the needs of affected students at a particular time and the learning facilities provided. However, the awareness of their reading difficulties (weaknesses) and learning styles (strengths) as well as their cognitive deficits should be en- couraged in order to facilitate the learning reflection and self-regulation of it. In this sense, technologies have the po- tential to make a real difference for those with special learning needs, so that all students can make the most of their skills, irrespective of their disabilities. Accordingly, we highlight the construction of learner models in order to gather students’ information and hence delivering of suit- able personalized and adapted learning to their needs in a learning context.
By opening this learner model we provide the student with additional information (e.g. reading difficulties, learning styles and cognitive deficits) about their learning process that is not usually available to them, so that they may then decide where they need or wish to improve their skills, and carry out the corresponding learning activities autonomously to achieve this improvement. In this sense,
we propose the use of learning analytics solutions to open
  1939-1382 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TLT.2016.2626292, IEEE Transactions on Learning Technologies
AUTHOR ET AL.: TITLE
the learner model.
Taking into account the foundations on Activity-based
Learner-models presented in [16], we think that this tech- nical framework is suitable for providing appropriate learning analytics to students with dyslexia or reading dif- ficulties. Therefore, this study raises new challenges to de- scribe activity-based learner-models for the effective appli- cation of learning analytics in the support of the affected students. On one hand, it is necessary to clarify how to im- plement this technical framework in independent educa- tional software such as PADA (see Section 5). On the other hand, new roles need to be supported as important sub- jects of educational activities for assistance of dyslexia or reading difficulties; in this work the role of psychological expert emerges and requires different communal perspec- tives. It is important to note that this role is not mentioned in the theories of learning orchestration. Finally, this work requires moderation of two types: activity centered and out- come centered. The prototypes of indicators implemented in [16] were only activity centered. The first type of modera- tion (activity centered) provides task support. This support can take many forms such as outlines, recommendations, storyboards, or key questions. It focuses on the modeling of a task, give advice or provide coaching. The second type of moderation (outcome centered) is guiding by feedback. This type of learning support tackles the problem solving skills of students by providing them an external view on their performance. Therefore, such learning support is re- lated to the assessment procedures that are defined for an educative process.
Figure 2 shows the Activity-based Learner-model tech- nical framework adapted from [16] to this work. This tech- nical framework was extended to Outcome-based Learner- model. Thus, the monitoring and assessment can be either activity centered (e.g., Activity-based Aggregators) or out- come centered (e.g., Outcome-based Aggregators). There- fore, learning analytics can be on activities and perfor- mance of the students in terms of demographics, reading difficulties, learning styles and cognitive processes. From these learning analytics, the provision of recommenda- tions by psychological experts could be planned. In section 5, the architecture of PADA is described and the challenges previously defined are answered.
Fig. 2. The Activity-based Learner-models technical framework adapted to PADA.
5
4 STUDENT MODEL AND DATA COLLECTION
The learner model called in this research work student model comprises four submodels: 1) the demographic sub- model considers variables related to student personal infor- mation such as their educational level, age and gender; 2) the reading profile submodel stores information about their reading difficulties as well as school life, personal and fam- ily history of learning difficulties, associated difficulties, and reading and writing habits; 3) the learning styles sub- model is used to include the students’ learning preferences; and 4) the cognitive traits submodel describes characteristics of the students that are gathered by evaluating the cogni- tive processes involved in reading such as phonological awareness, orthographic processing, lexical access, pro- cessing speed, verbal working memory, and semantic pro- cessing.
Collecting data from the student model is performed separately by a set of four web-based tools (1. Demo- graphic data forms, 2. ADDA, 3. ADEA, and 4. BEDA) which are independent of any educational software sys- tem. An overview of each one of these software tools is pre- sented below.
  Demographic data forms. To capture the student’s per- sonal details, web-based forms were developed. These forms retrieve information on: age, gender, academic level and program, origin country, and if they use or not assistive technology, among other details.
  Self-report questionnaire to detect dyslexia in adults (ADDA, acronym for Spanish name Autocuestionario de Detección de Dyslexia en Adultos). Using ADDA the stu- dent's reading profile is captured. This tool was de- signed and built using as references the instrument pro- posed in [22]. Particularly, ADDA allows to: detect stu- dents at the university that inform of having dyslexia or reading difficulties, know the most common difficul- ties presented by them, identify reading profiles, and to provide feedback. It is a set of 100 questions to be an- swered by making a simple or multiple choices, by marking YES-NO or using open-ended. The questions covered a wide range of aspects organized into eight sections: history of learning difficulties (6 items); school and learning to read experience (13 items); history of family learning difficulties (4 items); current specific learning difficulties (32 items); associated difficulties (29 items); affective issues (5 items); reading habits (8 items); and writing habits (3 items). This questionnaire was administered using a web-based software. Ques- tions were presented in text and audio format. Students used the mouse or keyboard to choose answers.
  Self-report questionnaire to detect learning styles (ADEA, acronym for Spanish name Autocuestionario de Detección del Estilo de Aprendizaje). Using ADEA the stu- dent's learning style is captured. This tool refers to the Spanish translation of the Felder-Silverman’s Index of Learning Styles (ILS) [29]. The ILS was selected after re- viewing numerous models in electronic and/or paper form. In particular, this model combines several learn- ing style models in its four dimensions (i.e., processing,
  1939-1382 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TLT.2016.2626292, IEEE Transactions on Learning Technologies
6
IEEE TRANSACTIONS ON JOURNAL NAME, MANUSCRIPT ID
open more fragments of learner model if they are later re- quired. Basically, the PADA components are: 1) the data- bases, which implement the sensor layer to collect data from the students and their activity and performance through forms (i.e., demographics), ADDA (i.e., reading profile), ADEA (i.e., learning styles) and BEDA (i.e., cogni- tive traits), 2) the aggregators, which implement the seman- tic layer to transform the data from the database according to the social plane (i.e., student, peers or class) and the so- cial perspective (i.e., student, teacher or expert) required, 3) the data mining, which implement the control layer to process the aggregators using different rules and statisti- cal analysis, 4) the learning analytics, which implement the indicator layer to display in the corresponding interface the visualizations (i.e., Overview, Reading Difficulties, Learning Styles, and Cognitive Processes), and 5) a web server that stores the implemented layers and allows com- munication between users and PADA by means of a browser.
Three challenges were raised in Section 3 related with the particular implementation of Activity-based Learner- models in PADA: how to implement this technical frame- work in an independent educational software such as PADA?, how can it include a new role of psychological ex- pert?, and how can be added an outcome-centered moder- ation?.
To answer these challenges, Figure 3 summaries tech- nology details to implement the four layers for PADA (an- swer to the first challenge). There are also details of aggre- gators’ elements (to answer the second and the third chal- lenge). The sensor layer uses PostgreSQL databases to save assessment results and monitor logs. The semantic layer is entirely implemented with PHP. In this layer a set of clas- ses and functions are used to define aggregators. An ag- gregator function receives at least two parameters: a) the social plane, and b) the perspective. Inside the aggregator function a SQL query is built and launched. This query changes depending on the value of parameters received. Thus, the same aggregator returns different semantic data.
Fig. 3. Architecture and technology behind PADA.
In PADA there are two kinds of aggregators. The out- come-based aggregators collect data of detection and as-
5
PADA: DASBOARD OF LEARNING ANALYTICS OF DYSLEXIA AND/OR READING DIFFICULTIES IN ADULTS
perception, input, and understanding), giving a more detailed description of the students. Additionally, it in- cludes learning strategies, motivation for learning and preferences for organizing information. It is a set of 44 questions which allow inquiring the strategies that a student employs or prefers to select, process and work with information. As mentioned above, it classifies dif- ferent kinds of learning styles along four dimensions: processing (active or reflective), perception (sensitive or intuitive), input (visual or verbal), and understand- ing (sequential or global). This questionnaire also was administered using a web-based software. Questions were presented in text and audio format.
  Assessment battery of dyslexia in adults (BEDA, acro- nym for Spanish name Batería de Evaluación de Dislexia en Adultos). To capture the student's cognitive traits we use BEDA. This tool is a novel computerized assess- ment battery for dyslexia and it focus on assessing cog- nitive processes associated with reading in Spanish. BEDA is web-based software and it is composed of 15 tasks that can be completed by students, 16-years old and older. The BEDA tasks detect and verify if a stu- dent has or not dyslexia and identify the associated cognitive processes that may be affected. BEDA con- sists of eight modules: six for the assessment of each cognitive process involved (i.e., 1. phonological pro- cessing, 2. orthographic processing, 3. lexical access, 4. processing speed, 5. verbal working memory, and 6. se- mantic processing), one for the analysis of results, and one for administration purposes. Figure 3 shows vari- ous interfaces of BEDA: the main menu, the pedagogi- cal agent who provides the instructions, and an assess- ment task to assess phonological processing. A task in BEDA is composed of different items (or exercises) that students must perform. Additionally, every task has an instruction and some example items.
 Once the data detection and assessment of students is saved, learning analytics of the data collected can be pro- duced. In this paper PADA tool is proposed to produce such learning analytics. PADA is a dashboard for visualiz- ing and inspecting fragments of information from the learner model related to reading difficulties for university students. It generates visualizations for each of the forms, questionnaires and cognitive assessment tasks presented above. These visualizations seek to create awareness among students about their reading difficulties, learning style, and cognitive deficits in order to facilitate reflection and encourage self-regulated of their learning, especially where reading is involved.
5.1 Architecture & Implementation
PADA’s architecture, as aforementioned, is based on the Activity-based Learner-models technical framework so as to have a flexible and extendable dashboard which let to
 1939-1382 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TLT.2016.2626292, IEEE Transactions on Learning Technologies
AUTHOR ET AL.: TITLE
sessment results from the database. The activity-based ag- gregators collect data from monitor logs from the database. The control layer holds several Javascripts based on the jQuery Javascript library. Scripts request particular aggre- gators. The request is an AJAX call to the server. Then in- telligent procedures (data mining algorithms) transform the information to send it to the indicator layer. A final pro- cessing is made in the indicator layer to produce adequate plots. These plots use HTML5 elements such as the <can- vas> element. A particular library is used also, the jpPlot library. The final aspects of the interface are arranged using CSS. In particular, de jqPlot library was selected after testing it and other five libraries. The jqPlot library has functions to plot a wide range of charts, the aspect of plots is nice and adaptable, and finally the time of response is adequate.
To sum up the results of this architecture and their im- plementation are visualizations for different social planes (student, peers, and class) and social perspectives (student, teacher, and expert); although in this paper we only con- sider the student’s perspective in the case study reported. Therefore, in this student's perspective, the visualizations that are shown are mostly for the student’s social plane, although some of them include views of the peers and class planes in order to provide comparisons and generate re- flection among students taking reference points. The social plane student shows visualizations of a single student (i.e. the student logged in PADA), the social plane peers shows visualizations of the entire class minus the student logged in PADA and the social plane class shows visualizations of the entire class.
5.2 Interfaces
The PADA interface was divided into four tabs de- pending on the learner submodel: 1. demographics, 2. reading profile, 3. learning styles or 4. cognitive traits. These tabs allow students to explore different visual repre- sentations of their activity and performance and provide feedback to support them to recognize strengths and weakness in their reading competences. These tabs also provide some parallel views of an individual students, her peers, and all as a class, in order to identify the severity of their difficulties according to the results of other matched by age and academic level. The analyses are made accord- ing to statistical analysis taking into account the criteria set for the construction of each of the data collection tools.
  Overview refers to personal details of the participant students (i.e. ages, genders, academic programs, etc.). The tab visualizes (1) the number of participant stu- dents, (2) the details of the student in session, (3) the time spent to complete the data collection tools, (4) the age distributions, and (5) the number of student per ac- ademic program.
  Reading difficulties (see Figure 4a) refers to reading profile of the participant students. The tab visualizes (1) the previous diagnosis of learning disabilities, (2) the number of learning difficulties in reading, writing and math reported by the students, (3) the number of asso- ciated difficulties with reading (i.e. language, memory,
7
motivation, perception, attention, and spatial-tem- poral) reported by the students, and (4) the reading and writing habits reported by the students. Figure 4b shows results of the reading (left) and writing (right) habits by the single student while Figure 4c shows feed- back provided to the student. Figure 5a displays results of the difficulties in reading, writing and math by the single student, peers and class while Figure 5b shows a summary by the single student. Figure 5c reports a summary of results of the associated difficulties with reading by the single student.
  1939-1382 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
Fig. 4. PADA interface: Tab of reading difficulties analytics.
 Fig. 5. PADA interface: Analytics with summaries of reading and asso- ciated difficulties.
   Learning styles (see Figure 6a) refers to ways in which participant students prefer to learn. The tab visualizes the major preferences of these students. Figure 6b shows result for a single student with learning style: ac- tive, sensory, visual and sequential. Figure 6c displays feedback provided to the student.
  Fig. 6. PADA interface: Tab of learning style analytics.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TLT.2016.2626292, IEEE Transactions on Learning Technologies
8
IEEE TRANSACTIONS ON JOURNAL NAME, MANUSCRIPT ID
University of Girona. Participants took part in an individual session that lasted an average of 60 minutes. Seven of them needed an average of two sessions to complete the tests without feeling tired.
6.1.2 Instruments
In previous research work done by authors in [61], a pilot study was conducted with some tools including PADA, in a computer lab using a Windows desktop computer equipped with a screen, a keyboard, a mouse, headphones, microphone, and Internet connection. In this work the case study was conducted in a different computed lab using the software and hardware conditions. Other software re- quirements were installing Java and an appropriate ver- sion of the Firefox web browser.
To carry out the case study all the web-based tools pre- sented in Section 4 were required (i.e., 1. forms, 2. ADDA, 3. ADEA, and 4. BEDA). After all data were collected, PADA tool was enabled and became operational for par- ticipants. To gather participants' feedback about whether PADA could assist them in understanding their student models and could be useful to identify reading difficulties, learning styles and cognitive deficits, we provided them with an online survey.
As shown in Table 1, the survey consisted of 37 state- ments as follows. One question to inquire whether partici- pants have previous diagnosis of dyslexia (‘yes’ or ‘no’). Eight 5-point Likert scale questions to inquire about navi- gation, and understanding of the visualizations were pre- sented in each tab of PADA ('1 = never' to '5 = always'). Nine 5-point Likert scale questions inquired about the agreement or disagreement with the contents of the stu- dent model in term of reading difficulties, learning style and cognitive deficits detected (‘1 = strongly disagree’ to ‘5 = strongly agree’). Seven 5-point Likert questions inquired about the awareness and self-regulation that can support PADA ('1 = never' to '5 = always'). There were three open- end questions to clarify the moments reported by students regarding awareness of difficulties, learning styles and cognitive deficits. Four open-end questions accompanied each of the awareness questions to inquire about more vis- ualizations which could improve the experience with P ADA. Four 5-point Likert questions inquired about P A- DA's big-picture usefulness ('1 = never' to '5 = always'). One rank order scale question to inquire about the opinion on the type of recommendations that the participants would prefer to receive. Finally, one open-ended question gave the opportunity for additional comments.
Table 1. Overview of case study survey
DESCRIPTIVE INFORMATION
DES.1. Have you been diagnosed with dyslexia?
Navigation
A.1. to Did you check graphical and textual visualizations in... Tab 1?, A.4. Tab 2?, Tab 3, Tab 4?
Understanding
B.1. to Was it easy for you to understand the meaning of the visualiza- B.4. tions displayed on... Tab 1?, Tab 2?, Tab 3?, Tab 4?
Inspection
C.1. Do you agree with the visualizations about your particular read- ing difficulties?
C.2. Do you agree with the visualizations about your associated dif- ficulties (i.e., languages, memory, etc.)?
  Cognitive processes (see Figure 7) refer to processes as- sociated with reading that were assessed. The tab visu- alizes (a) the results for each assessment task, (b) the percentages of successes/errors for each assessment task, (c) the result for cognitive process, and (d) the per- centages of success/errors for each cognitive process. Figure 7a shows results of the difficulties in the assess- ment task of phonological processing. Figure 7b illus- trates results of successes/errors for the assessment task “Segmentation into Syllables”. Figure 7c reports results for the cognitive processes by single student.
  Fig. 7. PADA interface: Tab of cognitive processes analytics.
6 CASE STUDY
Our concern is whether students would find PADA useful to detect their reading difficulties and their implications for learning and cognition. More specifically, our study was based on answering the following questions: 1. Could students view their student model?, 2. Could students un- derstand that model?, 3. Did students agree with the visu- alizations presented in that model?, 4. Were students aware on their difficulties, learning styles and cognitive deficits?, 5. Could PADA support students to perform self- regulated learning?, and 6. Were learning analytics useful for students?.
6.1 Method
6.1.1 Participant
Participants were 26 students (11 male and 15 female), with ages ranging from 21 to 53 years (M=27.538, SD=6.848) and coming from 15 classrooms of different programs (includ- ing, pedagogy, psychology, law, social work, business ad- ministration, economics, and engineering) and levels, (un- dergraduate and graduate) at the University of Girona with basic skills for understanding statistical graphs and tables. Eight participants had a previous diagnosis of dys- lexia, i.e., they had been formally diagnosed with dyslexia during their primary or secondary schooling, through an official psychoassessment procedure. All others students were affected with some reading difficulties which may be related to symptoms of dyslexia. Participants were re- cruited by the coordinators of the faculties and/or schools through e-mail and website announcements. However, most dyslexic participants were recruited in collaboration with the Program to Support People with Disabilities of the
        1939-1382 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TLT.2016.2626292, IEEE Transactions on Learning Technologies
AUTHOR ET AL.: TITLE
C.3. Do you agree with the visualizations about your reading habits?
C.4. Do you agree with the visualizations about your writing habits?
C.5. Do you agree with the visualizations about your learning style?
C.6. Do you agree with the visualizations about your successes/er-
rors in each cognitive assessment task?
C.7. Do you agree with the visualizations about your successes/er-
rors in each cognitive process?
C.8. Do you agree with the visualizations about your results in the
cognitive assessment tasks?
C.9. Do you agree with the visualizations about your cognitive defi-
cits?
Awareness
9
all tasks of BEDA which evaluates their cognitive pro- cesses associated with reading. They were then asked to enter PADA and navigate the entire tool. Finally, when participants had browsed through PADA, the teacher gave them access to the online survey.
During the process, participants were accompanied by one teacher with experience in using all tools. To com- plete this survey, they were left alone in order to not bias the responses. Approximately, this survey takes 10 to 20 minutes to complete.
6.2 Results
This study allows evaluating students’ opinions of the sup- port for activity and performance analytics (i.e., visualiza- tions) provided by PADA. These results presume to give a possible answer to the research questions.
From the navigation category, the results indicated that all 26 participants navigated through the different tabs and visualizations presented in the graphical user interface of PADA. In terms of understanding, 84.6% of the participants always (53.8%) and almost always (30.8%) understood the meaning of the visualizations shown in the tab of overview (question B.1.). 96.1% of the participants always (53.8%) or almost always (42.3%) understood the visualizations in tab of reading difficulties (question B.2.). 80.8% of the partici- pants always understood the visualizations in tab of learn- ing styles (question B.3.), while the remaining 19.2% of them almost always understood. Regarding the tab of cog- nitive processes (question B.4.), 69.3% of participants al- ways (30.8%) and almost always (38.5%) understood the visualizations presented, while 30.8% sometimes and 3.8% almost never understood them. A multivariate analysis of variance with the independent variable of dyslexia (dys- lexic or non-dyslexic) and the four dependent variables of understanding visualizations (overview, reading difficul- ties, learning styles, cognitive processes) was statistically significant (Roy’s largest root=1.01, F(4, 21)=5.31, p=0.004). However, a multivariate analysis of variance with the in- dependent variable of gender (female or male) and the four dependent variables of understanding visualizations was not significant (Roy’s largest root=0.33, F(4, 21)=1.71, p=0.18).
With regard to the remaining categories (i.e., inspec- tion, awareness, self-regulation, and usefulness), we pre- sent the results considering separated groups of partici- pants in students with and without diagnosis of dyslexia (i.e., dyslexic and non-dyslexic, respectively). This is be- cause participants with previous diagnosis of dyslexia may be familiar with similar tools, since they could have re- ceived training, they could have a clear picture of their dif- ficulties, and/or they could have adopted effective com- pensatory strategies to overcome their difficulties, which could provide us different perspectives on the visualiza- tions of PADA.
In the inspection category, participants indicated being agreed or disagreed with the visualizations of the student model. As shown in Table 2, 92.4% of the participants strongly agreed (46.2%) or agreed (46.2%) with the reading difficulties detected (question C.1.). 84.6% of the partici- pants strongly agreed (42.3%) or agreed (42.3%) with the
        D.1.
D.1.* D.2. D.2.* D.3. D.3.* D.4.
D.5.
Was it possible for you to be aware about your reading difficul- ties?
The former was possible by means of...
Was it possible for you to be aware about your learning style? The former was possible by means of...
Was it possible for you to be aware about your cognitive deficits? The former was possible by means of...
Was it helpful for your awareness process to view your learning analytics versus the performance of others (i.e., “peers” and “class”?
Did you learn more about your difficulties than you knew pre- viously?
What other visualizations do you think could improve your ex- perience in... Tab 1?, Tab 2?,Tab 3?, Tab 4?
D.6. to
D.9.
Self-regulation
E.1. Do you think that PADA can help you in reflecting and making
      decisions to self-regulate your learning process?
Usefulness
F.1. Was it useful for you to check the visualizations in multiple views (i.e., graphical and textual)?
F.2. Did the presented learning analytics provide feedback on your reading performance?
F.3. Do you think PADA helps to recognize strengths and weak- nesses in your reading process you could use to improve your academic performance?
F.4. Did you find all the visualizations you expected?
Recommendations
REC.1. Finally, if you could have a recommender system in PADA, what kind of recommender do you prefer? ‘1 - advices recommended by dyslexia-affected peers’, ‘2 - activities/tasks recommended by expert’, ‘3 - exercises, games, and other resources recommended by experts’.
Comments
COM.1 Please, if you have more comments about your experience with . PADA ...
Note. Tab 1=tab of overview; Tab 2=Tab of reading difficulties; Tab 3=Tab of learning styles; Tab 4=tab of cognitive processes.
6.1.3 Procedure
Prior to the case study, PADA was studied to evaluate the performance and usability in a pilot study with some stu- dents from the University of Girona. Once PADA was im- proved based on the findings of the pilot study, it was given to the participants. The present study focused on the use of PADA for visualizing and inspecting student mod- els. Participants were given an explanation of the web- based tools so as they could familiarize themselves with them, before commencing their sessions. Once the partici- pants started a session, firstly, they could register their per- sonal details information. Then, they were asked to enter ADDA and fill in the self-report questionnaire in order to detect reading difficulties. Afterwards, they were asked to access ADEA to fill in the self-report questionnaire for de- tecting their learning styles. Subsequently, they completed
                         1939-1382 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
                                                                                                                                                                                                                                                                                                                                       This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TLT.2016.2626292, IEEE Transactions on Learning Technologies
10
visualizations for associated difficulties with reading (question C.2.) and reading practices (questions C.3.), while 11.5% were indifferent to these visualizations. 76.9% of the participants strongly agreed (32.3%) or agree (34.6%) with their writing practices (question C.4.), 15.4% were in- different and 7.7% were disagreeing with them. Regarding the visualizations of learning style (question C.5.), all par- ticipants strongly agreed or agreed with them. With regard to the visualizations of cognitive processes (questions C.6. to C.9), we found that participants strongly agreed or agreed with 80-92% of the analytics presented. The dys- lexic participants not revealed significant differences in
IEEE TRANSACTIONS ON JOURNAL NAME, MANUSCRIPT ID
questions C.1. to C.7. compared to their non-dyslexic peers. However, in questions C.8. and C.9. some differences were showed. Multivariate analyses of variance of the nine de- pendent variables of inspection (reading difficulties, asso- ciated difficulties, reading habits, writing habits, learning style, cognitive assessment task, cognitive process, results, and cognitive deficits) was insignificant for both the inde- pendent variable of dyslexia (Roy’s largest root=1.03, F(9, 15)=1.71, p=0.17) and gender ( Roy’s largest root=0.76, F(9, 15)=1.27, p=0.32).
Table 2. Results of inspection category
    Responses (n=26)
Non-dyslexic (n=18)
Dyslexic (n=8)
     Question
C.1. C.2. C.3. C.4. C.5. C.6. C.7. C.8. C.9.
Strongly disagree
0 0
0
0
0
1 (3.8%)
2 (7.7%)
1 (3.8%) 2 (7.7%) 2 (7.7%) 1 (3.8%)
3 (11.5%) 3 (11.5%) 3 (11.5%) 0
16 (61.5%) 13 (50%) 14 (53.8%) 17 (65.4%)
6 (23.1%) 8 (30.8%) 7 (26.9%) 7 (26.9%)
4.11 0.832 3.88 4.11 1.023 3.88 4.17 0.857 3.63 4.28 0.752 3.63
0.991 0.463 0.463 0.518 0.354 0.354 0.744 1.061
                             Disagree
Indifferent
Agree
Strongly M Agree
SD M
SD
                               0
12 (46.2%)
12 (46.2%)
4.44
0.784
4.00
0.926
                           1 (3.8%)
3 (11.5%)
11 (42.3%)
11 (42.3%) 4.28
0.752
4.13
                       0
0
3 (11.5%)
14 (53.8%)
9 (34.6%) 4.22
0.732
4.25
                       0
2 (7.7%)
4 (15.4%)
11 (42.3%)
9 (34.6%) 3.94
1.056
4.25
                       0
0
0
9 (34.6%)
17 (65.4%) 4.78
0.428
4.38
                               Table 3 shows the findings in the awareness category that participants could achieve by interacting with PADA. Alt- hough most participants always (42.3%) or almost always (26.9%) indicated that they were able to be aware of their reading difficulties (question D.1.), it is worth noting that 28% of them indicated that sometimes (19.2%), almost never (7.7%) or never (3.8%) achieved such awareness. Re- garding the visualizations of learning style (question D.2.), we found that almost all participants achieved the aware- ness. For visualizations of cognitive processes (question D.3.), 77% of the participants indicated they achieved be aware of their cognitive deficits, while 23% indicated that they did it only sometimes (3.8%), almost never (11.5%) or never (7.7%). Comparing non-dyslexic and dyslexic, we did not find significant differences for these two groups in the aforementioned questions (D.1. to D.3.). However, an- alyzing questions about peer or group comparison (ques-
tion D.5.) and increased knowledge of difficulties (ques- tion D.5.) revealed some differences. In question D.4. (non- dyslexic M=4.11, SD=1.231; dyslexic M=3.88, SD=0.835) and question D.5. (non-dyslexic M=4.22, SD=0.808; dys- lexic M=3.50, SD=0.926).
A multivariate analysis of variance with the independ- ent variable of dyslexia (dyslexic or non-dyslexic) and the five dependent variables of awareness (reading difficul- ties, learning styles, cognitive deficits, peer or group com- parison, and increased knowledge) did not yield statisti- cally significant results (Roy’s largest root=0.40, F(5, 19)=1.52, p=0.23). Similarly, a multivariate analysis of var- iance with the independent variable of gender (female or male) and the five dependent variables of awareness was also not significant (Roy’s largest root=0.10, F(5, 19)=0.39, p=0.84). Questions D.6. to D.9. collected several opinions given by participants for improving their experience with each tab of PADA.
Responses (n=26)
Non-dyslexic (n=18) Dyslexic (n=8)
Always M SDM SD
Question Never D.1. 1 (3.8%) D.2. 0
D.3. 2 (7.7%) D.4. 0
D.5. 0
The self-regulation evaluation that can support PADA
was represented in the E.1. question. We found that 61.5% of the participants always (11.5%) or almost always (50%) took conscientious that PADA could encourage self-regu- lation in the learning process, 30.8% indicated that only sometimes could it, while 7.6% indicated that almost never or never could it. A univariate analysis of variance with the independent variable of dyslexia and the dependent
Almost never 2 (7.7%)
0
3 (11.5%)
3 (11.5%)
Sometimes 5 (19.2%)
2 (7.7%)
1 (3.8%)
6 (23.1%)
Almost always 7 (26.9%)
5 (19.2%)
12 (46.2%)
4 (15.4%)
11 (42.3%) 19 (73.1%) 8 (30.8%) 13 (50%) 8 (30.8%)
4.00 1.237 3.88 4.72 0.575 4.50 3.78 1.263 3.88 4.11 1.231 3.88 4.22 0.808 3.50
0.991 0.756 1.246 0.835 0.926
2 (7.7%)
4 (15.4%)
12 (46.2%)
Table 3. Results of awareness categories
1939-1382 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
variable of self-regulation was not significant (F (1, 23)=0.03, p=0.85) and neither were there any significant differences with respect to gender (F (1, 23)=0.14, p=0.71) In relation to the usefulness of PADA (see Table 4), partici- pants reacted positively to the idea of multiple views (question F.1.), with 88.5% always and 11.5% almost al- ways useful as response. Also, they were positive about
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TLT.2016.2626292, IEEE Transactions on Learning Technologies
AUTHOR ET AL.: TITLE
how easily they could recognize the strengths and weak- nesses of their reading process using PADA (question F.2.), with 23.1% always, 50% almost always and 23.1% some- times useful as response. But, they were not so positive about the use of PADA to improve their academic perfor- mance (question F.3.), since 23.1% of participants almost never found it useful. However, participants indicated that PADA showed the visualizations that they expected to find (question F.4.). Comparing the responses of dyslexic and non-dyslexic participants, we only found a significant difference in question F.4. (non-dyslexic M=4.22,
11
SD=0.548; dyslexic M=3.50, SD=0.535). A multivariate analysis of variance with the independent variable of dys- lexia (dyslexic or non-dyslexic) and the four dependent variables of usefulness (multiple views, reading perfor- mance feedback, reading process evaluation, and expected visualizations) was statistically significant (Roy’s largest root=0.58, F(4, 20)=2.87, p=0.05). However, a multivariate analysis of variance with the independent variable of gen- der (female or male) and the four dependent variables of usefulness was not significant (Roy’s largest root=0.17, F(4, 20)=0.83,p=0.52).
Table 4. Results of usefulness categories
        Responses (n=26) Question Never
F.1. F.2. F.3. F.4.
Concerning the rank order question, participants orga- nized three options of recommender systems from most to least important to them. Option R2 is the most preferred by participants (15 of 26; 57.7%). Option R3 is the second, which was selected by 7 participants (26.9%). While option R1 is the less preferred, since only 4 participants (15.4%) reported they preferred this option.
Finally, some additional comments made by partici- pants suggest their interest in PADA and willingness to contribute to its improvement:
  "It is a very useful tool - it helped me to reflect on the difficulties I have; it was good to learn about my learning style to reinforce my strategies for studying. I think the cognitive processes analytics are fine, but I missed more feedback on them."
  "It is very gratifying to see the results both textual and visual, since the reports I used to receive they are all textual and also hard to understand."
  "It would be interesting to have available a tutorial ex- plaining the issues addressed by PADA which in- cludes: symptoms, causes and solutions, cognitive processes, and so on."
  "I would appreciate to see some recommendations for dealing with my difficulties and strengthening my abilities."
  "I never had the opportunity to know such infor- mation, and I think that can be very helpful to im- prove my studying habits. I also believe that this infor- mation can be very useful for our teachers."
  “I think PADA may show more information. I spent almost 2 hours completing questionnaires and doing exercises. I would like to see more details of what I did.”
7 DISCUSSION
There is abundant evidence that dyslexia does not dis- appear with age or training [62], [63]. On the contrary, de- spite their effort, when compared to their peers, affected students still show significant difficulties in reading tasks
[64]–[67]. Despite their difficulties, many dyslexic students
Non-dyslexic (n=18)
Dyslexic (n=8)
0.463 0.463 0.744 0.535
                               Almost never
Sometimes
Almost always
Always M
SD M
SD
                                    0
0
0
3 (11.5%)
23 (88.5%)
4.94
0.236
4.75
                      1 (3.8%)
0
6 (23.1%)
13 (50%)
6 (23.1%)
3.94
1.056
3.75
                      0
6 (23.1%)
5 (19.2%)
8 (30.8%)
7 (26.9%)
3.72
1.274
3.38
                      0
0
5 (19.2%)
16 (61.5%)
5 (19.2%)
4.22
0.548
3.50
                    1939-1382 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
could develop compensatory strategies to help them suc- ceed in their studies [3], [4], [68], [69] and get into univer- sity, although they still underperform in reading-related tasks [62], [70]. For example, according to the Dyslexia As- sociation of Jaen between 6% and 8% of the university stu- dents are dyslexics.
Surprisingly, not all students whose performance is af- fected by dyslexia are diagnosed and/or assisted before starting their studies at university; therefore, there are many students with symptoms or reading difficulties who have not been diagnosed with an official psychoassess- ment procedure. Consequently, a considerable number of students enter university without having reading skills ex- pected, and would require support to cope with high read- ing demands.
Thus, higher educational institutions are in clear need of specific resources to detect students with or without a previous diagnosis of dyslexia that still show reading dif- ficulties, and to provide assistance to them. In this sense, PADA and its complementary services for data collection could be a suitable tool to achieve these two goals. Accord- ing to [22], [71], appropriate tools in Spanish cannot be found to assist these difficulties in adult dyslexics.
The main focus of this study was assessing the useful- ness of PADA in term of assisting university students with dyslexia or reading difficulties to achieve awareness, so that, reflection and self-regulation could be facilitated dur- ing their learning process. Findings of previous studies of [8]–[10], [42] revealed that awareness is a powerful predic- tor for their academic success. In this regard, a survey was created which explores PADA's aspects such as naviga- tion, understanding, and inspection capabilities. In partic- ular, the survey investigated awareness, as well as, sup- port for reflection and self-regulation which PADA sought to provide. Finally, overall usefulness of PADA was as- sessed.
According to the navigation results, the PADA tool is found to be well implemented. Students reported viewing their entire student model by browsing through all the tabs of PADA and checking different graphical and textual vis- ualizations. They also commented that PADA adequately
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TLT.2016.2626292, IEEE Transactions on Learning Technologies
12
respond to their navigation pace, it was friendly and intu- itive, and it graphical user interface was suitable.
On the other hand, although students reported that it was easy to understand the different visualizations dis- played, they also commented that it took them quite some time, mainly those related to cognitive processes. We be- lieve this could be due to two factors (1) there are many visualizations in this tab which makes students take longer to understand, and (2) the cognitive process concept is new for students which makes it difficult for quick comprehen- sion. Furthermore, multivariate analysis showed signifi- cant differences between dyslexic and non-dyslexic stu- dents. Surprisingly, dyslexic students found it a little more difficult to understand the meaning of the cognitive pro- cesses visualizations. It was expected that these students with previous diagnosis were more familiar with this con- cept. At this point, it is worth noting that dyslexia defini- tions and diagnostic criteria have changed since these stu- dents were first identified during childhood.
Regarding the inspection, overall, students were rather agreeing with the visualizations presented. This could in- dicate that PADA is reliable, though this claim may require further analysis of the system's confidence as presented by [48]. However, it is worth noting that a slightly significant percentage of students were “indifferent” and “disagreed” with the visualizations of reading and writing habits. We are assuming that students who were "indifferent" might not have understood the visualizations presented, while students "agreed" or "disagreed" had understood the visu- alizations. Accordingly, it was expected that the inspection of the student model would lead to awareness and subse- quent reflection and self-regulation. On the other hand, multivariate analyses were insignificant between dyslexic and non-dyslexic, but analysis showed slight differences in visualization of cognitive processes. Some dyslexic stu- dents expected greater cognitive deficits than those pre- sented in PADA.
In relation to awareness, although most students re- ported having reached awareness, a slight percentage re- ported not having succeeded with a few visualizations of reading difficulties and cognitive processes. On the one hand, we assumed that the positive perceptions of most of the students are due to the novelty of incorporating the cognitive processes concept across students' learning pro- cess. On the other hand, it was observed that both dyslexic and non-dyslexic students require more feedback on the visualizations presented to increase their awareness. In or- der to understand better why the slight percentage of stu- dents did not increased their awareness, we highlight some of their comments: “I need more feedback to understand the difficulties presented and be aware of them”, “I've had dys- lexia for years. So, I know what my reading difficulties are. Then, I didn't have an increased awareness”, “I enjoyed the visualiza- tions presented, but it’s not clear the meaning of each cognitive processes”, “I do not quite understand the percentages that it shows to me”. In addition, from the comments it was iden- tified that some dyslexic students did not increase their awareness because they already knew their particular dif- ficulties since childhood. This would indicate that the vis- ualizations of reading difficulties increase more awareness
IEEE TRANSACTIONS ON JOURNAL NAME, MANUSCRIPT ID
among non-dyslexic students. This was also supported by peer or group comparisons which are presented in these visualizations, as well as the acceptance of these students to increase knowledge of their difficulties.
Similarly, the results on self-regulation showed that PADA can provide successful mechanisms to encourage student independence in overcoming their difficulties dur- ing the learning process.
Although the results of usefulness category were gener- ally positive, i.e., students were satisfied about multiple views, reading performance feedback, and expected visu- alizations, these results were not so positive in recognizing the strengths and weaknesses in reading. Again, it con- firmed the need to include more feedback on the visualiza- tions presented. Furthermore, multivariate analysis showed significant differences between dyslexic and non- dyslexic students, particularly because dyslexic students expected more visualizations of their model. Emerging re- search findings indicate relationships between emotional, notational, and social aspects of learning analytics. These aspects could be considered in the usefulness evaluation of PADA's visualizations. For example, studies of [72] show that the traffic lights representations followed by smile no- tations have high emotional activation in students, because of their general socio-cultural availability and quick com- prehension. In [73], the notations and social aspects also are studied in order to increase the interest of the students in their learner models. Further, in [74] , authors note that release of model fragments to peers could help to find suit- able collaborators peers with common difficulties. Addi- tionally, facilitating collaboration between partners can improve understanding of themselves and each other by gaining information from their respective learner model. Here, it is worth noting that visualizations could play a role in guiding collaborative learning by amplifying cer- tain kind of social interactions [75], [76].
Regarding students’ gender, there were no significant differences between female and male students in any of the aspects evaluated (i.e., navigation, understanding, inspec- tion, awareness, self-regulation, and usefulness). This is a topic of interest since previous studies have often reported a higher rate of difficulties for males than females [77], [78], as well as differences in the particular difficulties pre- sented by males and females, and the specific assistance that they require [79], [80]. For instance, regarding attitude to technology, males generally are performing better in learning conditions which included visual resources, while females perform better with the traditional text-based re- sources [81].
Finally, over 100 comments were made by students. It was surprising that students made comments as the open- ended question was not mandatory. They made comments to clarify some answers, they suggested more visualiza- tions to improve their experience, and they made com- ments that contribute to the overall enrichment of PADA (see Section 5). All in all, based on the empirical case study results, we believe that PADA was well received, and it could be used to facilitate the learning process of students with dyslexia or reading difficulties.
Nevertheless, the findings of this study need to be viewed
1939-1382 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TLT.2016.2626292, IEEE Transactions on Learning Technologies
AUTHOR ET AL.: TITLE
in light of some limitations. First, PADA is a tool to visual- ize the presence of reading difficulties or subjective symp- toms implying dyslexia, the learning styles, and the pres- ence of cognitive deficits, as well as to provide feedback. However, this feedback is still limited, so this may lead to the creation of specific and necessary recommendations to support reflection and self-regulation of difficulties and learning process. Taking into consideration students’ sug- gestions, a recommender system of learning activities pro- vided by expert (e.g., psychologist, pedagogue, or counse- lor) could be implemented as future work. In addition, the students also offered some suggestions for improvement of the dashboard with new functionalities such as extend visualizations, giving more detail feedback on, and creat- ing a tutorial. Second, PADA displayed visualizations for each of the tools that collect data (namely demographic data forms, ADDA, ADEA, and BEDA), but it does not cre- ate aggregators that combine data between tools. Further research is proposed to analyze the influence between data collected in order to improve the assistance. Finally, future work includes the execution of additional experiments with more dyslexic students, so as to evaluate the feedback provided by them about the effectiveness of the dashboard compared with the feedback of non-dyslexic students (i.e. have a balanced group of dyslexic and non-dyslexic partic- ipants). We also believe that using PADA in a LMS can be useful in terms of improving the student’s academic per- formance in general and this needs to empirically investi- gated.
7 CONCLUSION
This paper has introduced PADA, a web-based tool for visualization and inspection of reading difficulties in uni- versity students. With PADA experts, teachers and stu- dents are involved in a new approach to support a better learning in higher education classes. Particularly, this novel tool generates automatic visualizations from the de- tection and assessment tools designed previously in collab- oration with experts in learning disabilities. A case study and survey with dyslexic and non-dyslexic students was described.
Based on the results of this study it has been demon- strated that PADA is a useful tool. It was shown that PADA’s functionality and navigability are good. Students were capable of understanding and inspecting their own student model through different visualizations. More in- terestingly, PADA was shown to be able to assist students in creating awareness, as well as facilitate reflection and self-regulation in the learning process.
To sum up, PADA is first to provide a tool of assistance for university students affected with dyslexia or reading difficulties. PADA is based on the fundamentals of open learner modeling and learning analytics [12]. Particularly, its architecture is based on Activity-based Learner-models technical framework proposed by [16] and extended with Outcome-based Learner-models for new roles in an inde- pendent web-based tool. PADA displayed visualizations
13
of different tools previously implemented such as forms, ADDA, ADEA, and BEDA. Results of the case study shown that PADA is a usefulness tool, however, it is nec- essary to continue this work to study this usefulness with larger samples of dyslexics and non-dyslexics. Future re- search may provide a clearer picture of the learning ana- lytics for awareness, reflection and self-regulation in the learning process among Spanish-speaking affected stu- dents. There is, of course, a need to replicate these findings and to validate them in other university contexts.
REFERENCES
[1] N. Gregg, “Underserved and Unprepared: Postsecondary Learning Disabilities,” Learn. Disabil. Res. Pract., vol. 22, pp. 219– 228, 2007.
[2] A. Díaz, “Perfiles cognitivos y académicos en adolescentes con dificultades de aprendizaje con y sin trastorno por déficit de atención asociado a hiperactividad (Ph.D. Thesis),” Universidad de la Laguna, La Laguna, Spain, 2007.
[3] D. L. Lefly and B. F. Pennington, “Spelling errors and reading fluency in compensated adult dyslexics,” Ann. Dyslexia, vol. 41, pp. 143–162, 1991.
[4] M. J. Ransby and H. L. Swanson, “Reading comprehension skills of young adults with childhood diagnoses of dyslexia.,” J. Learn. Disabil., vol. 36, no. 6, pp. 538–55, 2003.
[5] C. Mejia, J. Clara, and R. Fabregat, “DetectLD: Detecting university students with learning disabilities in reading and writing in the Spanish language,” in World Conference on Educational Multimedia, Hypermedia & Telecommunications - ED- MEDIA 2011, 2011, pp. 1122–1131.
[6] C. Mejia, A. Díaz, J. E. Jiménez, and R. Fabregat, “BEDA: A Computerized Assessment Battery for Dyslexia in Adults,” Procedia - Soc. Behav. Sci., vol. 46, pp. 1795–1800, 2012.
[7] C. Mejia, “Adaptation Process to Deliver Content based on User Learning Styles (Master Thesis),” Universitat de Girona, Girona, Spain, 2009.
[8] H.B.Reiff,P.J.Gerber,andR.Ginsberg,“InstructionalStrategies for Long-Term Success.,” Ann. Dyslexia, vol. 44, pp. 270–288, 1994. [9] R. J. Goldberg, E. L. Higgins, M. H. Raskind, and K. L. Herman, “Predictors of success in individuals with learning disabilities: a qualitative analysis of a 20-year longitudinal study,” Learn.
Disabil. Res. Pract., vol. 18, no. 4, pp. 222–236, 2003.
[10]E. E. Werner, “Risk and resilience in individuals with learning disabilities: Lessons learned from the Kauai longitudinal study,”
Learn. Disabil. Res. Pract., vol. 8, pp. 28–35, 1993.
[11] A. Mitrovic and B. Martin, “Evaluating the Effect of Open Student
Models on Self-Assessment,” Int. J. Artif. Intell. Educ., vol. 17, no.
2, pp. 121–144, Apr. 2007.
[12]S. Bull and J. Kay, “Open Learner Models,” in Advances in
Intelligent Tutoring Systems, R. Nkambou, J. Bordeau, and R. Miziguchi, Eds. Springer-Verlag, Berlin Heidelberg, 2010, pp. 318–338.
[13]G. Siemens, D. Gašević, C. Haythornthwaite, S. Dawson, S. Buckingham Shum, R. Ferguson, E. Duval, K. Verbert, and R. S. J. d. Baker, “Open Learning Analytics: an integrated & modularized platform,” 2011.
[14] K. Verbert, H. Drachsler, N. Manouselis, M. Wolpers, R. Vuorikari, and E. Duval, “Dataset-driven research for improving recommender systems for learning,” in 1st International Conference Learning Analytics & Knowledge, 2011, pp. 44–53.
[15]R. Vatrapu, C. Teplovs, N. Fujita, and S. Bull, “Towards Visual Analytics for Teachers’ Dynamic Diagnostic Pedagogical
1939-1382 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TLT.2016.2626292, IEEE Transactions on Learning Technologies
14
Analytics and Knowledge, 2011, pp. 93–98.
[16]B. Florian, C. Glahn, H. Drachsler, M. Specht, and R. Fabregat,
“Activity-Based Learner-Models for Learner Monitoring and Recommendations in Moodle,” in Towards Ubiquitous Learning, Lecture Notes in Computer Science Volume 6964, C. Kloos Delgado, D. Gillet, R. M. García Crespo, F. Wild, and M. Wolpers, Eds. Springer Berlin Heidelberg, 2011, pp. 111–124.
[17]R. Guzmán, J. E. Jiménez, R. Ortiz, I. Hernández-valle, A. Estévez, M. Rodrigo, E. García, and A. Díaz, “Evaluación de la velocidad de nombrar en las dificultades de aprendizaje de la lectura,” Psicothema, vol. 16, pp. 442–447, 2004.
[18]J. L. Luque, S. Bordoy, A. Giménez de la Peña, M. López-Zamora, and V. Rosales, “Severidad en las dificultades de aprendizaje de la lectura: diferencias en la percepción del habla y la conciencia fonológica,” Escritos Psicol., vol. 4, no. 2, pp. 45–55, 2011.
[19]R. I. Nicolson and A. J. Fawcett, “Automaticity: a new framework for dyslexia research?,” Cognition, vol. 35, no. 2, pp. 159–182, 1990. [20]D. L. Lefly and B. F. Pennington, “Reliability and validity of the adult reading history questionnaire.,” J. Learn. Disabil., vol. 33, no.
3, pp. 286–296, 2000.
[21]U. Wolff and I. Lundberg, “A technique for group screening of
dyslexia among adults,” Ann. Dyslexia, vol. 53, no. 1, pp. 324–339,
2003.
[22]A. Giménez de la Peña, J. J. Buiza, J. L. Luque, and J. López,
“Cuestionario de Autoinforme de Trastornos Lectores para Adultos (ATLAS),” in XXVII Congreso de la Asociación Española de Logopedia, Foniatría y Audiología (AELFA 2010), 2010.
[23]P. Kirschner and J. J. G. van Merriënboer, “Do learners really know best? Urban legends in education,” Educ. Psychol., vol. 48, no. 3, pp. 169–183, 2013.
[24]T. Mortimore, Dyslexia and learning style -A Practitioner’s Handbook. John Wiley & Sons, Ltd, 2008.
[25]R. Cooper, “Making Learning Styles Meaningful,” J. PATOSS, vol. 19, no. 1, pp. 58–62, 2006.
[26]G. Reid, “Metacognition, learning style and dyslexia,” in 5th BDA International Conference, 2001.
[27]P. Sumner, “The labyrinth of learning styles,” J. PATOSS, vol. 19, no. 1, pp. 52–57, 2006.
[28]F. Coffield, D. Moseley, E. Hall, and K. Ecclestone, “Should we be using learning styles? What research has to say to practice,” London., 2004.
[29]R. M. Felder and B. A. Soloman, “Index of Learning Styles,” North Carolina State University, 2008. [Online]. Available: http://www.engr.ncsu.edu/learningstyles/ilsweb.html.
[30] N. Beacham, J. Szumko, and J. Alty, “An initial study of computer- based media effects on learners who have dyslexia. The Media Effects on Dyslexic Individuals in Academia ( MEDIA ) Project .,” Loughborough, 2003.
[31]M. Bruck, “Component Spelling Skills of College Students with Childhood Diagnoses of Dyslexia,” Learn. Disabil. Q., vol. 16, no. 3, pp. 171–184, 1993.
[32]T. Lachmann and C. Van Leeuwen, “Different letter-processing strategies in diagnostic subgroups of developmental dyslexia,” Cogn. Neuropsychol., vol. 25, pp. 730–744, 2008.
[33] E. Rojas, “Diseño y validación de un videojuego para el tratamiento de la dislexia (Ph.D. Thesis),” Universidad de La Laguna, La Laguna, Spain, 2008.
[34]T. A. Barker and J. K. Torgesen, “An evaluation of computer- assisted instruction in phonological awareness with below average readers,” J. Educ. Comput. Res., vol. 13, no. 1, pp. 89–103, 1995.
[35]B. W. Wise, J. Ring, and R. K. Olson, “Individual differences in gains from computer-assisted remedial reading.,” J. Exp. Child
IEEE TRANSACTIONS ON JOURNAL NAME, MANUSCRIPT ID
Psychol., vol. 77, no. 3, pp. 197–235, 2000.
[36] J. Taylor, A. de Roeck, A. Anderson, M. Sharples, T. Boyle, W. Hall,
T. Rodden, and D. Scott, “A Grand Challenge for Computing:
Learning for Life,” 2004.
[37]O. E. Hetzroni and B. Shrieber, “Word Processing as an Assistive
Technology Tool for Enhancing Academic Outcomes of Students General Classroom,” J. Learn. Disabil., vol. 37, no. 2, pp. 143–154, 2004.
[38] C. MacArthur, “Overcoming barriers to writing: computer support for basic writing skills,” Read. Writ. Quartely, vol. 15, pp. 169–192, 1999.
[39]A. Collins and J. S. Brown, “The Computer as a Tool for Learning Through Reflection,” in Learning Issues for Intelligent Tutoring Systems, no. 376, New York: Springer-Verlag, 1988, pp. 1–18.
[40]S. Bull, A. T. Mcevoy, and E. Reid, “Learner Models to Promote Reflection in Combined Desktop PC / Mobile Intelligent Learning Environments,” in Workshop on Learner Modelling for Reflection, International Conference on Artificial Intelligence in Education, 2003, vol. 5, pp. 199–208.
[41]B. Y. White, T. A. Shimoda, and T. Hall, “Enabling Students to Construct Theories of Collaborative Inquiry and Reflective Learning : Computer Support for Metacognitive Development,” Int. J. Artif. Intell. Educ., vol. 10, pp. 151–182, 1999.
[42]M. H. Raskind, R. J. Goldberg, E. L. Higgins, and K. L. Herman, “Patterns of Change and Predictors of Success in Individuals with Learning Disabilities: Results from a Twenty-Year Longitudinal Study,” Learn. Disabil. Res. Pract., vol. 14, no. 1, pp. 35–49, 1999.
[43]P. Brusilovsky and E. Millán, “User Models for Adaptive Hypermedia and Adaptive Educational Systems,” Adapt. Web, vol. LNCS 4321, pp. 3–53, 2007.
[44]S. Bull and J. Kay, “Metacognition and Open Learner Models,” in
Workshop on Metacognition and Self-Regulated Learning in Educational Technologies, International Conference on Intelligent Tutoring Systems, 2008, pp. 7–20.
[45]I.-H. Hsiao, S. Sosnovsky, and P. Brusilovsky, “Guiding Students to the Right Questions: Adaptive Navigation Support in an E- learning System for Java Programming,” J. Comput. Assist. Learn., vol. 26, no. 4, pp. 270–283, 2010.
[46]K. A. Papanikolaou, M. Grigoriadou, H. Kornilakis, and G. D. Magoulas, “Personalizing the Interaction in a Web-Based Educational Hypermedia System: The Case of INSPIRE,” User- Modeling User-Adapted Interact., vol. 13, no. 3, pp. 213–267, 2003.
[47]J. D. Zapata-Rivera and J. E. Greer, “Interacting with Inspectable Bayesian Models,” in International Journal of Artificial Intelligence in Education, 2004, vol. 14, pp. 127–163.
[48]A. Mabbott and S. Bull, “Student Preferences for Editing , Persuading , and Negotiating the Open Learner Model,” in Intelligent Tutoring Systems, vol. LNCS 4053, M. Ikeda, K. Ashley, and T.-W. Chan, Eds. Springer-Verlag Berlin Heidelberg, 2006, pp. 481–490.
[49]R. Morales, H. Pain, and T. Conlon, “Understandable Learner Models for a Sensorimotor Control Task,” in Intelligent Tutoring Systems, G. Gauthier, C. Frasson, and K. VanLehn, Eds. Springer- Verlag Berlin Heidelberg, 2000, pp. 222–231.
[50]S. Mohanarajah, R. H. Kemp, and E. Kemp, “Opening a Fuzzy Learner Model,” in Workshop on Learner Modelling for Reflection, International Conference on Artificial Intelligence in Education, 2005, pp. 62–71.
[51]B. Kump, C. Seifert, G. Beham, S. N. Lindstaedt, and T. Ley, “Seeing What the System Thinks You Know - Visualizing Evidence in an Open Learner Model,” in LAK 2012, ACM, 2012, p. 2012.
[52]R. Vatrapu, P. Reimann, and A. Hussain, “Towards Teaching Analytics: Repertory Grids for Formative Assessment,” in
Decision-Making,” in 1st International Conference on Learning
1939-1382 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TLT.2016.2626292, IEEE Transactions on Learning Technologies
AUTHOR ET AL.: TITLE
International Conference of the Learning Sciences (ICLS) 2012, 2012. [53]T. Elias, “Learning Analytics : Definitions , Processes and
Potential,” 2011.
[54]L. B. Donald Norris, J. Leonard, L. Pugliese, and P. Lefrere,
“Action Analytics: Measuring and Improving Performance That
Matters in Higher,” Educ. Rev., vol. 43, no. 1, 2008.
[55]Y. Engeström, “Expansive Visibilization of Work: An Activity- Theoretical Perspective,” Comput. Support. Coop. Work, vol. 8, no.
1, pp. 63–93, 1999.
[56]P. Dillenbourg and P. Jermann, “Technology for classroom
orchestration,”NewSci.Learn.,pp.525–552,2010.
[57] A. Zimmermann, M. Specht, and A. Lorenz, “Personalization and Context Management,” User Model. User-adapt. Interact., vol. 15,
no. 3–4, pp. 275–302, Aug. 2005.
[58]Y. Engeström, “Activity Theory as a Framework for Analyzing
and Redesigning Work,” Ergonomics, vol. 43, no. 7, pp. 960–974,
2000.
[59]B. Mirel, “General Hospital: Modeling Complex Problem Solving
in Complex Work System,” in 21st annual international confer-ence
on Documentation - SIGDOC ’03, 2003, pp. 60–67.
[60]H. Lindgren, “Towards Personalized Decision Support in the Dementia Domain Based on Clinical Practice Guidelines,” User
Model. User-adapt. Interact., vol. 21, no. 4–5, pp. 377–406, 2011. [61]C. Mejía, R. Fabregat, and D. Salas, “Integration of a Framework for Detection, Assessment and Assistance of University Students with Reading Difficulties with a Learning Management System,”
Rev. EAN, vol. 79, pp. 98–115, 2015.
[62]M. Callens, W. Tops, and M. Brysbaert, “Cognitive profile of
students who enter higher education with an indication of
dyslexia.,” PLoS One, vol. 7, no. 6, p. e38081, Jan. 2012.
[63]H. L. Swanson and C.-J. Hsieh, “Reading Disabilities in Adults: A SelectiveMeta-AnalysisoftheLiterature,”Rev.Educ.Res.,vol.79,
no. 4, pp. 1362–1390, 2009.
[64]G. R. Lyon, S. E. Shaywitz, and B. A. Shaywitz, “A definition of
dyslexia,” Ann. Dyslexia, vol. 53, no. 1, pp. 1–14, 2003.
[65]S. Miller-Shaul, “The characteristics of young and adult dyslexics readers on reading and reading related cognitive tasks as compared to normal readers.,” Dyslexia Chichester Engl., vol. 11,
no. 2, pp. 132–151, 2005.
[66]F. Ramus, S. Rosen, S. C. Dakin, B. L. Day, J. M. Castellote, S.
White, and U. Frith, “Theories of developmental dyslexia: insights from a multiple case study of dyslexic adults,” Brain A J. Neurol., vol. 126, no. Pt 4, pp. 841–865, 2003.
[67] S. E. Shaywitz, R. Morris, and B. A. Shaywitz, “The education of dyslexic children from childhood to young adulthood.,” Annu. Rev. Psychol., vol. 59, no. 1, pp. 451–475, 2008.
[68]N. Firth, E. Frydenberg, and D. Greaves, “Perceived control and adaptive coping : programs for adolescent students who have learning disabilities,” Learn. Disabil. Q., vol. 31, no. 3, p. 151, Jul. 2008.
[69]D. Mellard, E. Fall, and K. Woods, “A path model of reading comprehension for adults with low literacy,” J. Learn. Disabil., vol. 43, no. 2, pp. 154–165, 2010.
[70]J. Hatcher, M. J. Snowling, and Y. M. Griffiths, “Cognitive assessment of dyslexic students in higher education,” Br. J. Educ. Psychol., vol. 72, no. Pt 1, pp. 119–133, 2002.
[71]J. E. Jiménez, N. Gregg, and A. Díaz, “Evaluación de habilidades fonológicas y ortográficas en adolescentes con dislexia y adolescentes buenos lectores Assessment of sublexical and lexical processing of Spanish young adults with reading disabilities and young adults normal readers,” Infanc. y Aprendiz., vol. 27, no. 1, pp. 63–84, Feb. 2004.
[72]R. Vatrapu, P. Reimann, M. Johnson, and S. Bull, “An Eye- Tracking Study of Notational, Informational, and Emotional
15
Aspects of Learning Analytics Representations. Proceedings of the,” in Third International Conference on Learning Analytics and Knowledge (LAK), 2013.
[73]S. Bull and J. Kay, “Student Models that Invite the Learner In: The SMILI Open Learner Modelling Framework,” Int. J. Artif. Intell. Educ., vol. 17, no. 2, pp. 89–120, 2007.
[74]S. Bull and M. Britland, “Group Interaction Prompted by a Simple Assessed Open Learner Model that can be Optionally Released to Peers,” in PING Workshop, User Modeling 2007, 2007.
[75]D. Suthers, R. Vatrapu, R. Medina, S. Joseph, and N. Dwyer, “Beyond Threaded Discussion: Representational Guidance in Asynchronous Collaborative Learning Environments,” Comput. Educ., vol. 50, no. 4, pp. 1103–1127, 2008.
[76]O. Scheuer, F. Loll, N. Pinkwart, and B. McLaren, “Computer- supported argumentation: A review of the state of the art,” Int. J. Comput. Collab. Learn., vol. 5, no. 1, pp. 43–102, 2010.
[77]R. A. Allred, “Gender differences in spelling achievement in Grades 1 through 6,” J. Educ. Res., vol. 83, no. 4, pp. 187–193, 1990. [78]S. Newman, H. Fields, and S. Wright, “A developmental study of specific spelling disability.,” Br. J. Educ. Psychol., vol. 63 ( Pt 2), pp.
287–296,1993.
[79]J. W. Rojewski, “Occupational and Educational Aspirations and
Attainment of Young Adults With and Without LD 2 Years After High School Completion,” J. Learn. Disabil., vol. 32, no. 6, pp. 533– 552, Nov. 1999.
[80] B. Heyman, J. Swain, and M. Gillman, “Organisational simplification and secondary complexity in health services for adults with learning disabilities,” Soc. Sci. Med., vol. 58, no. 2, pp. 357–367,Jan.2004.
[81]A. Holzinger, M. Kickmeierrust, S. Wassertheurer, and M. Hessinger, “Learning performance with interactive simulations in medical education: Lessons learned from results of learning complex physiological models with the HAEMOdynamics SIMulator,” Comput. Educ., vol. 52, no. 2, pp. 292–301, Feb. 2009.
Carolina Mejía received the PhD degree in Technology from the Univer- sitat de Girona, Spain, in 2013. She currently is Professor in the School of Studies in Virtual Environments at the EAN University, Colombia. She has an extensive experience doing active research and development in tech- nology-enhanced learning (TeL) and related areas, including adaptive sys- tems, user modelling, inclusive education, and learning analytics.
Beatriz Florian received the PhD degree in Technology from the Univer- sitat de Girona, Spain, in 2013. She currently works as an assistant profes- sor at the Computation and Systems Engineering School, University of Valle, Colombia. Her current research interests include adaptive hyperme- dia systems, e-assessment, learning analytics, and recommender systems.
Ravi Vatrapu received the PhD degree in Communication and Infor- mation Sciences from the University of Hawai‘i at Mānoa, USA, in 2007. He currently is Professor in the Department of IT Management at the Co- penhagen Business School, Denmark. He also is Professor in the Faculty of Technology at the Westerdals Oslo ACT, Norway.
Susan Bull received the PhD degree in Artificial Intelligence in Education from the University of Edinburgh in 1997, focussing on open learner mod- els. She currently is Professor in the School of Electronic, Electrical and Computer Engineering, Birmingham University, UK.
Sergio Gómez received the Ph.D. degree in Technology from the Univer- sitat de Girona, Spain, in 2013. He currently is Coordinator of a Master Program in Techologies applied to education in the Virtual Learning Unit at the University of Manuela Beltran, Colombia.
Ramon Fabregat received the PhD degree in Industrial Engineering from the Universitat de Girona, Spain, in 1999. He is an associate professor in the Department of Architecture and Computer Science at the Universitat de Girona. He is a codirector of the Broadband Communications and Dis- tributed Systems Research Group and a member of the Institute of Infor- matics and Applications at the University of Girona, Spain.
1939-1382 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 18, NO. 12, DECEMBER 2012 2421
Does an Eye Tracker Tell the Truth about Visualizations?: Findings while Investigating Visualizations for Decision Making
Sung-Hee Kim, Zhihua Dong, Hanjun Xian, Benjavan Upatising, and Ji Soo Yi, IEEE Member
(a) SimulSort (b) Typical Sorting
Fig. 1. Comparing two screenshots of the total aggregated fixation duration of 10 participants for 10 trials. The red area indi- cates longer duration of fixations. The two interfaces compared are (a) SimulSort, a tabular visualization with simultaneously sorted columns, and (b) Typical Sorting, a table with a one-column sorting feature.
Abstract—For information visualization researchers, eye tracking has been a useful tool to investigate research participants’ under- lying cognitive processes by tracking their eye movements while they interact with visual techniques. We used an eye tracker to better understand why participants with a variant of a tabular visualization called ‘SimulSort’ outperformed ones with a conventional table and typical one-column sorting feature (i.e., Typical Sorting). The collected eye-tracking data certainly shed light on the detailed cognitive processes of the participants; SimulSort helped with decision-making tasks by promoting efficient browsing behavior and compensatory decision-making strategies. However, more interestingly, we also found unexpected eye-tracking patterns with Simul- Sort. We investigated the cause of the unexpected patterns through a crowdsourcing-based study (i.e., Experiment 2), which elicited an important limitation of the eye tracking method: incapability of capturing peripheral vision. This particular result would be a caveat for other visualization researchers who plan to use an eye tracker in their studies. In addition, the method to use a testing stimulus (i.e., influential column) in Experiment 2 to verify the existence of such limitations would be useful for researchers who would like to verify their eye tracking results.
Index Terms—Visualized decision making, eye tracking, crowdsourcing, quantitative empirical study, limitations, peripheral vision.
     1 INTRODUCTION
An eye tracker is a potentially useful tool for information visualization (InfoVis) researchers because its basic premise is that it can tell where a person looks. In addition, as long as the “eye-mind hypothesis” [34] holds, eye-tracking results can reveal the underlying cognitive pro- cesses of a human user. In this case, the eye is literally the window of the mind. For this particular reason, some InfoVis researchers who are interested in the cognitive aspects of a visualization user often rely on eye-tracking methods (e.g., [6, 11, 46, 32]). In addition, visualization tools have been proposed to analyze eye-tracking data (e.g., [48]).
We are also researchers who would like to see the person’s mind while investigating visualization tools supporting multi-attribute deci- sion making, where one has to choose the best option among many candidates after reviewing the multiple attributes of each candidate (e.g., choosing a college or a nursing home). Since such multi-attribute decision making often involves overwhelming information and labo- rious cognitive processes, various visualization techniques have been proposed (refer to [25] for reviews). Some recent empirical evidence
• Sung-Hee Kim, Zhihua Dong, Benjavan Upatising, and Ji Soo Yi are in the School of Industrial Engineering at Purdue University. E-mail: {kim731, dong17, benjavan, yij}@purdue.edu.
• Hanjun Xian is in the School of Engineering Education at Purdue University. E-mail: hxian@purdue.edu.
Manuscript received 31 March 2012; accepted 1 August 2012; posted online 14 October 2012; mailed on 5 October 2012.
For information on obtaining reprints of this article, please send
e-mail to: tvcg@computer.org.
also has demonstrated that such techniques lead to better decision quality and satisfaction [1, 35, 38, 40, 15]; however, the gap in the pre- vious literature is that there is no empirical explanation of how these visualization techniques have helped with decision making beyond a simple confirmation of their effects. For example, studies using a vi- sualization tool called SimulSort (or SS) [16, 15] empirically showed that the participants who used SS made higher-quality decisions in a shorter amount of time than made the participants who used a regular table with a typical single-column sorting technique: Typical Sorting (or TS); however, these empirical studies cannot clearly explain how it happened.
To fill this gap, in this paper, we conducted an eye-tracking study to investigate how visual aids influenced the participants’ browsing behaviors and decision-making strategies that eventually influence de- cision quality [10, 29]. The eye-tracking study partially showed that the decision quality difference actually came from the changes in the decision strategies that the participants employed. Though this finding is only meaningful to a relatively small number of researchers who would like to combine InfoVis and decision science, such a finding is one of the first pieces of empirical evidence showing the how part and also one of major contributions of this paper.
Interestingly, we had another unexpected finding of potential value to a larger audience. While conducting the study (Experiment 1), we came across unexpected results: We believed that a certain part of the visualization interface was seen by participants, but the eye tracker did not capture it. To verify our suspicion, we conducted an additional crowdsourcing-based study (Experiment 2). It revealed that our suspi- cion was correct, and it turned out to be clear evidence of a limitation of the eye-tracking method: the incapability of capturing peripheral
 1077-2626/12/$31.00 © 2012 IEEE
Published by the IEEE Computer Society
2422 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 18, NO. 12, DECEMBER 2012
vision. In addition, the method to use a testing stimulus for Experi- ment 2 (influential column for our experiment) could be instrumental to other InfoVis researchers who would like to confirm the validity of their eye-tracker results.
In summary, the contributions of this paper are as follows:
• We believe this paper is one of the first eye-tracker studies em- pirically showing how visual aids can promote decision quality and efficiency;
• Weidentifiedalimitationofeye-trackingmethodsinunderstand- ing the effects of visual aids: the incapability of capturing periph- eral vision; and
• We suggested an approach to overcome the limitation: embed- ding testing stimuli (an influential column in our case).
2 BACKGROUND
for multi-attribute decision making. Parallel bargrams is an example that succeeds a generic tabular form and sorts all the attributes in paral- leled rows at the same time [53]. It is designed to help consumers with multi-attribute mechanized purchasing decisions by providing simul- taneously comparable attribute values for different alternatives. FO- CUS [45], EZChooser [53], and InfoZoom [44] are some visualization tools that apply the idea of parallel bargrams.
2.1.3 SimulSort
SimulSort (SS) is another visualization technique that also aims to help with daily multi-attribute decision making [16]. As shown in Figure 2(a), SS presents all columns sorted simultaneously so that one can see the relative values or utilities (i.e., pros and cons) of an alter- native over multiple attributes. This visual representation is expected to avoid the constant shuffling of rows induced by sorting a column in the TS interface (i.e., Figure 2(b)) and to offer insights to users by pre- senting the trend of the data at a glance; however, since it preserves the tabular form that reveals the values, there is a limitation of the number of alternatives and attributes that could be represented on the screen without additional interaction techniques (e.g., zooming).
A controlled laboratory study successfully demonstrated that par- ticipants using SS made higher-quality decisions (see Section 6.5 for the definition of “decision quality”) [15]; however, as discussed in the Introduction section, the controlled lab study failed to clearly show how the two representations caused the differences. A post-task sur- vey was given to ask what kinds of strategies the participants used dur- ing the task, but the descriptions of their strategies were unfortunately vague. We felt that a more systematic investigation was necessary to better understand the influence of different visual representations on decision-making quality.
Note that the versions of SS and TS used in the present study were much simpler than the original version [16] because additional features (i.e., horizontal bargram, multiple selection, filtering, and zoom in/out) actually became compounding factors in pilot studies; therefore, they were disabled for this study.
(a) SimulSort interface
(b) Typical Sorting interface
Fig. 2. Example of the two interfaces, SimulSort and Typical Sorting, comparing two alternatives; item 15 is highlighted in green, and item 5 is highlighted in yellow. (a) SimulSort: The comparison of the two items can be done by comparing the vertical positions of the highlighted cells. (b) Typical Sorting: The comparison of the two items can be done by reading the face values of all of the cells.
2.2 Two Potential Explanations
Based on our observations of how the participants used SS and TS, we came up with two general explanations for the performance difference between the two representations, (1) efficient browsing behavior and (2) different decision strategies, each of which is further described in subsequent sections.
2.1
2.1.1
Multi-attribute Decision Making and InfoVis
Multi-attribute Decision Making
Multi-attribute decision making means making a preference decision over all available alternatives that are characterized by multiple at- tributes [17]. It can be expressed in a matrix format, where rows repre- sent alternatives, and columns represent the attributes considered [55]. Multi-attribute decision making is intrinsically difficult because multi- ple attributes can conflict with each other, which often requires trade- off decisions. One also may need to consider his or her own preference regarding different attributes to assign weights to them; furthermore, different attributes usually have different units, which makes the selec- tion task even harder since it is very difficult to come up with an easy equation to calculate the value of each alternative. Using car selec- tion as an example, mileage is measured in miles, gas mileage in miles per gallon, and price in dollars while maker information is marked by brands. A customer often faces cars either low in mileage but high in price or low in price but also low in safety and tries to remember their makers and equipment at the same time. There is no simple algorithm that could help with the decision making; thus, multi-attribute deci- sion making in everyday life often involves high cognitive load and eventually induces a person to make a trade-off between effort and decision quality [2, 7]. Sometimes, decision makers even weigh ef- fort reduction higher than decision-quality maximization [21], which unfortunately lead them to make sub-optimal decisions.
2.1.2 Visualization Techniques
To alleviate such difficulties, various InfoVis techniques have been uti- lized and have helped with the decision-making process by presenting insights more interpretable and by lowering the cognitive load for de- cision makers [54]. We narrowed our interest to methods that solved the problem of representing multi-attribute data sets.
Parallel coordinates is an example of a classic approach to project- ing hyper-dimensional data onto a 2-D display. The attributes are rep- resented as parallel axes to each other, and a data point with multiple attributes is visualized as a poly line connecting each data dot on each axis [18, 51]. This technique is known to be effective in visualizing large multi-attribute data sets because it provides an overview of the data trend, which may help multi-attribute decision making.
Although parallel coordinates performs well at presenting high- dimensional data, the fact that it lacks a tabular view limits its appli- cation for helping with daily decision-making tasks. This is because the tabular form of presenting data with sorting features, such as Mi- crosoft Excel, not only is familiar to general users but also makes each data point visible. The data points in parallel coordinates are initially hidden and require additional interactions in order to retrieve the data.
In contrast, a problem with tabular data presentation is one-column sorting; as the sorting is done for a single column, data sequences for other columns are changed accordingly. This brings difficulty for users doing comparisons among alternatives since they keep losing the con- text of the previous sorting results as they have to sort the columns several times separately. Other visualization tools combining the ad- vantage of parallel coordinates and tabular form offer better solutions

KIM ET AL: DOES AN EYE TRACKER TELL THE TRUTH ABOUT VISUALIZATIONS?: FINDINGS WHILE INVESTIGATING... 2423
2.2.1 Efficient Information Browsing
A potential benefit of SS (also found in other InfoVis tools [12, 50]) is that users can get an overview of multiple alternatives and their associ- ated attributes through instantly grasping visual patterns without read- ing detailed information. Pattern detection includes detection of data distribution, trends, outliers, or other structures of a data set [54]. This pattern detection could happen within peripheral vision, especially for an interface that includes visual presentation rather than just text be- cause pictorial presentation can convey more useful visual information and increase the radius of what can be captured by the peripheral vi- sion [33]. Acquiring information using peripheral vision and pattern detection can help participants to grasp information quickly and to guide their focus to more worthy information [33, 54].
Taking SS as shown in Figure 2(a) as an example, one can quickly sense that item 5 (highlighted in yellow) is better than item 15 (high- lighted in green) without reading the actual values of all of the corre- sponding cells because the yellow cells are generally located above the green cells. In contrast, while using TS, one needs to read numbers on cells for items 5 and 15 and to compare them as shown in Figure 2(b). This could explain why the SS participants made faster decisions than the TS participants; however, it cannot fully explain why the SS partic- ipants made higher-quality decision outcomes. One might argue that the SS participants who relied on visual trends read less accurate in- formation than the TS participants, who relied on actual numbers. SS even induces distortion between two values; for example, the green cell and the yellow cell in the m4 column in Figure 2(a) are two cells apart even though the difference in value is just one. Thus, if efficient information browsing were the only advantage of SS over TS, the de- cisions of the TS participants should have been more accurate than or equivalent to those of the SS participants, but our previous experiment showed the opposite results [15]; therefore, we need to find another explanation.
2.2.2 Different Decision Strategies
Another potential advantage provided by visualization tools for deci- sion making is that these tools promote better decision-making strate- gies, which associated with how an individual would process given information to make a choice [31].
In order to understand different decision strategies, studies in de- cision science should be briefly reviewed. Over the past 30 years, several researchers from decision science and behavioral economics have researched how people actually make decisions. There have been several strategies introduced in the context of multi-attribute decision making [30, 31, 39], and they are categorized into the following two groups: compensatory strategies and non-compensatory strategies.
Compensatory Decision Strategies. If a decision maker ap- plies compensatory strategies, it means that by considering all of the attributes, a low value of one attribute can be compensated by a high value of another attribute; therefore, the final value of an alternative is calculated based on the trade-off among all of the attributes. Com- pensatory strategies are known to be closer to the normative approach, which leads to higher accuracy as all of the relevant information is pro- cessed. For example, a Weighted Additive (WADD) strategy would calculate the value of an alternative by the sum of each attribute multi- plied by the weight given to that attribute. An Equal Weighted (EQW) strategy is a particular case in which all of the attributes are equally weighted.
Non-compensatory Decision Strategies. In contrast, applying non-compensatory strategies means that an alternative can be elimi- nated from a set of candidates simply because one of its attributes has a lower value even though values in the other attributes might have high values. For example, Elimination by Aspects (EBA) begins by determining important attributes and eliminating alternatives that do not fulfill the cutoff values for the attributes [49]; therefore, not all of the attributes or alternatives get attention, which often leads to a relatively lower decision quality.
Although compensatory strategies lead to better decision outcomes, researchers found that people often deviated from compensatory
strategies. An individual decision maker has a limited cognitive ca- pacity that restrains information processing at a certain level. Because compensatory strategies are more cognitively demanding, people take mental shortcuts to reduce the amount of data to process [9, 42]. Some of these shortcuts are non-compensatory strategies, where the attributes to be considered are selectively chosen, leading people to consider fewer attributes and alternatives.
Given the two general categories of decision strategies (i.e., com- pensatory and non-compensatory), we suspected that the two visual aids (SS and TS) may lead to choose one or the other decision strategy. In order to test this hypothesis, we employed a measure of information search process, called “depth of search,” [52]; therefore, the decision strategies could be examined according to the different amounts of information searched. Depth of search reflects the amount of informa- tion processed, which could be interpreted as the number of attributes considered. In order to select the optimal solution, one has to con- sider the trade-off among all of the attributes; therefore, the number of attributes considered could be an indirect metric of the likelihood of employing compensatory strategies: the more attributes considered, the more likely compensatory strategies were applied.
2.3 Process Tracing
In order to verify the two potential explanations (i.e., Efficient Infor- mation Browsing and/or Different Decision Strategies), we needed to employ process-tracing techniques:
First, verbal protocol (think-aloud) analysis was briefly considered; however, our pilot study showed that generating concurrent or retro- spective verbal reports either interfered with visually taxing decision making or were not reliable, as was reported in previous literature [4].
Second, information board (or its electronic version, Mouse- lab) [20], which records the acquisition of information represented in a matrix form, is widely used among decision scientists. In Mouselab, the value of each cell is presented in a box that is first hidden. The value is revealed when the mouse cursor is over the cell and covered again when the cursor leaves the cell. Although it captures all of the data acquisition behavior explicitly, it does not seem to be appropri- ate because it allows a participant to retrieve only one value at a time, which defies the main purpose of visualization: allowing one to see the overall trends. It has also been shown that the information board method actually affects people to exert a certain strategy [23].
Thus, we ended up selecting the eye-tracking method because it al- lowed us to capture more natural behaviors [23]. Although it heavily relies on the eye-mind hypothesis [34], Glaholt Reingold [10] demon- strated that eye movements in decision-making processes could reflect the screening and evaluation of different alternatives. Previous work also has shown that eye tracking can identify the visual exploration behaviors in different visualizations [6, 11].
Among the various metrics employed in eye-tracking studies (e.g., gaze, fixation, and pupil dilation), we particularly employed fixation since it has been widely used to assess participants’ cognitive pro- cesses [19, 26]. Fixation duration reflects the task difficulty and infor- mation complexity [28, 33]. A longer duration indicates that the task complexity and difficulty is higher. We also employed visit as another metric, which is defined as the aggregated fixations and saccades of an individual visit to an Area of Interest (AOI) [47]. Fixation and visit are slightly different, as shown in Figure 3. Fixation duration within an AOI is the sum of the duration of all of the individual fixations in the corresponding AOI, while visit duration comprises all of the fixa- tions that occurred in one visit within the AOI as well as the saccadic duration among those fixations within that AOI until fixation is placed outside of the AOI. With eye-tracker data based on AOIs, we could examine the interaction between the participants and the attributes.
We defined AOIs by columns, as shown in Figure 4, because in the TS setting, the rows were shifting throughout the process as the partic- ipants tried to sort the different columns. With the AOIs fixed and the rows changing, it was challenging to capture gaze data within certain alternatives. Even though one could trace programmatically or man- ually the locations of the dynamically moving cells, detecting which cell was gazed at at any given moment was challenging due to the short
2424 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 18, NO. 12, DECEMBER 2012 4 EXPERIMENT 1: EYE-TRACKING STUDY
 Fig. 3. Definition of fixation and visit in the context of an AOI.
height of each cell (i.e., 20 pixels or 6 mm). In order to overcome such a limitation, we considered and captured mouse hovering data on all of the cells as another process tracing data source; however, we ended up ignoring the mouse hovering data because the SS participants were forced to hover over a cell to see what the associated attributes were while the TS participants were not (they used mouse hovering volun- tarily for marking purposes).
Fig. 4. A screenshot of SS, where all of the columns are sorted simulta- neously. Each highlighted color (i.e., green and yellow) corresponds to one item (i.e., items 14 and 10, respectively). The eight red boxes show how the AOIs are defined.
3 HYPOTHESES
Based on the above discussion of the two potential explanations for why SS has performed better than TS, we investigated browsing be- havior and decision strategies, respectively, using eye-movement data. If the participants applied peripheral vision to gain the trend infor- mation, then we expected to see shorter fixation duration and higher fixation counts as trend reading requires a lower cognitive load than reading and comparing numbers and promotes more scanning through data. If one applied the optimal compensatory strategy, then he or she should have evaluated each alternative, considering all of the at- tributes. We hypothesized that SS would promote the compensatory strategy with higher depth of search and less effort. Having adopted this compensatory strategy, the visit count should have been higher and uniformly distributed among the attributes.
H1 EfficientbrowsingbehaviorwouldappearmoreinSSthaninTS.
H1a FixationdurationsinSSwouldbeshorterthanthoseinTS. H1b FixationcountsinSSwouldbehigherthanthoseinTS.
H2 Compensatory strategies would be promoted more in SS than in TS.
H2a VisitcountwouldbehigherinSSthaninTS.
H2b Visit count would be more uniformly distributed in SS than in TS.
The goal of the eye-tracking study was to understand the underly- ing differences in cognitive processes while using different interfaces to make decisions. We replicated a controlled laboratory experiment done by Hur et al. [15] with the two interfaces: SS and TS. We also chose the same between-subject design to minimize the influence of extraneous factors. Within-subject design would have the risk of car- ryover effects, where the first interface could influence the strategies used with the following interface.
4.1 Participants
A total of 20 participants (11 females and 9 males) were recruited from undergraduate and graduate students at Purdue University. All of the participants had not participated in the previous studies [15, 16]. Seventeen of them were from engineering, two from science, and one from finance. Their average age was 24.2 years, ranging from 18 to 30 years. The participants earned $13.16 on average, depending on their performances. This will be explained more in Section 4.6.
4.2 Apparatus
An eye tracker, Tobii X60, was used to track the participants’ eye movements. Eye gazes on the screen were recorded at a sampling rate of 60Hz. The participants sat in front of a 19-inch computer screen at an approximate distance of 65 cm.
4.3 Procedure
Upon arriving, the participants were given instructions on how to use the interface and the goal of the given task. After instruction, the par- ticipants were asked to perform a nine-dot calibration with the eye tracker. Then, the participants completed a total of 15 rounds of tasks with the eye tracker monitoring their eye movements. To minimize the learning effect shown in the previous laboratory experiment, the first five rounds were regarded as practice and were not included in the analysis; however, in order to promote serious participation, the participants were not told that the first five rounds were for practice. We stopped the participants after every five rounds to let them take a rest and to recalibrate the eye tracker. This was to avoid the par- ticipants’ fatigue after the intense experiment sessions and to ensure the quality of the eye-tracker data. An open-ended interview was con- ducted after the participants finished all of the 15 rounds of tasks to better understand the strategies they used to complete the tasks as well as their opinions toward the assigned interface (i.e., TS or SS). At the end of the experiment, the participants filled out a simple demographic survey regarding their age, gender, and education information.
4.4 Tasks
For each round, the participant was asked to select the alternative with the highest utility out of 15 alternatives (i.e., rows) after considering seven attributes (i.e., columns). The term utility used here is borrowed from economics and decision science, where utility is a measure of sat- isfaction and is different from the face value of a cell. For example, the same face value of 40 means completely different things depending on which attribute it belongs to; 40 miles per hour is relatively high fuel efficiency (i.e., high utility), but 40 horsepower is relatively low horse- power (i.e., low utility) according to the standards of modern vehicles. To mimic such a reality, the attribute-wise utility of a face value of a cell was normalized within that attribute between the maximum and the minimum values of the attribute. The utility of an alternative was the summation of the attribute-wise utilities for the seven attributes, as shown in Equation 1:
7 7 Ti j − min T. j
utilityi = ∑utilityij = ∑ maxT −minT , (1)
j=0 j=0 .j .j
where utilityi is the ith alternative’s utility, utilityi j is the jth attribute- wise utility of the ith alternative, and Tij is the face value in the jth
attribute of the ith alternative in data set T . Due to the normalization, the range of the attribute-wise utility was [0, 1], and the range of utility

KIM ET AL: DOES AN EYE TRACKER TELL THE TRUTH ABOUT VISUALIZATIONS?: FINDINGS WHILE INVESTIGATING... 2425
 was theoretically [0, 7]; however, the maximum utilities of the alterna- tives of each round were different from each other and generally lower than 7.0 because the data sets were randomly generated in a way that there was no obvious best alternative (more details in Section 4.5). Although this calculation sounds intimidating, it turned out to be a re- alistic task; the participants needed to select an alternative that had as high an attribute-wise value as possible.
We also removed any contextual information from the table because it could bring in the participants’ biases in considering a certain col- umn to be more important than another (e.g., fuel efficiency could be more important than price), which would make it difficult to compare different individuals’ data. This kind of context-free experimental task has been widely used in other decision science and economic studies (e.g., [41]). Each task had a time limitation of 3 minutes.
4.5 Data Sets
The same 15 data sets used in the previous study [15] were used for this study. Each data set had 15 alternatives (i.e., rows) and seven attributes (i.e., columns) with a two-digit numerical value from 10 to 99. The dif- ficulty of each round was restricted to be similar by controlling the Av- erage of Inter-attribute Correlations (AIAC) value [24]. The AIAC was obtained by computing the average of the correlations among all of the combinations of the two columns out of all of the columns. When the AIAC is lower, selecting the best alternative is more difficult because it leads to considering more trade-off due to negative correlations be- tween columns. In contrast, when the AIAC is higher (maximum = 1), selecting the best alternative is easier because there are fewer trade- off situations, and the best option becomes obvious [8, 13]; thus, data sets with high AIAC would make the decision-making task over-easy, which would bias our study by making non-compensatory strategies sufficient for solving all of the problems. Therefore, in our study, the AIAC value was controlled around 0.01 to generate the appropriate level of difficulty [15]. Because of this, if one did not consider all of the columns, it would decrease the overall decision quality.
4.6 Rewards
The participants’ earnings were proportional to the utilities of the final alternatives selected, and the theoretical maximum earning for each round was $7.00. In reality, the participant was paid based on the util- ity of two randomly selected rounds out of the 15 rounds. This quota scheme payment was known to motivate the participants by increasing the perceived benefit for each round [3].
4.7 Measurements
In order to examine the depth of search while using the two interfaces, we chose to see how the participants interacted with the alternatives and the attributes. The eye-tracker data were used to capture their in- teractions with different attributes. Since we defined each column as a separate AOI (i.e., each red box, as shown in Figure 4), the analysis of the fixation and visit data based on the AOI could be used to examine how the participants perceived the attributes.
Fig. 5. Interval plot of average fixation duration (left). Interval plot of fixation count (right).
count between SS and TS suggests that the participants had quite dif- ferent reading patterns for these two interfaces. With lower fixation duration (i.e., H1a confirmed) and higher fixation count (i.e., H1b con- firmed), we can assume that while using SS, the participants spent less time retrieving information from each data point but visited more data points throughout the process.
Decision scientists suggest that decision processes can be decom- posed into a sequence of events, such as reading the values of two alternatives of an attribute, comparing the two values, calculating the difference, adding the values for the attributes, and so forth [21]. Dur- ing the decision-making process, the participants went through a series of such sub-tasks that had different cognitive loads. We assumed that the two interfaces associated with these sub-tasks comprised different patterns. Combining the participants’ interview data, we found that the SS participants gained maximum information by scanning the rel- ative positions of the green and yellow cells within one column and by reading the actual numbers when they wanted to acknowledge the numerical difference; therefore, the visual bars helped the participants in reading the values of the two alternatives and in comparing them by the participants just looking at the visual difference. The TS partici- pants were likely to read and to remember the actual numbers when they compared attribute values between two alternatives. As people have limited cognitive capacity, the visualization could have helped unburden some subtasks, which would leave some cognition room for processing more information. In the task in which comparison was im- portant, browsing behavior occurred more for SS users, which could be the reason for shorter fixation duration. Eventually, the participants could consider more attributes and alternatives with the SS interface.
5.2 Heatmaps
The overall patterns of eye movements in the SS and TS conditions are shown in Figure 1. The total fixation duration for each interface was aggregated for all of the tasks completed by 10 participants. Figure 1 shows that there were distinctive differences in the eye movement pat- terns between the two interfaces. In TS, the fixations had longer du- ration in the top areas of the columns, which might indicate that the participants focused on reading the maximum value of a sorted col- umn. In addition, the total duration of fixations looked stronger in the left columns (i.e., in AOI 2, 3, 4, and 5) than in the right columns (i.e., in AOI 6, 7, and 8), suggesting the participants might have spent more time comparing values within the columns on the left than within those on the right. This could be due to the nature of most of the participants’ reading patterns in which they started from the left and continued to right, which also has been shown commonly in reading web content [43]. Additionally, according to satisficing theory [5], an individual tends to make decisions based on information that is imme- diately perceptible; the options listed first in a sequence have a higher probability of being selected, while the last options listed may not even be observed. In contrast, with SS, the total duration of the fixations in the top areas of the columns was weaker, and, interestingly, the cen- ter columns (i.e., AOI 4, 5, and 6) received more fixations than the
5
5.1
RESULTS AND DISCUSSION Fixation Duration and Count
A fixation occurs when the foveal attention is focused on a particular
object. Fixation duration and fixation frequency are important metrics for revealing the cognitive load of users and the perceived importance of interface elements. We looked at the fixation count and fixation duration that occurred in the AOIs to examine the visual interaction patterns during the tasks.
We employed a repeated measures ANOVA test with a within- subject factor (i.e., AOIs) and a between-subject factor (i.e., visual- izations: SS vs. TS), and the results are depicted in Figure 5. First, the mean of fixation duration for SS was 0.36 seconds and 0.43 sec- onds for TS. The fixation duration for SS was significantly shorter than that for TS (F (1, 18) = 5.28, p = 0.0338). Second, the mean average fixation count for SS was 47.1 and 36.1 for TS. The fixation count for SS was significantly higher than that for TS (F (1, 18) = 6.17, p = 0.0231). The significant difference of fixation duration and fixation

2426 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 18, NO. 12, DECEMBER 2012
other columns even though we expected the seven columns to be more evenly browsed. To better understand the implications of the differ- ences, we analyzed the visit count data from the eye tracker.
5.3 Visit Count
Fig. 6. Interval plot of average visit count for the two interfaces.
Since a visit was defined as one observation within an AOI, and an AOI was defined by a column (Figure 4), visit count was a good met- ric to examine how users switched their fixations among different at- tributes. As stated in H2, we expected to see higher visit counts (H2a) with more evenly distributed patterns in SS than in TS (H2b). When we applied repeated measures ANOVA to visit count, both interface type (F (1, 18) = 15.50, p = 0.0010) and AOI (F (7, 126) = 27.78, p < 0.0001) came out to be significant main effects. The mean of visit count (for one participant and one task round) for the SS interface was statistically significantly higher with 28.3 for SS and 18.5 for TS. As in Figure 6, we saw that SS had a higher visit count for the seven attributes compared to TS (i.e., H2a was confirmed).
However, the center-column total fixation duration for SS, shown in both Figures 1 and 6, was an unexpected result that disconfirmed H2b. This could be because the SS participants considered the mid- dle columns to be more important and overlooked the columns on the sides, or they could have used their peripheral vision to look at the overall trend while generally fixating on the center of the screen. Sim- ply based on the eye-tracker data, which only tracked the gaze points, we could not determine which explanation was true; thus, we con- ducted a second experiment to further investigate these possibilities.
6 EXPERIMENT 2: CROWDSOURCING-BASED STUDY
This experiment was conducted to supplement the results from Ex- periment 1 discussed in Section 5.3, where we could not clearly see whether the SS interface promoted consideration of more attributes. In order to triangulate our confirmation of H2a and to further test whether SS promoted consideration of attributes in a more uniformed way (H2b), the data set was designed to capture the behavior of over- looking certain attributes in certain column positions.
We conducted the experiment through Amazon Mechanical Turk (MTurk), a well-known crowdsourcing platform. The crowdsourcing approach has several advantages over conventional, controlled labora- tory studies [22, 37], including recruiting a large number of partici- pants with diverse backgrounds in a more natural environment.
6.1 Participants
6.1.1 Demographic Summary
A total of 176 participants were originally recruited through MTurk, but 57 were identified as outliers and were excluded from the analysis (see Section 6.1.2 for details). The remaining, legitimate participants comprised 58 for the SS interface and 61 for the TS interface. Due to the analysis, we also removed additional data from 19 participants (i.e., 8 from SS and 11 from TS) to conveniently end up with a bal- anced data set (i.e., 50 for each group).
The remaining 100 participants consisted of 48 females and 52 males with a self-reported age range of 18 to 56, an average age of
28.6 years. They were evenly and randomly assigned to the two con- ditions (TS and SS), and none of them participated in both conditions. The education levels of the participants were as follows: 4-year col- lege, 34.5%; Master’s degree, 31%; and 2-year college, 18%. Their majors were computer and information systems, 25.5%; engineering, 20.5%; and science and math, 13%; followed by business, 12%. The baseline payment for participation was $0.10. An additional bonus re- ward was a maximum of $0.30 from two randomly selected rounds, as in Experiment 1. The participants earned $0.23 on average, depending on their performance.
6.1.2 Outliers
Previous crowdsourcing studies have shown that there were workers who completed the tasks without paying reasonable attention (e.g., by selecting random responses as quickly as possible to merely earn compensation) [14, 27]. To prevent such outliers from contaminating the present experiment’s data, we excluded the participants who re- sponded inconsistently (or randomly, in extreme cases) over the mul- tiple trials. We employed Pearson’s χ2 test to see how close the distri- bution of the rank of their responses was to the uniform distribution. We assumed that if the p-value of the Pearson’s χ2 test was bigger than 0.02, then the participant’s performance was sufficiently close to the uniform (in other words, random) distribution. The threshold of 0.02 was determined according to the data obtained from a controlled lab study.1 As a result, 57 participants were identified as outliers and were excluded from further analysis. Figure 7 shows the differences in the average time spent on finishing a single task between the two groups; legitimate participants spent an average of 51.3 seconds, while out- liers spent an average of 8.3 seconds, a duration too short for finding the best answer.
Fig. 7. The mean of time spent for each trial, comparing the participants between nonrandom clickers and random clickers.
6.2 Procedure
Our experiment was posted on the MTurk platform. After the partici- pants read the instructions on the MTurk website, they were redirected to our experimental website, which was separately hosted. Each par- ticipant was asked to complete 15 trials, where the first six trials were for practice, and the following nine trials were for actual analysis.
6.3 Experiment Design
With the aim of testing whether SS promoted strategies considering all of the attributes evenly rather than focusing on central columns, a testing stimulus (i.e., influential column) was devised for this experi- ment. The idea behind the influential column was to present a certain column so skewed in its value that the alternative with the highest util- ity would become too obvious if one paid attention to the column; however, if one failed to pay attention to it, the person would end up selecting a suboptimal alternative. More details on how the influen- tial column was constructed are discussed in Section 6.4. This column
1Detailed investigation of this issue will be published in a separate venue.

KIM ET AL: DOES AN EYE TRACKER TELL THE TRUTH ABOUT VISUALIZATIONS?: FINDINGS WHILE INVESTIGATING... 2427
was placed in three different positions (i.e., A, leftmost; B, center; and C, rightmost), as shown in Figure 8; thus, by comparing the decision qualities when using SS and TS under these three position settings, we could see whether SS or TS helped to avoid skewed consideration of attributes. A total of nine trials (three for each position: A, B, and C) were permuted to avoid ordering effects.
2. We found the lowest possible utility value that could be raised to the best alternative after adding 0.7 to the overall utility value; and
3. We assigned the highest value in the influential column to this row as well as randomized the assignments of the other values in the influential column to the other rows.
This strategy aimed to reduce the possibility of the best alternative being selected by the participants if they overlooked the influential column.
6.5 Measurement
As the data sets were generated to be different, the highest valued item in each set changed for each data set; therefore, we followed the mea- sure decision quality used in the previous study [15], calculated as follows:
Decision quality = utilityi −min(utility·) (2) max(utility·) − min(utility·)
where utilityi is derived from Equation 1. This normalized the perfor- mance from 0 to 1, where 1 was the maximum decision quality when the highest item was selected. The reason to introduce yet another metric over utility was that the maximum utility per each round was different from each other because each data set was randomly gener- ated.
Other than introducing the influential column and using the crowd- sourcing approach, the remaining experiment design stayed the same.
  Fig. 8. Sample tables comprising three variations in the position of the influential column among the seven attributes: position A at leftmost, position B at center, and position C at rightmost.
6.4 Data Sets
The data sets used in the first six practice trials were selected from the previous experiment, Experiment 1 (Section 4.5). For the actual data sets used in the analysis, nine data sets were generated to guarantee that no duplicate data sets were presented to a participant. Each data set had two-digit integers ranging from 10 to 99 that were presented as 15 alternatives (i.e., rows) with seven attributes (i.e., columns), the same as the data sets used in Experiment 1. For each data set, we used the following strategies to generate the non-influential columns and the influential column. Figure 9 shows one of the data sets we used in the experiment. First, for the non-influential columns, we maintained the same task difficulties across the data set by controlling the AIAC [24]. The AIAC for the six columns was controlled to be 0 ± 0.001. Sec- ond, for the influential column, there was a 70% drop from the highest number to the second highest number; however, the gap between the two adjacent values (sorted by values) was kept at 2.5%.
7
7.1
RESULTS AND DISCUSSION Decision Quality
  Fig. 9. An example of (a) six non-influential columns with an AIAC = 0.00027 and (b) an influential column.
After both the non-influential columns and the influential column were generated, we merged the two matrices to form a 15 × 7 table according to the following the steps:
1. We computed the overall utility value for each row of the six non-influential columns;
We employed a repeated measures ANOVA test on the decision quality
of the interface type, which had a significant main effect (F(1,98) = 5.60, p = 0.0199). On average, the decision quality with the SS inter- face (0.91) was higher than with the TS interface (0.88), as shown in Figure 10. This finding is consistent with an earlier controlled labora- tory study [15].
Fig. 10. The decision quality with the SS interface and the TS interface.
As our experiment was designed to penalize decision makers who overlooked the influential column, we could see whether more columns were likely to be considered in SS, which could have been due to the employment of compensatory strategies.
7.2 Column Order Effect
To see whether the three influential columns’ positions affected the performances with the different interfaces, we analyzed the data sepa- rately for SS and for TS. According to the data collected from SS, the repeated measures ANOVA did not show any main effect from influen- tial column position (F (2, 98) = 0.82, p = 0.4455); however, accord- ing to the data collected from TS, the column position had significant effects on the decision quality (F (2, 98) = 8.81, p = 0.0003). This is also shown in Figure 8, where, for TS, the decision quality dropped
2428 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 18, NO. 12, DECEMBER 2012
when the influential column was positioned toward the right side as in positions B and C; for SS, there was no significant difference among the different position settings.
Fig. 11. The decision quality for SS and TS with the three different influential column positions.
With the different effects that the column order had on the inter- faces, we could see that the interfaces promoted different decision- making strategies. For SS, as there was no difference among the in- fluential column positions, we saw that the participants considered all of the columns without paying differing degrees of attention (i.e., H2b was confirmed). Our hypothesis H2a, that SS would promote the par- ticipants to employ compensatory strategies by considering more at- tributes, was also indirectly explained. The strategies collected from the participants during the crowdsourcing study also confirmed that they used one of the compensatory strategies, the Majority of Con- firming Dimension (MCD), by doing a linear search among all of the alternatives:
[I] simply looked for the most top highlighted boxes. I se- lected the first selection (item) and visually compared it to [the] following selections until I found one that appeared to rank higher. Then, [I] selected [a] new selection (item) and repeated [this] until I finished the list.
I would select one option first and then compare the others against the selected one, one by one, down the list. At any one point in time, I would just be comparing two options.
Moreover, as mentioned in the previous literature, this compen- satory strategy should be highly demanding on information process- ing [31]; however, the cognitive load partially inferred from the fix- ation in Section 5.1 showed that the participants using SS had less cognitive pressure. Eventually, the visualization helped to promote optimal strategies for achieving higher accuracy with less effort.
For TS, the results also corresponded with those of the heatmap in Figure 1. The participants considered the leftmost column more than the other columns. We believe this resulted not because they consid- ered the left columns to be more important; rather, they started from the left due to their reading pattern [43]. While reading or compar- ing values toward the right, the information processing demands got higher, and the focus dropped drastically; therefore, a participant could have chosen less effort over accuracy, which would have led to lower decision quality [21]. This was also shown when participants selec- tively chose a few attributes to consider during the process.
The quotations collected from the participants using the TS setting also confirmed this strategy:
I opted for the second or third highest number in the first two columns.
It was helpful to look for columns with large variance and to try to narrow it down to two to three candidates.
With this crowdsourcing experiment, we believe that the longer du- ration of fixations shown in the middle columns in the heatmap (Fig- ure 1) and the visit count in Section 5.3 did not result due to the fact that the participants considered the center columns to be more impor- tant; rather, it was because the eye tracker could not capture clearly the visual scans of their peripheral vision.
8 CONCLUSIONS
We conducted an eye-tracking experiment and a crowdsourcing-based experiment to explore how a visualization technique, SS, could help people in the context of multi-attribute decision making. In the eye- tracker experiment, we analyzed fixation and visit data based on columns. We found that SS and TS resulted in vastly different fix- ation patterns (Figure 1); moreover, SS rendered higher visit counts, less fixation duration, and higher fixation counts. With these results, we confirmed our hypothesis that SS would help with decision making by providing pattern information for quicker browsing and promoting compensatory decision-making strategies.
However, the longer duration of the fixations in the central columns of SS were unexpected results, and the eye-tracker data did not render a clearer explanation; therefore, Experiment 2, which used the influen- tial column as a testing stimulus, was conducted with a crowdsourcing platform to explore the question. With the influential column placed in three different positions, decision quality was consistently higher in SS, while in TS we observed a drastic drop in decision quality when the influential column was placed in the middle or on the right side. This result further confirmed our finding from the eye-tracker experi- ment; the participants using SS had a higher depth of search.
Through the study, we verified the theory brought up by Rosen and Rosenkoetter [36] that different stimulus configurations (in our study, different interfaces), rather than the tasks themselves, could affect information-processing strategies when people made choices. More specifically, we demonstrated that SS, as a visualization tool, could help people with multi-attribute decision making by changing their behavior, which enhanced decision accuracy with lowered effort. Through exploring the use of process tracing techniques to study these behavior changes, we saw that quantitative methods could translate the originally illusive concepts into a concrete understanding.
We also showed that the eye-mind hypothesis did not hold in the SS condition probably because SS promoted more information brows- ing behavior using peripheral vision. We do not believe that this evi- dence completely nullifies the utility of eye-tracking methods in Info- Vis studies; instead, this could be a warning to other researchers who use an eye-tracker. They should be cautious while analyzing collected eye-movement data. Particularly, when an average fixation duration is below 400 milliseconds (which might indicate cursory browsing behaviors, based on our results), the experimenter should make sure whether the eye-mind hypothesis holds. We tested the eye-mind hy- pothesis using purposefully manipulated data (i.e., the influencing col- umn), which could be applicable to other experimental settings.
We hope that this study will serve as an example showing that eye- movement data could be used to trace cognitive procedures if used carefully. We hope that other researchers join this interesting research direction to evolve the research methods.
9 FUTURE WORK
In order to clearly understand how visual aids could effectively pro- mote higher decision quality and efficiency, we had to control several factors. This led to unrealistic settings, such as a primitive visual in- terface design and small artificial data sets. This was inevitable in eliminating compound factors. Our next step would be to extend the study to a more realistic setting with large-scale and real-world data sets. Different decision strategies could be applied and influence de- cision quality. We could also compare this performance with existing InfoVis tools, such as parallel coordinates. Another direction could be analyzing the sequential eye-movement data. The measures we used (i.e., fixation and visit) were an aggregation of eye-movement data, which did not reveal sequential patterns. Scanpath data could help de- tect distinct sequential patterns for different decision strategies. This

KIM ET AL: DOES AN EYE TRACKER TELL THE TRUTH ABOUT VISUALIZATIONS?: FINDINGS WHILE INVESTIGATING... 2429
would help in understanding more deeply why the visual aids were efficiently helping the decision maker to produce better decisions.
ACKNOWLEDGMENTS
We appreciate Zhicheng Liu for his insightful feedback and sugges- tions, and we also thank the students and crowdsourcing workers who participated in this study.
REFERENCES
[1] J.BautistaandG.Carenini.Anempiricalevaluationofinteractivevisual- izations for preferential choice. In Proceedings of the working conference on Advanced visual interfaces, pages 207–214, Napoli, Italy, 2008. ACM.
[2] J.R.Bettman,E.J.Johnson,andJ.W.Payne.Acomponentialanalysisof cognitive effort in choice. Organizational Behavior and Human Decision Processes, 45(1):111–139, Feb. 1990.
[3] S. E. Bonner, R. Hastie, G. Sprinkle, and S. M. Young. A review of the effects of financial incentives on performance in laboratory tasks: Impli- cations for management accounting. Journal of Management Accounting Research, pages 19–64, Jan. 2000.
[4] D. M. Boush and B. Loken. A Process-Tracing study of brand extension evaluation. Journal of Marketing Research, 28(1):16–28, Feb. 1991.
[5] T. Brinck, D. Gergle, and S. Wood. Designing Web Sites That Work:
Usability for the Web. Morgan Kaufmann, 2002.
[6] M. Burch, N. Konevtsova, J. Heinrich, M. Hoeferlin, and D. Weiskopf.
Evaluation of traditional, orthogonal, and radial tree diagrams by an eye tracking study. Visualization and Computer Graphics, IEEE Transactions on, 17(12):2440–2448, 2011.
[7] E. H. Creyer, J. R. Bettman, and J. W. Payne. The impact of accuracy and effort feedback and goals on adaptive decision behavior. Journal of Behavioral Decision Making, 3:1–16, Jan. 1990.
[8] B. Fasolo, G. H. McClelland, and P. M. Todd. Escaping the tyranny of choice: When fewer attributes make choice easier. Marketing Theory, 7(1):13–26, Mar. 2007.
[9] G. Gigerenzer and W. Gaissmaier. Heuristic decision making. Annual Review of Psychology, 62(1):451–482, Jan. 2011.
[10] M. G. Glaholt and E. M. Reingold. Eye movement monitoring as a pro- cess tracing methodology in decision making research. Journal of Neu- roscience, Psychology, and Economics, 4(2):125–146, May 2011.
[11] J. Goldberg and J. Helfman. Eye tracking for visualization evaluation: Reading values on linear versus radial graphs. Information Visualization, 10(3):182–195, 2011.
[12] V.GonzalezandA.Kobsa.Benefitsofinformationvisualizationsystems for administrative data analysts. In Proceedings of the Seventh Interna- tional Conference on Information Visualization, IV ’03, pages 331–336. IEEE Computer Society, 2003.
[13] C. P. Haugtvedt, K. A. Machleit, and R. Yalch. Online Consumer Psy- chology: Understanding and Influencing Consumer Behavior in The Vir- tual World. Psychology Press, Jan. 2005.
[14] J.J.HortonandL.B.Chilton.Thelaboreconomicsofpaidcrowdsourc- ing. In Proceedings of the 11th ACM conference on Electronic commerce, EC ’10, pages 209–218, New York, NY, USA, 2010. ACM.
[15] I. Hur, S.-H. Kim, A. Samak, and J. S. Yi. A comparative study of three sorting techniques in performing cognitive tasks on a tabular repre- sentation. International Journal of Human-Computer Interaction, 2012. doi:10.1080/10447318.2012.713802.
[16] I. Hur and J. S. Yi. SimulSort: multivariate data exploration through an enhanced sorting technique. In J. Jacko, editor, Human-Computer Interaction. Novel Interaction Methods and Techniques, volume 5611 of Lecture Notes in Computer Science, pages 684–693. Springer Berlin / Heidelberg, 2009.
[17] C. Hwang and K. Yoon. Multiple Attribute Decision Making: Methods And Applications: A State-of-the-art Survey. Springer-Verlag, 1981.
[18] A. Inselberg and B. Dimsdale. Parallel coordinates: A tool for visual-
izing multi-dimensional geometry. In Proceedings of the 1st conference on Visualization ’90, VIS ’90, pages 361–378, Los Alamitos, CA, USA, 1990. IEEE Computer Society Press.
[19] R. J. Jacob and K. S. Karn. Eye tracking in human-computer interaction and usability research: Ready to deliver the promises. Mind, 2(3):4, 2003.
[20] E. Johnson, J. Payne, J. Bettman, and D. Schkade. Monitoring informa- tion processing and decisions: The mouselab system. Technical report,
[21] E. J. Johnson and J. W. Payne. Effort and accuracy in choice. Manage- ment Science, 31(4):395–414, Apr. 1985.
[22] A. Kittur, E. H. Chi, and B. Suh. Crowdsourcing user studies with me- chanical turk. In Proceeding of the twenty-sixth annual SIGCHI confer- ence on Human factors in computing systems, pages 453–456, Florence, Italy, 2008. ACM.
[23] G. Lohse and E. Johnson. A comparison of two process tracing meth- ods for choice tasks. In System Sciences, 1996., Proceedings of the Twenty-Ninth Hawaii International Conference on,, volume 4, pages 86– 97, 1996.
[24] N. Lurie. Decision making in information-rich environments: The role of information structure. Journal of Consumer Research, 30(4):473–486, 2004.
[25] N. H. Lurie and C. H. Mason. Visual representation: Implications for decision making. Journal of Marketing, 71(1):160–177, 2007.
[26] S. P. Marshall. Identifying cognitive state from eye metrics. Avia- tion, space, and environmental medicine, 78(Supplement 1):B165–B175, 2007.
[27] W.MasonandS.Suri.Conductingbehavioralresearchonamazon’sme- chanical turk. SSRN eLibrary, pages 1–23, Oct. 2010.
[28] B.Pan,H.Hembrooke,G.Gay,L.Granka,M.Feusner,andJ.Newman. The determinants of web page viewing behavior: An eye-tracking study. In Proceedings of the 2004 symposium on Eye tracking research & appli- cations, pages 147–154, 2004.
[29] A.L.Patalano,B.J.Juhasz,andJ.Dicke.Therelationshipbetweeninde- cisiveness and eye movement patterns in a decision making informational search task. Journal of Behavioral Decision Making, pages 353–368, 2009.
[30] J.Payne.Taskcomplexityandcontingentprocessingindecisionmaking: An information search and protocol analysis. Organizational behavior and human performance, 16(2):366–387, 1976.
[31] J. Payne, J. Bettman, and E. Johnson. Adaptive strategy selection in de- cision making. Journal of Experimental Psychology: Learning, Memory, and Cognition, 14(3):534–552, 1988.
[32] D.PeeblesandP.C.Cheng.Modelingtheeffectoftaskandgraphicalrep- resentation on response latency in a graph reading task. Human Factors: The Journal of the Human Factors and Ergonomics Society, 45(1):28–46, Jan. 2003.
[33] K. Rayner. Eye movements in reading and information processing. Psy- chological Bulletin, 85(3):618–660, 1978.
[34] K. Rayner. Eye movements in reading and information processing: 20 years of research. Psychological bulletin, 124(3):372, 1998.
[35] N.Reisen,U.Hoffrage,andF.W.Mast.Identifyingdecisionstrategiesin a consumer choice situation. Judgment and decision making, 3(8):641– 658, 2008.
[36] L. Rosen and P. Rosenkoetter. An eye fixation analysis of choice and judgment with multiattribute stimuli. Memory & Cognition, 4(6):747– 752, Nov. 1976.
[37] J. Ross, L. Irani, M. S. Silberman, A. Zaldivar, and B. Tomlinson. Who are the crowdworkers?: Shifting demographics in mechanical turk. In Proceedings of the 28th of the international conference extended ab- stracts on Human factors in computing systems, pages 2863–2872, At- lanta, Georgia, USA, 2010. ACM.
[38] S. Rudolph, A. Savikhin, and D. Ebert. FinVis: applied visual analytics for personal financial planning. In Visual Analytics Science and Technol- ogy, 2009. VAST 2009. IEEE Symposium on, pages 195–202, 2009.
[39] J. Russo and B. Dosher. Strategies for multiattribute binary choice. Journal of Experimental Psychology: Learning, Memory, and Cognition, 9(4):676–696, 1983.
[40] A. Savikhin, R. Maciejewski, and D. Ebert. Applied visual analytics for economic decision-making. In Visual Analytics Science and Technology, 2008. VAST’08. IEEE Symposium on, pages 107–114, 2008.
[41] C. Seidl and S. Traub. Testing decision rules for multiattribute decision making. In Current trends in economics: theory and applications: pro- ceedings of the third international meeting of the Society for the Advance- ment of Economic Theory, Antalya, Turkey, June 1997, volume 8, page 413. Springer Verlag, 1999.
[42] A.ShahandD.Oppenheimer.Heuristicsmadeeasy:Aneffort-reduction framework. Psychological bulletin, 134(2):207–222, 2008.
[43] S. Shrestha, K. Lenz, B. Chaparro, and J. Owens. “F” pattern scanning of text and images in web pages. Proceedings of the Human Factors and Ergonomics Society Annual Meeting, 51(18):1200–1204, Oct. 2007.
[44] M.SpenkeandC.Beilken.InfoZoom-analysingformulaoneracingre-
DTIC Document, 1989.
2430 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 18, NO. 12, DECEMBER 2012
sults with an interactive data mining and visualisation tool. In in Ebecken,
N. Data mining II, pages 455–464, 2000.
[45] M. Spenke, C. Beilken, and T. Berlage. FOCUS: the interactive table for
product comparison and selection. In Proceedings of the 9th annual ACM symposium on User interface software and technology, UIST ’96, pages 41–50, New York, NY, USA, 1996. ACM.
1990.
[52] M. Westenberg and P. Koele. Multi-attribute evaluation processes:
Methodological and conceptual issues. Acta Psychologica, 87(2–3):65–
84, 1994.
[53] K. Wittenburg, T. Lanning, M. Heinrichs, and M. Stanton. Parallel bar-
grams for consumer-based information exploration and choice. In Pro- ceedings of the 14th annual ACM symposium on User interface software and technology, UIST ’01, pages 51–60, New York, NY, USA, 2001.
[46] M. Steinberger, M. Waldner, M. Streit, A. Lex, and D. Schmalstieg. Context-Preserving visual links. Visualization and Computer Graphics,
IEEE Transactions on, 17(12):2249–2258, Dec. 2011. ACM.
[47] Tobii Technology AB. Tobii studio 2.2 user manual.
[48] H. Y. Tsang, M. Tory, and C. Swindells. eSeeTrack - visualizing sequen-
tial fixation patterns. IEEE Transactions on Visualization and Computer
Graphics, 16(6):953–962, Dec. 2010.
[49] A. Tversky. Elimination by aspects: A theory of choice. Psychological
review, 79(4):281–299, 1972.
[50] C. Ware. Information visualization: perception for design. Morgan Kauf-
mann, Apr. 2004.
[51] E. J. Wegman. Hyperdimensional data analysis using parallel coordi-
nates. Journal of the American Statistical Association, 85(411):664–675,
[54] J. S. Yi, Y.-a. Kang, J. T. Stasko, and J. A. Jacko. Understanding and characterizing insights: How do people gain insights using information visualization? In Proceedings of the 2008 conference on BEyond time and errors: novel evaLuation methods for Information Visualization, pages 1– 6, Florence, Italy, 2008. ACM.
  Eye tracking evaluation of visual analytics
Kuno Kurzhals1, Brian Fisher2, Michael Burch1 and Daniel Weiskopf1
Abstract
Information Visualization
1–19
Ó The Author(s) 2015
Reprints and permissions: sagepub.co.uk/journalsPermissions.nav DOI: 10.1177/1473871615609787 ivi.sagepub.com
 The application of eye tracking for the evaluation of humans’ viewing behavior is a common approach in psy- chological research. So far, the use of this technique for the evaluation of visual analytics and visualization is less prominent. We investigate recent scientific publications from the main visualization and visual analytics conferences and journals, as well as related research fields that include an evaluation by eye tracking. Furthermore, we provide an overview of evaluation goals that can be achieved by eye tracking and state-of- the-art analysis techniques for eye tracking data. Ideally, visual analytics leads to a mixed-initiative cognitive system where the mechanism of distribution is the interaction of the user with the visualization environment. Therefore, we also include a discussion of cognitive approaches and models to include the user in the evalua- tion process. Based on our review of the current use of eye tracking evaluation in our field and the cognitive theory, we propose directions for future research on evaluation methodology, leading to the grand challenge of developing an evaluation approach to the mixed-initiative cognitive system of visual analytics.
Keywords
Eye tracking, visual analytics, visualization, evaluation methods, visual cognition
Introduction
Eye tracking has been widely used to measure the dis- tribution of visual attention, often in connection with analyzing how well participants perform with certain tasks on visual stimuli. The task might be dependent on the environment in which eye tracking is applied. Traditionally, eye tracking has been applied in areas such as psychology and marketing research.1
The canonical early eye tracking work was that of
2
Alfred Yarbus, who demonstrated that the path taken
by the gaze of his observer across paintings of various naturalistic scenes was determined by the interaction of scene information and the nature of the observer’s task. For example, after an initial inspection of the painting ‘‘They Did Not Expect Him’’, Yarbus asked the viewer to ‘‘Estimate the material circumstances of the family in the picture’’, ‘‘Surmise what the family had been doing before the arrival of the ‘unexpected visitor’’’, and ‘‘Estimate how long the ‘unexpected
visitor’ had been away from the family’’. This led to different patterns of eye movement (scanpaths) across the scene as the observer sought the required informa- tion. In naturalistic scenes such as these, it is thought that the ‘‘gist’’ of the entire scene is perceived quickly, and interacts with the observer’s task and top-down knowledge to determine what areas are to be fixated and in what order.3,4 As eye tracking technology became more available, a large number of studies were conducted using a variety of technologies, scenes, tasks, etc.3,5,6
1University of Stuttgart, Germany 2Simon Fraser University, Canada
Corresponding author:
Kuno Kurzhals, University of Stuttgart, Universit ̈atsstraße 38, Stuttgart 70569, Germany.
Email: kuno.kurzhals@vis.uni-stuttgart.de
 Downloaded from ivi.sagepub.com at Zhejiang University of Technology on September 21, 2016
2
Information Visualization
 However, only recently eye tracking has become increasingly popular in visualization research, as a means of evaluating visualization techniques, but also as a source of data for which visualization can be used for analysis.7 For evaluation purposes, one typically records the eye movements of study participants when they perform a given task with a visual stimulus depict- ing some data visualization. It is thought that the mea- surement of spatiotemporal eye movement data may well be more diagnostic than popular summative per- formance variables, such as completion time and accu- racy, recorded in traditional user studies. In addition, because eye movements are recorded in an ongoing basis throughout the visualization task, they can pro- vide insight into the process of working with a visuali- zation environment. On the challenging side, however, this spatiotemporal aspect of the eye movement data requires more sophisticated data analysis and visualiza- tion methods tailored to the tasks and stimuli of interest.
A simple mapping of the psychology research methods mentioned above onto the interpretation of eye position in visualization tasks is complicated by a number of perceptual and cognitive factors. Both natural scenes and the reduced-cue experiments used in laboratory studies by psychologists tend to be composed of discrete objects about which a decision can be made, e.g. the material circumstances ques- tion in the Yarbus example might be answered by inspection of clothing and room decor. In contrast, understanding a visualization often requires a judg- ment to be made based on the configuration of multi- ple objects or aspects of a given object (e.g. seeking clusters of points in a scatterplot), chromatic patterns as in a color map used for scalar-data visualization, the orientation of elongated objects such as in a line chart, or the relative area of sections of a pie chart. While it is clear that these kinds of tasks are not pro- cessed in the same way as object categorization,8 it is not obvious where one would expect an observer to look in order to perform tasks based on important aspects of a visualization, or how a given scanpath might be interpreted as a predictor of some particular cognitive operation. Some studies in the psychology literature used scenes that were entirely artificial (e.g. counting and subitizing in point cloud displays9), and these may provide a basis for investigation. However, the problems are further complicated when interactive visualization is evaluated because dynamic stimuli require even more advanced models and eva- luation methods. The evaluation of visual analytics is challenging because it, in the ideal case, forms a mixed-initiative cognitive system, with the user inter- acting with the visualization environment.
These problems might be one of the reasons that the number of eye tracking studies is much smaller than the number of other user studies in visualization and visual analytics. In the past, high prices of eye tracking hardware and technology might have been another roadblock. Since hardware components become cheaper and easier to handle, the potential of this technology for extending evaluation in our research community is considerable. Another issue might be that it is not clear to what class of evaluation problems in visualization and visual analytics eye tracking is applied best, and which analysis methods can be employed to derive knowledge from the recorded gaze data.
We provide an overview of how eye tracking is cur- rently used in the evaluation of visualization tech- niques and how the gaze data is analyzed. As another contribution, we describe existing cognitive models and how they can be related to eye tracking for visual analytics. Based on these ingredients, we propose a number of promising areas in which eye tracking could advance evaluation methods, sketch ways how to approach these evaluation problems, and identify open research challenges.
This article is an extension of our previous position paper10 presented at the BELIV workshop 2014. Our extension includes an extensive literature research for eye tracking evaluation in visualization-related com- munities. We also updated the list of relevant publica- tions in the visualization community. This literature research is available as an interactive survey that can be browsed online: http://go.visus.uni-stuttgart.de/eye trackingevaluation. As a result, we provide an overview and an extended discussion with examples of addi- tional methods and how they can be applied to the evaluation of visual analytics scenarios.
With this paper, we hope that we can stimulate other researchers to work with eye tracking in visuali- zation and visual analytics.
Related work
The evaluation of visualization techniques is challen- ging, but it has been acknowledged in our research community that we need good ways of assessing visua- lization11,12 and visual analytics.13 In particular, the series of BELIV workshops addresses the issue of how we can evaluate visualization, going beyond traditional measurements of task accuracy and completion time. For example, in the 2012 BELIV workshop, Elmqvist and Yi14 proposed a general approach to evaluating visualizations based on a collection of patterns. For a most recent overview of user study-based evaluation in visualization, we refer to Tory.15 She provides her
Downloaded from ivi.sagepub.com at Zhejiang University of Technology on September 21, 2016
Kurzhals et al.
3
 reflection on user studies and a categorization of test- ing methods, based on major goals such as under- standing versus evaluation as well as common methodological approaches such as quantitative experiment, qualitative observational study, inspec- tion, and usability study. In another recent publica- tion, Freitas et al.16 discuss usability evaluation for information visualizations by particularly looking at it from a user-centered perspective.
Although eye tracking has a very long tradition and has been widely used in many fields, there is remarkably little work in the visualization literature that would address eye tracking as a means of evaluating visualiza- tion or visual analytics. For example, the paper by Goldberg and Helfman17 is the only paper from any of the previous BELIV workshops that would specifically address the issue of eye tracking evaluation methodol- ogy. Goldberg and Helfman present how eye tracking can be applied to evaluate simple information graphics, such as bar charts and line graphs. They focus on statis- tical analysis of common eye tracking metrics and visual analysis of scanpaths. Based on our research on similar user studies that included eye tracking for the evaluation of visualization techniques, we will provide a broader overview of analysis methods applied for different research questions.
Despite such little prior work on eye tracking-based testing methodology in our community, we have been witnessing a rapid increase in the number of user stud- ies that, at least in part, use eye tracking. One contri- bution of our paper is a summary and categorization of such papers. The majority of the papers were pub- lished in the last 4 years, showing a steep gradient of related papers. Despite the still small absolute number of such studies, we believe that this strong increase shows that eye tracking evaluation methodology is a timely topic for our research community.
Our paper is in line with previous work that reflects on how visualization is evaluated. In the context of evaluating information visualization in general, Lam et al.18 describe seven scenarios, based on an extensive literature review of more than 800 visualization papers. They consider the ‘‘understanding of environ- ments and work practices’’ and the evaluation of ‘‘visual data analysis and reasoning’’, ‘‘communication through visualization’’, ‘‘collaborative data analysis’’, ‘‘user performance’’, ‘‘user experience’’, and ‘‘visualiza- tion algorithms’’ in their survey. In a follow-up paper, Isenberg et al.19 extend the literature review to include papers from scientific visualization. Isenberg et al. adopt the coding scheme by Lam et al., with only minor changes and extensions: they add a new cate- gory ‘‘qualitative result inspection’’ and change the
evaluation of ‘‘visualization algorithms’’ to ‘‘algorithm performance’’. One result of both Lam et al.’s and Isenberg et al.’s reflection on the research field is that we have been witnessing a strong increase in the por- tion of user-related evaluation, across the different subfields of visualization. Another observation is the dominating role of the categories ‘‘user performance’’ and ‘‘user experience’’. Our paper builds a link in par- ticular to ‘‘user performance’’ because eye tracking is most often used in controlled laboratory experiments that aim to measure and understand the performance of users with visualization.
However, we also want to go beyond the traditional user performance with (isolated) visualization: in fact, we target the evaluation of visual analytics in the sense of the ‘‘science of analytical reasoning facilitated by interactive visual interfaces’’.20 As Ribarsky et al.21 dis- cuss, there is the general need for a science of analyti- cal reasoning, including a human cognitive model, but they do not detail any evaluation methodology. We will later describe some cognitive models and how they relate to the evaluation of the combined cognitive sys- tem of user and visualization interface. In this sense, our paper addresses the evaluation scenario ‘‘visual data analysis and reasoning’’ identified by Lam et al. and Isenberg et al. However, as discussed in their papers, there is little previous work that would address the combined evaluation of such distributed cognition; much of the previous evaluation is based on case studies.
Except for the aforementioned paper by Goldberg and Helfman,17 none of the above papers deals with eye tracking evaluation in detail. With our paper, we want to fill this gap. We provide reflections on the cur- rent state of how eye tracking is used for evaluation in visualization and visual analytics, and we discuss the directions for future research that will allow for a broader use of eye tracking. We focus on eye tracking evaluation in the context of controlled laboratory experiments.
It should be noted that there is yet another aspect of eye tracking related to visualization and visual analytics: their use for the visual analysis of eye tracking data. This paper does not focus on this con- nection between eye tracking and visualization or visual analytics. Instead, we refer the reader to a recent state of the art report by Blascheck et al.7 and a review of visual analytics techniques for eye track- ing data by Andrienko et al.22 However, we do dis- cuss some of the analysis problems for eye tracking data as far as they are concerned with analyzing the results of eye tracking studies with visualization and visual analytics.
Downloaded from ivi.sagepub.com at Zhejiang University of Technology on September 21, 2016
4 Information Visualization
      Figure 1. Pipeline of user-oriented evaluation. Stimulus and task represent the independent variables, measurements from a user study represent the dependent variables. The data analysis can then be performed with statistical methods, or in combination with visual data analysis. Cognitive models will influence both ends of the pipeline.
Evaluation pipeline
A typical user study for visualization techniques can be described by a pipeline as shown in Figure 1 (gray parts). Here, we assume a controlled laboratory experi- ment, even though many aspects carry over to other variants of user studies. A task is given to the study participant, that is to be solved by using visualization or visual analytics. The visual stimuli and choice of tasks serve as independent variables of the study. In this context, different visualization techniques and/or variations of one technique provide the basis for the visual stimuli. The task often requires the user to search and report certain aspects, or interpret the stimulus.
The performance with the task is assessed in the form of dependent variables. Standard measurements are the completion time and accuracy. However, pro- tocol analysis is often employed as well, in particular, the ‘‘think aloud’’ protocol analysis.23 Finally, the data acquired through the dependent variables is analyzed, eventually leading to conclusions regarding the study. Very well accepted is data analysis in the form of statis- tical inference for hypothesis testing. However, descriptive statistics and statistical modeling might be employed, too. To some degree, visually oriented data analysis might appear here as well. Nonetheless, the standard procedure for the overall user study process is oriented along the goal of hypothesis testing with statistical methods.
With eye tracking, the evaluation pipeline is extended; see the color highlighting in Figure 1. First, eye tracking provides additional dependent variables, in particular, spatiotemporal data in form of eye
movements that provide information about the partici- pant’s viewing behavior or physiological data by the pupil diameter, which can be an indicator of cognitive load.24,25 Eye movement data is meaningful since it contains the information about where and when parti- cipants waste time when performing a given task, which may hint at design issues in the visual analytics system. This aspect is not contained in traditional completion time measures where the dependent vari- able ‘‘completion time’’ is stored as an aggregated value. Neither time-varying nor spatial behavior can be extracted from this dependent variable, in contrast to what is supported by eye movement data. However, algorithmic and visual analysis becomes much more complex and statistical hypothesis testing is a difficult problem.
In visualization and visual analytics systems, the analyst is typically confronted by a difficult task that consists of several stages for the subtasks, demanding for interacting with one or more visualizations. Consequently, the traditional error rates and comple- tion time variables abstract from the viewing behavior. This is unsatisfying for performing meaningful analy- ses on viewing behavior with the goal to understand difficulties that people have when working with an interactive visualization or when they use a visual ana- lytics technique. However, analyzing the recorded eye movement data from a complex stimulus with chang- ing content demands for sophisticated visual analytics approaches to find patterns and insights in the data and, finally, to derive knowledge. The displayed sti- muli do not have to be restricted to the domain of visual analytics. Also other interface designs might be worth exploring by recording people’s eye movements.
Downloaded from ivi.sagepub.com at Zhejiang University of Technology on September 21, 2016
Kurzhals et al.
5
 This might be in particular useful for integrated devel- opment environments (IDEs) used in software engineering.
Due to the complexity of the spatiotemporal gaze data, we usually have to derive other, more simplified dependent variables from the raw gaze data in order to perform data analysis. The following section reviews typical examples of such aggregated metrics for eye tracking data. However, with data aggregation, we lose much of the information about the spatiotemporal nature of the eye tracking data. Therefore, as second major change, eye tracking studies often come with a visual spatiotemporal analysis of the gaze data.
In fact, eye tracking experiments and the accompa- nying visual data analysis may often be used for hypothesis building, not just hypothesis testing, because they allow for a detailed ‘‘window’’ into how the participant works with the visualization over time. The data analysis methods suitable for eye tracking data are reviewed in the next section.
It should also be noted that the upper part of the pipeline from Figure 1 targets the classical evaluation of visualization techniques. For visual analytics, the much more complex distributed cognitive system that includes the user and machine needs to be evaluated. To this end, we also have to include cognitive modeling of the user, as discussed later in this paper. As shown in Figure 1, the cognitive models will have an influence on the task design and the stimuli, which will have to fit the properties of the underlying models. In addition, the cognitive models will influence the data analysis in both directions, as models can be derived as well as be evaluated with data analysis.
Eye tracking data analysis
With the spatiotemporal eye tracking data recorded in a user study, the data analysis can be performed by two different approaches: statistical and visual analysis.
Statistical analysis
The raw gaze data is usually preprocessed by an appro- priate filter algorithm to detect fixations and saccades; for further reading on eye tracking terminology, we refer to Holmqvist et al.26 and Blascheck et al.7 The preprocessed data can then undergo statistical analysis. Typically, the data has to be further aggregated to allow for the application of statistical methods. An important class of analysis approaches is based on eye tracking metrics computed from the (preprocessed) eye tracking data.
Objects or specific regions on a stimulus can be of special interest. By defining boundary shapes around these areas of interest (AOIs), fixation data can be
mapped to the areas. The common eye tracking metrics can be separated into the following three cate- gories, according to Poole and Ball.27
  Fixation-derived metrics. Fixations with or without AOI information can be processed. A common metric is defined by the number of fixa- tions per AOI, which indicates the relevance of the AOI for the users. The sum of fixation durations may be used to compare the distribution of atten- tion between AOIs.
  Saccade-derived metrics. The characteristics of the saccades may indicate the quality of visual cues in the stimulus or the extent of visual searching. For example, large saccade amplitude can indicate meaningful cues that draw the attention from a dis- tance, or a high frequency of saccades could come from much visual searching. Therefore, saccade- derived metrics can serve to indicate difficulties with the visual encoding.
  Scanpath-derived metrics. The scanpath con- sists of the full sequence of fixations and saccades. Therefore, scanpath-derived metrics can acquire information about visual reading strategies or pin- point specific problems with the visualization design during the task. The transition matrix is the common approach to analyzing transition patterns between AOIs, albeit it does not represent the full sequence but only the collection of pairs of fixa- tions from the sequence.
Once we have values from any of these metrics, we can directly apply statistical methods, including infer- ential or descriptive statistics as well as statistical mod- eling. Therefore, these metrics can serve as a basis for hypothesis testing.
A major problem is that the eye tracking metrics have to be interpreted with caution because they are no unambiguous indicator for certain characteristics of cognitive or perceptual processing. In fact, they pro- vide a rather coarse and aggregated perspective on the participant’s viewing behavior. Therefore, they are best accompanied by complementary indicators, or the eye tracking study is specifically designed to evoke and test clearly specified hypotheses. Another problem is that the metrics were typically developed for visual stimuli that are different from those from visualization; there- fore, it still needs to be demonstrated that the metrics are indicators for the same characteristics.
Eye tracking data usually contains much more information than represented by the above, aggregated metrics. Therefore, statistical analysis can also be applied to data that is closer to the original gaze data. In particular, statistical modeling to predict and clas- sify scanpaths on stimuli provides a promising
Downloaded from ivi.sagepub.com at Zhejiang University of Technology on September 21, 2016
6 Information Visualization
      Figure 2. Common eye tracking visualization techniques: Attention map (left) and gaze plot (right).
approach for a more comprehensive analysis for visua- lization stimuli. Here, one issue is to generate the appropriate model for the scanpath (e.g. define the appropriate AOIs) and employ the appropriate statisti- cal methods. In this context, one can use data-mining techniques such as scanpath clustering,17 layered hid- den Markov models,28 or measures for the similarity between aggregated scanpaths.29
To this point, these metrics are primarily applied to analyze user performance. With respect to visualization and visual analytics systems, performance is important but not the only relevant component to evaluate. In addition, the evaluation of users’ experience and gain of insight30 plays an important role. Quantitative mea- sures derived from eye tracking (e.g. cognitive load) could help quantify such complex cognitive aspects.24
The metrics summarized in this section are just the most common that can be found in the evaluation pro- cedures of our literature review. For the evaluation of visual analytics, some metrics will provide valuable information, such as the distribution of attention between multiple views, but none of them alone will capture all of the cognitive processes that are involved. Therefore, cognitive models for visual analytics will be required. We will have to evaluate which metrics are suitable for the application to visual analytics and the development of new models.
Visual analysis
In general, visualization can complement statistical analysis by providing additional insight into the data by exploratory search, building hypotheses, or the pre- sentation of confirmed analysis results.31 The same is true for the special cases of eye tracking data analysis. In particular, visualization is a very good means of
examining the spatial, temporal, or spatiotemporal aspects of the data.7
The most common visualization techniques are attention maps and gaze plots (see Figure 2). Attention maps display the spatial distribution of eye tracking data on a stimulus. The data can be aggre- gated over time for one participant or multiple partici- pants. Although attention maps can provide a good overview of important areas of interest on a static sti- mulus, the temporal component of the data is lost. In contrast, gaze plots provide a spatiotemporal perspec- tive on fixation sequences and can be investigated to identify potential reading strategies. However, with increasing length of the scanpath, or with scanpaths from multiple participants, the visualization becomes cluttered and hard to interpret. Alternatively, transi- tion matrices allow us to analyze transition patterns but lack the interpretation of longer transition sequences (beyond just pairs of fixations). In sum- mary, the traditional visualization techniques are well prepared to provide a qualitative picture of the distri- bution of attention aggregated over time (attention maps) or of a short scanpath of a single participant, both for static stimuli. In these cases, they can also be used for eye tracking experiments with visualization or visual analytics, in particular, for exploratory data analysis and hypothesis building.
However, for more challenging research questions, including those that work with dynamic stimuli, many participants or groups of participants, and coupled spatial and temporal structures of the gaze data, the above visualization techniques are not sufficient. Therefore, there is much, mostly recent, work in the visualization community to develop improved visuali- zation techniques, for example, for displaying time- oriented AOI data,32 complete sequences of
Downloaded from ivi.sagepub.com at Zhejiang University of Technology on September 21, 2016
Kurzhals et al.
7
 scanpaths,33 or spatiotemporal gaze data.34 A particu- larly interesting approach is visual analytics for eye tracking data,22 combining statistical and data-mining techniques with interactive visualization; recent exam- ples combine scanpath clustering with visualization35 or multiple coordinated views with statistical graphics.36
Cognitive models in interaction design
Visualization and human–computer interaction (HCI) researchers frequently conduct usability testing, pro- viding a summative evaluation of the effectiveness of a specific application or visualization in the context of use. We believe, as do many of our colleagues, that we must also go beyond usability testing to build an understanding of human perception, cognition, and interaction in order to design environments in which people perform cognitive tasks through interaction with visualizations. We think, however, that the mechanisms by which those theories might inform design and evaluation of visualization systems are not entirely straightforward. It is not sufficient to simply apply general principles of psychology as they are implemented in design guidelines (e.g. ‘‘Follow Gestalt Laws’’). Any given psychological phenomenon will apply only to a specific set of situations, and the effect size of its factors will vary for different environ- ments and for different individuals. Perceptually com- plex environments such as those we utilize for visual analytics contain many factors that could potentially affect human performance, not all of which can be optimized in any single design.
Models of cognitive architecture
Because of this, we conclude that an increased empha- sis on models of cognitive architecture and multi- factor models of human information processing that are known to generalize across a wide range of individ- uals, environments, and tasks is needed. For example, the two-visual-systems theory of Trevarthen37 is based on neuroanatomical evidence and is known to predict changes in response to visual environments when tasks are motor versus cognitive in nature. Since it is derived from neuroanatomy, it is architectural in the sense that it will hold true (or have predictive validity) over a large number of visual illusions that can be generated in visualization environments. Individual subjects may differ in their sensitivity to the effects it describes, but once they are affected, their performance will change in predictable ways. We propose an increased empha- sis on cognitive architecture, including computational theories of mind such as ACT-R.38,39
Directed fundamental research
Another proposed emphasis in cognitive visualization research would be to focus our models more precisely on the situation of interest, to move beyond naturalis- tic studies and laboratory investigations abstracted from those environments to focus more closely on the artificial scenes (e.g. dashboards) that we generate and the analytical cognitive processes that our visualiza- tions are meant to support. This ‘‘directed fundamen- tal research’’ approach does not mean that we limit our evaluation to visualization systems and tasks per se, but that stimuli and tasks used in our studies should demonstrate aspects of human cognitive architecture that are important for the design and evaluation of visualization systems and the ways in which they are used to understand situations and make decisions.
Directed fundamental research studies provide knowledge of human capabilities and limitations that can be used by designers of systems for a variety of applications. The goal here is, in essence, to build a basic ‘‘science of analytical reasoning’’ specific to the kinds of operations that might be ‘‘facilitated by inter- active visual interfaces’’.20 This kind of study will necessarily build upon general theories, frameworks, and methods from psychology, and many findings that result will be of interest to those publication channels.
An example of directed fundamental research is the work of Liu et al.40 This study used reduced-cue experimental methods typical for psychology studies. However, its purpose was to determine the impact of transformations of the visual environment that are important for graphical visualization environments, in this case for air traffic control, so that its results will be immediately applicable to the design and analysis of these environments. Their first experiment replicated a psychological study by Pylyshyn et al.41 on an aspect of human cognitive architecture, the FINST (‘‘fingers of instantiation’’) attentional token mechanism. FINSTs track multiple moving objects in the visual scene in order to support performance in a variety of tasks that depend on rapid access to information about those objects. Liu et al. hypothesized that FINSTs were important for air traffic controllers’ ability to associate information from memory (such as being low on fuel) with a specific aircraft representation dis- played on their screen. If the FINST link to a particu- lar display object were to fail, the cognitive load on a controller increases, as they must consciously recall the information and reestablish its relationship with the proper display object. Thus, we can see how a gen- eral psychological model, the FINST hypothesis, might play a role in cognitive task performance by air traffic controllers. The second and third experiments, however, extend the general model to focus specifically
Downloaded from ivi.sagepub.com at Zhejiang University of Technology on September 21, 2016
8
Information Visualization
 on threats to the FINST mechanism that come about when camera movements are made in a simulated 3D ‘‘fishtank VR’’ air traffic environment (the NextGen ATC system).
At this point, Liu et al. are conducting directed fun- damental research with the goal of informing the design of air traffic control environments. They found that participants in their experiments were able to track multiple moving targets through a surprising range of display transformations, even when those scenes were displayed in two dimensions. This finding was interesting enough to psychology that it found a home in one of the top-ranked journals in the field. From the perspective of visualization interface design, the lack of effect of these transformations on partici- pants’ psychophysical tracking functions suggested that their use in NextGen ATC systems was not con- traindicated. Since the study extended previous funda- mental research and was conducted using similar laboratory stimuli and tasks, the results should gener- alize across a range of applications. In that sense, the work was scientifically progressive, supporting the cre- ation of new hypotheses, empirical methods and scien- tific theory. We propose a greater use of this approach to research in visualization environments.
Translational studies
While eye movement methods can make substantial contributions to (directed) fundamental research, they are likely to be particularly useful for translational studies that build upon what is known about the cog- nitive architecture to examine how it is utilized in spe- cific situations and tasks that are more similar to a visualization approach. These studies are intended not to contribute to the psychology literature nor to evalu- ate a specific visualization but rather constitute an intermediate ‘‘translational’’ study whose results would come in the form of more structured guidance, guide- lines, visual queries,42 or design actions,43 that might be of use in the design and evaluation of a class of visualizations or visual information systems.
An example of this type of study is that of Po et al.44 that mapped the two-visual-systems theory of Trevarthen onto the kinds of displays used in CAD of large aircraft. The results of this study were interpreted with respect to the impact of individual differences on the design of interaction with these displays. While studies designed to elucidate cognitive architecture in general and focused models of interaction are rare in visualization research, the study of individual and cohort differences is gaining some attention. We pick up on this in the subsection ‘‘Comparison between user groups’’ later in this paper. We feel that this trend towards focusing research on individual and cohort
differences is a valuable one, and support the further development of methods such as those utilized by Po et al.44 by which we can predict the propensity of a given individual or cohort to shift their performance in ways predicted by the model. A combination of com- prehensive models of general cognitive architecture, focused models of visualization cognition, and analysis of individual and cohort differences (the ‘‘personal equation’’) hold the promise of creating a more scien- tific basis for the design of visualization systems.
In summary, there are already a few promising aspects that one could adopt from cognitive models in order to extend the evaluation methodology for visual analytics. However, these are covered only in a few examples. Overall, we are still missing a good integra- tion of cognitive models. In particular, there is need for further translational studies; see our discussion of future research directions later in this paper.
Literature overview
Over recent years, we have been witnessing an increas- ing number of publications that included eye tracking in user studies to evaluate visualization techniques. First, we will describe our systematic review approach, including the investigated publication channels and search and classification strategies. Readers who want to skip the methodology and go directly to our research results, may continue reading in Section ‘‘Eye tracking evaluation in the visualization community’’.
For our systematic review, we checked the main journals (including special issues of conferences) and proceedings for visualization and visual analytics, each spanning their whole time span of publications. These include the current publication channels:
  IEEE Transactions on Visualization and Computer Graphics (TVCG);
  Computer Graphics Forum (CGF);
  Information Visualization Journal (IVS);
  IEEE Conference/Symposium on Visual Analytics
Science and Technology (VAST);
  IEEE Pacific Visualization Symposium (PacificVis);   International Conference on Information
Visualisation (IV).
We also included older proceedings that are no longer published in this form (because they now appear in one of the above journals, or the conferences are suc- ceeded by other conferences):
  IEEE Conference on Visualization (VIS);
  IEEE Symposium on Information Visualization
(INFOVIS);
Downloaded from ivi.sagepub.com at Zhejiang University of Technology on September 21, 2016
Kurzhals et al.
9
   EG/IEEE TCVG Symposium on Visualization (VISSYM);
  EG Conference on Visualization (EuroVis);
  Asia–Pacific Symposium on Visualization
(APVIS).
From these sources, we identified 17 publications that include eye tracking in a user study for the evalua- tion. Our search strategy was to investigate the differ- ent channels individually and perform a search for the terms (eye tracking) and (study or evaluation) with the
provided web search interfaces (e.g. IEEE Xplore). The resulting publications were then investigated manually to identify if a user-based evaluation was performed. Several of the publication channels did not contain any papers with eye tracking evaluation.
Note that also publications in the visualization com- munity exist that applied eye tracking for interactions with applications. Those papers investigated how gaze data can be used as an input device, e.g. to replace mouse input. Also, attentive interfaces that react to where the user is looking have been presented. Although such work could also provide important improvements for visual analytics applications, we focused on the evaluation of gaze data in the visualiza- tion community.
Obviously, there are many more user study papers with eye tracking, albeit outside our research commu- nity and, therefore, in other publication channels. Further research in related communities reveals that more eye-tracking-related evaluation methods have been applied to investigate human viewing behavior. In particular, in HCI research, there is a related com- munity that investigated more aspects of the recorded eye tracking data than mentioned above. Therefore, we chose to extend our research to the:
  SIGCHI Conference on Human Factors in Computer Systems (CHI);
  ACM Symposium on Eye Tracking Research and Applications (ETRA);
as the most important representatives for visualization- related communities with eye tracking evaluation. The HCI community also evaluates the usability of visual interfaces (e.g. graphical user interfaces (GUIs), web- sites) which can be similar to visual analytics systems. The ETRA proceedings also contain much related work on the evaluation of visual interfaces as well as publications from other fields. Therefore we chose the major HCI conference and the major eye tracking con- ference to provide a glimpse into related research.
For initial filtering of CHI publications, we per- formed a keyword search on the ACM Digital Library for (eye tracking) and (study or evaluation), resulting in
Figure 3. Histogram of all 368 investigated publications (visualization community, CHI, ETRA).
Figure 4. Comparison of publications with evaluation of gaze data: CHI (red), ETRA (yellow), and visualization community (purple).
552 publications that have been examined for content where eye tracking was either used for interaction or evaluation, leaving 189 publications of relevance.
The proceedings of ETRA were examined com- pletely, filtering out technical publications and focus- ing also on work that applied eye tracking for evaluation purposes. Here, our research resulted in 162 publications.
In summary, our research comprises 368 publica- tions of all mentioned communities from which 224 contained an evaluation of gaze data.
Figure 3 shows a histogram of all investigated publi- cations, displaying the increasing importance of this research field. Interesting about the histogram is the fact that until 1997 with the work of Faraday and Sutcliffe,45 who applied eye tracking to infer guidelines for multimedia presentations, the few publications before did not consider eye tracking for usability eva- luation. Starting with the publication from Bolt,46 eye movement data was investigated as an input device for attentive displays.
Figure 4 displays a detailed comparison of eye tracking publications, focusing on publications after 1995, with an evaluation of gaze data. Since 2000, the proceedings of ETRA appeared biannually (yellow), increasing the overall number of eye tracking evalua- tions in these years. To this point, the number of pub- lications with eye tracking in the visualization community is slowly increasing.
We coded the surveyed literature with the main key- words distribution of attention, sequential analysis, group comparison, visual analysis, statistical analysis. In addi- tion, we included the keywords community and stimulus to further describe in which community the
                                                                                                         Downloaded from ivi.sagepub.com at Zhejiang University of Technology on September 21, 2016
10
Information Visualization
 publication was published and what kind of stimulus has been investigated.
Since it is not possible to include all of these refer- ences in our paper, we will only refer to all publica- tions found in the visualization community and to examples with the highest citation counts in the related communities. However, we provide an overview of all literature from our research on an interactive website where the reader is free to explore all references by keywords, according to the descriptions in this paper. This interactive website was created with SurVis.47 The survey is available at http://go.visus.uni-stutt- gart.de/eyetrackingevaluation
Eye tracking evaluation in the visualization community
In the surveyed publications, different visualizations have been investigated. Based on the established statis- tical and visual analysis methods for eye tracking data, all publications contained at least one of the above- mentioned methods. We identified three main approaches to evaluate visualizations: evaluating the distribution of visual attention, evaluating sequential characteristics of eye movements, and comparing the viewing behavior of different participant groups.
In this section, we will summarize these approaches and discuss the points that have to be considered for the application to visual analytics.
Distribution of visual attention.48–55 The inves- tigated visualizations were static node-link graphs,48,52 matrices,49 parallel coordinates,53 three-dimensional meshes with various rendering styles,50 and different user interfaces.55 For dynamic stimuli, eye tracking was applied to measure the distribution of attention on objects with different video visualizations,51 and to create perceptual motion blur for rendered scenes.54 The visualization techniques were compared by fixa- tion metrics for the attention on different regions to investigate how the techniques are perceived and to identify possible usability issues. Attention maps were applied to visualize the spatial distribution of attention on the stimuli and support the statistical results. The majority of these publications investigated the spatial distribution of attention directly on the stimulus. If applied, AOIs were defined for rather coarse regions on the screen (i.e. multiple views). For visualizations that contain small regions of interest (e.g. nodes in a graph), the definition of AOIs can be difficult since the accuracy of current eye tracking hardware might be insufficient to retrieve such small areas. Therefore, the person evaluating a study of a complex visual ana- lytics system has to decide whether it is reasonable to investigate small visual components, or to consider a
coarser scale for AOI annotation (e.g. individual views).
Sequential characteristics of eye movement.56–62
The analyzed stimuli include node-link dia- grams,56,57,60,61 linear and radial charts,58 and visuali- zations with multiple coordinated views.59,62 In addition to fixation-related metrics on AOIs, the tran- sition frequencies between AOIs with transition matrices,57 transition graphs,62 and visual scanpath analysis58 were analyzed to gain insights into how users investigate a visualization (e.g. as an explanation for a decrease in task performance). Also, gaze analysis by visual analytics was applied to identify reading stra- tegies in tree diagrams.56 As mentioned above, the def- inition of AOIs in complex visual analytics systems might be problematic but is often necessary to perform most of the analysis related to sequential characteris- tics. For multiple views, the view itself can be consid- ered an AOI, but also the content of a view could contain multiple AOIs. For such complex structures, the definition of hierarchical AOIs could be considered to investigate the behavior both between and within different views. Since many visualizations consist of rendered content with known geometry, the definition of potential AOIs based on this content can be considered.
Comparison between user groups.63,64 The visual stimuli in these publications are virtual charac- ter models63 and cross-sectional medical images.64 Complementary to the previous two points, the distri- bution of attention between different groups was investigated. Group comparisons were performed between healthy and mentally disordered persons, or between novice and expert groups. Comparisons were based on a statistical analysis of AOI fixation metrics63 or visual comparison of gaze point distributions.64 For visualization analysis tasks, the expertise of a partici- pant also plays an important role. For the application to visual analytics, one point that should be considered more in the future, is the influence of the visual span of participants. For example, Reingold et al.65 investi- gated the viewing behavior of chess players with differ- ent levels of expertise. As a result, they observed that novice players fixated more on individual pieces, whereas expert players had a greater proportion of fixations between chess pieces, indicating a larger visual span to investigate more pieces at once. For visualizations, this behavior needs to be investigated in much more detail. As a consequence of an increased visual span, the accuracy of the eye tracking device is less problematic, since a much larger area on the screen with potential AOIs has to be considered. Approaches that count fixation hits on AOIs might be not sufficient for evaluations with expert participants.
Downloaded from ivi.sagepub.com at Zhejiang University of Technology on September 21, 2016
Kurzhals et al.
11
 Therefore, an uncertainty factor could be applied to distribute the visual attention between potential AOIs.
These eye tracking studies mainly relied on the sta- tistical analysis of AOI-based fixation metrics. The main focus of these studies was on static visualizations where the definition of AOIs is less complicated than with dynamic content. As discussed, the proper defini- tion of AOIs and the influence of the visual span are two important points that have to be considered dur- ing the design process of a study.
If performed, the visual data analysis was often lim- ited to the investigation of attention maps and scan- path visualizations. For the identification of visual reading strategies, more advanced visual analytics tech- niques were applied. However, none of the above stud- ies investigated the full sequence length of scanpaths or any complex spatiotemporal characteristics of eye tracking for dynamic stimuli, let alone any cognitive aspects related to the mixed-initiative distribution of cognition in visual analytics.
Considering the applied eye tracking hardware, the main part of the studies was conducted with a remote eye tracking system, which should be sufficient for studies with one participant. In collaborative scenar- ios, for example a visualization expert working with a domain expert, wearable eye tracking glasses for each expert are required to capture eye movements from both participants.
In addition to these important points, we take a look into the related communities to obtain further inspira- tion of how eye tracking evaluation of visual analytics might be performed in the future.
Related communities
In this section, we discuss publications of the CHI and ETRA proceedings that provided extended evaluation methods using eye tracking. Technically, not only video- based eye tracking, but also electrooculography and head tracking was applied to estimate the participants’ point of regard. Since these approaches are less precise and less suited for an application to visual analytics sce- narios, we focused on the video-based systems.
In the CHI literature, eye tracking is applied for two main reasons: hands-free interactions with com- puters and evaluation for usability testing. Evaluations mainly focused on websites, text, and GUIs. Also, the gaze behavior during driving simulations, on mobile devices, and in code programming was investigated several times.
The ETRA literature also contains publications with similar research, since various authors published work at both conferences. Investigated stimuli
comprise those from CHI with more work investigat- ing videos and photographs, as well as artificial stimuli from psychological research.
In summary, the evaluation of standard eye tracking metrics can be found in most of these publications. In addition to these metrics, we identified additional pur- poses and approaches to analyze eye tracking data. Those could also be beneficial for the evaluation of visualizations.
Cognitive modeling and machine learning.66–70
Eye tracking data was analyzed to infer statistical mod- els that can predict and classify human behavior. Common examples comprise research to identify time spans of visual search and reading behavior, as well as visual saliency models that predict regions of interest in interactive environments. Such approaches can also be found in smaller numbers in the visualization commu- nity.71 Such models could also be applied to visual analy- tics. A predictive model could influence the design process of a system, telling the developer how the layout of visual components could be optimized. In addition, it could be used during the analysis to guide the user’s attention to relevant parts of a visualization.
Correlation of gaze and mouse data.72,73
Another important aspect of usability evaluation with eye tracking is to find out how mouse input and visual attention work together in different scenarios and tasks. The main focus of these publications was on the interaction behavior with websites. The application to complex GUIs such as visual analytics systems was not performed and will provide a challenge for future research.
Pupil dilation measurements.74–76 Physiological data from pupil dilation is often available from the recorded eye tracking data. The identified work on this topic considers the data as an indicator for cognitive load, arousal, and vigilance. A direct application of these measurements to other stimuli seems reasonable but to this point, we are not aware of any evaluation procedures in the visualization community that include these measures.
Retrospective think aloud.77,78 As a variant of the think aloud method, eye tracking data was included in a retrospective analysis (RTA). Gaze data was either displayed to the participants as a visual cue during the replay of their task performance or applied to check the validity of retrospective think aloud protocols. Although this combination still requires further investi- gation, a general application of the RTA method to a visual analytics context might be a good approach to produce reliable results, since eye tracking data and task performance are influenced by the think aloud method if performed during the task.
Downloaded from ivi.sagepub.com at Zhejiang University of Technology on September 21, 2016
12
Information Visualization
 We noticed that much more work on the sequential analysis of scanpaths exists in the CHI and ETRA publications. Here, the quantitative analysis of com- mon transition sequences between AOIs and similar scanpath patterns was also applied for the analysis of viewing strategies.
Future directions
With the availability of cheap eye tracking hardware and its ease of use, there are no longer any technologi- cal obstacles for using eye tracking in user-based eva- luation; in particular, in controlled laboratory studies, we can essentially record gaze data for free along with any traditional study procedure that aims to test task performance. Therefore, the big overall challenge is to make sense out of the eye tracking data and relate this data to something we want to learn about the visuali- zation tested and the cognitive processes involved. As discussed before, there are already several examples of eye tracking studies in visualization: they mostly work with statistical analysis of quite aggregated data, for well-defined hypotheses, and with traditional visual analysis by attention maps and gaze plots. In fact, many other laboratory studies could adopt these approaches to testing and data evaluation, adding a better understanding of reasons for task performance. Therefore, our general recommendation is that eye tracking should be considered as a testing method whenever you plan and design a laboratory study.
However, we see the real value of eye tracking going beyond what is possible now. Based on our reflections on the state of the art in the previous sections, we dis- cuss relevant directions for future research on evalua- tion methodology. We begin with more technologically oriented research questions asking for short-term action, and end with long-term grand challenges.
Study design
The study design for future evaluation procedures in visual analytics will have to consider some changes for the applied stimuli and tasks.
The visual stimuli (i.e. interactive visual analytics systems) should include the possibility to produce data to identify AOIs on the screen. Given that the ren- dered content is known, dynamic changes of position and size of a visual component can be tracked and logged. This preparation step will help increase the efficiency of the evaluation. The study design should already consider the granularity and type of potential AOIs.
For future research, the classical task performance analysis will not be sufficient to evaluate the insight gain of a participant using a visualization or visual
analytics system.30 Referring to the evaluation pipeline (Figure 1), this means that the ‘‘Task’’ section will sig- nificantly differ from classical performance analysis. New classes of tasks will be required that are less restrictive than classical ‘‘search and report’’ tasks. This approach leaves more freedom to the participant to explore a dataset, but increases the difficulty for the evaluation later on. In addition to the qualitative, open-ended protocol approach suggested by North,30 the analysis of eye tracking data, for example the iden- tification of reading strategies, could provide a quanti- tative component on the way to measure insight.
Exploratory data analysis and hypothesis building
Well-known statistical methods can be applied once we have a clearly defined hypothesis and an eye track- ing experiment set up accordingly. The interesting question is how we can design such an eye tracking experiment, in particular, for the complex visual repre- sentations and tasks in applications of visualization and visual analytics. Here, we see a great potential for improved data analysis methods that could work on eye tracking data acquired in less-constrained prelimi- nary studies. In fact, visual analytics will certainly play a major role here,22 in particular, for the complex spa- tiotemporal nature of the eye tracking data and the (dynamic) stimulus data, and by combining data-min- ing, statistical, and interactive visualization methods.
Scanpath comparison. One analysis aspect is most relevant, albeit difficult: improved scanpath anal- ysis. So far, the studies in the visualization community focused mainly on the spatial aspect of the recorded gaze data. Temporal aspects of the data, such as AOI sequences, provide important information about read- ing strategies but were often neglected completely or only partially covered through transition matrices. More work in this field has been performed in the related communities, often applying algorithms for statistical analysis. For a full understanding of com- mon scanpath patterns, a combination of automatic algorithms for processing these patterns and visualiza- tions for interpreting the patterns could be the best solution. Therefore, better visual analysis techniques for long sequence information are required. Since gaze plots tend to cause visual clutter with an increasing number of participants and scanpath length, a visual comparison becomes problematic with standard approaches. Alternative visualization approaches that combine automatic and visual scanpath compari- sons35,79 exist, but have not been used for an extended evaluation of visualization techniques to this point. Hence, we consider a visual analytics approach to fit
Downloaded from ivi.sagepub.com at Zhejiang University of Technology on September 21, 2016
Kurzhals et al.
13
 best for analyzing eye tracking data recorded from using visual analytics systems.
Data fusion. A third aspect is the combination of eye tracking data with additional time-oriented data. For exam- ple, the temporal evolution of the dynamic stimuli needs to be understood to build the context for the gaze data. Or, the eye tracking data can be combined with infor- mation about logged interaction such as mouse or key- stroke data, as to obtain deeper insights into the usabil- ity of interactive visualization applications and visual analytics systems. The evaluation of interactive systems solely based on gaze data and performance measure- ments might lack details for a full interpretation of the participants’ cognitive processes. Continuing the pre- liminary work from other communities, the fusion of multiple data source (e.g. eye tracking with interaction logs80) could provide this missing data for the interpreta- tion. In the field of visual analytics, where evaluated sys- tems are often far more complex than simple menus and websites, this approach opens a new research field where only little work exists to this point.
Another trend is to include other physiological mea- sures into an eye tracking experiment. For example, EEG measures can already be included in the software suites of known eye tracking vendors. Including such measures could help understand the interrelation between these components.
Since pupil dilation is already recorded by many eye tracking devices as an additional measurement, the current research from the other communities could also be applied for the evaluation of visual analytics and visualization techniques. For example, in long testing sessions when using a complex analysis tool, the participants could get tired and time spans when they just stare at the screen might occur. In these time spans, long fixations would be identified without any cognitive processing of the participants. Hence, an additional temporal measure for vigilance and cogni- tive load would increase the reliability of the gaze anal- ysis afterward.
Evaluation tools. A practical aspect is concerned with making the newly developed analysis methods available to other researchers. Reflecting a general dis- cussion in our community, we see the need for dissemi- nating codes, tools, and systems so that improved analysis can be adopted quickly. One way is to have advanced analysis methods included in professional software by the vendors of eye tracking hardware; however, this approach might not always work due to the latency in this software development process and because not all of our analysis problems will be sufficiently relevant for the broader eye tracking audience. Hence, there should also be dissemination of software (prototypes) developed, including complete analysis systems but also partial codes.
For example, we have shared our system ISeeCube35 with other eye tracking researchers and plan a public release for the future. Figure 5 shows an overview of the system for the comparison of multiple participants’ gaze data. For the temporal distribution of attention, timeline visualizations for individual AOIs as well as for the participants are displayed. The color-coded AOIs (a) display histograms of all participants’ atten- tion on them over time, allowing for efficient analysis when individual AOIs were relevant to the participants. For scanpath comparison, the individual timelines of all participants (b) can be investigated. In a scarf plot representation, the color-coded time spans of gaze data on AOIs is synchronized with the histograms. The spa- tiotemporal content of the data is presented in a space- time cube (c). Applying such a visual analytics system for an exploratory data analysis and hypothesis build- ing of other visual analytics systems provides a promis- ing direction for future research.
Evaluation procedures
We not only see the need for improved data analysis but also for extended evaluation procedures and protocols.
Think aloud. The ‘‘think aloud’’ protocol analy- sis23 is a method commonly used by HCI researchers to elicit user reports of subtasks that take place in the course of accomplishing a given task with a specific user interface. This approach has much to recommend when the goal is to produce a GOMS (Goals, Operators, Methods, and Selection rules81) model of the task as it takes place in a particular interface, to detect operational errors, and to document the process of repair of those errors. Since many perceptual pro- cesses are unconscious, user reports of how they detect known patterns and discover new patterns in data are not well elicited by this method. Taken in the context of cognitive models of task performance, the analysis of eye movement patterns may provide greater insight into unconscious aspects of task performance. The combination of think aloud protocol analysis and eye track- ing may well lead to insight into both task performance using visual information systems and the perceptual processes that allow users to understand information presented by those systems.
One approach to this could be taken from the work of Tanenhaus et al.82 By interpreting eye movements as they occur in response to verbal task instructions, they were able to show a much tighter integration of linguistic understanding and overt attention to objects in the environment. Tanenhaus et al. showed that spo- ken instructions can guide eye movements in real time, with close coupling of hearing and eye position. The application of this method to visualization tasks might
Downloaded from ivi.sagepub.com at Zhejiang University of Technology on September 21, 2016
14 Information Visualization
      Figure 5. Visual analysis of eye tracking data with ISeeCube: distribution of attention from participants can be analyzed on AOIs (a), for participants individually (b), and spatiotemporally in a space–time cube (c).
support a deeper theoretical understanding of visuali- zation use as well as guidance for the design and eva- luation of visualization environments. A translational approach to the use of eye movement research that might be productive for visualization researchers would be to adapt the methods used by Tanenhaus et al., leading to user studies with verbal task instruc- tions in combination with eye tracking.
Considering the results from our literature research, the option of a retrospective think aloud, where the participants verbalize their thoughts after the experi- ment, might also be considered. Especially when think aloud is performed during a task that is recorded with eye tracking, the resulting gaze patterns could signifi- cantly differ from the patterns without the verbaliza- tion of thoughts. In these cases, the retrospective approach could be the better choice, since the task itself can be performed without the additional effort of constantly thinking aloud.
Cognitive models. The prediction and classifica- tion of cognitive processes during complex tasks based on eye tracking data are a promising field of research for the evaluation of visual analytics systems. On the one hand, such models could be applied to predict design flaws in a visualization, on the other hand, the classification of eye movements belonging to certain cognitive processes (e.g. classifying a time span in the eye tracking protocol as visual search83) provides a deeper understanding of the recorded data.
Preliminary work on generating such models in the context of static information graphics84,85 and click- down menus66 already exists. Further research will be required to investigate whether and how these models can be adapted for more complex visual analytics scenarios.
Translational evaluation of human cognition
To move beyond the evaluation of usability and the techniques of visualization we must build an under- standing of human cognition as it is shaped by visual information systems. This builds upon work done in psychology and cognitive sciences. Therefore, we need to build bridges between cognitive and computing sci- ence, as it is done between HCI and visualization.86 Since these studies are not well-suited for application to visualization stimuli and tasks, we must move beyond off-the-shelf psychology and build transla- tional studies in partnership with interested cognitive scientists.
Eye movement records are a strong candidate for a boundary object:87 a method and data source that can be interpreted from both psychological and visualiza- tion perspectives, acting as a bridge between cognitive and computing science. Defining boundary objects such as eye tracking protocols and methodologies con- stitutes a grand challenge for visualization and visual analytics.
Downloaded from ivi.sagepub.com at Zhejiang University of Technology on September 21, 2016
Kurzhals et al.
15
 As an evaluation methodology, we see such bound- ary objects as linking functions for multi-scale evalua- tion. In this context, multi-scale is interpreted in an abstract sense: it reaches from low-level perception, over mid-level descriptions of tasks, all of the way to human cognitive processes. The challenge is to define the details of such linking so that the existing models on the different levels can be connected quantitatively.
We can speculate as to how eye tracking might play a role in this broader translational science agenda. We discussed attempts by Ribarsky et al.21 and others to build a high-level human cognitive model of visual analysis. These high-level models are necessarily descriptive, specifying the nature of the information processing operations that must occur for a visual analysis to take place. They do not of themselves make predictions about how a particular analysis process takes place over time and what bottlenecks might impede it. Adding temporal data from eye movements, think-aloud protocols, etc. gives us a richer perspective on the specific task and temporal processes, but the lack of connection between these two levels of analysis makes it difficult to see how they might support each other.
By taking a translational perspective on the human cognition model we can address this challenge, defin- ing the characteristics of the ‘‘Rosetta Stone’’ research that might allow us to bridge conceptual and process levels of analysis to advance a comprehensive and pre- dictive theory of visual analysis. We can see indications of this kind of bridging mechanism in Pylyshyn’s41 FINST hypothesis. FINSTs are proposed as a funda- mental mechanism of object-based attention that enables observers to parse complex visual scenes, e.g. to implement Gestalt grouping principles. Taking our eye tracking/protocol analysis data together with the human cognitive model and the FINST hypothesis, we can design a set of laboratory studies and field experiments that examine the interplay of multiple mechanisms that enable analysts to accomplish visuali- zation tasks:
  Overt attention: the sequence of eye positions by which we foveate display objects and attend to them, measured in eye movement studies in analy- sis task performance.
  Covert attention: the ways in which events taking place in the environment and the unfolding analy- sis process identifies the display item that is to be processed next, initiating processing and in many cases, specifying the eye movement that should occur next. These mechanisms are typically stud- ied in laboratory experiments, but their effects can be seen in field studies.
  Attentional token allocation: identification of multiple display items whose information and posi- tion carries information necessary for the analysis process. These are allocated a FINST so that they may be processed preferentially, e.g. for Gestalt group formation, subtilizing, and visual search. These mechanisms are also studied in laboratory experiments, but can be seen in field studies as well.
Taken together these measures may provide suffi- cient guidance for the development of a new (or adap- tation of an existing) computational theory of human cognitive architecture that will be sufficiently predic- tive that it might guide development of novel approaches to the design and evaluation of visual information systems. Architectures such as ACT-R,38 EPIC,88 and CLARION89 may come into play as we seek a greater level of insight into visual analysis and visual analytics.
Bridging cognitive and computing science with eye tracking as a boundary object is only a temporary solu- tion. As systems become more richly interactive and users develop perceptual and cognitive capabilities based on their experience with increasingly sophisti- cated visualization environments, we will find our- selves in the position of studying cognitive processes that are distributed between one or more human users and complex computational processes that provide non-trivial contribution to the overall cognitive system. At this point, we face the grandest challenge: to under- stand and design mixed-initiative cognitive systems where the mechanism of distribution is interaction with visua- lization environments.
Building the interdisciplinary scientific community
Computational analysis methods and the visualization environments that enable them to be used by humans grow increasingly complex. Methods such as eye movement monitoring can provide valuable insight into real-time cognitive processing in visual analysis, but can best be interpreted within the context of scien- tific theory of human cognition and performance. We have argued here and elsewhere90 that a translational cognitive science of human interaction with visualiza- tion environments is the best, and perhaps the only, pathway by which we can achieve this objective.
This will require computer scientists to build application development and user testing methods that incorporate scientific principles in software engi- neering processes. It will also call upon a new group of translational researchers whose background is in the
Downloaded from ivi.sagepub.com at Zhejiang University of Technology on September 21, 2016
16
Information Visualization
 cognitive and social sciences, to evaluate visualization systems and performance requirements, conducting field experiments91 as needed in order to generate fun- damental research questions for laboratory investiga- tion by cognitive scientists. The findings from these laboratory studies must then be mapped onto the design process, generating a spiral of directed scientific research, application to visualization system, and refo- cused scientific investigation. If we are successful, this will lead to a progressive science of analytical reason- ing facilitated by interactive visual interfaces together with a new approach to visualization system develop- ment that is grounded in scientific facts and evaluated using scientific methods.
Conclusion
We presented a literature overview of the current appli- cation of eye tracking technology for the evaluation of visualization techniques. The common research ques- tions that have been investigated comprise the compar- ison of different techniques, the comparison of user groups, the influence of a visualization on the distribu- tion of attention, and the identification of visual read- ing strategies.
To this point, only standard eye tracking metrics and basic visual analysis approaches are applied for the analysis of gaze data. For the extension of future evaluations, we proposed the application of other eva- luation techniques and cognitive models from other fields to the evaluation of visualization and visual ana- lytics. As a long-term grand challenge, we discussed the translational evaluation of human cognition for visual analytics. With an extended focus not only on the evaluation of usability, but also on the user experi- ence and the gain of insight, these extended tech- niques and approaches have the potential to provide insights far beyond completion times and error rates.
Funding
This work was funded by the German Research Foundation (DFG) (grant numbers WE 2836/5-1, WE 2836/1-2, SFB/Transregio 161/B01).
References
1. Duchowski A. A breadth-first survey of eye-tracking applications. Behav Res Meth Instr Comput 2002; 34: 455–470.
2. Yarbus AL. Eye Movements and Vision. New York: Ple- num Press, 1967.
3. Biederman I. Perceiving real-world scenes. Science 1972; 177(4043): 77–80.
4. Loftus GR and Mackworth NH. Cognitive determinants of fixation location during picture viewing. J Exp Psy- chol: Human Percept Perform 1978; 4: 565–572.
5. DeAngelus M and Pelz JB. Top-down control of eye movements: Yarbus revisited. Vis Cogn 2009; 17: 790–811.
6. Torralba A, Oliva A, Castelhano MS and Henderson JM. Contextual guidance of eye movements and atten- tion in real-world scenes: The role of global features in object search. Psychol Rev 2006; 113: 766–786.
7. Blascheck T, Kurzhals K, Raschke M, et al. State-of- the-art of visualization for eye tracking data. In Proceed- ings EuroVis state of the art reports, 2014, pp. 63–82.
8. Biederman I and Gerhardstein PC. Viewpoint-depen- dent mechanisms in visual object recognition: Reply to Tarr and Bu ̈lthoff. J Exp Psychol: Human Percept Perform 1995; 21(6): 1506–1514.
9. Zelinsky GJ, Rao RPN, Hayhoe MM, Ballard DH and Ballard DH. Eye movements reveal the spatiotem- poral dynamics of visual search. Psychol Sci 1997; 8(6): 448–453.
10. Kurzhals K, Fisher BD, Burch M and Weiskopf D. Evaluating visual analytics with eye tracking. In Pro- ceedings of the fifth workshop on beyond time and errors: novel evaluation methods for visualization (BELIV ’14), pp. 61–69.
11. Carpendale S. Evaluating information visualizations. In Kerren A, Stasko JT, Fekete JD and North C (eds) Infor- mation Visualization: Human-Centered Issues and Perspec- tives. New York: Springer, 2008, pp. 19–45.
12. Plaisant C. The challenge of information visualization evaluation. In Proceedings of the working conference on advanced visual interfaces AVI ’04. New York: ACM Press, pp. 109–116.
13. van Wijk JJ. Evaluation: A challenge for visual analytics. IEEE Comput 2013; 46(7): 56–60.
14. Elmqvist N and Yi JS. Patterns for visualization evaluation. In Proceedings of the workshop on beyond time and errors: novel evaluation methods for visualization (BELIV ’12), pp. 12:1– 12:8.
15. Tory M. User studies in visualization: A reflection on methods. In Huang W (ed.) Handbook Human Centric Visualization. New York: Springer, 2014, pp. 411–426.
16. Freitas CMDS, Pimenta MS and Scapin DL. User-cen- tered evaluation of information visualization techniques: Making the HCI-InfoVis connection explicit. In Huang W (ed.) Handbook of Human Centric Visualization. New York: Springer, 2014, pp. 315–336.
17. Goldberg JH and Helfman JI. Comparing information graphics: A critical look at eye tracking. In Proceedings of the workshop on beyond time and errors: novel evaluation methods for visualization (BELIV ’10), pp. 71–78.
18. Lam H, Bertini E, Isenberg P, et al. Empirical studies in information visualization: Seven scenarios. IEEE Trans Vis Comput Graph 2012; 18: 1520–1536.
19. Isenberg T, Isenberg P, Chen J, et al. A systematic review on the practice of evaluating visualization. IEEE Trans Vis Comput Graph 2013; 19: 2818–2827.
Downloaded from ivi.sagepub.com at Zhejiang University of Technology on September 21, 2016
Kurzhals et al. 17
 20. Thomas JJ and Cook KA (eds.) Illuminating the Path: The Research and Development Agenda for Visual Analytics. Los Alamitos, CA: IEEE Computer Society Press, 2005.
21. Ribarsky W, Fisher BD and Pottenger WM. Science of analytical reasoning. Info Vis 2009; 8: 254–262.
22. Andrienko GL, Andrienko NV, Burch M and Weiskopf D. Visual analytics methodology for eye movement stud- ies. IEEE Trans Vis Comput Graph 2012; 18: 2889–2898.
23. Ericsson KA and Simon HA. Protocol Analysis: Verbal Reports as Data. Cambridge, MA: MIT Press, 1993.
24. Anderson EW. Evaluating visualization using cognitive
measures. In Proceedings of the workshop on beyond time and errors: novel evaluation methods for visualization (BELIV ’12), pp. 5:1–5:4.
25. Klingner J, Kumar R and Hanrahan P. Measuring the task-evoked pupillary response with a remote eye tracker. In Proceedings of the symposium on ETRA ’08, pp. 69–72.
26. Holmqvist K, Nystro ̈m M, Andersson R, et al. Eye Tracking: A Comprehensive Guide to Methods and Mea- sures. Oxford: Oxford University Press, 2011.
27. Poole A and Ball L. Eye tracking in human–computer interaction and usability research: Current status and future prospects. In Ghaoui C (ed.) Encyclopedia of Human–Computer Interaction. Idea Group Inc., 2006, pp. 211–219.
28. Courtemanche F, A ̈ımeur E, Dufresne A, et al. Activity recognition using eye-gaze movements and traditional interactions. Interact Comput 2011; 23: 202–213.
29. Grindinger T, Duchowski AT and Sawyer MW. Group- wise similarity and classification of aggregate scanpaths. In Proceedings of the symposium on ETRA ’10, pp. 101– 104.
30. North C. Toward measuring visualization insight. Com- put Graph Applic 2006; 26(3): 6–9.
31. Schulz HJ, Nocke T, Heitzler M and Schumann H. A design space of visualization tasks. IEEE Trans Vis Com- put Graph 2013; 19: 2366–2375.
32. Burch M, Kull A and Weiskopf D. AOI Rivers for visua- lizing dynamic eye gaze frequencies. Comput Graph Forum 2013; 32: 281–290.
33. Tsang HY, Tory M and Swindells C. eSeeTrack – visualizing sequential fixation patterns. IEEE Trans Vis Comput Graph 2010; 16: 953–962.
34. Kurzhals K and Weiskopf D. Space-time visual analytics of eye-tracking data for dynamic stimuli. IEEE Trans Vis Comput Graph 2013; 19: 2129–2138.
35. Kurzhals K, Heimerl F and Weiskopf D. ISeeCube: Visual analysis of gaze data for video. In Proceedings of the symposium on ETRA ’14, pp. 43–50.
36. Ristovski G, Hunter M and Olk B and Linsen L. EyeC: coordinated views for interactive visual exploration of eye-tracking data. In Proceedings of the conference on infor- mation visualization (IV), 2013, pp. 239–248.
37. Trevarthen C. Two mechanisms of vision in primates. Psychol Res 1968; 31: 299–337.
38. Anderson JR. The Architecture of Cognition. Cambridge, MA: Psychology Press, 2013.
39. Anderson JR, Bothell D, Byrne MD, et al. An integrated theory of the mind. Psychol Rev 2004; 111: 1036–1060.
40. Liu G, Austen EL, Booth KS, et al. Multiple-object tracking is based on scene, not retinal, coordinates. J Exp Psychol: Human Percept Perform 2005; 31: 235–247.
41. Pylyshyn ZW. Situating vision in the world. Trends Cogn Sci 2000; 4: 197–207.
42. Ware C. Visual queries: The foundation of visual think- ing. In Tergan SO and Keller T (eds.) Knowledge and Information Visualization (Lecture Notes in Computer Science, vol. 3426). New York: Springer, 2005, pp. 27–35.
43. Spence B. The broker. In Ebert A, Dix A, Gershon ND, et al. (eds.) Human Aspects of Visualization (Lecture Notes in Computer Science, vol. 6431). New York: Springer, 2011, pp. 10–22.
44. Po BA, Fisher BD and Booth KS. Pointing and visual feedback for spatial interaction in large-screen dis- play environments. In Proceedings of the third interna- tional symposium on smart graphics (Lecture Notes in Computer Science, vol. 2733). New York: Springer, 2003, pp. 22–38.
45. Faraday P and Sutcliffe A. Designing effective multime- dia presentations. In Proceedings of the ACM SIGCHI con- ference on human factors in computing systems (CHI ’97). New York: ACM Press, pp. 272–278.
46. Bolt RA. Eyes at the interface. In Proceedings of the 1982 conference on human factors in computing systems (CHI ’82). New York: ACM Press, pp. 360–362.
47. Beck F, Koch S and Weiskopf D. Visual analysis and dis- semination of scientific literature collections with SurVis. IEEE Trans Vis Comput Graph 2016; 22: DOI:10.1109/ TVCG.2015.2467757.
48. Huang W. Using eye tracking to investigate graph layout effects. In Proceedings of the Asia–Pacific symposium on visualization, 2007, pp. 97–100.
49. Kim SH, Dong Z, Xian H, et al. Does an eye tracker tell the truth about visualizations? Findings while investigat- ing visualizations for decision making. IEEE Trans Vis Comput Graph 2012; 18: 2421–2430.
50. Kim Y and Varshney A. Persuading visual attention through geometry. IEEE Trans Vis Comput Graph 2008; 14: 772–782.
51. Kurzhals K, Ho ̈ferlin M and Weiskopf D. Evaluation of attention-guiding video visualization. Comput Graph Forum 2013; 32(3): 51–60.
52. Netzel R, Burch M and Weiskopf D. Comparative eye tracking study on node-link visualizations of trajec- tories. IEEE Trans Vis Comput Graph 2014; 20: 2221–2230.
53. Siirtola H, Laivo T, Heimonen T and Raiha KJ. Visual perception of parallel coordinate visualizations. In Pro- ceedings of the conference on information visualization (IV), 2009, pp. 3–9.
54. Stengel M, Bauszat P, Eisemann M, et al. Temporal video filtering and exposure control for perceptual motion blur. IEEE Trans Vis Comput Graph 2014; 21: 663–671.
55. Swindells C, Tory M and Dreezer R. Comparing para- meter manipulation with mouse, pen, and slider user interfaces. Comput Graph Forum 2009; 28: 919–926.
Downloaded from ivi.sagepub.com at Zhejiang University of Technology on September 21, 2016
18
Information Visualization
 56. Burch M, Andrienko G, Andrienko N, et al. Visual task solution strategies in tree diagrams. In Proceedings of the IEEE Pacific visualization symposium, 2013, pp. 169–176.
57. Burch M, Konevtsova N, Heinrich J, et al. Evaluation of traditional, orthogonal, and radial tree diagrams by an eye tracking study. IEEE Trans Vis Comput Graph 2011; 17: 2440–2448.
58. Goldberg J and Helfman J. Eye tracking for visualization evaluation: Reading values on linear versus radial graphs. Info Vis 2011; 10: 182–195.
59. Griffin A and Robinson A. Comparing color and leader line highlighting strategies in coordinated view geovisua- lizations. IEEE Trans Vis Comput Graph 2015; 21: 339– 349.
60. Huang W and Eades P. How people read graphs. In Pro- ceedings of the Asia–Pacific symposium on visualization, 2005, pp. 51–58.
61. Jianu R, Rusu A, Hu Y and Taggart D. How to display group information on node-link diagrams: An evalua- tion. IEEE Trans Vis Comput Graph 2014; 20: 1530– 1541.
62. Tory M, Atkins MS, Kirkpatrick AE, et al. Eyegaze anal- ysis of displays with combined 2D and 3D views. In Pro- ceedings of the IEEE visualization conference, 2005, pp. 519–526.
63. Bekele E, Zheng Z, Swanson A, et al. Understanding how adolescents with autism respond to facial expres- sions in virtual reality environments. IEEE Trans Vis Comput Graph 2013; 19: 711–720.
64. Song H, Yun J and Kim B and Seo J. GazeVis: interac- tive 3D gaze visualization for contiguous cross-sectional medical images. IEEE Trans Vis Comput Graph 2014; 20: 726–739.
65. Reingold EM, Charness N, Pomplun M, et al. Visual span in expert chess players: Evidence from eye move- ments. Psychol Sci 2001; 12: 48–55.
66. Byrne MD, Anderson JR, Douglass S and Matessa M. Eye tracking the visual search of click-down menus. In Proceedings of the SIGCHI conference on human factors in computing systems (CHI ’99). New York: ACM Press, pp. 402–409.
computing systems (CHI ’99). New York: ACM Press, pp.
254–261.
71. Hillaire S, Lecuyer A, Regia-Corte T, et al. Design and
application of real-time visual attention model for the exploration of 3D virtual environments. IEEE Trans Vis Comput Graph 2012; 18: 356–368.
72. Guan Z and Cutrell E. An eye tracking study of the effect of target rank on web search. In Proceedings of the SIGCHI conference on human factors in computing systems (CHI ’07). New York: ACM Press, pp. 417–420.
73. Huang J, White RW and Dumais S. No clicks, no prob- lem: Using cursor movements to understand and improve search. In Proceedings of the SIGCHI conference on human factors in computing systems (CHI ’11). New York: ACM Press, pp. 1225–1234.
74. Iqbal ST, Adamczyk PD, Zheng XS and Bailey BP. Towards an index of opportunity: Understanding changes in mental workload during task execution. In Proceedings of the SIGCHI conference on human factors in computing systems (CHI ’05). New York: ACM Press, pp. 311–320.
75. Palinko O, Kun AL, Shyrokov A and Heeman P. Esti- mating cognitive load using remote eye tracking in a driving simulator. In Proceedings of the symposium on ETRA ’10, pp. 141–144.
76. Partala T, Jokiniemi M and Surakka V. Pupillary responses to emotionally provocative stimuli. In Proceedings of the symposium on ETRA ’00, pp. 123–129.
77. Elling S, Lentz L and de Jong M. Retrospective think- aloud method: Using ey movements as an extra cue for participants’ verbalizations. In Proceedings of the SIGCHI conference on human factors in computing systems (CHI ’11). New York: ACM Press, pp. 1161–1170.
78. Guan Z, Lee S, Cuddihy E and Ramey J. The validity of the stimulated retrospective think-aloud method as mea- sured by eye tracking. In Proceedings of the SIGCHI con- ference on human factors in computing systems (CHI ’06). New York: ACM Press, pp. 1253–1262.
79. Raschke M, Herr D, Blascheck T, et al. A visual approach for scan path comparison. In Proceedings of the symposium on ETRA ’14, pp. 135–142.
80. Blascheck T and Ertl T. Towards analyzing eye tracking data for evaluating interactive visualization systems. In Proceedings of the workshop on beyond time and errors: novel evaluation methods for visualization (BELIV ’14), pp. 70–
67. Ehret BD. Learning where to look: Location learning
in graphical user interfaces. In Proceedings of the
SIGCHI conference on human factors in computing systems
(CHI ’02). New York: ACM Press, pp. 211–218. 77.
68. Hornof AJ and Halverson T. Cognitive strategies and eye movements for searching hierarchical computer displays. In Proceedings of the SIGCHI conference on human factors in computing systems CHI ’03. New York: ACM Press, pp. 249–256.
69. Peters RJ and Itti L. Computational mechanisms for gaze direction in interactive visual environments. In Pro- ceedings of the symposium on ETRA ’06. New York: ACM Press, pp. 27–32.
70. Salvucci DD. Inferring intent in eye-based interfaces: Tracing eye movements with process models. In Proceed- ings of the SIGCHI conference on human factors in
81. Card SK, Newell A and Moran TP. The Psychology of Human–Computer Interaction. L. Erlbaum Associates Inc. Hillsdale, NJ, USA, 1983.
82. Tanenhaus MK, Spivey-Knowlton MJ, Eberhard KM and Sedivy JC. Integration of visual and linguistic infor- mation in spoken language comprehension. Science 1995; 268(5217): 1632–1634.
83. Halverson T and Hornof AJ. A minimal model for pre- dicting visual search in human–computer interaction. In Proceedings of the SIGCHI conference on human factors in computing systems (CHI ’07). New York: ACM Press, pp. 431–434.
Downloaded from ivi.sagepub.com at Zhejiang University of Technology on September 21, 2016
Kurzhals et al. 19
 84. Raschke M, Blascheck T, Richter M, et al. Visual analy- sis of perceptual and cognitive processes. In Proceedings of the international joint conference on computer vision, ima- ging and computer graphics theory and applications (IVAAP), 2014, pp. 284–291.
85. Raschke M, Engelhardt S and Ertl T. A framework for simulating visual search strategies. In Proceedings of the 11th international conference on cognitive modeling, Ottawa, 2013, pp. 221–226.
86. Ebert A, van der Veer G, Domik G, et al. (eds.) Building Bridges: HCI, Visualization, and Non-formal Modeling. Berlin: Springer-Verlag, 2014.
87. Star SL. The structure of ill-structured solutions: Boundary objects and heterogeneous distributed prob- lem solving. In Huhns M and Gasser L (eds.) Readings
in Distributed AI. San Mateo, CA: Morgan Kaufmann,
1989.
88. Kieras DE and Meyer DE. An overview of the EPIC
architecture for cognition and performance with applica- tion to human–computer interaction. Human–Comput Interact 1997; 12: 391–438.
89. Sun R, Merrill E and Peterson T. From implicit skills to explicit knowledge: A bottom-up model of skill learning. Cogn Sci 2001; 25: 203–244.
90. Fisher BD, Green TM and Arias-Hernandez R. Visual analytics as a translational cognitive science. Top Cogn Sci 2011; 3(3): 609–625.
91. Kaastra LT, Arias-Hernandez R and Fisher BD. Evalu- ating analytic performance. In: Proceedings of the work- shop on: beyond time and errors: novel evaluation methods for visualization (BELIV ’12), pp. 14: 1–14:3.
Downloaded from ivi.sagepub.com at Zhejiang University of Technology on September 21, 2016
Eye Tracking for Everyone
Kyle Krafka∗† Aditya Khosla∗‡ Petr Kellnhofer‡⋆ Harini Kannan‡
Suchendra Bhandarkar† Wojciech Matusik‡ Antonio Torralba‡
†University of Georgia ‡Massachusetts Institute of Technology ⋆MPI Informatik {krafka, suchi}@cs.uga.edu {khosla, pkellnho, hkannan, wojciech, torralba}@csail.mit.edu
  Abstract
From scientific research to commercial applications, eye tracking is an important tool across many domains. Despite its range of applications, eye tracking has yet to become a pervasive technology. We believe that we can put the power of eye tracking in everyone’s palm by building eye tracking software that works on commodity hardware such as mobile phones and tablets, without the need for additional sensors or devices. We tackle this problem by introducing GazeCap- ture, the first large-scale dataset for eye tracking, contain- ing data from over 1450 people consisting of almost 2.5M frames. Using GazeCapture, we train iTracker, a convolu- tional neural network for eye tracking, which achieves a sig- nificant reduction in error over previous approaches while running in real time (10–15fps) on a modern mobile de- vice. Our model achieves a prediction error of 1.71cm and 2.53cm without calibration on mobile phones and tablets respectively. With calibration, this is reduced to 1.34cm and 2.12cm. Further, we demonstrate that the features learned by iTracker generalize well to other datasets, achieving state-of-the-art results. The code, data, and models are available at http://gazecapture.csail.mit.edu.
1. Introduction
From human–computer interaction techniques [16, 23, 26] to medical diagnoses [12] to psychological studies [27] to computer vision [3, 18], eye tracking has applications in many areas [6]. Gaze is the externally-observable indica- tor of human visual attention, and many have attempted to record it, dating back to the late eighteenth century [14]. To- day, a variety of solutions exist (many of them commercial) but all suffer from one or more of the following: high cost (e.g., Tobii X2-60), custom or invasive hardware (e.g., Eye Tribe, Tobii EyeX) or inaccuracy under real-world condi-
∗ indicates equal contribution
Corresponding author: Aditya Khosla (khosla@csail.mit.edu)
Figure 1: In this work, we develop GazeCapture, the first large-scale eye tracking dataset captured via crowdsourc- ing. Using GazeCapture, we train iTracker, a convolutional neural network for robust gaze prediction.
tions (e.g., [25, 34, 43]). These factors prevent eye tracking from becoming a pervasive technology that should be avail- able to anyone with a reasonable camera (e.g., a smartphone or a webcam). In this work, our goal is to overcome these challenges to bring eye tracking to everyone.
We believe that this goal can be achieved by develop- ing systems that work reliably on mobile devices such as smartphones and tablets, without the need for any external attachments (Fig. 1). Mobile devices offer several benefits over other platforms: (1) widespread use—more than a third of the world’s population is estimated to have smartphones by 2019 [32], far exceeding the number of desktop/laptop users; (2) high adoption rate of technology upgrades—a large proportion of people have the latest hardware allow- ing for the use of computationally expensive methods, such as convolutional neural networks (CNNs), in real-time; (3) the heavy usage of cameras on mobile devices has lead to rapid development and deployment of camera technology, and (4) the fixed position of the camera relative to the screen reduces the number of unknown parameters, potentially al-
                                                 1
arXiv:1606.05814v1 [cs.CV] 18 Jun 2016
iTracker GazeCapture
 lowing for the development of high-accuracy calibration- free tracking.
The recent success of deep learning has been apparent in a variety of domains in computer vision [20, 7, 36, 28, 19], but its impact on improving the performance of eye tracking has been rather limited [43]. We believe that this is due to the lack of availability of large-scale data, with the largest datasets having ∼50 subjects [13, 34]. In this work, us- ing crowdsourcing, we build GazeCapture, a mobile-based eye tracking dataset containing almost 1500 subjects from a wide variety of backgrounds, recorded under variable light- ing conditions and unconstrained head motion.
Using GazeCapture, we train iTracker, a convolutional neural network (CNN) learned end-to-end for gaze predic- tion. iTracker does not rely on any preexisting systems for head pose estimation or other manually-engineered features for prediction. Training the network with just crops of both eyes and the face, we outperform existing eye tracking ap- proaches in this domain by a significant margin. While our network achieves state-of-the-art performance in terms of accuracy, the size of the inputs and number of parameters make it difficult to use in real-time on a mobile device. To address this we apply ideas from the work on dark knowl- edge by Hinton et al. [11] to train a smaller and faster net- work that achieves real-time performance on mobile devices with a minimal loss in accuracy.
Overall, we take a significant step towards putting the power of eye tracking in everyone’s palm.
2. Related Work
There has been a plethora of work on predicting gaze. Here, we give a brief overview of some of the existing gaze estimation methods and urge the reader to look at this ex- cellent survey paper [8] for a more complete picture. We also discuss the differences between GazeCapture and other popular gaze estimation datasets.
Gaze estimation: Gaze estimation methods can be di- vided into model-based or appearance-based [8]. Model- based approaches use a geometric model of an eye and can be subdivided into corneal-reflection-based and shape- based methods. Corneal-reflection-based methods [42, 45, 46, 10] rely on external light sources to detect eye features. On the other hand, shape-based methods [15, 4, 39, 9] in- fer gaze direction from observed eye shapes, such as pupil centers and iris edges. These approaches tend to suffer with low image quality and variable lighting conditions, as in our scenario. Appearance-based methods [37, 30, 22, 21, 38, 2] directly use eyes as input and can potentially work on low-resolution images. Appearance-based methods are be- lieved [43] to require larger amounts of user-specific train- ing data as compared to model-based methods. However, we show that our model is able to generalize well to novel faces without needing user-specific data. While calibration
# People [24] 20
Poses Targets Illum. 1 16 1 [40] 20 19 2–9 1
Images videos 1,236 5,880 videos 64,000 213,659 videos 2,445,504
    [31] 56 [25] 16 [34] 50 [43] 15 [13] 51 Ours 1474
5 21 1
 cont. 8+synth. cont. cont. cont.
cont. 2 160 1
cont. cont. 35 cont. 13+cont. cont.
      Table 1: Comparison of our GazeCapture dataset with pop- ular publicly available datasets. GazeCapture has approx- imately 30 times as many participants and 10 times as many frames as the largest datasets and contains a signif- icant amount of variation in pose and illumination, as it was recorded using crowdsourcing. We use the following abbre- viations: cont. for continuous, illum. for illumination, and synth. for synthesized.
is helpful, its impact is not as significant as in other ap- proaches given our model’s inherent generalization ability achieved through the use of deep learning and large-scale data. Thus, our model does not have to rely on visual saliency maps [5, 33] or key presses [35] to achieve accurate calibration-free gaze estimation. Overall, iTracker is a data- driven appearance-based model learned end-to-end without using any hand-engineered features such as head pose or eye center location. We also demonstrate that our trained networks can produce excellent features for gaze predic- tion (that outperform hand-engineered features) on other datasets despite not having been trained on them.
Gaze datasets: There are a number of publicly available gaze datasets in the community [24, 40, 31, 25, 34, 43, 13]. We summarize the distinctions from these datasets in Tbl. 1. Many of the earlier datasets [24, 40, 31] do not contain sig- nificant variation in head pose or have a coarse gaze point sampling density. We overcome this by encouraging par- ticipants to move their head while recording and generating a random distribution of gaze points for each participant. While some of the modern datasets follow a similar ap- proach [34, 25, 43, 13], their scale—especially in the num- ber of participants—is rather limited. We overcome this through the use of crowdsourcing, allowing us to build a dataset with ∼30 times as many participants as the current largest dataset. Further, unlike [43], given our recording permissions, we can release the complete images without post-processing. We believe that GazeCapture will serve as an invaluable resource for future work in this domain.
3. GazeCapture: A Large-Scale Dataset
In this section, we describe how we achieve our goal of scaling up the collection of eye tracking data. We find that
most existing eye tracking datasets have been collected by researchers inviting participants to the lab, a process that leads to a lack of variation in the data and is costly and inef- ficient to scale up. We overcome these limitations through the use of crowdsourcing, a popular approach for collecting large-scale datasets [29, 19, 44, 28]. In Sec. 3.1, we describe the process of obtaining reliable data via crowdsourcing and in Sec. 3.2, we compare the characteristics of GazeCapture with existing datasets.
3.1. Collecting Eye Tracking Data
Our goal here is to develop an approach for collecting eye tracking data on mobile devices that is (1) scalable, (2) reliable, and (3) produces large variability. Below, we de- scribe, in detail, how we achieve each of these three goals.
Scalability: In order for our approach to be scalable, we must design an automated mechanism for gathering data and reaching participants. Crowdsourcing is a popular technique researchers use to achieve scalability. The pri- mary difficulty with this approach is that most crowdsourc- ing platforms are designed to be used on laptops/desktops and provide limited flexibility required to design the de- sired user experience. Thus, we decided to use a hybrid approach, combining the scalable workforce of crowdsourc- ing platforms together with the design freedom provided by building custom mobile applications. Specifically, we built an iOS application, also named GazeCapture1, capable of recording and uploading gaze tracking data, and used Ama- zon Mechanical Turk (AMT) as a platform for recruiting people to use our application. On AMT, the workers were provided detailed instructions on how to download the ap- plication from Apple’s App Store and complete the task.
We chose to build the GazeCapture application for Ap- ple’s iOS because of the large-scale adoption of latest Ap- ple devices, and the ease of deployment across multiple de- vice types such as iPhones and iPads using a common code base. Further, the lack of fragmentation in the versions of the operating system (as compared to other platforms) sig- nificantly simplified the development process. Additionally, we released the application publicly to the App Store (as op- posed to a beta release with limited reach) simplifying in- stallation of our application, thereby further aiding the scal- ability of our approach.
Reliability: The simplest rendition of our GazeCapture application could involve showing workers dots on a screen at random locations and recording their gaze using the front- facing camera. While this approach may work well when calling individual participants to the lab, it is not likely to produce reliable results without human supervision. Thus, we must design an automatic mechanism that ensures work- ers are paying attention and fixating directly on the dots shown on the screen.
1 http://apple.co/1q1Ozsg
Display Dot
0.5s
Start Recording
1.5s
Display Letter
Hide Dot, Wait for Response
                     Figure 2: The timeline of the display of an individual dot. Dotted gray lines indicate how the dot changes size over time to keep attention.
First, to avoid distraction from notifications, we ensure that the worker uses Airplane Mode with no network con- nection throughout the task, until the task is complete and ready to be uploaded. Second, instead of showing a plain dot, we show a pulsating red circle around the dot, as shown in Fig. 2, that directs the fixation of the eye to lie in the middle of that circle. This pulsating dot is shown for ap- proximately 2s and we start the recording 0.5sec. after the dot moves to a new location to allow enough time for the worker to fixate at the dot location. Third, towards the end of the 2sec. window, a small letter, L or R is displayed for 0.05sec.—based on this letter, the worker is required to tap either the left (L) or right (R) side of the screen. This serves as a means to monitor the worker’s attention and provide en- gagement with the application. If the worker taps the wrong side, they are warned and must repeat the dot again. Last, we use the real-time face detector built into iOS to ensure that the worker’s face is visible in a large proportion of the recorded frames. This is critical as we cannot hope to track where someone is looking without a picture of their eyes.
Variability: In order to learn a robust eye tracking model, significant variability in the data is important. We believe that this variability is critical to achieving high- accuracy calibration-free eye tracking. Thus, we designed our setup to explicitly encourage high variability.
First, given our use of crowdsourcing, we expect to have a large variability in pose, appearance, and illumination. Second, to encourage further variability in pose, we tell the workers to continuously move their head and the distance of the phone relative to them by showing them an instructional video with a person doing the same. Last, we force workers to change the orientation of their mobile device after every 60 dots. This change can be detected using the built-in sen- sors on the device. This changes the relative position of the camera and the screen providing further variability.
Implementation details: Here, we provide some imple- mentation details that may be helpful for other researchers conducting similar studies. In order to associate each mo- bile device with an AMT task, we provided each worker with a unique code in AMT that they subsequently typed into their mobile application. The dot locations were both random and from 13 fixed locations (same locations as Fig. 3 of [41])—we use the fixed locations to study the effect
“Tap left or right side”

 Figure 3: Sample frames from our GazeCapture dataset. Note the significant variation in illumination, head pose, appearance, and background. This variation allows us to learn robust models that generalize well to novel faces.
of calibration (Sec. 5.3). We displayed a total of 60 dots2 for each orientation of the device3 leading to a task duration of ∼10min. Each worker was only allowed to complete the task once and we paid them $1–$1.50. We uploaded the data as individual frames rather than a video to avoid compres- sion artifacts. Further, while we did not use it in this work, we also recorded device motion sensor data. We believe that this could be a useful resource for other researchers in the future.
3.2. Dataset Characteristics
We collected data from a total of 1474 subjects: 1103 subjects through AMT, 230 subjects through in-class re- cruitment at UGA, and 141 subjects through other var- ious App Store downloads. This resulted in a total of 2,445,504 frames with corresponding fixation locations. Sample frames are shown in Fig. 3. 1249 subjects used iPhones while 225 used iPads, resulting in a total of ∼ 2.1M and ∼ 360k frames from each of the devices respectively.
To demonstrate the variability of our data, we used the approach from [43] to estimate head pose, h, and gaze direction, g, for each of our frames. In Fig. 4 we plot the distribution of h and g on GazeCapture as well as existing state-of-the-art datasets, MPIIGaze [43] and TabletGaze [13]. We find that while our dataset contains a similar overall distribution of h there is a significantly larger
2This was the number of dots displayed when the user entered a code provided via AMT. When the user did not enter a code (typical case when the application is downloaded directly from the App Store), they were shown 8 dots per orientation to keep them engaged.
3Three orientations for iPhones and four orientations for iPads follow- ing their natural use cases.
proportion of outliers as compared to existing datasets. Further, we observe that our data capture technique from Sec. 3.1 introduces significant variation in the relative posi- tion of the camera to the user as compared to other datasets; e.g., we have frames where the camera is mounted below the screen (i.e., when the device is turned upside down) as well as above. These variations can be helpful for training and evaluating eye tracking approaches.
50 50 50
000
-50 -50 -50
-50 0 50 -50 0 50 -50 0 50
Yaw [deg] Yaw [deg] Yaw [deg]
(a) h(TabletGaze) (b) h(MPIIGaze) (c) h(GazeCapture)
20 20 20 10 10 10 000 -10 -10 -10 -20 -20 -20
-20 0 20 -20 0 20 -20 0 20
Yaw [deg] Yaw [deg] Yaw [deg]
(d) g(TabletGaze) (e) g(MPIIGaze) (f) g(GazeCapture)
Figure 4: Distribution of head pose h (1st row) and gaze direction g relative to the head pose (2nd row) for datasets TabletGaze, MPIIGaze, and GazeCapture (ours). All inten- sities are logarithmic.
Pitch [deg]
Pitch [deg]
Pitch [deg]
Pitch [deg]
Pitch [deg]
Pitch [deg]
         y
x
               right eye
left eye
face
face grid
shared weights
                      input frame
gaze prediction
  Figure 5: Overview of iTracker, our eye tracking CNN. Inputs include left eye, right eye, and face images detected and cropped from the original frame (all of size 224 × 224). The face grid input is a binary mask used to indicate the location and size of the head within the frame (of size 25 × 25). The output is the distance, in centimeters, from the camera. CONV rep- resents convolutional layers (with filter size/number of kernels: CONV-E1,CONV-F1: 11 × 11/96, CONV-E2,CONV-F2: 5 × 5/256, CONV-E3,CONV-F3: 3 × 3/384, CONV-E4,CONV-F4: 1 × 1/64) while FC represents fully-connected layers (with sizes: FC-E1: 128, FC-F1: 128, FC-F2: 64, FC-FG1: 256, FC-FG2: 128, FC1: 128, FC2: 2). The exact model configuration is available on the project website.
4. iTracker: A Deep Network for Eye Tracking
In this section, we describe our approach for building a robust eye tracker using our large-scale dataset, GazeCap- ture. Given the recent success of convolutional neural net- works (CNNs) in computer vision, we use this approach to tackle the problem of eye tracking. We believe that, given enough data, we can learn eye tracking end-to-end without the need to include any manually engineered features, such as head pose [43]. In Sec. 4.1, we describe how we de- sign an end-to-end CNN for robust eye tracking. Then, in Sec. 4.2 we use the concept of dark knowledge [11] to learn a smaller network that achieves a similar performance while running at 10–15fps on a modern mobile device.
4.1. Learning an End-to-End Model
Our goal is to design an approach that can use the infor- mation from a single image to robustly predict gaze. We choose to use deep convolutional neural networks (CNNs) to make effective use of our large-scale dataset. Specifi- cally, we provide the following as input to the model: (1) the image of the face together with its location in the im- age (termed face grid), and (2) the image of the eyes. We believe that using the model can (1) infer the head pose rela- tive to the camera, and (2) infer the pose of the eyes relative to the head. By combining this information, the model can infer the location of gaze. Based on this information, we
design the overall architecture of our iTracker network, as shown in Fig. 5. The size of the various layers is similar to those of AlexNet [20]. Note that we include the eyes as indi- vidual inputs into the network (even though the face already contains them) to provide the network with a higher resolu- tion image of the eye to allow it to identify subtle changes.
In order to best leverage the power of our large-scale dataset, we design a unified prediction space that allows us to train a single model using all the data. Note that this is not trivial since our data was collected using multiple devices at various orientations. Directly predicting screen coordinates would not be meaningful beyond a single device in a sin- gle orientation since the input could change significantly. Instead, we leverage the fact that the front-facing camera is typically on the same plane as, and angled perpendicu- lar to, the screen. As shown in Fig. 6, we predict the dot location relative to the camera (in centimeters in the x and y direction). We obtain this through precise measurements of device screen sizes and camera placement. Finally, we train the model using a Euclidean loss on the x and y gaze position. The training parameters are provided in Sec. 5.1.
Further, after training the joint network, we found fine- tuning the network to each device and orientation helpful. This was particularly useful in dealing with the unbalanced data distribution between mobile phones and tablets. We denote this model as iTracker∗ .
CONV-E4 CONV-E3
CONV-E4 CONV-E3
CONV-F5 CONV-F4 CONV-F3
CONV-E2 CONV-E1
CONV-E2 CONV-E1
CONV-F2 CONV-F1
FC -E1
FC -F2 FC - F1
FC -FG2 FC - FG1
FC2 FC1
 25 20 15 10
5
0 -5 -10 -15 -20 -25
-20 -10 0 10 20
X [cm]
calibration. Further, we demonstrate the importance of hav- ing a large-scale dataset as well as having variety in the data in terms of number of subjects rather than number of exam- ples per subject. Then, we apply the features learned by iTracker to an existing dataset, TabletGaze [13], to demon- strate the generalization ability of our model.
5.1. Setup
Data preparation: First, from the 2,445,504 frames in GazeCapture, we select 1,490,959 frames that have both face and eye detections. These detections serve as important inputs to the model, as described in Sec. 4.1. This leads to a total of 1471 subjects being selected where each person has at least one frame with a valid detection. Then, we divide the dataset into train, validation, and test splits consisting of 1271, 50, and 150 subjects5, respectively. For the vali- dation and test splits, we only select subjects who looked at the full set of points. This ensures a uniform data distri- bution in the validation/test sets and allows us to perform a thorough evaluation on the impact of calibration across these subjects. Further, we evaluate the performance of our approach by augmenting the training and test set 25-fold by shifting the eyes and the face, changing face grid appropri- ately. For training, each of the augmented samples is treated independently while for testing, we average the predictions of the augmented samples to obtain the prediction on the original test sample (similar to [20]).
Implementation details: The model was implemented using Caffe [17]. It was trained from scratch on the Gaze- Capture dataset for 150, 000 iterations with a batch size of 256. An initial learning rate of 0.001 was used, and after 75, 000 iterations, it was reduced to 0.0001. Further, simi- lar to AlexNet [20], we used a momentum of 0.9 and weight decay of 0.0005 throughout the training procedure. Further, we truncate the predictions based on the size of the device.
Evaluation metric: Similar to [13], we report the error in terms of average Euclidean distance (in centimeters) from the location of the true fixation. Further, given the differ- ent screen sizes, and hence usage distances of phones and tablets, we provide performance for both of these devices (even though the models used are exactly the same for both devices, unless otherwise specified). Lastly, to simulate a realistic use case where a stream of frames is processed for each given fixation rather than just a single frame, we re- port a value called dot error. In this case, the output of the classifier is given as the average prediction of all the frames corresponding to a gaze point at a certain location.
5.2. Unconstrained Eye Tracking
Here, our goal is to evaluate the generalization ability of iTracker to novel faces by evaluating it on unconstrained
5Train, validation and test splits contain 1,251,983, 59,480, and 179,496 frames, respectively.
Y [cm]
Figure 6: Our unified prediction space. The plot above shows the distribution of all dots in our dataset mapped to the prediction space. Axes denote centimeters from the camera; i.e., all dots on the screen are projected to this space where the camera is at (0, 0).
4.2. Real-Time Inference
As our goal is to build an eye tracker that is practically useful, we provide evidence that our model can be applied on resource-constrained mobile devices. Encouraged by the work of Hinton et al. [11], we apply dark knowledge to reduce model complexity and thus, computation time and memory footprint. First, while we designed the iTracker network to be robust to poor-quality eye detections, we use tighter crops (of size 80 × 80) produced by facial landmark eye detections [1] for the smaller network. These tighter crops focus the attention of the network on the more dis- criminative regions of the image, while also being faster due to the reduced image size. Then, we fine-tune the ar- chitecture configuration using the validation set to optimize efficiency without sacrificing much accuracy. Specifically, we have a combined loss on the ground truth, the predic- tions from our full model, as well as the features from the penultimate layer to assist the network in producing qual- ity results. We implemented this model on an iPhone using Jetpac’s Deep Belief SDK4. We found that the reduced ver- sion of the model took about 0.05sec. to run on a iPhone 6s. Combining this with Apple’s face detection pipeline, we can expect to achieve an overall detection rate of 10–15fps on a typical mobile device.
5. Experiments
In this section, we thoroughly evaluate the performance of iTracker using our large-scale GazeCapture dataset. Overall, we significantly outperform state-of-the-art ap- proaches, achieving an average error of ∼ 2cm without cal- ibration and are able to reduce this further to 1.8cm through
4 https://github.com/jetpacapp/DeepBeliefSDK
error (cm) 3.5
2.5 1.5
error (cm) 11
6
1
mobile phones
tablets
           Aug.
   Mobile phone error dot err.
 tr + te
2.99 2.40
None
2.04 1.62
te
1.84 1.58
tr
1.86 1.57
tr + te
1.77 1.53
tr + te
1.71 1.53
  None
 2.11 1.72
  None
 2.15 1.69
  None
 2.23 1.81
Model
Tablet error dot err.
  Baseline       5.13 4.54 iTracker       3.32 2.82
iTracker       3.21 2.90 iTracker       2.81 2.47 iTracker       2.83 2.53 iTracker∗       2.53 2.38
        iTracker (no eyes) iTracker (no face) iTracker (no fg.)
3.40 2.93 3.45 2.92 3.90 3.36
Figure 7: Distribution of error for iTracker (with train and test augmentation) across the prediction space, plotted at ground truth location. The black and white circles represent the location of the camera. We observe that the error near the camera tends to be lower.
   Table 2: Unconstrained eye tracking results (top half) and ablation study (bottom half). The error and dot error values are reported in centimeters (see Sec. 5.1 for details); lower is better. Aug. refers to dataset augmentation, and tr and te refer to train and test respectively. Baseline refers to apply- ing support vector regression (SVR) on features from a pre- trained ImageNet network, as done in Sec. 5.4. We found that this method outperformed all existing approaches. For the ablation study (Sec. 5.5), we removed each critical input to our model, namely eyes, face and face grid (fg.), one at a time and evaluated its performance.
(calibration-free) eye tracking. As described in Sec. 5.1, we train and test iTracker on the appropriate splits of the data. To demonstrate the impact of performing data augmentation during train and test, we include the performance with and without train/test augmentation. As baseline, we apply the best performing approach (pre-trained ImageNet model) on TabletGaze (Sec. 5.4) to GazeCapture. The results are sum- marized in the top half of Tbl. 2 and the error distribution is plotted in Fig. 7.
We observe that our model consistently outperforms the baseline approach by a large margin, achieving an error as low as 1.53cm and 2.38cm on mobile phones and tablets re- spectively. Further, we find that the dot error is consistently lower than the error demonstrating the advantage of using temporal averaging in real-world eye tracking applications. Also note that both train and test augmentation are helpful for reducing the prediction error. While test augmentation may not allow for real-time performance, train augmenta- tion can be used to learn a more robust model. Last, we observe that fine-tuning the general iTracker model to each device and orientation (iTracker∗) is helpful for further re- ducing errors, especially for tablets. This is to be expected, given the large proportion of samples from mobile phones (85%) as compared to tablets (15%) in GazeCapture.
5.3. Eye Tracking with Calibration
As mentioned in Sec. 3.1, we collect data from 13 fixed dot locations (per device orientation) for each subject. We use these locations to simulate the process of calibration.
Tablet error dot err.
2.83 2.53 4.41 4.11
iTracker       3.50
3.04 2.59
2.81 2.38
2.53 2.38 3.12 2.96
   # calib. points
   Mobile phone error dot err.
   0 4 5 9 13
       1.77 1.53 1.92 1.71 1.76 1.50 1.64 1.33 1.56 1.26
     0 4 5 9 13
       1.71 1.53 1.65 1.42 1.52 1.22 1.41 1.10 1.34 1.04
   Model
  iTracker*       2.56
2.29 1.87
2.12 1.69
Table 3: Performance of iTracker using different numbers of points for calibration (error and dot error in centimeters; lower is better). Calibration significantly improves perfor- mance.
For each subject in the test set, we use frames from these 13 fixed locations for training, and evaluate on the remaining locations. Specifically, we extract features from the fc1 layer of iTracker and train a model using SVR to predict each subject’s gaze locations. The results are summarized in Tbl. 3. We observe that the performance decreases slightly when given few points for calibration. This likely occurs due to overfitting when training the SVR. However, when using the full set of 13 points for calibration, the perfor- mance improves significantly, achieving an error of 1.34cm and 2.12cm on mobile phones and tablets, respectively.
5.4. Cross-Dataset Generalization
We evaluate the generalization ability of the features learned by iTracker by applying them to another dataset, TabletGaze [13]. TabletGaze contains recordings from a to- tal of 51 subjects and a sub-dataset of 40 usable subjects6. We split this set of 40 subjects into 32 for training and 8
6 [13] mentions 41 usable subjects but at the time of the experiments, only data from 40 of them was released.
3.13
  2.30

   Error
  7.54
  4.77
  4.04
  3.63
  3.17
  3.09
  2.58
Method Center
TurkerGaze [41] TabletGaze MPIIGaze [43] TabletGaze[13] AlexNet [20] iTracker (ours)
Description
Simple baseline
pixel features + SVR
Our implementation of [13] CNN + head pose Random forest + mHoG eyes (conv3) + face (fc6) + fg. fc1 of iTracker + SVR
4 3.8 3.6 3.4 3.2 3 2.8 2.6 2.4
4.4
4.3
4.2
4.1
4
3.9
3.8
              20 subjects, variable samples
100 samples, variable subjects
                                                                                                                        0 200
400 600
0.5 1 1.5 2
 Table 4: Result of applying various state-of-the-art ap- proaches to TabletGaze [13] dataset (error in cm). For the AlexNet + SVR approach, we train a SVR on the concate- nation of features from various layers of AlexNet (conv3 for eyes and fc6 for face) and a binary face grid (fg.).
for testing. We apply support vector regression (SVR) to the features extracted using iTracker to predict the gaze lo- cations in this dataset, and apply this trained classifier to the test set. The results are shown in Tbl. 4. We report the performance of applying various state-of-the-art approaches (TabletGaze [13], TurkerGaze [41] and MPIIGaze [43]) and other baseline methods for comparison. We propose two simple baseline methods: (1) center prediction (i.e., always predicting the center of the screen regardless of the data) and (2) applying support vector regression (SVR) to im- age features extracted using AlexNet [20] pre-trained on ImageNet [29]. Interestingly, we find that the AlexNet + SVR approach outperforms all existing state-of-the-art ap- proaches despite the features being trained for a completely different task. Importantly, we find that the features from iTracker significantly outperform all existing approaches to achieve an error of 2.58cm demonstrating the generalization ability of our features.
5.5. Analysis
Ablation study: In the bottom half of Tbl. 2 we report the performance after removing different components of our model, one at a time, to better understand their significance. In general, all three inputs (eyes, face, and face grid) con- tribute to the performance of our model. Interestingly, the mode with face but no eyes achieves comparable perfor- mance to our full model suggesting that we may be able to design a more efficient approach that requires only the face and face grid as input. We believe the large-scale data allows the CNN to effectively identify the fine-grained dif- ferences across people’s faces (their eyes) and hence make accurate predictions.
Importance of large-scale data: In Fig. 8b we plot the performance of iTracker as we increase the total number of train subjects. We find that the error decreases signif- icantly as the number of subjects is increased, illustrating the importance of gathering a large-scale dataset. Further, to illustrate the importance of having variability in the data, in Fig. 8b, we plot the performance of iTracker as (1) the
number of train subjects
total samples
(b) Subjects vs. samples
4 x 10
(a) No. of subjects vs. error
Figure 8: Dataset size is important for achieving low error. Specifically, growing the number of subjects in a dataset is more important than the number of samples, which further motivates the use of crowdsourcing.
number of subjects is increased while keeping the number of samples per subject constant (in blue), and (2) the num- ber of samples per subject is increased while keeping the number of subjects constant (in red). In both cases the total number of samples is kept constant to ensure the results are comparable. We find that the error decreases significantly more quickly as the number of subjects is increased indicat- ing the importance of having variability in the data.
6. Conclusion
In this work, we introduced an end-to-end eye track- ing solution targeting mobile devices. First, we intro- duced GazeCapture, the first large-scale mobile eye track- ing dataset. We demonstrated the power of crowdsourcing to collect gaze data, a method unexplored by prior works. We demonstrated the importance of both having a large- scale dataset, as well as having a large variety of data to be able to train robust models for eye tracking. Then, us- ing GazeCapture we trained iTracker, a deep convolutional neural network for predicting gaze. Through careful evalu- ation, we show that iTracker is capable of robustly predict- ing gaze, achieving an error as low as 1.04cm and 1.69cm on mobile phones and tablets respectively. Further, we demonstrate that the features learned by our model gener- alize well to existing datasets, outperforming state-of-the- art approaches by a large margin. Though eye tracking has been around for centuries, we believe that this work will serve as a key benchmark for the next generation of eye tracking solutions. We hope that through this work, we can bring the power of eye tracking to everyone.
Acknowledgements
We would like to thank Kyle Johnsen for his help with the IRB, as well as Bradley Barnes and Karen Aguar for helping to recruit participants. This research was supported by Samsung, Toyota, and the QCRI-CSAIL partnership.
error (cm)
error (cm)
References
[1] T. Baltrusaitis, P. Robinson, and L.-P. Morency. Constrained local neural fields for robust facial landmark detection in the wild. In Com- puter Vision Workshops (ICCVW), 2013 IEEE International Confer- ence on, pages 354–361. IEEE, 2013. 6
[2] S. Baluja and D. Pomerleau. Non-intrusive gaze tracking using arti- ficial neural networks. Technical report, 1994. 2
[3] A. Borji and L. Itti. State-of-the-art in visual attention modeling. PAMI, 2013. 1
[4] J. Chen and Q. Ji. 3d gaze estimation with a single camera without ir illumination. In ICPR, 2008. 2
[5] J. Chen and Q. Ji. Probabilistic gaze estimation without active per- sonal calibration. In CVPR, 2011. 2
[6] A. Duchowski. Eye tracking methodology: Theory and practice. Springer Science & Business Media, 2007. 1
[7] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hier- archies for accurate object detection and semantic segmentation. In CVPR, 2014. 2
[8] D. W. Hansen and Q. Ji. In the eye of the beholder: A survey of models for eyes and gaze. PAMI, 2010. 2
[9] D.W.HansenandA.E.Pece.Eyetrackinginthewild.CVIU,2005. 2
[10] C. Hennessey, B. Noureddin, and P. Lawrence. A single camera eye- gaze tracking system with free head motion. In ETRA, 2006. 2
[11] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a
neural network. arXiv:1503.02531, 2015. 2, 5, 6
[12] P. S. Holzman, L. R. Proctor, D. L. Levy, N. J. Yasillo, H. Y. Meltzer,
and S. W. Hurt. Eye-tracking dysfunctions in schizophrenic patients
and their relatives. Archives of general psychiatry, 1974. 1
[13] Q. Huang, A. Veeraraghavan, and A. Sabharwal. TabletGaze: A dataset and baseline algorithms for unconstrained appearance-based gaze estimation in mobile tablets. arXiv:1508.01244, 2015. 2, 4, 6,
7, 8
[14] E. B. Huey. The psychology and pedagogy of reading. The Macmil-
lan Company, 1908. 1
[15] T. Ishikawa. Passive driver gaze tracking with active appearance
models. 2004. 2
[16] R. Jacob and K. S. Karn. Eye tracking in human-computer interaction
and usability research: Ready to deliver the promises. Mind, 2003. 1
[17] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for
fast feature embedding. arXiv:1408.5093, 2014. 6
[18] S.Karthikeyan,V.Jagadeesh,R.Shenoy,M.Ecksteinz,andB.Man-
junath. From where and how to what we see. In ICCV, 2013. 1
[19] A.Khosla,A.S.Raju,A.Torralba,andA.Oliva.Understandingand predicting image memorability at a large scale. In ICCV, 2015. 2, 3
[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classifica- tion with deep convolutional neural networks. In NIPS, 2012. 2, 5,
6, 8
[21] F. Lu, T. Okabe, Y. Sugano, and Y. Sato. Learning gaze biases with
head motion for head pose-free gaze estimation. Image and Vision
Computing, 2014. 2
[22] F. Lu, Y. Sugano, T. Okabe, and Y. Sato. Adaptive linear regression
for appearance-based gaze estimation. PAMI, 2014. 2
[23] P. Majaranta and A. Bulling. Eye tracking and eye-based human– computer interaction. In Advances in Physiological Computing.
Springer, 2014. 1
[24] C. D. McMurrough, V. Metsis, J. Rich, and F. Makedon. An eye
tracking dataset for point of gaze detection. In ETRA, 2012. 2
[25] K.A.F.Mora,F.Monay,andJ.-M.Odobez.Eyediap:Adatabasefor the development and evaluation of gaze estimation algorithms from
rgb and rgb-d cameras. ETRA, 2014. 1, 2
[26] C. H. Morimoto and M. R. Mimica. Eye gaze tracking techniques
for interactive applications. CVIU, 2005. 1
[27] K. Rayner. Eye movements in reading and information processing: 20 years of research. Psychological bulletin, 1998. 1
[28] A. Recasens, A. Khosla, C. Vondrick, and A. Torralba. Where are they looking? In NIPS, 2015. 2, 3
[29] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 2014. 3, 8
[30] W. Sewell and O. Komogortsev. Real-time eye gaze tracking with an unmodified commodity webcam employing a neural network. In SIGCHI, 2010. 2
[31] B. A. Smith, Q. Yin, S. K. Feiner, and S. K. Nayar. Gaze locking: Passive eye contact detection for human-object interaction. In UIST, 2013. 2
[32] Statista. Global smartphone user penetration 2014 - 2019.
http://www.statista.com/statistics/203734/
global- smartphone- penetration- per- capita- since- 2005/, 2015. 1
[33] Y. Sugano, Y. Matsushita, and Y. Sato. Appearance-based gaze esti- mation using visual saliency. PAMI, 2013. 2
[34] Y. Sugano, Y. Matsushita, and Y. Sato. Learning-by-synthesis for appearance-based 3d gaze estimation. In CVPR, 2014. 1, 2
[35] Y. Sugano, Y. Matsushita, Y. Sato, and H. Koike. An incremental learning method for unconstrained gaze estimation. In Computer Vision–ECCV 2008, pages 656–667. Springer, 2008. 2
[36] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface: Closing the gap to human-level performance in face verification. In CVPR, 2014. 2
[37] K.-H. Tan, D. J. Kriegman, and N. Ahuja. Appearance-based eye gaze estimation. In WACV, 2002. 2
[38] D. Torricelli, S. Conforto, M. Schmid, and T. D’Alessio. A neural- based remote eye gaze tracker under natural head motion. Computer methods and programs in biomedicine, 2008. 2
[39] R. Valenti, N. Sebe, and T. Gevers. Combining head pose and eye location information for gaze estimation. TIP, 2012. 2
[40] U. Weidenbacher, G. Layher, P.-M. Strauss, and H. Neumann. A comprehensive head pose and gaze database. 2007. 2
[41] P. Xu, K. A. Ehinger, Y. Zhang, A. Finkelstein, S. R. Kulkarni, and J. Xiao. Turkergaze: Crowdsourcing saliency with webcam based eye tracking. arXiv:1504.06755, 2015. 3, 8
[42] D. H. Yoo and M. J. Chung. A novel non-intrusive eye gaze estima- tion using cross-ratio under large head motion. CVIU, 2005. 2
[43] X. Zhang, Y. Sugano, M. Fritz, and A. Bulling. Appearance-based
gaze estimation in the wild. In CVPR, 2015. 1, 2, 4, 5, 8
[44] B.Zhou,A.Khosla,A.Lapedriz,A.Torralba,andA.Oliva.Places2:
A large-scale database for scene understanding. arXiv, 2016. 3
[45] Z. Zhu and Q. Ji. Eye gaze tracking under natural head movements.
In CVPR, 2005. 2
[46] Z. Zhu, Q. Ji, and K. P. Bennett. Nonlinear eye gaze mapping func-
tion estimation via support vector regression. In Pattern Recognition, 2006. 2
                                 Personal Visualization and Personal Visual Analytics
Eye Tracking for Personal Visual Analytics
Kuno Kurzhals and Daniel Weiskopf ■ University of Stuttgart
Eye tracking for the analysis of gaze behav- ior is common in many scienti c  elds and marketing research. So far, the high cost of eye-tracking hardware, a result of the re- quirements of high precision and reliability for research measurements, has prevented wide ap- plication in personal, nonprofessional scenarios. However, this situation has been changing as af- fordable consumer hardware has become more
widely available. The established eye-tracking vendors have devel- oped consumer versions of por- table eye-tracking hardware that can be used on any monitor or TV. The development is not re- stricted to stationary eye-track- ing devices, and “how to build your own eye-tracking glasses” instructions can even be found in various publications.1 There- fore, easy-to-use mobile eye tracking integrated in wearable glasses is already available.
In combination with the industry’s interest in intelligent glasses, we expect that wearable mobile eye tracking will be available for everyone in the near future. The main purpose of this development is the use of eye tracking as a device for human- computer interaction—for example, to adapt the user interface. However, we see a great opportu- nity in using such hardware for personal analytics as well. How can users of intelligent glasses reca- pitulate their viewing behavior, understand their interactions with others and the environment, or just have fun with their personal data?
The possible application scenarios for personal eye tracking cover diverse  elds. With the addi- tional information about the user’s visual atten- tion, important events in the video database can be extracted to allow users to re-experience these events. Possible scenarios might include applica- tions that support self-re ection and self-insight2 via video analysis with gaze information. This could involve analyzing interaction logs for personal re- lations with others, vigilance optimization during driving situations, or cognitive activity recognition that can be applied for quanti ed-self scenarios.3 For example, users could set a goal to read at least 10,000 words a day and then monitor their read- ing behavior and time spent on reading texts. Also, recommender systems could generate catalogs of interest based on the objects that attracted the user’s attention. Viewing behavior could also be analyzed to present similar suggestions for future media consumption. The time spent on a personal visual analytics application strongly depends on the scenario. For example, users who bene t from the analysis for health or social reasons will be more motivated to spend time with the application than users who browse recorded data just for fun.
With the changes in technology and new appli- cations, new opportunities and challenges for data analysis will arise. Mobile eye tracking produces massive amounts of complex data because it both produces spatiotemporal information of eye gazes and provides video recordings of the person’s en- vironment. Without such video information, we are missing the semantic context of the gaze data; we would not be able to relate visual attention to objects in the environment or to other people
 Eye tracking can help record massive amounts of data about the distribution of visual attention in various scenarios. Such data could support nonexpert user self-re ection and self-insight. However, challenges arise when eye tracking is applied to everyday situations and personal visual analytics.
 64 July/August 2015 Published by the IEEE Computer Society 0272-1716/15/$31.00 © 2015 IEEE
                                   Figure 1. Mobile eye tracking in a supermarket scenario. The person in the front is wearing glasses with integrated mobile eye tracking. In this consumer study, the participants’ eye movements are recorded when they perform shopping tasks. The picture was taken as part of a research project on mobile, unconstrained eye tracking.
with whom the user interacts. In other words, we are facing a coupled analysis problem: analysis of spatiotemporal gaze data and video analysis. Each of these analysis problems comes with challenges of its own, even in a professional setup. (See the related discussions of professional visual analyt- ics for eye tracking4 and video5 for more details.) The combined analysis problem raises even more interesting questions for personal visual analytics. In particular, with the vast amount of personal video data with gaze information being avail- able, standard personal information visualization methods—such as replaying the videos—will fail. Therefore, new visual analytics methods must be developed to  lter and summarize information that is important to the user.
In this article, we discuss how eye tracking  ts into the design space of existing personal visual analytics applications as well as the special re- quirements and research perspectives of personal eye tracking. Because this personal information can be applied for the user’s self-re ection, it also  ts into the concept of personal informat- ics.6 In this context, the focus of our discussion is on the analysis of personal data rather than on data acquisition. As one example of the visualiza- tion of personal eye-tracking data, we present a new approach, the areas of interest (AOI) cloud, to display information about the distribution of attention across multiple videos. With our tech- nique, AOIs (which might be objects or people) can be displayed in an annotated overview us- ing a representation similar to a tag cloud. Addi- tional rings on the AOIs allow for easy navigation through several videos to examine time spans that received the user’s attention.
Current Use of Mobile Eye Tracking
Mobile eye tracking is often used for user studies that do not restrict the participants as much as a laboratory experiment under controlled conditions might. Figure 1 shows a typical example of mobile eye tracking. In this scenario, to investigate the viewing behavior of people in a supermarket, par- ticipants perform a shopping task while wearing eye-tracking glasses. The glasses record eye move- ments and a video of the participant’s  eld of view. To analyze the recorded data, statistical methods (in particular, statistical inference for hypothesis testing) and/or visualization are used. However, statistical methods cannot be applied as easily as in laboratory studies because of the less controlled environment and stimuli. For mobile eye-tracking scenarios “in the wild,” changing conditions exac- erbate the statistical comparison of multiple par-
ticipants. Therefore, qualitative visual evaluation of the data is often required.
For qualitative and quantitative analysis of mo- bile eye-tracking data, most techniques rely on the de nition of AOIs to relate the stimuli contents between participants. Unfortunately, these analy- sis methods require extensive manual processing and labeling; there are only a few automatic com- puter vision techniques that detect and recognize trained objects from a database to generate AOIs.7 Today’s analysis methods for mobile eye tracking are restricted to professional users and require extensive work to setup the experiments and postprocess the data recorded. Therefore, these methods cannot be applied directly for personal visual analytics.
Personal Eye Tracking
Here, we investigate how personal eye tracking can be categorized in the general context of personal visual analytics and what special requirements and challenges have to be considered for applica- tions of personal eye tracking.
In the Context of Personal Visual Analytics
To apply eye tracking in a personal context, we will  rst investigate the design dimensions of per- sonal visual analytics and how an application for personal eye tracking  ts in. To this end, here we examine the classi cation introduced by Dandan Huang and his colleagues,8 which consists of four categories: data, context, interaction, and insight.
The scope of the recorded data is a combination of data about oneself and data about other people.
IEEE Computer Graphics and Applications 65
© 2014 VISUS, University of Stuttgart
                                Personal Visualization and Personal Visual Analytics
Data about oneself is recorded by gaze information and by the video camera of the eye-tracking device that captures data about the environment, includ- ing other people. This data is personal and has to be handled with care. Under the assumption that eye-tracking devices will become more and more comfortable in the future and comparable to wear- ing regular glasses, the effort to record data will be reduced to sensor recording only. Current eye- tracking devices still require elaborate calibration procedures that increase the effort to record data. Regarding the controllability of the data acquisi- tion, the user has partial control over whether to record the surrounding.
The in uence context of mobile eye-tracking analysis is mainly personal, functioning to inform the user wearing the eye-tracking device. However, other people will often be involved in the recorded
Personal eye tracking will cover much longer time spans than traditional eye-tracking experiments, requiring more time-compressed visual representations.
data, so the user could communicate extracted events through social media to involved persons— for example, to recapitulate parts of a conversa- tion. The design context of an application depends on the scenario. In the example we describe in the next section, the application to examine the recorded data is designed by the researcher. How- ever, users can freely organize the components of the visualization, such as to arrange groups of peo- ple or extract and summarize important personal events in an easily accessible visual representation. For scenarios with automatic data analysis (such as recommender systems), prede ned representations of the results should be suf cient in many cases.
The degree of attentional demand for interaction also depends on the scenario. In cases when the analysis is performed automatically and the user just has to choose between different results (for example, recommended media), the attentional demand will be low. For the analysis of personal encounters, the user must focus attention on the visualization to investigate interesting events in the data, so a high attentional demand is required. The high explorability of the data in the application allows users to investigate multiple video streams simultaneously for interesting events that received much attention during the recording of the data.
Apart from technical issues, fully automatic analysis of the data can only be applied in a sub- set of scenarios and for preprocessing. An analysis of subjective events cannot be automated, and it requires the user to make conclusions based on the data. Also, the degree to which extracted in- sight from the application can in uence future actions varies. In the best case, examination of the recorded data leads to an identi cation of self-de ned misbehavior that can be avoided in the future. For example, a close friend may have received less attention than the user would con- sider appropriate. Now aware of this situation, the user can then spend more time with this person to strengthen their friendship.
Special Requirements
For the personal analysis of mobile eye-tracking data, we have to consider certain aspects that differentiate personal from professional visual analytics. From our perspective, the following characteristics and requirements of personal eye tracking are most relevant.
In professional eye tracking, the accuracy of the analysis is critical because research results, prod- uct design, security-relevant decisions, or other factors rely on the quality of the analysis. For ex- ample, both recall and precision of pattern recog- nition in the eye-tracking data are highly relevant. Fortunately, personal eye tracking is less critical in terms of analysis accuracy. Therefore, some leeway exists when designing personal visual analytics.
Personal eye tracking will cover much longer time spans than traditional eye-tracking experi- ments, requiring more time-compressed visual representations. Similarly, different reasoning artifacts are relevant.9 For example, patterns in the transitions between  xations are of lesser in- terest than events or objects extracted from the eye-tracking data (such as people with whom the person interacted). Speci c aspects of tasks for personal eye tracking will be complemented by general observations for casual visualization.10
Because personal eye tracking focuses on iden- tifying relevant events or objects, it bene ts from linking those to semantic information and em- bedding them into the context of “outside” in- formation. For example, people identi ed as being important could be associated with information from their Web pro les.
Like any personal visual analytics application, the design of the visual interface has to be easy to use for nonexpert users. The design should be in- tuitive and not require a steep learning curve. The automatic processing for the analysis should be ro-
  66
July/August 2015
 Input data
AOI annotation
Interactive visualization
Gaze data
Video ( rst-person perspective)
User-de ned objects Computer vision
Overview Filtering Event browsing
Figure 2. Data processing pipeline. Eye-tracking and video data need a semantic annotation of AOIs. The annotated data can then be displayed in an interactive visualization for video event browsing based on the distribution of attention on AOIs.
bust so that there is little or no need for the user to interfere and  ne-tune data mining or computer vision techniques. Similar to many of the apps in mobile personal use on smartphones, visual ana- lytics software for personal eye tracking will most likely be application-speci c. In contrast, profes- sional tools tend to be generic so that they can work with any study setup.
Personal visual analytics has to incorporate mechanisms to protect privacy because poten- tially sensitive information is recorded from the environment. Therefore, the analysis needs to be designed to work with the principle of data mini- mization (for example, to work with video record- ings in which faces of persons or license plates of cars are modi ed to make them unrecognizable). Also, high data security is required to protect the user’s personal gaze data.
These aspects will be critical in the design of appropriate visual interfaces and the development of automatic analysis techniques to be integrated within visual analytics. We expect that personal eye tracking will come with many challenging research questions related to design, interaction techniques, visualization, computer vision, pattern recogni- tion, and semantic modeling. Although there is substantial research in these areas, we believe that the personal perspective will require us to devise new variants of existing techniques or develop com- pletely new ones. To illustrate the personal visual analysis of eye-tracking data, we implemented a prototype for a commonly representative scenario: the analysis of a user’s personal encounters.
Personal Encounter Analysis Case Study
The analysis of interactions between people plays an important role in psychology and cognitive science.11 For a private user, the analysis of personal encoun- ters can also be interesting, be it a self-re ection of social behavior or just for re-experiencing situ- ations that received much attention.
In our example scenario, the user was wearing eye-tracking glasses during coffee breaks, a re- curring event over one week. During the coffee breaks, groups of between three and six people, including the person wearing the eye-tracking glasses, gathered to discuss miscellaneous themes.
The recordings during these breaks lasted between three and nine minutes with a varying set of par- ticipants. All participants agreed to be recorded on video if their faces were anonymized. Consider- ing the privacy issues discussed earlier, this was an important prerequisite for all participants. We also agreed not to include the recorded audio in any form of publication of the data. One coffee breakparticipant(P1)didnotagreetoberecorded in any form, so P1 sat next to the user wearing the eye-tracking glasses so as not to be visible to the camera, and therefore, P1’s face was not an- notated as an AOI. This situation exempli es the issues that occur when other people are recorded on video and that have to be considered for per- sonal eye-tracking applications.
Automatic preprocessing of this data requires an algorithm to detect faces in the videos, store them in a database, and recognize the faces when they reappear. In this scenario, the faces are the AOIs. Compared with other tasks in computer vi- sion, this can be performed without much user interaction because there is no semantic gap that requires human interpretation of situations. The user might identify a person once, while the rest of the data is processed automatically. With the information about which faces can be seen in the videos and where they appear, an attention mea- sure can be calculated by the AOIs of faces and the eye-tracking data. Although computer vision approaches can nowadays be applied for automatic segmentation and classi cation of such events,12 we decided to showcase our example with manu- ally annotated data because current automatic approaches often face dif culties with changing environment conditions, as in our case.
Data Processing
The visualization of personal eye-tracking data requires a preprocessing step that is necessary to map gaze data to semantic AOI information. Fig- ure 2 shows the processing pipeline the data has to pass before it can be displayed in an interactive visualization for data exploration.
Assuming the user wears a set of eye-tracking glassesduringanarbitraryoccupation,twotypesof recorded data are of special interest. An integrated
IEEE Computer Graphics and Applications 67
                                Personal Visualization and Personal Visual Analytics
 (a)
Figure 3. Visualization of one AOI. (a) This representative image of a person includes a name label, and the radius indicates the attention spent on the person. (b) The inner ring has segments for all the videos the person appeared in. (c) The outer ring shows the currently selected video. (d) Reference images can be created with markers on the outer ring.
(d)
(c)
(b)
68 July/August 2015
camera records a video from a  rst-person perspec- tive comprising most of the user’s  eld of view. The video data serves as a foundation for semantic in- terpretations of the user’s viewing behavior. The eye-tracking hardware can map eye-gaze positions to the coordinate system of the video. Following the eye-mind hypothesis,13 we can assume that the  xated regions in the video were those to which the user’s attention was directed. In combination with the video images, semantic interpretations can be derived.
To collect aggregated information about how much attention was directed to a particular object, it has to be annotated for a semantic mapping of gaze data to this object. By de ning an object as an AOI, attention metrics can be aggregated even over several recordings that contain the same object. De- pending on the user’s interest, the AOIs can consist of a set of tools that are used during a work task or of the people the user interacts with, as in our example. Because this annotation of AOIs is task- speci c, an automatic computer vision approach will be not suf cient in most cases. We suggest a semiautomatic approach where the user can de-  ne interesting objects once and the detection and
tracking of these objects will then be performed au- tomatically. Although computer vision approaches still need improvements to work in everyday situ- ations, the semantic gap14 that requires user input can be closed by such an interactive approach. For our example, we annotated the data manually to show how the interactive visualization works with a ground truth annotation. After the annotation, the processed data consists of AOI information about when and where an object appears in the videos and how much attention was directed to this object.
AOI Cloud Visualization
To visualize the distribution of attention on AOIs, common visualization principles such as an over- view and interactive  ltering of the data have to be available. For personal eye-tracking data, the overview of all AOIs and how much attention was spent on them play an important role. The inter- active visualization has to meet the requirements that we discussed earlier for personal eye tracking and enable the user to browse the recorded video data for events and time spans where attention was spent on a speci c object.
In our visualization approach, the annotated peo- ple (or AOIs) are represented as circles consisting of a representative image and an inner and outer ring (see Figure 3). Radial visualization approaches are applied in cases where hierarchical structures, rela- tionships among disparate entities, or as in our case, time series data have to be displayed in a dense rep- resentation.15 We decided to use a radial approach because of its accessibility for novices,16 possibili- ties for fast interactions, and its compact represen- tation of the temporal dimension on the rings that can be interpreted by using a clock metaphor.
The radius of the circle can be determined by an appropriate attention metric. In this example, we applied the total amount of gaze points on the person from all videos. Notice that other metrics such as transition counts between AOIs or mean  xation durations could also be applied, depending on the analysis question. Hence, our visualization approach is independent from the applied metric.
Because some people appear only in one video and others in three, the difference between the attention of the AOIs with the lowest and highest values can be large. This leads to extreme differ- ences in the size of the circles, resulting in the problem that at least one of the AOIs is either too small or too big to be readable. Hence, we used a logarithmic scaling of the metric to adjust the vi- sualization for a better representation of all AOIs. The representative image of a person is determined by the  rst appearance in the data. Alternative ap-
                                 Figure 4. The touch-friendly design of the AOI cloud allows for analysis of the data on mobile devices such as tablets.
proaches to determine the representative image based on a special event in the data or on a pro le image from social networks could also be applied.
The inner ring consists of segments that each represent a video containing the AOI. Hence, the inner circle contains all videos where the AOI ap- peared, and the size of a segment is determined by the relative length of the corresponding video. Segments in the inner ring are connected to the outer ring by identical colors. To visualize when at- tention was spent on the AOI, we use an approach similar to AOI timeline visualizations known from various approaches in this  eld.17 Time spans with- out attention on the AOI are displayed darker, whereas time spans with attention are displayed with full brightness. This way, important events can be identi ed ef ciently by directly selecting the time spans with attention on the AOI. No- tice that approaches with AOI timelines usually consider only one video. In our approach, multiple video stimuli are combined in one visualization to investigate the data more ef ciently. To distin- guish between the different videos, we use an HSV (hue-saturation-value) color scheme where neigh- boring segments receive colors with a distant hue.
By selecting a segment of the inner ring, a sec- ond ring appears outside representing the selected segment zoomed over the whole ring. Time scales for the start and end of the video as well as for the quarters help the user to navigate clockwise through the video. Initially, one marker is avail- able on the rim of the outer ring. It can be moved around the ring to navigate through the video. A thumbnail image next to the marker shows the currently selected frame as a reference to the video content. By clicking on the thumbnail, the corre- sponding video appears in a separate player win- dow and can be played back directly at the selected position. The user can also create additional mark- ers to select multiple events of potential interest to compare them or just summarize the gist of im- portant interactions with the person in this video. With this approach, the user can generate a set of interesting events that can be assessed simply by clicking on the corresponding thumbnails.
The complete dataset can  nally be visualized with items for each AOI that can be arranged in a layout similar to a tag cloud.18 Important AOIs are placed in the center of the cloud, and less impor- tant AOIs appear in the outer regions. This setup makes our visualization accessible because tag clouds are familiar to most users and already estab- lished in everyday life. Selected items appear in the foreground, while the other items can be faded out. From that point on, the user is free to rearrange all
the items to build groups or rank people based on subjective criteria. For example, a user could rank people based on friendship relations and investigate if their received attention relates to this ranking.
The time spans when a person received atten- tion are easily accessible by the inner and outer rings. By adding markers to the outer ring, the user can de ne interesting events in the data and di- rectly play back the corresponding video. With this approach, we simplify the exploration of multiple video sources in an easy-to-understand interactive visualization.
Due to the touch-friendly design of our visual- ization, users can also examine their data on the go on mobile devices (see Figure 4). This enables an easier integration of the application into the everyday life of the user, which is important for the long-term use of the application.
Use Case
Figure 5 shows a summarization of four videos from the coffee break dataset. Two videos (green and purple) are from the same session because the constellation of people changed after the  rst record ended. Altogether, eight individuals participated in the breaks and received different amounts of atten- tion from the user wearing the eye-tracking glasses.
The user organized the participants in three groups based on the amount of attention they received:
■ Group 1: Dylan and Russel appeared just once in different videos. Both received less attention than the others, especially Russel, who was sit- ting next to the user and only received attention when he was talking because the user had to turn to look at him. Dylan was watched when he was not talking because he was sitting in front of the user. Both people could have received a similar
IEEE Computer Graphics and Applications 69
                                Personal Visualization and Personal Visual Analytics
  Figure 5. AOI cloud for eight people over four videos. The items are freely adjustable and can be arranged by the user. In this example, three groups were created: Group 1 (Dylan and Russel) received the least attention; group 2 (Anya, John, and Steve) received medium attention; and group 3 (Jack, Oliver, and Chris) received the most attention.
70 July/August 2015
amount of attention as those in group 2 if they had appeared in another video and if Russel had been seated in a better position.
■ Group 2: Anya, John, and Steve appeared in two videos and were watched occasionally by the user. Steve could also be shifted to group 1 be- cause he received little attention during his at- tendance in the coffee break.
■ Group 3: Jack, Oliver, and Chris received most of the attention, although the distribution of attention depended on the constellation of peo- ple. For example, Oliver received a lot of atten- tion in video 3 (green), when Chris, Steve, and he were present. During this coffee break, Chris left the room for half of the time (see markers at 00:02:35 and 00:05:59), at which points the main attention was on Oliver. In video 2 (blue), Oliver received less attention. In this video, as well as in video 1 (red), Jack was the attention catcher. Because Jack talked most of the time in both videos, the user gave him a good deal of attention. Hence, he received most of the at- tention although he was only present in two videos.
In this coffee break example, we can see that the amount of attention people received strongly depends on their position in the room, their ac- tive participation in discussions, and the other people attending at the time. People who talked less and required the user to turn to see them received less attention, especially when an atten- tion-catching person was present. Thus, if the user would like to give more attention to some of the people from groups 1 or 2, talking with these people outside the coffee breaks when an atten- tion-catching person such as Jack is not present might be an option.
Discussion
The AOI cloud provides an accessible approach to investigating the personal distribution of attention over several videos. The visualization approach is not restricted to people and could be applied to an arbitrary set of objects, assuming that it is possible to annotate the objects.
Although the most important AOIs will always be in the center of the initial cloud, a large number of AOIs and videos might reduce the readability of the visualization. Therefore, the scalability of our approach can be improved with additional  ltering of the AOIs and video segments. By thresholding the attention values, AOIs that received less atten- tion than the given threshold could be removed from the visualization. The same approach could be applied to the video segments of an AOI.
The presented visualization approach focuses on the analysis of individual relations between the user and other individuals. For future extensions, an analysis of group interactions would be bene cial for a re ection on personal social activity. By add- ing new options for examining attention changes between different people and how these changes correlate with people’s activities, we could cover a comprehensive set of personal analysis interests.
M
cameras or head tracking. Its main advantage lies in the additional gaze information. That is, when multiple objects are in the center area of a re- corded image, we can derive detailed information about particular objects. A typical example could be a person looking at a picture collection. In this case, it would not be possible to identify the spe- ci c picture of interest without determining the
obile eye tracking comprises most scenarios
that can be achieved with head-mounted
                                user’s gaze position. In addition, because the focus of this research is on personal scenarios, design- ing interfaces to combine mobile eye-tracking data with existing applications for personal visual ana- lytics would be desirable.
To extend the possibilities of personal eye track- ing in the near future, the challenges linked to the requirements we discussed here must be ad- dressed. First, to increase accuracy, self-calibrating approaches need to be developed. Current tech- niques still rely on calibration procedures that are not feasible for a personal application. Also, man- aging the in uence of uncontrolled lighting condi- tions in the environment introduces problems that require further research.
Second, de ning areas or objects of interest by solely relying on computer vision might be hard to achieve in the near future. Arbitrary user-de ned queries (for example, searching all cars in the videos of the database that received the user’s at- tention) are required to process the recorded data to its full extent. Semiautomatic approaches and crowdsourcing could bridge the semantic gap in automatic approaches. Hence, visual analytics could help support such semiautomatic analysis.
Lastly, regarding cognitive processes, the inter- pretation of the gaze data itself has to be consid- ered. Current approaches using cognitive modeling and machine learning to predict and classify gaze behavior (for example, detecting arousal or vigi- lance) need further development to provide more information than just distributions of attention. In our example, this information could be applied to weight the AOI circles. Additional information from measured pupil dilation can be included be- cause current eye-tracking devices already record this data and preliminary work to correlate pu- pil changes with emotional states already exists. Supplementary sensors (such as heart rate sensors) can also provide such information and are already combined with mobile eye tracking.
Acknowledgments
This work was funded by the German Research Foun- dation (DFG) as part of the Scalable Visual Analyt- ics Priority Program (SPP 1335). We thank Albrecht Schmidt of the University of Stuttgart for the discus- sion during the development of this work, and our reviewers for their constructive feedback.
References
1. R. Mantiuk et al., “Do-It-Yourself Eye Tracker: Low- Cost Pupil-Based Eye Tracker for Computer Graphics
Applications,” Advances in Multimedia Modeling, K. Schoeffmann et al., eds., LNCS 7131, Springer, 2012, pp. 115–125.
2. J.G. Hixon and W.B. Swann, “When Does Introspection Bear Fruit? Self-Re ection, Self- Insight, and Interpersonal Choices,” J. Personality and Social Psychology, vol. 64, no. 1, 1993, pp. 35–43.
3. K. Kunze et al., “Activity Recognition for the Mind: Toward a Cognitive ‘Quanti ed Self,’” Computer, vol. 46, no. 10, 2013, pp. 105–108.
4. G. Andrienko et al., “Visual Analytics Methodology for Eye Movement Studies,” IEEE Trans. Visualization and Computer Graphics, vol. 18, no. 12, 2012, pp. 2889–2898.
5. B. Höferlin et al., “Scalable Video Visual Analytics,” Information Visualization, vol. 14, no. 1, 2015, pp. 10–26.
Semiautomatic approaches and crowdsourcing could bridge the semantic gap in automatic approaches.
6. I. Li, A. Dey, and J. Forlizzi, “A Stage-Based Model of Personal Informatics Systems,” Proc. SIGCHI Conf. Human Factors in Computing Systems, 2010, pp. 557–566.
7. T. Toyama et al., “Gaze Guided Object Recognition Using a Head-Mounted Eye Tracker,” Proc. Symp. Eye Tracking Research and Applications (ETRA), 2012, pp. 91–98.
8. D. Huang et al., “Personal Visualization and Personal Visual Analytics,” IEEE Trans. Visualization and Computer Graphics, vol. 21, no. 3, 2014, pp. 420–433.
9. J.J. Thomas and K.A. Cook, eds., Illuminating the Path: The Research and Development Agenda for Visual Analytics, IEEE CS Press, 2005.
10. D. Sprague and M. Tory, “Exploring How and Why People Use Visualizations in Casual Contexts: Modeling User Goals and Regulated Motivations,” Information Visualization, vol. 11, no. 2, 2012, pp. 106–123.
11. A. Navab et al., “Eye-Tracking as a Measure of Responsiveness to Joint Attention in Infants at Risk for Autism,” Infancy, vol. 17, no. 4, 2012, pp. 416–431.
12. R.S. Jasinschi et al., “Integrated Multimedia Processing for Topic Segmentation and Classi cation,” Proc. 2001 Int’l Conf. Image Processing, 2001, pp. 366–369.
13. M.A. Just and P.A. Carpenter, “A Theory of Reading: From Eye Fixations to Comprehension,” Psychological Rev., vol. 87, no. 4, 1980, pp. 329–354.
14. A. Smeulders et al., “Content-Based Image Retrieval at the End of the Early Years,” IEEE Trans. Pattern
    IEEE Computer Graphics and Applications 71
                                Personal Visualization and Personal Visual Analytics
Analysis and Machine Intelligence, vol. 22, no. 12,
2000, pp. 1349–1380.
15. G. Draper, Y. Livnat, and R. Riesenfeld, “A Survey of
Radial Methods for Information Visualization,” IEEE Trans. Visualization and Computer Graphics, vol. 15, no. 5, 2009, pp. 759–776.
16. G. Draper and R. Riesenfeld, “Who Votes for What? A Visual Query Language for Opinion Data,” IEEE Trans. Visualization and Computer Graphics, vol. 14, no. 6, 2008, pp. 1197–1204.
17. T. Blascheck et al., “State-of-the-Art of Visualization for Eye Tracking Data,” Proc. EuroVis State of the Art Reports, 2014; http://dx.doi.org/10.2312/eurovisstar.20141173.
18. F.B. Viéas and M. Wattenberg, “Timelines Tag Clouds and the Case for Vernacular Visualization,” Interactions, vol. 15, no. 4, 2008, pp. 49–52.
Kuno Kurzhals is a research assistant working in the Visu- alization Research Center at the University of Stuttgart. His research interests include eye tracking, visualization, visual analytics, and computer vision, especially developing new visualization methods to analyze eye-movement data from dynamic stimuli. Kurzhals has a Diplom in computer sci-
ence from the University of Stuttgart. Contact him at kuno. kurzhals@vis.uni-stuttgart.de.
Daniel Weiskopf is a professor of computer science in the Visualization Research Center and the Visualization and Interactive Systems Institute at the University of Stuttgart. His research interests include visualization, visual analytics, GPU methods, computer graphics, and special and general relativity. Weiskopf has a doctor rerum naturalium (PhD) in physics from Eberhard-Karls-Universität Tübingen. Con- tact him at weiskopf@visus.uni-stuttgart.de.
Selected CS articles and columns are also available for free at http://ComputingNow.computer.org.
  ADVERTISER INFORMATION
 Advertising Personnel
Marian Anderson: Sr. Advertising Coordinator Email: manderson@computer.org
Phone: +1 714 816 2139 | Fax: +1 714 821 4010
Sandy Brown: Sr. Business Development Mgr. Email sbrown@computer.org
Phone: +1 714 816 2144 | Fax: +1 714 821 4010
Advertising Sales Representatives (display)
Central, Northwest, Far East: Eric Kincaid
Email: e.kincaid@computer.org Phone: +1 214 673 3742
Fax: +1 888 886 8599
Northeast, Midwest, Europe, Middle East:
Ann & David Schissler
Email: a.schissler@computer.org, d.schissler@computer.org Phone: +1 508 394 4026
Fax: +1 508 394 1707
Southwest, California:
Mike Hughes
Email: mikehughes@computer.org Phone: +1 805 529 6790
Southeast:
Heather Buonadies
Email: h.buonadies@computer.org Phone: +1 973 304 4123
Fax: +1 973 585 7071
Advertising Sales Representatives (Classi ed Line)
Heather Buonadies
Email: h.buonadies@computer.org Phone: +1 973 304 4123
Fax: +1 973 585 7071
Advertising Sales Representatives (Jobs Board)
Heather Buonadies
Email: h.buonadies@computer.org Phone: +1 973 304 4123
Fax: +1 973 585 7071
    72 July/August 2015
  Harri Siirtola, Poika Isokoski
TAUCHI / VIRG
School of Information Sciences University of Tampere, Finland
harri.siirtola@uta.fi, poika.isokoski@uta.fi
Abstract—Digitalization is changing how research is carried out in all areas of science. Humanities is no exception – materials that used to be hand-written or printed on paper are increasingly available in digital form. This development is changing how scholars are interacting with their material.
We are addressing the problem of interactive text visual- ization in the context of sociolinguistic language study. When a scholar is reading and analyzing text from a computer screen instead of a paper, we can support this by providing a dashboard for reading, and by creating visualizations of the text structure, variation, and change.
We have designed and developed a software tool called Text Variation Explorer (TVE) for sociolinguistic language study. It is based on interactive visualization with a direct manipulation user interface, and aimed for exploratory corpus linguistics.
The TVE software tool has proven to be useful in supporting the study of language variation and change in its social contexts, or sociolinguistics. It is, to a certain degree, language- independent, and generic enough to be useful in other linguistic contexts as well.
We are now in the process of designing and implementing the next iteration of TVE. We present the lessons learned from the first version, discuss the old and the new design, and welcome feedback from the communities involved.
Keywords-Information visualization; text visualization; inter- action
I. INTRODUCTION
Text is a challenging data type to visualize. Firstly, text itself is a visual encoding, and it does not provide much to vary. In the spirit of Bertin [1], we can vary the size, color, orientation, style, and typeface of text, but this usually hinders the readability and spoils the reading experience. Secondly, text is not just a sequence of characters that always has the same meaning – a fragment of text detached from context may carry a completely different meaning, or might even be open to several interpretations. Finally, visualizations and visual summaries of text are often crafted to avoid seeing detailed data, which is unacceptable in many tasks involving text.
2375-0138/16 $31.00 © 2016 IEEE DOI 10.1109/IV.2016.57
Tanja Sa ̈ily, Terttu Nevalainen
Department of English University of Helsinki, Finland tanja.saily@helsinki.fi, terttu.nevalainen@helsinki.fi
A. Sociolinguistics
Sociolinguistics is the study of language in social and cul- tural context. Typical background variables in sociolinguistic studies include, e.g., the author’s age, gender, ethnicity, domicile, social class, and the date of speaking/writing. Examples of quintessential research questions involve which social and linguistic factors influence language variation and change, how language change begins and proceeds, and the effect of the change upon linguistic structure and communication [19]. Also of interest is how personal and communal styles of speech and writing evolve in interaction. Research methods cover the full spectrum, including the information visualization approach [16], and often the research material is compiled into a corpus of representative texts.
B. Text visualization
Text visualization is a thriving subfield of information visualization. Kucher and Kerren [8] have recently made a survey of text visualization techniques and maintain an interactive, online browser of currently published techniques [17]. At the writing of this paper (the browser is con- stantly updated, initially there were 141 techniques) the browser lists and categorizes 272 text visualization tech- niques. Searching for linguistically-motivated text visualiza- tions returns six techniques, including the first version of our tool.
Often we visualize data to avoid seeing it in detail, because the sheer volume of the data might render the close reading impossible. In sociolinguistic research, the close reading is an essential activity, and visualization provides a dashboard for the reader – not unlike the dashboard in a car.
This paper describes experiences gained from the devel- opment of Text Variation Explorer (TVE), and presents our plans for the version we are currently developing.
330
2016 20th International Conference Information Visualisation
Interactive Text Visualization with Text Variation Explorer

II. INTERACTIVE TEXT VISUALIZATION
Text visualizations can be broadly divided into three classes based on how they combine the text and visualiza- tion. We can characterize them as direct, indirect, or hybrid visualizations. Please see Kucher and Kerren [8] for an extensive classification.
A. Direct text visualizations
The direct visualizations rely on the visual properties of text, in a ‘bertinian’ sense: size, color, location, orientation, style, value, and shape (pattern deliberately left out). A popular example in this class is the tag cloud (see [18] for the history), although it does not present the whole text, just the n most frequent words. Tag clouds have been criticized for using just the size to carry information — other visual variables have only a decorative function (color, location, orientation), and may mislead the reader. A classic direct text visualization is SeeSoft which used color to encode programmers in software systems and characters in novels [4].
B. Indirect text visualizations
The indirect text visualizations quantify some aspect of text, and visualize the numbers with a suitable technique. A popular approach is to compute term vectors or weights of each word in a document and visualize the vectors. This approach allows the use of, e.g., Self-Organizing Maps (SOMs) [7] or ThemeScapes [10] to create maps of the document or document collection. From the sociolinguistic perspective, these techniques serve as overviews, but lose connection to the detail.
C. Hybrid visualizations
Finally, the hybrid visualizations combine the text and the visualization in a meaningful way. This requires easy movement between the text, the corresponding spot in the visualization, and vice versa.
A significant portion of the text visualization tools have a ‘zero-interaction’ user interface, i.e., they are static. How- ever, in an exploratory tool it is essential to be able to interact, to adjust visualization parameters and test ideas.
III. TEXT VARIATION EXPLORER
Text Variation Explorer (TVE) is a linguistically-oriented visualization tool for gaining insight into text [14], [15]. It was not designed to be a tool to run statistical tests (there are unsurpassed tools for that [11]), but rather a tool for quick exploration of text structure, complexity, and variation. It is a tool to raise questions rather than give answers. In the following, we describe the design goals, the features, and the use of TVE.
A. Language-independence
TVE was designed to be as simple and general as the intended task reasonably allows. One of the issues was how to keep this kind of a tool as language-independent as possible. We chose to limit the input into plain text (Unicode, utf8) and leave the definition of a ‘word’ to the user. What constitutes a word is defined by negation, by giving the set of characters that can’t appear in a word (Fig. 1). The input is then parsed into words according to this set, and the resulting word count is displayed. These choices essentially limit the use of TVE to western, left-to-right written languages.
 3
3
3
3
1
1
Figure 1.
Definition of ‘word’ and setting of the sample size and overlap.
B. Size of text window
In corpus-linguistic analysis one of the important parame- ters is the window size of the text sample, which affects how some of the linguistic measures behave. Often the length of the sample window is set to a value that has been used in previous research (400 words is common), although it would pay off to explore a range of values. TVE allows a quick experimentation by providing a direct-manipulation slider both for the text window size and overlap (Fig. 1).
C. Measures
Besides language-independence, another important design goal in TVE is to keep it responsive, even with novel-length inputs and beyond. The linguistic measures were limited to the three most important ones that are highly useful when analyzing the structure, complexity, and variation of text. These measures are type-token ratio, hapax legomena, and average word length. The type-token ratio is the proportion of unique words in a sample, the hapax legomena is the proportion of words appearing exactly once in a sample, and the average word length as characters is self-explanatory. These three measures are displayed as a line graph (Fig. 2), and they describe the vocabulary richness and style of text. The first two of these measures are affected by the change of text window size, and they are known to stabilize around 1,300 words [6].
The interplay of these measures reveals to an expert interesting things about the text. As a trivial example, when all three measures have a low or high value, it may signal that the text type is dialog (conversation of two or more people) or narrative (representation of an event or a series of events), respectively.

  Figure 2. The linguistic measurements as a line graph: type-token ratio, hapax legomena, and average word length.
D. Text clustering
There is also a more generic method to explore the structure of text. The user can define a list of words that is used to compute frequency vectors for those words, per each text fragment (Fig. 3). When a text fragment is selected, the word list view will show the corresponding word frequencies.
Figure 3. Frequency vectors.
Then the principal component analysis (PCA, the WEKA library [5]) is used to compute a user-defined number of text fragment clusters, based on the first two principal components. The result of the computation is shown as an xy-plot of the two first components, and the points representing text fragments are colored according to the assigned clusters (Fig. 4). The points of each cluster can also be represented with a minimal convex hull having a transparent color-coding. The word frequency vectors can also be exported from TVE to continue the analysis with other tools.
In Fig. 6 “Seven Brothers” by Aleksis Kivi, the national author of Finland, is read into TVE. If we define a list of Finnish Pronouns (Fig. 5), then we can cluster the text fragments into three according to them. The areas indicated are dominated either by ‘she/he’ (‘ha ̈n’), ‘they’ (‘he’), or ‘I’ (‘mina ̈’). Based on close reading of the text fragments falling within each area, it seems that the ‘she/he’ area corresponds to narratives within the narrative of the novel, such as folk tales told by the brothers, while the ‘they’ area indicates narrative sections of the novel itself, i.e., what the brothers
Figure 4.
Principal Component View (PCA). Each point is a text fragment.
3
33
32
2
were doing, and the ‘I’ area indicates dialog. In a sociolin- guistic corpus, differences in the use of personal pronouns might be interpreted, e.g., as different communicative styles employed by people representing different social groups.
  Figure 5.
button in Fig. 3.
The list of Finnish pronouns, opened from the ‘Edit words...’
E. Interaction
TVE provides two methods to interact with the text. Firstly, there are sliders to adjust the text window size and overlap (Fig. 1). As the sliders are manipulated, the line graph view (Fig. 2) will be continuously updated, making it possible to review a large number of parameter settings in a short time. Secondly, there is a three-way brushing interaction between the data views (Fig. 7). When a text fragment is selected either in the text view, line graph, or the principal component view, all the other views are updated to show the same fragment. The text view will also scroll to the corresponding point to make the text fragment visible.

   The ”Seven Brothers” by Aleksis Kivi, clustered according to
IV. DISCUSSION
We have seen that TVE can provide a quick overview of similarities and differences across corpora, highlighting sections that require more careful analysis. It can also be used to explore variation both across and within social cate- gories. However, both of these functions could be enhanced by further development.
In the following, we have gathered the development ideas and issues we have collected from the users of TVE. We refer to the current version as ‘TVE’ and the new version as ‘TVE2’. TVE has been demonstrated in seminars and conferences, and it has been used in teaching as well.
A. From texts to corpora
In the current TVE, the input is simply pasted into text view, and the design assumption was that we analyze a single text. Obviously, it is possible to paste several texts into text view, in conjunction, so we added a special non- word marker, ‘dammocmark’, that can be placed between texts, and which is shown as a blue line in the line graph (Fig. 7). However, this afterthought does not really solve the problem, because the line graph runs out of pixels to represent the text fragments. TVE2 will have a setup screen to define the files that a corpus consists of, and separate screens for each text and the corpus they form. Texts can be analyzed separately, and we can use a more suitable method to visualize the corpus.
B. Metadata and scatterplot
Sociolinguistic text materials are invariably described by metadata, such as author, year, gender, ethnicity, domicile, social class, data, etc. TVE2 will read metadata from a csv text file with a header. The header defines the names for metadata items and avoids fixing the set of metadata in advance. We add a general-purpose scatterplot to TVE2 where the user can set the x, y, size, and color variables. The available set of variables includes the metadata items and linguistic measures, so it will possible to create plots like ‘hapax legomena of men over time’.
C. PCA and line graph views
Users find it confusing that changing the sample size just by one word may produce a completely different PCA view. This is due to the nature of principal component analysis, and is the correct behavior. The interpretation of PCA view can be simplified if we show the highest frequency word on top of the corresponding area (Fig. 9).
In addition, TVE2 will implement the vector space model [13] for text fragments and use PCA to show the similarities between corpus texts.
As noted earlier (subsection III-C), it is usually the interplay of linguistic measures that gives insight about the text. TVE2 will have an option to stack the line graphs, which will amplify the changes and make it easier to locate interesting spots (Fig. 10).
Figure 6.
pronouns. The view of first two principal components (top), data clustered into three (middle), and clusters shown as regions (bottom). The text window size was set to 370 words.
F. Use cases
It is known that there is a gender difference in the use of pronouns in the Corpus of Early English Correspondence [3]. Women use more pronouns than men [12]. This socio- linguistic difference can be seen in TVE by clustering the text samples according to pronouns and dividing the text fragments into two. The first principal component signifies gender fairly accurately. Knowing this fact one might be interested to ask how homogenous is the language use of women writers. If we look at two historical figures, Dorothy Osborne (1671) and Lady Arabella Stuart (1605), the clus- tering according to pronouns does not suggest difference in use. However, if we cluster according to function words [2], we see that their use of language is quite different (Fig. 8).
TVE has also been used in comparing the different versions of International Corpus of English (ICE) although the current user interface is not really designed for this. TVE was able to point out similarities and differences at the level of corpus, genre, and subgenre [9].
 3
3
3
3
3
3
   Figure 7.
The user interface of TextVariationExplorer application.
HE
MINÄ
Figure 9. PCA clusters labeled with the highest-frequency word (continues the example of Fig. 4).
  HÄN
 Figure 8.
Clustering according to function words.
D. Data export for analysis
TVE can export the frequency data of user-defined word vectors as csv files. TVE2 is able to export the complete text fragment data in R [11] format for further analysis.
E. Wildcards in word vectors
The current text clustering is based on the frequencies of words that are given literally, i.e., only the exact match
Figure 10.
Linguistic measures as a stacked line graph.
3
3
3
4
4
is considered (subsection III-D). This is a problem with languages like Finnish where we have about 15 cases for nouns, which are expressed by suffixes. Adding a single noun to the wordlist would then mean inserting about fifteen
 3
entries. In TVE2, the wordlist entries will allow wildcards.
V. CONCLUSION
We have presented and discussed the design of the Text Variation Explorer tool we are developing for exploratory corpus linguistics and for sociolinguistics. Both the current and the upcoming versions are freely available [14].
We would welcome any ideas or experiences about visu- alizing sociolinguistic data you may have, especially if you have tried our Text Variation Explorer.
ACKNOWLEDGMENTS
This research was funded by the Academy of Finland, Digital Humanities Programme, project ‘Interfacing struc- tured and unstructured data in sociolinguistic research on language change (STRATAS)’, sub-project #293441.
REFERENCES
[1] J. Bertin, Graphics and Graphic Information- Processing. Berlin: Walter de Gruyter, 1981, 273 p. Translated by William J. Berg and Paul Scott.
[2] J. N. G. Binongo, “Who wrote the 15th book of Oz? an application of multivariate analysis to authorship attribution,” Chance, vol. 16, no. 2, pp. 9–17, 2003.
[3] CEEC, Corpus of Early English Correspondence, Compiled by T. Nevalainen, H. Raumolin-Brunberg, J. Kera ̈nen, M. Nevala, A. Nurmi, and M. Palander- Collin, Department of English, University of Helsinki, 1998.
[4] S. G. Eick, J. L. Steffen, and E. E. Sumner Jr., “Seesoft – a tool for visualizing line oriented software statistics,” IEEE Trans. Softw. Eng., vol. 18, no. 11, pp. 957–968, 1992. [Online]. Available: http://dx.doi. org/10.1109/32.177365.
[5] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute- mann, and I. H. Witten, “The WEKA data mining software: An update,” SIGKDD Explor. Newsl., vol. 11, no. 1, pp. 10–18, Nov. 2009. [Online]. Available: http://doi.acm.org/10.1145/1656274.1656278.
[6] D. Keim and D. Oelke, “Literature fingerprinting: A new method for visual literary analysis,” in IEEE Symposium on Visual Analytics Science and Technol- ogy (VAST 2007), 2007, pp. 115–122. DOI: 10.1109/ VAST.2007.4389004.
[7] T. Kohonen, Self-Organizing Maps, Third Extended, ser. Springer Series in Information Sciences. Springer- Verlag, 2001.
[8] K. Kucher and A. Kerren, “Text visualization tech- niques: Taxonomy, visual survey, and community in- sights,” in Visualization Symposium (PacificVis), 2015 IEEE Pacific, Apr. 2015, pp. 117–121. DOI: 10.1109/ PACIFICVIS.2015.7156366.
[9] T. Nevalainen and T. Sa ̈ily, Comparing like with like? tools for exploring families of corpora, Talk at ChangE 2013, Helsinki, 2013. [Online]. Available: https : / / www. cs . helsinki . fi / u / tsaily / presentations / change2013 tn ts.pdf.
[10] K. A. Pennock and D. B. Lantrip, “Themescapes: A landscape representation of themes in text,” in Symposium on Advanced Intelligence Processing and Analysis, Tysons Corner, VA, USA, 1995, p. 47.
[11] R Core Team, R: A language and environment for statistical computing, R Foundation for Statistical Computing, Vienna, Austria, 2016. [Online]. Avail- able: http://www.R-project.org/.
[12] T. Sa ̈ily, T. Nevalainen, and H. Siirtola, “Variation in noun and pronoun frequencies in a sociohistorical cor- pus of English,” Literary and Linguistic Computing, vol. 26, no. 2, pp. 167–188, 2011. [Online]. Available: http://dx.doi.org/10.1093/llc/fqr004.
[13] G. Salton, A. Wong, and C. S. Yang, “A vector space model for automatic indexing,” Commun. ACM, vol. 18, no. 11, pp. 613–620, Nov. 1975. [Online]. Available: http://doi.acm.org/10.1145/361219. 361220.
[14] H. Siirtola, Text Variation Explorer (TVE), Computer program, 2011. [Online]. Available: http://www.uta. fi/sis/tauchi/virg/projects/dammoc/tve.html.
[15] H.Siirtola,T.Sa ̈ily,T.Nevalainen,andK.-J.Ra ̈iha ̈, “Text Variation Explorer: Towards interactive visu- alization tools for corpus linguistics,” International Journal of Corpus Linguistics, vol. 19, no. 3, pp. 417– 429, 2014. [Online]. Available: http://dx.doi.org/10. 1075/ijcl.19.3.05sii.
[16] M. Stone, “Information visualization: Challenge for the humanities,” in Report of a Workshop Cospon- sored by the Council on Library and Information Re- sources and the National Endowment for the Human- ities, Council on Library and Information Resources, 2009, pp. 43–56. [Online]. Available: http://www.clir. org/pubs/abstract/reports/pub145.
[17] Text Visualization Browser, 2016. [Online]. Available: http://textvis.lnu.se.
[18] F. B. Vie ́gas and M. Wattenberg, “Timelines: Tag clouds and the case for vernacular visualization,” Interactions, vol. 15, pp. 49–52, 4 2008. DOI: http: //doi.acm.org/10.1145/1374489.1374501.
[19] U. Weinreich, W. Labov, and M. Herzog, “Directions for historical linguistics: A symposium,” in, W. P. Lehmann and Y. Malkiel, Eds. University of Texas Press, 1968, ch. Empirical Foundations for a Theory of Language Change, pp. 95–188.
Pers Ubiquit Comput (2013) 17:371–385 DOI 10.1007/s00779-011-0500-3
ORIGINAL ARTICLE
On the effectiveness of Overview+Detail visualization on mobile devices
Stefano Burigat • Luca Chittaro
Received: 16 September 2011 / Accepted: 5 December 2011 / Published online: 30 December 2011 Ó Springer-Verlag London Limited 2011
  Abstract Overview?Detail visualization is one of the major approaches to the display of large information spaces on a computer screen. Widely used in desktop applications, its feasibility on mobile devices has been scarcely investi- gated. This paper first provides a detailed analysis of the literature on Overview?Detail visualization, discussing and comparing the results of desktop and mobile studies to highlight strengths and weaknesses of the approach. The analysis reveals open issues worthy of additional investi- gation and can provide useful indications to interface designers. Then, the paper presents an experiment that studies unexplored aspects of the design space for mobile interfaces based on the Overview?Detail approach, inves- tigating the effect of letting users manipulate the overview to navigate maps and the effect of highlighting possible objects of interest in the overview to support search tasks. Results of the experiment suggest that both direct manipu- lation of the overview and highlighting objects of interest in the overview have a positive effect on user performance in terms of the time to complete search tasks on mobile devices, but do not provide specific advantages in terms of recall of the spatial configuration of targets.
Keywords Overview?Detail   Visualization   Small-screen devices   Mobile interaction   Experimental evaluation
S. Burigat (&)   L. Chittaro
HCI Lab, Department of Mathematics
and Computer Science, University of Udine, Udine, Italy e-mail: stefano.burigat@uniud.it
L. Chittaro
e-mail: luca.chittaro@uniud.it
1 Introduction
Today, mobile devices are powerful enough to display maps, images, web pages, and other large and complex information spaces, supporting an ever increasing number of people in carrying out work and leisure activities any- time, anywhere. Map-based systems, content-rich web sites, imaging software, and other applications and services are no longer limited to the desktop domain. Unfortunately, visualizing information effectively on mobile devices is not trivial [14] and there is no guarantee that effective solutions for desktop visualization could be successfully employed in the mobile domain. Indeed, mobile devices have smaller displays, less powerful hardware, different input mecha- nisms compared to desktop computers, and most of these limitations are not likely to disappear in the near future without sacrificing device portability.
One of the most complex steps in the process of designing appropriate visualizations for the mobile context is laying out the information on the available screen space (the presentation problem). When the information to accommodate is larger than the available viewing area, users need access to fine-grained details as well as coarse- grained context information to effectively explore the visualization [13]. Interface design choices have then to focus on how to provide details as well as context infor- mation when screen space is at a premium. The typical approach to face this issue is to provide users with pan and zoom mechanisms, thus introducing a temporal separation between detail and context information [6, 26]. However, temporal separation makes it difficult for users to focus on the details of a visualization while keeping track of the global context [12, 22].
Researchers have investigated four classes of solutions to solve or at least mitigate the presentation problem
 1 3
372
Pers Ubiquit Comput (2013) 17:371–385
 on mobile devices: Overview?Detail, Focus?Context, Contextual Cues, and custom pan and zoom mechanisms. The Overview?Detail approach is commonly used in commercial desktop applications (Fig. 1) and provides both detail and context information by typically displaying two separate views simultaneously, one for the context and one for the detail [28]. Focus?Context [23] seamlessly integrates detail and context information in the same view, usually by exploiting some form of geometric distortion. Contextual Cues techniques augment the detail view with glyphs meant to help locate parts of interest that are outside the view area. Typically, this is obtained by displaying abstract shapes (e.g., arrows or arcs) in the border region of the screen as visual references to the off-screen context [2, 9]. Custom pan and zoom mechanisms adopt the tra- ditional idea of navigating a visualization by panning and zooming but adapt it to the specific features of mobile devices to reduce the complexity of navigation for the user.
In this paper, we focus specifically on Overview?Detail visualization (hereinafter, O?D), which has received lim- ited attention by the mobile community. Indeed, while several studies have compared O?D to other presentation techniques on desktop computers [15], few studies have investigated the effectiveness of the O?D approach on small screens, with conflicting results. For example, Bu ̈ring et al. [11] found that there was no advantage in navigating a scatterplot with the aid of an overview and that the overview was actually detrimental to user navigation per- formance in case of users with high spatial ability. Burigat et al. [10] found instead that users benefit from the avail- ability of an overview in map search tasks.
The contribution of this paper is twofold. First, we provide a detailed analysis of the strengths and weaknesses of O?D visualization on mobile devices in light of results
of both desktop and mobile studies. Our survey points out open issues worthy of additional investigation and aims to help designers of mobile interfaces determine if and when O?D visualization could be advantageous over other pre- sentation techniques. Second, we present a follow-up study to [10] that further explores the design space of mobile O?D visualization, delivering additional actionable infor- mation on the topic.
The survey we present in the first part of the paper is complementary to the surveys of Cockburn et al. [15] and Hornbaek and Hertzum [21], which mainly focus on the desktop domain. In particular, Cockburn et al. [15] discuss O?D in the context of a more general review of approa- ches that allow users to work at multiple levels of detail. Hornbaek and Hertzum [21] take a different perspective, exploring how the concept of overview (defined as awareness of some aspect of an information space) is used in the Information Visualization literature.
The study we describe in the second part of the paper explores the effect of two features of O?D interfaces on mobile devices: (1) adding interactive capabilities to the overview, i.e., letting users manipulate the overview as an interactive navigation control, and (2) highlighting possi- ble objects of interest in the overview, thus adding an additional layer of semantic information. Our main motivation for the study was the general lack of such investigations in the mobile as well as desktop literature. Indeed, while there are several comparisons between O?D and other presentation approaches, investigations of the effect of specific O?D interface features on user performance and preference are rare. Results of our study would thus contribute to the understanding of which features offer the greatest performance advantages and under what conditions.
 Fig. 1 Two examples of Overview?Detail visualization in desktop photo-editing example (Paint Shop Pro), the overview is displayed at applications. In the map example (Google Maps), the overview the right of the detail view
overlaps the detail view at the bottom right corner of the screen. In the
1 3
Pers Ubiquit Comput (2013) 17:371–385
373
 2 O1D research results in desktop and mobile contexts
In this section, we first outline the design space for O?D visualization, identifying core features and possible design alternatives that have been proposed in the literature. Then, we distill the most significant results of the studies on O?D visualization on desktop computers and discuss them in relation to mobile device capabilities and current research findings in the mobile context.
2.1 The design space for O?D visualization
Most interfaces based on the O?D approach are charac- terized by a common core of functionality, but can vary substantially in terms of presentation and usage.
2.1.1 O?D presentation
Figure 1 shows typical O?D layouts, comprising a pair of coordinated views, with one small overview displayed either over or beside a larger detail view. Overlapping views are typically used in map-based applications while non-overlapping views are more common in drawing and photo-editing tools. In both cases, the overview is usually a small-scale thumbnail of the whole information space that includes a properly positioned graphical highlight (here- inafter, viewfinder) of the portion of space which is cur- rently displayed by the detail view.
In current applications, the viewfinder is displayed in the overview as a simple polygonal outline, or as a shaded polygonal area (see Fig. 1 left), or by shading the context area in the overview (see Fig. 1 right). To the best of our knowledge, no comparative study of the effectiveness of the three alternatives is available in the literature.
In terms of size, the overview is almost always smaller than the detail view but there is no standard value for the relative size of views. Less common layouts make choices such as reserving the same amount of screen space for the two views or allowing the overview to use most of the screen. In general, as suggested by Plaisant et al. [28], the size of the overview and the detail view should be task dependent. For example, a large detail view should sim- plify drawing or open-ended exploration of a map while a large overview should be preferable in monitoring tasks.
The number of views is another parameter of O?D interfaces: while most applications display two views, complex configurations based on three or more views are possible. Empirical evidence shows that the number of views should depend on the zoom factor, i.e., the level of magnification between overview and detail view: when the zoom factor is higher than 25–30, intermediate overview levels are recommended [28, 30].
While there are validated recommendations for the design of O?D interfaces in the desktop domain, the mobile context is lacking specific guidelines. Given the limited screen space of mobile devices, it would seem sensible to aim at optimizing use of screen space, e.g., by using overlapping views and choosing a low zoom factor to limit the number of views to the minimum. However, as we will see in the following sections, existing studies in the literature have mostly focused on comparisons of O?D visualization with other solutions to the presentation problem and we thus have limited knowledge on the rela- tive merits of different O?D design options.
2.1.2 O?D usage
All O?D interfaces support navigation of the information space they display, through traditional panning and zooming mechanisms such as dragging the detail view to move it in the desired direction and changing magnification level with left and right mouseclicks, or by direct manip- ulation of the overview. In this latter case, dragging the viewfinder within the overview results in a corresponding change in the portion of information space shown by the detail view and highlighting a region of the overview with a click-and-drag operation implements a combination of panning and zooming, making the detail view display the selected portion of information space.
Early papers on O?D visualization such as [28] recom- mend to coordinate overview and detail view in the form of tight coupling to properly support navigation, regardless of the specific panning and zooming technique. Tight coupling consists in immediately reflecting manipulation of the detail view (panning, zooming) as variations in the position or size of the viewfinder and vice versa [1]. However, most of today’s widely used applications (e.g., Google Maps, Adobe Reader) adopt a less strict implementation of coordination in which manipulation of the overview results in an update of the detail view only when users complete their panning action. This behavior helps reducing computational and network load but its effect on users has not been investigated.
Another increasingly common feature in O?D inter- faces is manual control of overview visibility: users can hide the overview when they do not need it. This option allows one to maximize the area devoted to the detail view, which is desirable when the screen space is limited, but is likely to be useful only in tasks that do not require frequent examination of the overview.
2.2 Empirical evaluations of O?D visualization
Several interfaces based on the O?D approach have been developed since the 1980s but only in the last decade there has been a significant research effort aimed at studying
1 3
 374
Pers Ubiquit Comput (2013) 17:371–385
their effectiveness. In this section, we discuss implications of major desktop studies on O?D interfaces and then examine the state of O?D in the mobile domain.
2.2.1 Implications of desktop O?D studies
Looking at the studies on desktop O?D, summarized in Table 1, one immediately notices the wide variety of dif- ferent information spaces, user tasks, interface designs, navigation mechanisms that have been considered by researchers over the years. Such variety provides several starting points for discussion but, at the same time, makes it difficult to compare and generalize findings and to explain the inconclusive and sometimes contradictory results that have been produced.
Table 1 Desktop O?D studies
Only a couple of studies provided some insight into the O?D design space, comparing variants of O?D interfaces. In particular, Beard and Walker [5] let users manipulate the overview in two alternative ways, by dragging the view- finder or by highlighting regions to zoom into, but did not find any significant performance difference between them. The study revealed that displaying the semantic content in the overview was instead an essential feature. Indeed, users performed worse when they did not have access to a miniature of the explored information space in the over- view, even if they could still manipulate the overview to navigate. North and Shneiderman’s study [25] highlights the effect of coordination in O?D interfaces: coordination between overview and detail view is absolutely critical in tasks where access to details is important (the majority
1 3
 Pers Ubiquit Comput (2013) 17:371–385 375
Table 1 continued
Unless otherwise specified, ‘‘O?D interface’’ means a pair of coordinated views, with one small overview displayed over a larger detail view
1 3
376
Pers Ubiquit Comput (2013) 17:371–385
 in common applications) but is not essential when the overview can directly provide users with the information needed to carry out tasks.
In terms of pure task completion time, we note that O?D interfaces typically outperformed scrolling interfaces [4, 5, 20, 25] but often did not compare favorably to zooming and Focus?Context interfaces [3, 18, 20, 25]. There is one widely mentioned reason for the difficulties users experience with O?D visualization: the mental and motor effort required to integrate overview and detail views might strain memory and increase the time needed for visual search of an information space [3, 12, 22]. However, Pietriga et al. [27] showed in their study that an O?D interface combined with zooming is superior to zooming and Focus?Context interfaces in terms of the low-level motor and perceptual effort required in generic search tasks. This result suggests that different factors could have negatively affected user performance in those studies where O?D interfaces were outperformed by other solutions. For example, Hornbaek et al. [22] recognize that the addition of semantic zooming in their study probably provided users with rich navigation cues, making the overview often unnecessary. In a similar way, the lack of performance effects of overviews in Nekrasovski et al.’s [24] may be explained by the presence of guaranteed vis- ibility in all interfaces. Indeed, the authors speculate that coloring important tree nodes in the detail view may have provided users with the orientation information they could otherwise find only through the overview. In Baudisch et al.’s study [3], the fact that the O?D interface used two different, physically separated screens while both other interfaces were displayed on one single screen could have had an influence on the results. An analysis of reading patterns provides a possible explanation of the results in Hornbaek et al.’s 2003 study [20]: in the O?D condition, users often abused of the capability to easily navigate the document using the overview, doing unnecessarily frequent and longer explorations even when a satisfactory answer to the given task had already been obtained.
The study by Hornbaek et al. [20] also reveals that per- formance of users with the O?D interface was significantly better than performance with other interfaces, including a Focus?Context one, when a different metric, i.e., text comprehension, was considered. This finding suggests that O?D interfaces can provide benefits to users in terms of information acquisition during navigation. Hornbaek et al.’s study [22] seems to provide a contradictory result on this aspect since users showed better spatial recall of map objects after using the zooming interface compared to the O?D interface. However, semantic zooming could have played a significant role also in this case. Unfortunately, since almost all studies in the literature have focused on task completion time as the primary metric to measure user
performance, there is limited knowledge of other possible positive effects of the O?D approach.
There is instead significant evidence of the preference of users for O?D interfaces, even in those studies that found O?D to be worse than other approaches in terms of performance (with the notable exception of Gutwin and Skopik [18]). Some researchers suggest that the overview probably helps users in building a more comprehensible internal model of the visualization [24]. In those studies where O?D did not compare favorably to other interfaces, this internal model was probably insufficient to counter- balance the additional factors that did negatively affect performance, yet it improved users’ perception of the benefits of O?D interfaces, which could explain preference results. In such cases, as recommended by Hornbaek et al. [22], designers should consider whether to shoot for sub- jective satisfaction or user performance and provide an overview or not in their interfaces accordingly. For example, overviews should be avoided when the informa- tion space provides enough cues for navigation and navi- gation time is the most important performance metric.
The study by Gutwin and Skopik [18] was the only one where, consistently with their performance results, users preferred the Focus?Context approach to the O?D one. This is an important result because it suggests that in tasks where the goal is to locally manipulate the visualization at high magnification (e.g., tracing the edges of an object in an image), the benefits (real and perceived) of Focus? Context interfaces far exceed those of O?D interfaces. However, most of the studies we examined focused on tasks that required users to navigate an information space in order to visually search for targets. Further studies are thus needed to get a more comprehensive picture of the relation between task category and presentation techniques.
2.2.2 O?D on mobile devices
It is reasonable to expect that most high-level results of desktop O?D studies would hold on mobile devices as well: coordinated views should be more effective than uncoordinated overview and detail views [25], displaying a miniature of the information space should be essential to properly support navigation [5], providing additional ways to get orientation information (e.g., through semantic zooming) should have a negative impact on the usefulness of an overview [22, 24]. However, there is no easy way to generalize to the mobile domain all the performance and preference results found in desktop O?D studies. Indeed, conditions are extremely different and it may be the case that mobile device limitations affect different interfaces in dissimilar ways with respect to the desktop domain. For example, in the study by Pietriga et al. [27], the overview covered a 200 9 200 pixels region of the screen, which
1 3
Pers Ubiquit Comput (2013) 17:371–385
377
 represented 4.5% of the total available display area. This configuration cannot be produced on the screen of mobile devices, which are limited to low resolutions. Even in the absence of a formal investigation of the role of the size of views in O?D interfaces, the direct applicability of the results of that study to the mobile domain is doubtful.
In general, fitting overview and detail views on a limited screen space is problematic: reducing the overview in size negatively affects the readability of its content but increasing the size subtracts screen space from the detail view, which is typically the primary focus of user’s inter- est. Some researchers suggest that designers should use overviews at least one-sixteenth the size of the detail window in desktop applications and that the overview might need to be larger to support navigation on small devices [22]. However, design guidelines on overview sizes are lacking. Necessarily, overview and detail views are smaller than on a desktop screen and this could make it more difficult to relate them, increasing the effort required to integrate the information they provide [14].
Several desktop studies also highlighted difficulties users had in manipulating the overview to carry out pan and zoom operations when the zoom factor was too high [5, 18, 22]. In such cases, besides the difficulty of interacting with a very small viewfinder, it came out that the small size of the overview resulted in large jumps in the detail view for even a small movement of the viewfinder. Even more so, the small size of overviews could have a significant impact on the ease with which users manipulate the over- view itself on mobile devices.
On the positive side, we note that both overview and detail views on a small screen should be relatively easy to see at once. Compared to the desktop case, where the overview is typically in the peripheral view area when the user focuses on the detail view, fewer and shorter eye movements should probably be necessary on a mobile device to correlate the information the two views provide.
2.2.3 Mobile O?D studies
Despite the differences between desktop and mobile sce- narios, only a few empirical studies, summarized in Table 2, have been carried out to determine how mobile device limitations affect the design and use of O?D interfaces.
Roto et al.’s work [29] on web page visualization on small screens clearly shows the effect of designing an O?D interface for a mobile platform, proposing an approach that differs significantly from those found in desktop studies in terms of features of the overview. Since the target device had no pointing capabilities, the overview did not provide pan and zoom mechanisms and was aimed primarily at supporting orientation. Moreover, to limit its
intrusiveness, the overview was overlaid transparently over the detail view and, more importantly, it was visible only during continuous scrolling of a page. Compared to a more traditional mobile browser, the O?D approach scored better in usability ratings and user preference, similarly to what was found in Baudisch et al.’s desktop study on web page navigation [4]. Unfortunately, the design of the study makes it impossible to determine whether the results were due to the page reformatting technique used, the overview, or to the combination of the two factors. Neither it is possible to understand the effect of any of the specific features of the overview.
Buring et al.’s study [11] on scatterplot visualization seems to provide the most compelling proof of the draw- backs of mobile O?D visualization. Results of the study revealed that participants with high spatial ability solved tasks significantly faster with the zooming interface while no performance difference between the two considered solutions was found for subjects with low spatial ability. As pointed out by the authors, these results seem to confirm the negative effect of the reduced size of the detail view in mobile O?D interfaces: on small screens, a larger detail view can outweigh the benefits gained from the presence of an overview window. However, another possible motiva- tion for the results is that users could get additional navi- gation cues beyond those provided by the overview, like in the studies by Hornbaek et al. [22] and Nekrasovski et al. [24] in the desktop domain. Indeed, not only did the system use an implementation of semantic zooming as in [22] but users could also refer to the labeled axes of the scatterplot to guide their navigation. These factors, combined with the reduced size of the detail view and the problems users encountered in interacting with the small overview, might also explain why, unlike in most desktop studies, the O?D interface did not show any advantage in terms of user preference over the zooming interface.
Unlike what was found by Buring et al. [11] and Hornbaek et al. [22], our 2008 study on map, diagram, and web page navigation showed that an O?D interface is comparable or can provide advantages over a more tradi- tional zooming interface in terms of task completion time [10]. This suggests that orientation cues that are external to the overview, which were available in the two cited studies but not in ours, might play indeed a significant role in supporting navigation, making the overview unnecessary. We also found that trading semantic content in the over- view for increased visibility of the detail view, as we did in the wireframe O?D interface, was not useful to improve user performance. The spatial recall task we designed to determine which interface better supported user creation of a mental map of the information space revealed that users were more accurate with O?D interfaces than with the zooming interface, especially in the case of maps. This
1 3
 378 Pers Ubiquit Comput (2013) 17:371–385
Table 2 Mobile O?D studies
Unless otherwise specified, ‘‘O?D interface’’ means a pair of coordinated views, with one small overview displayed over a larger detail view
contradicts the results obtained by Hornbaek et al. [22], which were probably affected by the availability of semantic zooming. Similarly to desktop studies, we also found a clear user preference for traditional O?D over the other two interfaces for map and diagram navigation. Interestingly, user comments highlighted that the overview was considered detrimental to web navigation but did not point out the same drawback for the other two information spaces. This might be due to the fact that web pages have a well-defined structure that is familiar to users and helps navigation.
As we found in our 2011 study [8], O?D on mobile devices is also useful when the user needs to reason in terms
of the spatial configuration of the objects of interest con- tained in an information space. Unlike previous desktop and mobile studies on O?D, which required users to actively navigate an information space to search for specific data, the tasks in our study aimed at assessing how well the different conditions conveyed information about off-screen objects, i.e., objects of an information space that fall outside the detail view area. In the object-ordering task, users were significantly slower with O?D than they were with Wedge [17] and Scaled Arrows [9], probably because it was easier for users to compare the glyphs encoding direction and distance of off-screen objects with Wedge or Scaled Arrows than it was to obtain distance information from a small-
1 3
Pers Ubiquit Comput (2013) 17:371–385
379
 scale overview. In this case, the small size of the overview nullified the advantage of having direct visual access to object configurations. In the pair-of-closest-objects task, users were significantly faster and were more accurate with O?D than they were with Wedge. This task revealed the effectiveness of O?D in complex spatial tasks that depend on knowing the spatial configuration of all off-screen objects. As in the desktop domain, we found evidence of the preference of users for O?D interfaces, even for those tasks in which O?D was worse than other approaches in terms of performance. Probably, users prefer having direct visual access to the configuration of off-screen objects even if the small size of the overview makes it actually difficult to easily extract accurate information.
2.2.4 Implications of mobile O?D studies
Overall, it is difficult to draw general conclusions from the few studies on mobile O?D. As we pointed out in the discussion of desktop O?D studies, availability of multiple means to obtain orientation cues seems to reduce the effectiveness of the mobile O?D approach. When an information space provides these cues, as in the case of the scatterplot in [11] or web pages in [10], O?D interfaces do not provide advantages in terms of navigation performance compared to more traditional presentation techniques. However, an O?D interface is comparable or can provide performance advantages when additional orientation cues are not available in the considered information space [10]. This is particularly noticeable in the case of spatial tasks, as we found in [8] and [10], even if the small size of the overview might sometimes negatively affect geometric assessments (as in the object-ordering task in [8]). Unlike in the desktop domain, there also seems to be a tighter correlation between user performance and subjective preference in target search tasks. Probably because of the smaller size of the views, users did perceive O?D inter- faces to be detrimental in the studies that found O?D to be worse in terms of task completion time.
However, many unclear points still remain to be clar- ified through further investigations. For example, are the general results of Pietriga et al.’s study [27] about the low-level motor and perceptual effort advantages of O?D still valid on small-screen devices? How do O?D inter- faces compare to Focus?Context interfaces in the mobile domain? What is the effectiveness of O?D interfaces in common mobile scenarios such as during walking or under sunlight? What are the effects of different design options on user performance with mobile O?D inter- faces? In the second part of this paper, we present one study that starts to take into consideration this last ques- tion, exploring two possible design dimensions for mobile O?D interfaces.
3 User study
Most of the studies on O?D visualization, in both desktop and mobile domains, have focused on comparing a specific O?D implementation with interfaces based on different approaches to the presentation problem such as scrolling, zooming, or Focus?Context visualization. Very few stud- ies [5, 10, 25] have instead explored, at least in part, the design space for O?D visualization, investigating the effect of specific interface features on user performance and preference. As a consequence, implementations of the O?D approach are often arbitrary and sometimes even ignore the guidelines we highlighted in previous sections, such as keeping the zoom factor under a certain threshold and using tight coupling.
To deepen the analysis of the O?D design space and provide actionable indications to interface designers, we carried out a follow-up to our 2008 study, with a twofold goal. First, we wanted to better understand the effect of highlighting objects of interest in the overview, which introduces an additional layer of semantic content with respect to a standard overview. In our previous study [10], we introduced highlighting in the overview during map search tasks with the traditional O?D interface. Besides having access to a miniature of the information space, users could thus look at the highlighted objects in the overview to guide their search toward possible targets. In the present study, we controlled the display of objects of interest in the overview to assess how much this specific cue could affect user performance in search and spatial recall tasks. The second goal of the study was to investigate if letting users navigate an information space by direct manipulation of the viewfinder within the overview could benefit performance on mobile devices despite the likely interaction difficulties due to the small size of the overview. Almost all previous O?D studies integrated some form of overview manipu- lation to support pan, zoom, or both operations. However, none of them could determine whether results were due to the information displayed in the overview, the direct manipulation capabilities, or a combination of the two factors. Our study will help clarify this point.
Intuitively, the two O?D interface features we consid- ered should significantly benefit user performance. How- ever, we were unsure about how much the small size of the overview could negatively affect their effectiveness. We were also interested in determining the relative impact of the two features in terms of magnitude of their effect. For designers, this could be useful to estimate how much they could gain by including each feature in their interfaces.
As in our previous experiment, we designed a navi- gation task that required users to search for specific tar- gets in the considered information space and a spatial memory task that assessed recall of information after
1 3
380
Pers Ubiquit Comput (2013) 17:371–385
 exploration of the information space. The first task is useful to compare our results with those of the related literature in terms of task completion time while the second task allows us to continue our study of the O?D approach using a different metric to measure user per- formance. This time we focused only on map navigation since maps are at the core of several widely used mobile applications and services (e.g., navigation systems, mobile guides, Geographic Information Systems) and were found to be the information space that derived the most benefit from O?D in our first study.
3.1 Hypotheses
In general, we expected that both highlighting objects of interest in the overview and supporting navigation through direct manipulation of the overview would have a positive effect on user performance. More specifically, our hypotheses were:
– Users should be faster in searching for targets when objects of interest are highlighted in the overview. Highlighting, together with the additional orientation cues provided by viewfinder size and position, should enable users to directly navigate toward possible targets, thus reducing search time by avoiding a blind search in the considered information space.
– Users should be faster in carrying out search tasks when they can manipulate the viewfinder in the overview to pan the detail view. Moving the viewfinder toward the desired destination should allow users to be faster with respect to the traditional panning technique based on dragging the portion of information space displayed in the detail view.
– Users should be more accurate in remembering target location when objects of interest are highlighted in the overview. With visible objects of interest, users can see the global configuration of possible targets in the overview, which should simplify construction of an accurate mental map of the information space.
Our hypotheses relied on the (optimistic) expectation that the advantages provided by direct manipulation of the overview and highlighting objects of interest would exceed the negative impact of mobile device limitations, in par- ticular the small size of the overview, on user performance.
3.2 Interfaces
The need to control two binary variables led us to the design of four interface conditions, based on the traditional O?D visualization we employed in our 2008 study. In all conditions, the overview was displayed as a small 80 9 80 pixels thumbnail, covering about 10% of the 240 9 268 pixels detail view, in line with the suggestion of [22] for overview sizes. The only difference among the four inter- faces concerned the manipulability (or lack of manipula- bility) of the viewfinder, and the highlighting (or lack of highlighting) of possible objects of interest in the overview. Figure 2 shows the O?D visualization without (Fig. 2a) and with (Fig. 2b) highlighting of objects of interest in the overview. In all four conditions, users could pan by drag- ging the portion of information space displayed in the detail view and zoom by tapping on the two icons with a plus (zoom in) and a minus (zoom out) in the upper area of the screen. During the evaluation, the zoom factor ranged from a minimum of 3 to a maximum of 10, thus fully complying with the guidelines suggested in [28] and [30]
 Fig. 2 O?D visualization without a and with
b highlighted objects of interest in the overview
1 3
Pers Ubiquit Comput (2013) 17:371–385
381
 for two-views O?D layouts, and the viewfinder reached a minimum size of about 24 9 27 pixels. In the two condi- tions with manipulable viewfinder, users could also pan by dragging the viewfinder within the overview in the desired direction, and the detail view updated accordingly in real time.
3.3 Participants
Twenty-eight subjects (11 female, 17 male) participated in the study. They were all recruited by direct contact among undergraduate or graduate students from the Computer Science and Engineering courses at our university. Their age ranged from 21 to 28, averaging at 25, and they were all mobile phone users. Only two of the subjects had often used map-based applications on their devices, 13 had used them occasionally, and the remaining 13 had never used map-based applications on mobile phones or PDAs.
3.4 Materials
The study was carried out on an Asus P535 Windows Mobile phone featuring a 520 MHz processor and a 2.8- inch touchscreen with 240 9 320 resolution. As in our 2008 study, the detail view covered a 240 9 268 area in the middle of the screen, and the rest of the screen displayed two standard Windows Mobile menu bars at the top and bottom. We used 4 city maps for the experimental tasks and 1 for training. The cities we chose turned out to be unfamiliar to users. All city maps included 10 possible targets depicted as numbered color icons. Targets were manually placed in random positions on maps. Four zoom levels were available to users, thus requiring three taps on the zoom-in icon to move from the lowest to the highest zoom level. Zoom icons were semi-transparent to minimize occlusion on the detail view. All maps were initially dis- played at the coarsest level of detail so that they were almost entirely displayed in the detail view at the start of tasks. However, fine details such as street names and icon numbers were visible only at the highest zoom level, at which the resolution of each map was 800 9 800 pixels.
3.5 Tasks
Each participant carried out one MapNavigation task and one SpatialMemory task for each interface (8 tasks in total).
In the MapNavigation task, users had to navigate a city map to find the location of two specific hotels and tap on their icons on the detail view. Users were informed that all hotels were depicted as numbered color icons. When highlighting of objects of interest was active, hotels were displayed in the overview as small color dots (see Fig. 2b).
An example of the task was: ‘‘Find out hotels 2 and 5 on the map and tap on their icons as soon as you locate them.’’ The two hotels to search for were always located in dif- ferent areas of the map to prevent users to find both in a single screen (at the maximum zoom factor).
The SpatialMemory task required users to mark the location of the targets they had searched for in the Map- Navigation task on a paper sheet that reproduced the con- sidered information space at the coarsest level of detail. To carry out this task, users could not use the mobile device and had to rely only on the spatial knowledge they had previously acquired during the MapNavigation task.
3.6 Experimental design and procedure
The experimental design was within-subjects. Participants were initially briefed about the nature of the study and were provided with an introduction and demonstration of the interfaces. Before carrying out the experimental tasks, users were presented with training tasks to let them familiarize with the interfaces and clarify possible doubts concerning interfaces or tasks. After training, users carried out the 4 pairs of experimental tasks (8 tasks total), each pair including one MapNavigation task and the corre- sponding SpatialMemory task. Participants had access to a printed sheet that provided clear instructions for each task. To start the MapNavigation task, users were required to tap on a ‘‘Start Task’’ button that was initially displayed on the screen. Each MapNavigation task ended when users tapped on the last target. The SpatialMemory task did not require users to interact with the mobile device and ended when users marked the last target on the paper reproduction of the considered map. After completing all tasks, users were asked to order the four interfaces from the best to the worst according to their preference (draws were allowed) and were briefly interviewed to collect their comments.
The order of presentation of experimental conditions as well as their association with maps and target configurations were counterbalanced using a Latin-square design to mini- mize order effects. Four maps and four target configurations were used during the study. Configurations were kept as similar as possible in terms of relative distance of targets.
We automatically recorded the following data for each task:
– The time users spent to complete a MapNavigation task, from the instant they tapped on the ‘‘Start Task’’ button to the instant they tapped on the last target.
– The number of distinct pan, zoom, and target selection actions during each MapNavigation task. A pan action was counted each time users dragged the stylus on the information space, a zoom action each time users tapped on zoom buttons, and a target selection action each time users tapped on any object of interest on the detail view.
1 3
382
Pers Ubiquit Comput (2013) 17:371–385
 –
The duration of each pan action, from the instant users began dragging the stylus on the map to the instant they lifted the stylus from the screen.
3.7.2 User interface actions
Figures 4 and 5 show means of the number of zoom and pan actions performed by users. The Shapiro–Wilk test of normality we performed prior to further analysis revealed a right skew in the data distribution, and none of the trans- formations (roots, logarithm, inverse) which are typically used to deal with this kind of deviation could normalize the data. We thus employed the non-parametric ANOVA-Type Statistic (ATS) [7] to analyze main and interaction effects. For zoom actions, a significant main effect of manipula- bility was detected (ATS = 18.56, p \ 0.0001): users made more zoom actions with the non-manipulable over- view than they did with the manipulable overview. A significant main effect of highlighting was also detected (ATS = 6.49, p = 0.01): users made more zoom actions when no object of interest was highlighted in the overview. There was also a significant interaction effect (ATS = 6.58, p = 0.01): for the non-manipulable overview, users made more zoom actions when objects of interest were not highlighted than when objects of interest were highlighted in the overview, but no such pattern was found for the manipulable overview. For pan actions, the ATS revealed
We also manually computed the distance between actual target location and the location indicated by the user in the SpatialMemory task.
3.7 Results
3.7.1 Task completion times
Figure 3 shows mean completion times for the MapNavi- gation task, for all four possible combinations of the two within-subjects factors (manipulability of the overview, and highlighting of objects of interest in the overview). Both factors have two levels: manipulable overview (abbreviated as MAN in figures) and non-manipulable overview (abbreviated as NMAN in figures) for manipu- lability; highlighting enabled (abbreviated as HIGH in figures) and highlighting disabled (abbreviated as NHIGH in figures) for highlighting. Task completion times were subjected to the Shapiro–Wilk test of normality prior to further analysis. The test revealed moderate deviations from the normal distribution and data were normalized using a log transformation. A two-way repeated measures analysis of variance (ANOVA) was then employed on the log-transformed times. The ANOVA did not reveal a significant interaction between manipulability and high- lighting (F (1, 27) = 0.94, p = 0.340). A significant main effect of manipulability was detected, (F(1, 27) = 49.96, p \ 0.001): users took less time to complete the task with the manipulable overview than they did with the non- manipulable overview. A significant main effect of high- lighting was also detected, (F(1, 27) = 8.39, p \ 0.01): users took less time to complete the task when objects of interest were highlighted in the overview.
 Fig. 4 Mean number of zoom actions
 Fig. 3 Mean completion times for the search task. MAN manipulable overview, NMAN non-manipulable overview, HIGH highlighting enabled, NHIGH highlighting disabled
 Fig. 5 Mean number of pan actions
1 3
Pers Ubiquit Comput (2013) 17:371–385
383
  Fig. 7 Error in the SpatialMemory task
no significant interaction (ATS = 0.28, p = 0.6) and no significant main effect of highlighting (ATS = 1.61, p = 0.2). However, a significant main effect of manipula- bility was detected (ATS=136.96, p\0.0001): users made more actions with the non-manipulable overview than they did with the manipulable overview.
3.7.3 Pan time
As with task completion times, we used the Shapiro–Wilk test of normality prior to further analysis of pan times, whose means are shown in Fig. 6. The test revealed a moderate deviation from the normal distribution, which was corrected using a log transformation. ANOVA was then used to analyze the data. No significant main effect was found for highlighting (F(1, 27) = 3.21, p [ 0.05), while a significant main effect was found for manipula- bility (F (1, 27) = 121.60, p \ 0.001), with users taking longer pan actions with the manipulable overview than with the non-manipulable overview. The ANOVA also revealed a significant interaction effect (F(1, 27) = 11.79, p = 0.02): for the manipulable overview, users made longer pan actions when no object of interest was high- lighted in the overview than they did when objects of interest were highlighted in the overview. No such pattern was found for the non-manipulable overview.
3.7.4 Error
Two-way ANOVA was used to analyze error in the Spatial Memory task, where the amount of error for each user was measured as the average of the distance (in pixels) between the location indicated by users and the correct location for the two considered targets (results are shown in Fig. 7). The ANOVA did not reveal a significant interaction effect (F(1, 27) = 0.094, p = 0.76), nor any significant main effect for manip- ulability (F(1, 27) = 0.001, p = 0.98) and for highlighting (F (1, 27) = 2.08, p = 0.16).
3.7.5 Subjective preference
To analyze the data on subjective preference (Fig. 8), we employed the non-parametric ATS statistic. Since users were asked to rate the four interfaces from the best to the worst, we assigned a score of 4, 3, 2, and 1, respectively, to the first, second, third, and fourth interface. An appropriate fractionary score was assigned to draws, which were allowed. The analysis did not reveal a significant interac- tion effect (ATS = 1, p = 0.32) but pointed out a signifi- cant main effect for manipulability (ATS = 216.16, p\0.0001) with users preferring the manipulable over- view to the non-manipulable overview, as well as for highlighting (ATS = 348.79, p \ 0.0001) with users pre- ferring highlighting to no highlighting in the overview.
3.8 Discussion
As we had hypothesized, the analysis of task completion times revealed that users benefit from the availability of manipulability of the overview and highlighting of objects of interest in the overview. The role of overviews as tools that users can manipulate to perform navigation actions
 Fig. 8 Mean preference for each interface (higher numbers corre- spond to better scores)
 Fig. 6 Mean pan times
1 3
384
Pers Ubiquit Comput (2013) 17:371–385
 was taken for granted in almost all previous studies on the O?D approach, which always implemented some form of overview-supported panning or zooming mechanism. However, the difficulties users had in directly interacting with the overview in some of these studies [5, 18, 22] and the additional constraints on overview size introduced by the mobile context raised doubts about the actual effec- tiveness of this feature. As we found out, if the small size of the overview had a negative impact, it was not suffi- cient to counter the positive effects of direct manipula- bility on user performance. Providing an additional layer of semantic information to the overview through high- lighting of objects of interest proved useful as well. A similar feature was introduced in the study of Nekrasovski et al. in the desktop domain [24]. In that case, however, it was not found to affect user performance, likely because orientation cues were also provided in the detail view through other means. As we previously remarked, over- views become redundant in terms of orientation support when interfaces simultaneously integrate other sources for the same information.
The performance gains users obtained because of the availability of one of the two features were not affected by the presence or absence of the other feature. However, the difference in performance increase associated to the two variables is interesting: highlighting improved performance by about 15–20% while manipulability resulted in a stronger 40% improvement. Moreover, while it is not always possible to highlight objects of interest in the overview, for example because the location of such objects is not known in advance, introduction of a manipulable overview to support panning can always be a very effective solution to considerably reduce search time. There are multiple reasons that can explain why users were so much faster in carrying out tasks when the overview was manipulable. One is that users needed less effort to pan a certain distance by moving the viewfinder compared to operating directly on the detail view. For example, moving the viewfinder by 10 pixels at the maximum zoom factor (10) corresponded to moving the detail view by 100 pixels (10 9 10). A comparable pan action on the detail view required instead users to drag the pen on the screen for 100 pixels, which is the typical behavior of traditional panning mechanisms. However, one must also consider that it might be more difficult for users to properly control the large jumps of the detail view when moving the viewfinder, as pointed out by Gutwin et al. [18] in their study. The analysis of user interface actions revealed another possible motivation for user performance in the study. Users made significantly less pan and zoom actions when the manipu- lable overview was available. This probably decreased the total motor effort required to complete the tasks, which led to a lower task completion time. The number of pan actions
is likely related to the above mentioned difference between panning in the detail view and panning by moving the viewfinder but is also affected by the overall strategy users employed when searching for targets in the different con- ditions. The availability of a manipulable overview allowed users to perform a sort of continuous navigation, characterized by long pan actions, while users employed sequences of short pan actions when they had to navigate maps by dragging the detail view, regardless of object highlighting. However, the navigation strategy with a manipulable overview seemed to depend on the availability of highlighting: with highlighting enabled, users made shorter pan actions than they did when objects of interest were not highlighted in the overview, probably because the highlighting allowed users to directly home on targets without requiring to blindly explore the whole information space. For the same reason, highlighting also helped users in carrying out search tasks with less overall actions.
The SpatialMemory task did not reveal significant effects of the factors we considered on user error. Contrary to our hypothesis, there were no differences in spatial memory performance whether objects of interest were highlighted or not in the overview. This might be due to the small size of the overview, which could have made it more difficult for users to easily discriminate the relative position of targets and support their memorization. However, there is a definite possibility that it is not the visualization of targets but the position and size of the viewfinder that play a major role in helping users construct a mental map of the configuration of targets. Indeed, the relative size of the error was about 10–12% of the size of the map, meaning that users were fairly accurate in their position estimation. This hypothesis might also explain the similar results we obtained in our previous study [10] when comparing the traditional and the wireframe O?D interfaces.
Finally, subjective preference was consistent with per- formance results, revealing that users perceived both manipulability and highlighting as useful and effective features in mobile O?D interfaces.
4 Conclusions
This paper investigated Overview?Detail visualization, one of the major approaches to the display of large infor- mation spaces on a computer screen, focusing on its applicability to mobile devices. While O?D visualization is now common in many desktop interfaces, its adoption on mobile devices is rare, even in those commercial applica- tions, such as Google Maps Mobile, whose desktop coun- terpart include an overview. Our examination of the few research studies on mobile O?D provided evidence of its possible beneficial effects, especially for those information
1 3
Pers Ubiquit Comput (2013) 17:371–385 385
 spaces (e.g., maps) that do not provide additional orienta- tion cues in the detail view, but also pointed out the neg- ative effects of the limited space of mobile screens which could make O?D ineffective. The experiment we pre- sented in the paper explored the role played by two specific features of O?D interfaces, manipulability of the overview and highlighting of objects of interest in the overview, and revealed that both features are beneficial to users in search tasks, with manipulability providing the highest perfor- mance improvement. However, knowledge of the strengths and weaknesses of the O?D approach on mobile devices is still limited. Further empirical analyses are needed, for example, to obtain general guidelines on the impact of different overview designs on different kinds of task or to understand the relative effectiveness of O?D visualization compared to the other approaches to the presentation problem on mobile devices. Important questions for devi- ces with limited screen space, e.g., the effect of overview size on user performance, need also answers.
Acknowledgments The authors acknowledge the financial support of the Italian Ministry of Education, University and Research (MIUR) within the FIRB project number RBIN04M8S8.
References
1. Ahlberg C, Shneiderman B (1994) The alphaslider: a compact and rapid selector. In: Proceedings of CHI 1994, ACM Press, pp 365–371
2. Baudisch P, Rosenholtz R (2003) Halo: a technique for visual- izing off-screen locations. In: Proceedings of CHI 2003, ACM Press, pp 481–488
3. Baudisch P, Good N, Bellotti V, Schraedley P (2002) Keeping things in context: a comparative evaluation of focus plus context screens, overviews, and zooming. In: Proceedings of CHI 2002, ACM Press, pp 259–266
4. Baudisch P, Lee B, Hanna L (2004) Fishnet, a fisheye web browser with search term popouts: a comparative evaluation with overview and linear view. In: Proceedings of AVI 2004, ACM Press, pp 133–140
5. Beard D, Walker J (1990) Navigational techniques to improve the display of large two-dimensional spaces. Behav Inform Technol 9(6):451–466
6. Bederson B, Hollan J (1994) Pad??: a zooming graphical interface for exploring alternate interface physics. In: Proceed- ings of UIST 1994, ACM Press, pp 17–26
7. Brunner E, Munzel U (1999) Rank-score tests in factorial designs with repeated measures. J Multivariate Anal 70:286–317
8. Burigat S, Chittaro L (2011) Visualizing references to off-screen content on mobile devices: a comparison of arrows, wedge, and overview?detail. Interact Comput 23(2):156–166
9. Burigat S, Chittaro L, Gabrielli S (2006) Visualizing locations of off-screen objects on mobile devices: a comparative evaluation of three approaches. In: Proceedings of MobileHCI 2006, ACM Press, pp 239–246
10. Burigat S, Chittaro L, Parlato E (2008) Map, diagram, and web page navigation on mobile devices: the effectiveness of zoomable user interfaces with overviews. In: Proceedings of MobileHCI 2008, ACM Press, pp 147–156
11. Bu ̈ring T, Gerken J, Reiterer H (2006) Usability of overview- supported zooming on small screens with regard to individual differences in spatial ability. In: Proceedings of AVI 2006, ACM Press, pp 233–240
12. Card S, Mackinlay J, Shneiderman B (1999) Readings in infor- mation visualization. Morgan Kaufmann, Massachusetts
13. Chen C (2006) Information visualization: beyond the horizon. Springer, Berlin
14. Chittaro L (2006) Visualizing information on mobile devices. IEEE Comput 39(3):40–45
15. Cockburn A, Karlson A, Bederson B (2008) A review of over- view?detail, zooming, and focus?context interfaces. ACM Comput Surv 41(1):1–31
16. Ghosh P, Shneiderman B (1999) Zoom-only versus overview- detail pair: a study in browsing techniques as applied to patient histories. In: Technical report HCIL-99-12, Human-Computer Interaction Lab, University of Maryland, College Park
17. Gustafson S, Baudisch P, Gutwin C, Irani P (2008) Wedge: clutter-free visualization of off-screen locations. In: Proceedings of CHI 2008, ACM Press, pp 787–796
18. Gutwin C, Skopik A (2003) Fisheyes are good for large steering tasks. In: Proceedings of CHI 2003, ACM Press, pp 201–208
19. Hornbaek K, Frokjaer E (2001) Reading of electronic documents: The usability of linear, fisheye, and overview?detail interfaces. In: Proceedings of CHI 2001, ACM Press, pp 293–300
20. Hornbaek K, Frokjaer E (2003) Reading patterns and usability in visualizations of electronic documents. ACMT Comput Hum Int 10(2):119–149
21. Hornbaek K, Hertzum M (2011) The notion of overview in infor- mation visualization. Int J Hum Comput Stud 69(7-8):509–525 22. Hornbaek K, Bederson B, Plaisant C (2002) Navigation patterns
and usability of zoomable user interfaces with and without an
overview. ACMT Comput Hum Int 9(4):362–389
23. Leung YK, Apperley MD (1994) A review and taxonomy of distortion-oriented presentation techniques. ACMT Comput Hum
Int 1(2):126–160
24. Nekrasovski D, Bodnar A, McGrenere J, Guimbretiere F,
Munzner T (2006) An evaluation of pan&zoom and rubber sheet navigation with and without an overview. In: Proceedings of CHI 2006, ACM Press, pp 11–20
25. North C, Shneiderman B (2000) Snap-together visualization: can users construct and operate coordinated visualizations? Int J Hum-Comput Stud 53(5):715–739
26. Perlin K, Fox D (1993) Pad: an alternative approach to the computer interface. In: Proceedings of SIGGRAPH 1993, ACM Press, pp 57–64
27. Pietriga E, Appert C, Beaudouin-Lafon M (2007) Pointing and beyond: an operationalization and preliminary evaluation of multi-scale searching. In: Proceedings of CHI 2007, ACM Press, pp 1215–1224
28. Plaisant C, Carr D, Shneiderman B (1995) Image-browser tax- onomy and guidelines for designers. IEEE Softw 12(2):21–32
29. Roto V, Popescu A, Koivisto A, Vartiainen E (2006) Minimap: a web page visualization method for mobile phones. In: Proceed- ings of CHI 2006, ACM Press, pp 35–44
30. Shneiderman B (1998) Designing the user interface. Addison- Wesley, Boston
 Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI-16)
Predicting Confusion in Information Visualization
from Eye Tracking and Interaction Data
Sébastien Lallé, Cristina Conati, Giuseppe Carenini
The University of British Columbia, Vancouver B.C., Canada {lalles, conati, carenini}@cs.ubc.ca
Abstract
Confusion has been found to hinder user experience with visualizations. If confusion could be predicted and resolved in real time, user experience and satisfaction would greatly improve. In this paper, we focus on predicting occurrences of confusion during the interaction with a visualization using eye tracking and mouse data. The data was collected during a user study with ValueChart, an interactive visualization to support preferential choices. We report very promising results based on Random Forest classifiers.
1 Introduction
Confusion has been found to hinder user experience and satisfaction with user interfaces [e.g., Nadkarni and Gupta 2007] and Information Visualization (InfoVis) [e.g., Lee et al. 2016; Yi 2008]. To date, most work on developing methods to detect and prevent or resolve confusion in real- time during interaction has been in the field of Intelligent Tutoring Systems (ITS), [Bosch et al. 2015; Baker et al. 2012; D’Mello and Graesser 2007]. Building on this work, we investigate how to predict confusion during the interaction with a visualization-based interface, with the long-term goal of devising intelligent user-adaptive visualizations that can provide personalized interventions to help confused users. Such user-adaptive visualizations would be especially beneficial as complex visualizations are becoming increasingly used by broad audiences, not only in professional settings, but also for personal usage (e.g., for monitoring health and fitness, interactions in social media, and home resources consumption) [Huang et al. 2015].
In this paper, we investigate machine learning models to predict occurrences of confusion during the interaction with ValueChart, an interactive visualization to support multi- criteria preferential choice. To make such predictions in real-time, we leverage both interaction data as well as eye tracking capturing users’ gaze patterns, pupil size and head distance to the screen. Interaction data have been used before to predict confusion in computer games [Pentel 2015] and ITS [Baker et al. 2012]. However, we are the first to study how eye tracking can be used to predict confusion, with the rationale that eye tracking should be particularly
informative in predicting confusion during visualization processing, as visual scanning is a fundamental component of working with a visualization. Furthermore, eye tracking has been shown to be a good predictor of affective states in educational systems [Jaques et al. 2014; Muldner et al. 2010] as well as user characteristics in InfoVis, [e.g., Steichen et al. 2014; Jang et al. 2014].
This work has two main contributions. The first is a proof of concept that confusion can be predicted in real-time in InfoVis, with 61% accuracy (significantly better than chance) for occurrences of confusion and a false positive rate of only 7.4%. The second contribution is evidence of the importance of eye tracking for building predictors of confusion in InfoVis, with the most informative sources of information being differences in the user’s attention to the labels of the InfoVis, as well as variations in user pupil size and head distance to the screen.
2 Related Work
Previous work suggests that confusion can negatively impact user experience or satisfaction with an interface, e.g., [Rickenberg and Reeves 2000; Nadkarni and Gupta 2007]. In InfoVis, confusion has been linked to lower user performance and satisfaction in completing decision making tasks [Yi 2008], and was shown to affect novice users when they process an unfamiliar visualization [Lee et al. 2016].
In the field of Intelligent Tutoring Systems, predictors of confusion have been built by leveraging facial expressions [Bosch et al. 2015; D’Mello and Graesser 2007], posture [D’Mello and Graesser 2007], or students’ interface actions and studying behavior [Baker et al. 2012]. In HCI, Pentel [Pentel 2015] leveraged mouse usage information to predict occurrences of confusion in a simple computer game. We extend this work by showing the feasibility of predicting confusion in real time in InfoVis, and we extend previous work on confusion prediction in general by using a new sensor, eye tracking.
Eye tracking has been shown to be a good predictor of other emotional or attentional states such as mind wandering while reading [Bixler and D’Mello 2015], as well as boredom, curiosity and excitement, while learning with educational software [Jaques et al. 2014; Muldner et al. 2010]. In InfoVis, gaze and pupil data have been
2529

 investigated to predict long-term user traits (e.g., perceptual speed, visual and verbal working memory), as well as short- term properties such as task completion time, learning curve, or intention for visual search [Steichen et al. 2014; Lallé et al. 2015; Jang et al. 2014]. Still in InfoVis, [Yelizarov and Gamayunov 2014] tracked mouse and keyboard events to predict users’ level of cognitive load and adjust the amount of information to be displayed accordingly. Their results showed that their adaptations positively impacted users’ performance.
3 User Study
ValueCharts. Complex decisions can often be framed as preferential choices, i.e., the process of selecting the best option out of a set of alternatives characterized by a variety of attributes (e.g., select a car to buy, a university to attend, etc.). The dataset1 used in this paper was collected from a user study using ValueChart, an interactive visualization for preferential choice [Conati et al. 2014]. Figure 1 shows an example of ValueChart for selecting rental properties among ten available alternatives (listed in the leftmost column), based on a set of relevant attributes (e.g., location, appliances, etc.). These attributes are arranged hierarchically in the top part of the ValueChart, with a column for each attribute in the central part of the display. The width of each column indicates the relative weight assigned to the corresponding attribute. The available alternatives (i.e., rental homes) are represented as the rows in the display. Each cell specifies how the alternative in that row fares with respect to the attribute in that column, indicated by the amount of filled cell color. In the rightmost part of the ValueChart, all values for each alternative are accumulated and presented as stacked bars, displaying the overall value of each alternative (e.g., home4 is the best home in the example in terms of overall value).
The interactive functionalities available to support the decision process include: (i) inspecting the specific domain value of each attribute (e.g., the rent of home1 being equal to $500), by left clicking on the related alternative; (ii) sorting the alternatives with respect to a specific attribute (by double-clicking on the attribute name); (iii) swapping attribute columns (initiated via a left click on one of the attributes); and (iv) resizing the width of an attribute's column to see how that would impact the decision outcome (initiated via a left click on the column edge).
ValueChart has been extensively evaluated for usability and adopted in several applications (e.g., [Yi 2008; Wongsuphasawat et al. 2012]). It has, however, inherent complexity due to the nature of the task, which can still generate confusion in some users [Yi 2008; Conati 2013].
Procedure. 136 participants (age range 16 to 40, 75 female) were recruited from various departments at our university to perform 5 different types of tasks with ValueChart (e.g., retrieve the cheapest home or select the best home based on the aggregation of price and size). After 10 min of training
1 Data available at http://www.cs.ubc.ca/~lalles/IJCAI16.html
with ValueChart, each participant repeated each task type 8 times in a randomized fashion to account for within-user variability, for a total of 40 tasks. This results in a total of 5440 trials (136 users × 40 trials).
While performing the tasks, the user’s gaze was tracked with a Tobii T120, a non-intrusive eye-tracker embedded in the study computer monitor. In order to avoid possible confounds on pupil size due to lighting changes, the study was administered in a windowless room with uniform lighting. To compensate for physiological differences in pupil size among users, pupil diameter baselines were collected for each user by having them stare at a blank screen for ten seconds at the beginning of the study.
Figure 1: An example of the main elements of ValueChart.
Collecting data on user confusion. Collecting ground truth labels is one of the main challenges for building user models that can predict transient user states in an adaptive interface (e.g., [Porayska-Pomsta et al. 2013]), and a variety of meth-
 ods have been proposed in the literature to address this chal- lenge (see [Conati et al. 2013] for an overview).
After careful consideration of various options, we chose to have users self-report their confusion by clicking on a button labeled “I am confused” (see Fig. 1, top right). Users were instructed to use the button as follows (rephrased for brevity): “[you should click the confusion button] if you feel that you want to ask the experimenter a question about something; if you are confused about the interface; if you are confused about the wording of a question. ...These are just a few examples, to show that confusion can occur in many unforeseeable ways, [which are all] OK reasons to click the confusion button.” Participants were told that clicking on the button would have no effect on the
 interaction, it was just included for data collection. At the end of the study, each participant was shown replays of interaction segments centered around their reports of confusion, to verify that the report was intended and elicit the reason of the confusion. This collection method was evaluated via pilot studies before being deployed in the main experiment [Conati et al. 2013].
Confusion was reported in 112 trials (2% of all trials), with 80 users (59%) reporting confusion at least once during the study, with an average of 1.4 clicks (SD=1.9). There was never more than one click per trial. Reasons for confusion reported by participants include not understanding the tasks, perceived ambiguity in the textual or visual components of
  2530
the interface, interactive functionalities not working as expected, alleged missing functionalities to solve the task.
 To investigate the impact of confusion on users’ performance, we ran an MANOVA with task accuracy and task completion time as the dependant variables, and the occurrence of confusion (2 levels: YES or NO) and types of task (5 levels) as factors. The MANOV A revealed a significant main effect of confusion on both accuracy (F1,2159=108, p < .001, η2p=.02) and completion time (F1,2159=125, p < .001, η2p=.02), with confusion trials having a lower accuracy and being longer than no-confusion trials. These results confirm the negative impact of confusion on performance reported in [Yi 2008] for a visualization based on a variation of ValueChart. It is notable that the impact exists even with the relatively low number of confusion reports in our dataset.
4 Predicting Confusion in ValueChart
We label trials as “confusion” when the user pressed the confusion button at least once during the trial, or “no- confusion” otherwise. Thus, predicting confusion in our dataset is a binary classification task: classify each trial as one in which the user might be experiencing confusion, or not. We compare a variety of features sets for this classification task (Section 4.2). In Section 4.1, we describe the datasets we generated to compute these feature sets.
4.1 Data Windows
To simulate the real-time prediction of confusion episodes, we use only users’ data prior to the click on the confusion button. As there is no such click in no-confusion trials, we randomly generate a “pivot point” in each of those trials. In order to ascertain how much data leading up to an episode of confusion is needed to predict it, we built our feature sets (described below) by using two different windows of data: a short window captures data 5 seconds immediately before a confusion click2 (or pivot point); a full window captures the whole episode of confusion by including data from a click (or pivot point) back to the beginning of the trial3. Full windows were on average 13.7s in length (SD=11.3s).
5.2 Predictive features
The eye tracking data collected during the study provide information on user gaze patterns (Gaze, from now), on changes in a user’s pupil width (Pupil), and on the distance of the user’s head from the screen (Head Distance, defined as the averaged distance of each eye to the screen), which are all good candidates to explore as predictors of confusion. Confusion is likely to impact how a user attends to elements of the visualization (Gaze), which has already been successfully used to predict user performance and
2 Windows actually end 1 second before a confusion click to avoid confounds associated with the specific intent to report confusion (e.g., fast straight saccades toward the button).
3 Note that this window cannot include another confusion click because there is only one per trial in our dataset. Otherwise, full windows would go back only to the last episode of confusion.
other abilities with visualizations [Steichen et al. 2014; Lallé et al. 2015; Nazemi et al. 2014]. Pupil size has been associated to cognitive load [e.g., Granholm and Steinhauer 2004], which might be affected when the user experiences confusion. Pupil has also been shown to be a predictor of other mental states, such as mind wandering [e.g., Bixler and D’Mello 2015]. Head distance provides a rough indication of user posture, which has been shown to be a predictor of user engagement with a task [D’Mello and Graesser 2007]. Since ValueChart is interactive, we also leverage data on mouse events, which have been shown to predict user confusion during interaction with a computer game [Pentel 2015]. From all these measures (Gaze, Pupil, Head Distance, and Mouse Events) we derive four groups of features (listed in Table 1) that we evaluate in terms of their ability to predict user confusion with ValueChart.
a) Gaze Features (149)
Overall Gaze Features (9)
Fixation rate
Mean & Std. deviation of fixation durations Mean & Std. deviation of saccade length
Mean & Std. deviation of relative saccade angles Mean & Std. deviation of absolute saccade angles
AOI Gaze Features for each AOI (140)
Fixation rate in AOI
Longest fixation in AOI, Time to first & last fixation in AOI Proportion of time, Proportion of fixations in AOI
Number & Prop. of transitions from this AOI to every AOI
b) Pupil Features (6) and Head Distance Features (6)
Mean, Std. deviation, Max., Min. of pupil width/head distance
Pupil width/head distance at the first and last fixation in the data window
c) Mouse Event Features (Overall and for each AOI) (32)
Left click rate, Double click rate
Time to first left click, Time to first double click
Table 1: Sets of feature considered for classification.
Gaze features (Table 1a) describe user’s gaze patterns in terms of fixations (gaze maintained at one point on the screen), and saccades (quick movement of gaze from one fixation point to another). From this raw gaze data, we generated features that capture overall gaze activity on the screen, as well as activity over specific Areas of Interest (AOI), shown in Figure 2. We selected these features because they have been extensively used in HCI to capture differences in users’ attention patterns over an interface [e.g., Holmqvist et al. 2015].
Pupil and Head Distance features (Table 1b) are generated by averaging the corresponding measures for each eye. Pupil size is adjusted using the pupil baseline collected during the study to get the percentage change in pupil size (PCPS), as defined in [Iqbal et al. 2005]. Features like the mean, min, max and std.dev are standard ways to measure fluctuations in a measure of interest, and have been used with pupil size to reveal individual differences while users work with an interface [Holmqvist et al. 2015] and with head distance to predict user boredom [Jaques et al. 2014]. We also included pupil size & head distance of the first and
                         2531
 last fixation in the data window (see Section 4.1) as a way to capture variations of the measures between the closest and farthest data-points to the confusion click in that window.
Mouse Event features (Table 1c) are built upon summary statistics on left and double mouse clicks (the only two with associated functionalities), measured both over the whole screen and for the 7 specific AOIs defined in Figure 2. Mouse click rate has been shown to be effective at predicting cognitive states in other interactive tasks [Lim et al. 2015]. The time to first click has been used for early prediction of user failure with an interface [Liu et al. 2010], thus it might also be relevant to predict confusion because confused users are likely to make more errors.
From all the features described above, we derive a total of 8 feature sets that we will compare to predict confusion:
  One feature set for each of Gaze (G), Pupil (P), Head
Distance (HD), and Mouse Events (ME) features.
  All features together (ALL).
  Only features derived from eye tracking data (G+P+HD),
as these can be used in non-interactive InfoVis as well.
  Head distance and Pupil (P+HD) as these features are
entirely independent from the layout of the visualization.
  Head Distance+MouseEvent (HD+ME), as the expensive eye tracker is not needed to collect these features (a
webcam can reliably infer the distance to the screen).
We used each of the two data windows described in Section 4.1 to generate each of the 8 features sets above, for a total of 16 combinations.
Figure 2: Areas of Interest (AOI) defined for ValueChart.
4.3 Addressing data imbalance
Our dataset is highly imbalanced, with only 2% of confusion trials. Building meaningful classifiers on imbalanced dataset is challenging: a majority class classifier that always predicts no confusion would have a very high accuracy, but would be useless. Thus, to train our predictors of confusion, we balanced training data by using the well- known SMOTE algorithm for data over-sampling [Chawla et al. 2002]. Specifically, using SMOTE we generated “synthetic” confusion trials based on k nearest neighbors in the minority class (we used the default value k=5). To do so, SMOTE randomly duplicates a confusion trial (ct) and modifies it by sampling for each feature a new value along the line between ct and its k nearest neighbors. Next, no- confusion trials are randomly discarded (down-sampled) until the dataset is balanced. In our study, we over-sampled confusion trials by 200% (i.e., number of confusion trials is
doubled) and 500%. These percentages appeared to be among the best ones when applied to similar imbalanced datasets in [Chawla et al. 2002], where they also show that more over-sampling is pointless.
4.4 Machine learning set up
To build our classifiers, we use Random Forest tuned with 100 trees using the Caret package in R [Kuhn 2008]. We chose Random Forest because previous work has shown that this learning algorithm performed well on similar prediction tasks [e.g., Pentel 2015; Wu 2015]. The classifiers are trained for all combinations of features sets (8), data window lengths (2) and SMOTE configurations (2) for a total of 32 classifiers. These classifiers are trained and evaluated with a process of 20-runs-10-folds nested cross- validation, which includes two levels (inner and outer) of cross-validation.
At the outer level the following process is repeated 20 times (runs) to strengthen the stability and reproducibility of the results. Data is randomly partitioned in 10 folds; in turn, each of the 10 folds is selected as a test set; the remaining 9 folds are SMOTE-balanced and used to train a classifier which is then tested on the test fold. The performance of each classifier is averaged across the outer test sets, and then again over the 20 runs. It should be noted that test sets at the outer level are not altered by SMOTE in any way.
We performed cross-validation over users, meaning that in each cross-validation fold, all trials of a given users are either in the training or in the test set. We kept in the folds a distribution of confusion data points similar to that in the whole dataset. As the outer test folds are strongly imbalanced, model performance is measured via sensitivity and specificity, two suitable measures when data are skewed [Kotsiantis et al. 2006], defined as follow:
  Sensitivity (or true positive rate): proportion of confusion
trials that are correctly identified as such. It indicates the
ability of the model to predict occurrences of confusion.
  Specificity (or true negative rate): proportion of no- confusion trials that are correctly identified as such. It indicates the ability of the model to avoid false positives
(as 1 − Specificity is the proportion of false positives).
At the inner level of the nested cross-validation, Correlation Feature Selection [Kuhn 2008] is applied to remove highly correlated features. Next, the best decision threshold is selected for each Random forest classifier using ROC curves plotted on inner train data only. The threshold selected is the one that gives the best trade-off between sensitivity and (1 – specificity) by taking the closest point of the ROC curve to the point (0, 1) representing perfect classification [Fawcett 2006].
5 Results
We analyze the performance of the 32 classifiers defined in Section 4.4 by running a 3-way MANOVA with:
  sensitivity and specificity of the classifiers as the two
dependent variables,
 2532
  feature set (8 levels), window length (2 levels) and SMOTE configuration (2 levels) as the factors.
The MANOVA reveals a main effect of both feature set (F8,67=18.65, p < .001, η2p=.59) and SMOTE configuration (F1,26=9.58, p = .005, η2p=.17). No main effect of data window length, nor any interaction effects were found. To investigate further the two main effects, we run univariate pairwise comparisons using the Holm-Bonferroni adjustment for family-wise error4.
SENSI ALL>G+P+HD>P+HD>P>HD+MV>G>ME>HD
SPECIF ALL>G+P+HD>P+HD>P>HD+ME>G>ME>HD
Table 2: Effect of feature set on model performance, with G=Gaze, P=Pupil, HD=Head Distance, ME=Mouse Event.
Effects of Feature set. Table 2 summarizes the results of the pairwise comparisons by ordering feature sets according to their mean sensitivity and specificity over data windows and SMOTE configurations. Bold underlining indicates models for which there are no statistically significant differences. For example, for sensitivity, these is no difference between G+P+HD and HD+P, they are both better than P but worse than ALL.
Results in Table 2 show that combining various data sources works usually better than using a single data source, for both sensitivity and specificity. In particular, the best feature set for both performance measures includes all the features (ALL). This indicates that eye tracking features work well together to predict confusion, and that adding mouse events can lead to significantly better accuracy. It is interesting to note that G+P+HD (the second best model) is not significantly better than P+HD, meaning that adding Gaze data to Pupil and Head Distance features sets does not lead to a significant improvement.
Effects of SMOTE configuration. The pairwise comparisons show that:
  Sensitivity is better with SMOTE-200% than with
SMOTE-500%, (t(198)=3.19, p = .002, η2p=.15).
  Specificity is better with SMOTE-500% than with
SMOTE-200%, (t(198)=2.41, p = .017, η2p=.04).
These results show that there is a trade-off between SMOTE-200% and SMOTE-500% in terms of sensitivity and specificity. In particular, generating more synthetic data with SMOTE has a substantial negative effect on the sensitivity (see medium effect size η2p=.15). This is likely due to the fact that synthetic data start to dilute the information captured in the real confusion data points. This effect is2inverted for specificity, although the effect size is small (η p=.04), indicating limited implications in practice.
5.1 Model performance
In this section, we analyze in more detail the performance (in terms of sensitivity and specificity) of the classifiers using the ALL and P+HD feature sets and trained over “Full
4 Actual values for average sensitivity and specificity will be discussed later for specific models.
Window”5 data. We focus on these two feature sets because ALL showed the best overall performance (Table 2) and P+HD is the second best model along with G+P+HD, but it has the advantage to be fully independent from the layout of the visualization. For each of these feature sets and Full window, we report results for both SMOTE-200% and SMOTE-500% (Table 3), since Section 5.1 showed that there is a trade-off between these two configurations.
Overall, results in Table 3 indicate that SMOTE models have very similar specificities, with variations of only about .02 for both ALL and P+HD, whereas SMOTE-200 achieved by far the highest sensitivity6, reaching .61 for ALL. Thus SMOTE-200 used with all features together appears to be the most promising model to predict user confusion during interaction with ValueCharts. However, layout independent information captured only by pupil and head distance features appears to successfully predict 57% of confusion trials on unseen data, a very encouraging result in terms of building visualization-independent classifiers.
SMOTE 200%   SMOTE 500%   Baseline
SENSI         0 0
                       ALL
   .61
    .54
     P+HD
.57
  .55
     ALL
 .926
     .942
    P+HD
    .905
      .921
       SPECIF
1 1
   Table 3: Performance of ALL and P+HD with Full window.
5.2 Feature importance
Our results show that multiple data sources together (i.e., ALL Features) can better predict confusion, than any single source. To ascertain which features best predict confusion, we use the method described in [Liaw and Wiener 2002] to measure feature importance in Random forest classifiers. Table 4 shows the top 10 selected features for our best performing classifier (using the ALL feature set and SMOTE -200%), with relative importance normalized between 0 and 100. In the table, a positive (negative) direction of the effect column (D) indicates that the value of the feature is higher (lower) in confusion than in no-confusion trials.
Pupil features. Table 4 shows that overall pupil size features are the most important predictors of confusion, as the top three best features belong to this set. Generally, increase in pupil size is correlated with higher cognitive load [Granholm and Steinhauer 2004]. Consistently with this result, end, max, and mean pupil size are higher in confusion trials, where confused users might be experiencing a higher cognitive load. Also, users in the confusion trails have higher std.dev pupil size, which might be an indication of a higher variability in their cognitive load compared to more stable non-confused users. The prominence of pupil-based features as predictors of confusion is especially promising for the generalization of
5 Windows length had no effect on performance, so either length could be used here.
6 When differences in specificity is higher, measures such as the practical utility could be used to investigate the trade-off between sensitivity and specificity [Gena 2005].
  2533
 our work, since these features are independent from the layout of the visualization, and thus may be used to predict confusion in other InfoVis.
Head distance. There is one head distance feature among the top 10, namely std.dev head distance. Confused users have a higher value for this feature, suggesting that confusion generates more fluctuations in the user’s position. Distance to the screen has been shown to be correlated with user engagement [D’Mello and Graesser 2007], thus more fluctuations in the position of confused users might indicate that these users get closer to the screen to better attend to the unclear information before eventually disengaging. As with pupil size, head distance to the screen does not depend on the visualization layout and thus may be a good predictor of confusion using other InfoVis. Also as noted before, head distance may be inferred with a cheaper webcam.
Gaze features. Five of the 10 most important features in Table 4 are Gaze features related to attention to the AOI that includes the name of the attributes (Labels_attr) in the decision problem (see Fig. 2). These features are time to last fixation in the AOI, longest fixation, proportion of time spent, as well as transitions from and to the AOI that encloses the name of the problem alternatives (Labels alter). Confused users have higher values for all these features, suggesting that they need to process the names of the attributes more extensively. One possible explanation for this trend is that labels are a source of confusion (e.g., due to ambiguous wording). Another is that looking at names of attributes and alternatives is a way to reduce confusion as text in visualization is meant to support data comprehension. It should be noticed that all Gaze features here relate to a particular AOI, thus are layout dependent. This indicates that although many independent features (e.g., pupil and head distance) are important predictors of confusion, additional information about specific components of the visualizations can improve prediction.
criteria preferential choice. To this end, we leveraged a user study that collected ground truth labels for confusion, along with eye tacking and interaction data. Then, we compared various combinations of these data sources to train Random forest classifiers for confusion, and technically had to deal with data imbalance.
Our results show that eye tracking is valuable to predict confusion in real time. Remarkably, we found that 61% of the occurrences of confusion can be predicted, while getting a false positive rate of only 7.4%. More tellingly, when we examine the most important features used by the classifiers, it appears that our models are able to capture aspects of the interaction that are very plausibly related to confusion. Furthermore, some of these features may well generalize to other visualizations. For instance, we found that features of pupil size (which are independent of the layout of the current visualization) are strong predictors of confusion, consistently with the fact that pupil size is correlated to cognitive load, which plausibly correlates with confusion. Another strong visualization-independent predictor was a feature related to head distance. Again this makes sense, because confusion can affect engagement with a task, which has been shown to be predictable by head distance. Additionally, prominent gaze features reveal differences in user’s attention to the labels of the visualization among confused and non-confused users; however, these features may not generalize so easily to other visualizations.
To increase the performance of our models, future work includes improving our features and models selection via ensemble modeling, leveraging additional features such as facial expressions, and using past observed data to optimize prediction for each individual user. Another thread of future work relates to further investigating the generalizability of our findings to other visualizations, as well as researching how confusion can be addressed once predicted.
Acknowledgement
This research was funded by NSERC Grant 227795-10. We thank Enamul Hoque and Dereck Toker for their help with data collection, and Dereck for his help in revising the paper.
References
[Baker et al., 2012] R. Baker, S.M. Gowda, M. Wixon, et al. Towards Sensor-Free Affect Prediction in Cognitive Tutor Algebra. In Proc. of EDM 2012, 126–133, 2012.
[Bixler and D’Mello, 2015] R. Bixler and S. D’Mello. Automatic Gaze-Based Prediction of Mind Wandering with Metacognitive Awareness. In Proc. of UMAP 2015, 31–43, 2015. Springer.
[Bosch et al., 2015] N. Bosch, S. D'Mello, R. Baker, et al. Automatic Prediction of Learning-Centered Affective States in the Wild. In Proc. of IUI 2015, 379–388. ACM.
[Chawla et al., 2002] N.V. Chawla, K.W. Bowyer, L.O. Hall, et al. SMOTE: synthetic minority over-sampling technique. J. Artif. Intell. Res., 16:321–357, 2002.
   Features
End pupil size
Max pupil size
Stddev pupil size
Labels_attr: proportional time spent Stddev distance
Mean pupil size
Labels_vis: time to last fixation Labels_attr: time to last fixation Labels_attr: num of transitions to Labels_alter
Labels_alter: number of transitions to Labels_attr
Table 4: Top 10 features for predicting confusion.
Conclusion
Set Score D
    Pupil
    100
   Pupil
    88
   Pupil
    67
   Gaze
    45
   Head
    39
   Pupil
    37
   Gaze
      36
   Gaze
    33
   Gaze
  29
   Gaze
      26
    + + + + + + + +
+ +
                    If confusion could be predicted and resolved in real time, user experience and satisfaction with InfoVis would be greatly improved. In this paper, we focused on predicting occurrences of confusion during the interaction with ValueChart, an interactive visualization to support multi-
2534
 [Conati et al., 2014] C. Conati, G. Careninni, B. Steichen, et al. Evaluating the impact of user characteristics and different layouts on an interactive visualization for
Sensemaking. IEEE Trans. On Vis. Comput. Graph., 22(1):499–508, 2016.
[Liaw and Wiener, 2002] A. Liaw and M. Wiener. Classification and regression by randomForest. R News, 2(3):18–22, 2002.
[Lim et al., 2015] Y. Lim, A. Ayesh, and M. Stacey. Using Mouse and Keyboard Dynamics to Predict Cognitive Stress During Mental Arithmetic. Intelligent Systems in Science and Information, 335–350, 2015.
[Liu et al., 2010] C. Liu, R.W. White, and S. Dumais. Understanding web browsing behaviors through weibull analysis of dwell time. In Proc. of ACM SIGIR 2010, 379–386, 2010.
[Muldner et al, 2010] K. Muldner, W. Burleson, and K. VanLehn. “Yes!”: using tutor and sensor data to predict moments of delight during instructional activities. In Proc. of UMAP 2010, 159–170, 2010.
[Nadkarni and Gupta, 2007] S. Nadkarni and R. Gupta. 2007. A task-based model of perceived website complexity. Mis Q., 31(3):501–524.
[Nazemi et al., 2014] K. Nazemi, W. Retz, J. Kohlhammer, et al. User similarity and deviation analysis for adaptive visualizations. In Human Interface and the Management of Information, 64–75, 2014. Springer.
[Pentel, 2015] A. Pentel. Patterns of Confusion: Using Mouse Logs to Predict User’s Emotional State. In Proc. of the PALE Workshop, 40–45, 2015.
[Porayska-Pomsta et al., 2013] K. Porayska-Pomsta, M. Mavrikis, S. D'Mello, et al. Knowledge elicitation methods for affect modelling in education. Int. J. Artif. Intell. Educ., 22(3):107–140, 2013.
[Steichen et al., 2014] B. Steichen, C. Conati, and G. Carenini. Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities from Eye Gaze Data. ACM Trans. Interact. Intell. Syst., 4(2):11, 2014.
[Wongsuphasawat et al., 2012] K. Wongsuphasawat, C. Plaisant, et al. Querying event sequences by exact match or similarity search: Design and empirical evaluation. Interact. Comput., 24(2):55–68, 2012.
[Wu, 2015] M. Wu. Inferring user cognitive abilities from eye-tracking data. University of British Columbia, MsC Thesis, 2015.
[Yelizarov and Gamayunov, 2014] A. Yelizarov and D. Gamayunov. Adaptive Visualization Interface That Manages User’s Cognitive Load Based on Interaction Characteristics. In Proc. of ACM VINCI 2014, 1–8, 2014.
[Yi, 2008] J.S. Yi. Visualized decision making: development and application of information visualization techniques to improve decision quality of nursing home choice, Georgia Institute of Technology. PhD Thesis, 2008.
decision making. 33(3):371–380, 2014.
Computer Graphics Forum,
[Conati et al., 2013] C. Conati, E. Hoque, D. Toker, et al. When to Adapt: Predicting User’s Confusion During Visualization Processing. In Proc. of WAUV, 2013.
[D’Mello and Graesser, 2007] S. D’Mello and A. Graesser. Mind and Body: Dialogue and Posture for Affect Prediction in Learning Environments. In Proc. of AIED 2007, 161–168, 2007.
[Fawcett, 2006] T. Fawcett. An introduction to ROC analysis. Pattern Recognit. Lett., 27(8):861–874, 2006.
[Gena, 2005] C. Gena. Methods and techniques for the evaluation of user-adaptive systems. Knowl. Eng. Rev., 20(1):1–37, 2005.
[Granholm and Steinhauer, 2004] E. Granholm and S.R. Steinhauer . Pupillometric measures of cognitive and emotional processes. Int. J. Psychophysiol., 52(1):1–6, 2004.
[Holmqvist and Nyström, 2015] K. Holmqvist and M. Nyström. Eye Tracking: A Comprehensive Guide to Methods and Measures. OUP, 2015.
[Huang et al., 2015] D. Huang, M. Tory, B.A. Aseniero, et al. Personal Visualization and Personal Visual Analytics. IEEE Trans. Vis. Comput. Graph., 21(3):420–433, 2015.
[Iqbal et al., 2005] S.T. Iqbal, P.T. Adamczyk, X.S. Zheng, et al. Towards an index of opportunity: understanding changes in mental workload during task execution. In Proc. of ACM SIGCHI 2005, 311–320, 2005.
[Jang et al., 2014] Y.-M. Jang, R. Mallipeddi, and M. Lee. Identification of human implicit visual search intention based on eye movement and pupillary analysis. User Model. User-Adapt. Interact., 24(4):315–344, 2014.
[Jaques et al., 2014] N. Jaques, C. Conati, J.M. Harley, et al. Predicting Affect from Gaze Data during Interaction with an Intelligent Tutoring System. In Proc. of ITS 2014, 29–38, 2014. Springer.
[Kotsiantis et al., 2006] S. Kotsiantis, D. Kanellopoulos, and P. Pintelas. Handling imbalanced datasets: A review. GESTS Int. Trans. Comput. Sci. Eng., 30(1):25–36, 2006.
[Kuhn, 2008] M. Kuhn. Building predictive models in R using the caret package. J. Stat. Softw., 28(5):1–26, 2008.
[Lallé et al., 2015] S. Lallé, D. Toker, C. Conati, and G. Carenini. Prediction of Users’ Learning Curves for Adaptation While Using an Information Visualization. In Proc. of IUI 2015, 357–368, 2015. ACM.
[Lee et al., 2016] S. Lee, S. Kim, Y. Hung, et al. How do People Make Sense of Unfamiliar Visualizations? A Grounded Model of Novice’s Information Visualization
Static Visualization of Temporal Eye-Tracking Data
Kari-Jouko Räihä, Anne Aula, Päivi Majaranta, Harri Rantala, and Kimmo Koivunen
Tampere Unit for Computer-Human Interaction, Department of Computer Sciences, FIN-33014 University of Tampere, Finland
Tel. +358-3-35518871
            {kjr, aula, curly, hjr, kimmo}@cs.uta.fi
Abstract. Existing static visualization techniques for eye-tracking data do not make it possible to easily compare temporal information, that is, gaze paths. We review existing techniques and then propose a new technique that treats time as the prime attribute to be visualized. We successfully used the new technique for analysing the visual scanning of web search results listings. Based on our experiences, the new visualization is a valuable tool when the temporal order of visiting Areas of Interest (AOI) is the main focus in the analysis, the AOIs have a natural linear order, there are many AOIs to produce interesting patterns, and the AOIs fill most of the coordinate space being studied.
1 Introduction
Eye-tracking is increasingly being used to provide insight on a variety of tasks involving visual perception. Eye-trackers provide massive amounts of data that needs to be summarized to make it useful. A variety of numerical metrics, such as fixation duration, number of fixations, and saccade length, are used for this purpose [6].
Before the data can be analyzed statistically with appropriate metrics, it is essential to understand the characteristics of gaze behaviour in the current context. A number of gaze data visualization techniques have been developed for this purpose. They are currently being supported by commercial analysis tools [4, 8].
The problem of visualizing temporal gaze data (that is, gaze paths) statically in a compact and usable way still remains a challenge. We first present the existing solutions and then propose a new technique, time plots, that focus on static visualization of the temporal data without cluttering the visualization.
2 Existing Visualization Techniques
Most current eye-trackers are based on video technology. They produce coordinates that indicate where the user is looking at using typically a sampling rate from 50 Hz to 250 Hz. Thus, new coordinates are produced every 20 ms to every 4 ms, respectively.
A straightforward way of visualizing eye-tracking data is to plot the coordinate stream on top of the observed target. However, this is often problematic for visual analysis of the data: the individual data points carry little information, and in these
M.F. Costabile and F. Paternò (Eds.): INTERACT 2005, LNCS 3585, pp. 946 – 949, 2005. © IFIP International Federation for Information Processing 2005
Static Visualization of Temporal Eye-Tracking Data 947
visualizations there are 50 to 250 of them per second. It is a challenging task for the human analyzer to make sense of this stream because of the sheer volume of data. Therefore, the most common way of representing eye-tracking data is to draw a scan path on top of the target image. It consists of fixations, typically shown as circles, and saccades, shown as lines connecting the circles. Figure 1 (a) shows a scan path of a user viewing a web search result listing.
Scan path visualizations are typically more useful than visualizations of the individual data points, since they group the information into meaningful chunks. However, Figure 1 (a) illustrates the typical problem of gaze path visualizations: the fixations and saccades overlap, making it difficult to figure out the order in which the fixations occurred. The fundamental reason for this problem is that a two-dimensional image is used to represent three-dimensional data: x-coordinates, y-coordinates, and time. To solve this problem, the third dimension (time) is usually handled in some special way, such as by using colour coding to indicate the order of the saccades. However, as there is no natural ordering of colours for the human observer, the interpretation of the coding is cognitively demanding. Moreover, colour can only give a rough illustration of the order, as these visualizations typically contains tens of saccades.
Another possibility is to collapse the time dimension, so that only the fixations are visualized, not their order. This is a useful way of summarizing numerous gaze paths, even from different users, and it can be effectively used to highlight the points of interest. When such cumulative data is displayed smoothly using fixation maps by David Wooding [9] or its variations, the result is expressive and informative. A sample fixation map is shown in Figure 1 (b). The visualization is less cluttered than the gaze path on the left, but the time dimension has completely disappeared.
   Fig. 1. (a) On the left, a scan path. (b) On the right, the same data visualized as a fixation map.
948 K.-J. Räihä et al.
3 Static Visualization of Temporal Gaze Data
In some situations the timing information is the crucial aspect in the analysis. Our visualization technique is especially suitable for situations where (1) the exact locations of the fixations are less important than how they land on predefined areas of interest (AOI) in the stimulus; and when (2) the AOIs have a natural linear ordering. This is the case, for instance, when studying gaze behaviour on web pages that are composed of horizontally or vertically arranged blocks of information. Another natural application is studies of menu usage [2, 3].
Prime examples of such web pages are the result listings of web search engines. Figure 2 shows a miniaturized image of such a page, with a gaze path positioned next to it. The data is the same that was used in Figure 1. In the visualization, the y- coordinate corresponds to the position in the result listing, and the x-coordinate is used to visualize the ordering in time. We call this a time plot of the gaze data. The horizontal location of where the gaze has landed within the area of interest is not shown.
Fig. 2. A time plot of the scan path shown in Figure 1 (a)
Time plot visualizations make it possible to visually observe differences in the gaze behaviour of different users. We have used the technique for analyzing how users perceive search result listings (see [2] for more examples).
The basic time plot approach can be used with variations. For instance, the y- coordinate could denote the exact position of the AOI in the coordinate space, and similarly, the x-coordinate could denote a relative point in time. In Figure 3 we are visualizing only the order of AOIs, both for the vertical locations (y) and for their visiting order (x). For this analysis task, this solution was found to work well.

Static Visualization of Temporal Eye-Tracking Data 949
4 Discussion
We have proposed a simple new way of visualizing gaze data that facilitates the analysis of gaze paths. We have used it successfully for analyzing the scanning of web search result listings, and believe it to be useful for other visual search tasks as well. Figure 3 illustrates another study where we tried to use this technique. Here the objective was to study the differences of gaze behaviour of designers and consumers while viewing design products. Figure 3 shows that in this case the time plot was less illustrative.
Based on our experiences, we believe the time plot to be a valuable tool when (1) the temporal order of visiting AOIs is the main target of the analysis, (2) the AOIs have a natural linear order, (3) there are sufficiently many AOIs to produce interesting patterns in the time plot, and (4) the AOIs fill most of the coordinate space being studied.
References
1. Aaltonen, A., Hyrskykari, A., and Räihä, K.-J.: 101 spots, or how do users read menus? In Proc. CHI 1998, ACM Press (1998) 132-139
2. Aula, A., Majaranta, P., and Räihä, K-J.: Eye-tracking reveals the personal styles for search result evaluation. In Proc. INTERACT 2005, Rome, September 2005
3. Byrne, M.D., Anderson, J.R., Douglass, S., and Matessa, M.: Eye tracking the visual search of click-down menus. In Proc. CHI 1999, ACM Press (1999) 402-409
4. EyeResponseTechnologies:GazeTracker.Availableathttp://www.eyeresponse.com/
5. Feusner,M.:Eye-trackingvisualizations.Availableat
http://www.hci.cornell.edu/eyetracking/visualizations.php
6. Jacob, R.J.K., and Karn, K.S.: Eye tracking in human-computer interaction and usability
research: Ready to deliver the promises (section commentary). In The Mind’s Eye: Cognitive and Applied Aspects of Eye Movement Research, ed. by J. Hyönä, R. Radach, and H. Deubel, pp. 573-605, Amsterdam, Elsevier Science, 2003
7. Outing, S., and Ruel, L.: The Best of Eyetrack III: What We Saw When We Looked Through Their Eyes. Available at http://www.poynterextra.org/eyetrack2004
8. Tobii Technology: ClearView. Available at http://www.tobii.com/
9. Wooding, D.: Fixation maps: quantifying eye-movement traces. In Proc. ETRA’02, ACM
   Fig. 3. Time plot (on the right) of a user inspecting the image of a mobile phone. AOIs are highlighted with polygons.
Press (2002), 31-36
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 J Vis (2016) 19:461–474
DOI 10.1007/s12650-015-0338-2
REGULAR PAPER
Jie Li • Zhao Xiao • Han-Qing Zhao • Zhao-Peng Meng • Kang Zhang
Visual analytics of smogs in China
Received: 12 August 2015 / Accepted: 8 December 2015 / Published online: 5 January 2016 Ó The Visualization Society of Japan 2015
Abstract Smog is one of the most important environmental problems in China. Scientists have attempted to explain the causes from chemistry, physics, atmosphere and other perspectives. Many meteorologists believe that meteorology is a crucial reason. In this paper we present a new multi-view approach to visual analytics of the recent smog problems in China. This approach integrates four interrelated visualizations, each specialized in a different analysis task. To reveal the relationship between smog and meteorological attributes, we design a Correlation Detection View that simultaneously visualizes the air quality change patterns across multiple cities and related meteorological attributes. Component Trend View is used to show the variation patterns of six air components, while Aggregation View can reveal the regional overall pollution situations. A case study has been conducted using the China Air Quality Observation Data and European Centre for Medium-Range Weather Forecasts re-analysis data to verify the effectiveness of the proposed approach. By visually analyzing the meteorological data of Beijing and other major cities, we have found several interesting patterns, which prove the validity of our work in identifying sources of smog.
Keywords Smog   Spatiotemporal visualization   Meteorological visualization   Visual analytics 1 Introduction
Recently, smog has become one of the most serious problems in China, which has attracted nationwide attention (Ma et al. 2012). Many cities are immersed in smog and the blue skyline has been barely visible over the past few years. Smog not only affects air quality, but also brings a series of negative effects on almost every aspect of our life. It can cause all kinds of respirational diseases (Chen et al. 2013), increased transportation risks, and reduced urban sustainable competitive advantages (Chen et al. 2014). To tackle the
J. Li   Z. Xiao   H.-Q. Zhao   Z.-P. Meng
School of Computer Software, Tianjin University, Tianjin, China E-mail: jie.li@tju.edu.cn
Z. Xiao
E-mail: zxiao@tju.edu.cn
H.-Q. Zhao
E-mail: hanqingzh@tju.edu.cn
Z.-P. Meng
E-mail: zpmeng@tju.edu.cn
K. Zhang (&)
Department of Computer Science, University of Texas at Dallas, MS EC31, Box 830688, Richardson, TX 75083-0688, USA
E-mail: kzhang@utdallas.edu

462 J. Li et al.
 smog problem, researchers have conducted various studies on air pollution, and both the state and local governments have legislated the related laws and regulations. The pollution situations, however, are still serious, and the real pollution sources for major cities, such as Beijing, have not been accurately detected.
Due to our limited understanding on the causes of smog, all the analytical results of the existing methods are with inherent uncertainties (Zhou et al. 2013), and even conflicting conclusions were often drawn by different researchers and organizations. For example, once an official claimed that smog was mainly caused by the smoke of cooking, but Beijing government considered the vehicle exhaust as the main cause of smog. They tried to reduce car exhaust gases by a ‘‘Private Vehicle Restriction’’ policy and limiting the vehicles from other provinces to enter the city. However, smog in Baoding (a small city near Beijing) is heavier than in Beijing, yet Baoding has far fewer vehicles than Beijing, making one to doubt about such measures. In fact, although the discharge amount of pollution gases in a city is in a stable state, the air quality of the city has always been changing amount of pollution gases in a city is in a stable state, the air quality of the city has always been changing affected by meteorological factors, such as atmosphere movement and inversion layer (the air temperature at a low altitude is lower than that at a high altitude at the same location). It is impossible to analyze smog in a city without considering the meteorological attributes of its surrounding area (Hulek et al. 2013).
We thus attempt to design a visualization framework that can intuitively and interactively reveal the evolutionary nature of air quality at different spatiotemporal scales. Our goal is to identify spatiotemporal patterns of air quality changes and the relations between air quality and meteorological attributes through visual analysis. The major contributions of this paper include:
• A comprehensive visual analytics framework for discovering air pollution patterns at different spatiotemporal scales.
• Three visual analytic views for different analysis tasks in smog studies.
• Case studies on China Air Quality Observation Data and ECMWF re-analysis data, through which the
effectiveness and the scalability of the approach have been verified.
The remaining part of this paper is organized as follows: Sect. 2 reviews the related work. The approach overview is then described in Sect. 3, followed by three concrete visualization views in Sect. 4. Section 5 describes a case study, and an expert review is described in Sect. 6. Finally, we conclude the paper in Sect. 7.
2 Related work
Our current work builds upon our previous system that has been developed to explore and visualize oceanographic applications in a distributed environment (Li et al. 2014a, b). In this section, we first discuss the existing spatiotemporal data visualization techniques, and then review several related environment visualization applications.
2.1 Spatiotemporal data visualization
Thematic map (Slocum et al. 2009), which can be viewed as an overlay of Heatmap or glyphs on a map, is perhaps the most traditional method of geo-related data visualization. Taggram (Nguyen and Schumann 2010) is another type of thematic map, in which tag clouds (Lee et al. 2010) are plotted on a map to represent the area characteristics. These methods work well for static data, but provide limited supports for visualizing time-oriented geographic data. Many researchers have attempted to overcome this problem by combining a map with other time-series visualization techniques. Malik et al. (2012) comprehensively used maps, bar charts, line charts and pie charts to analyze the correlations between urban crime activities and spatiotemporal dimensions. Landesberge et al. (2012) designed a dynamic categorical data view (DCDV) to visualize human position transitions in 1 day, associated it with a geography view. Furthermore, parallel coordinates (Johansson and Jern 2007; Tominski et al. 2004), ThemeRiver (Havre et al. 2000, 2002), stacked bar charts (Nocke et al. 2004), and many other existing time-series visualization techniques can be com- bined with a map to form effective spatiotemporal data visualization tools.
Visual analytics of smogs in China
463
 Table 1 Attributes of the collected air quality data ID Name
1 AQI (Air Quality Index)
2 CO
3 NO2
4 SO2
5 O3
6 PM2.5
7 PM10
2.2 Visual analysis environment data
Unit
Computed based on 2–7 mg/m3
ug/m3
ug/m3
ug/m3 ug/m3 ug/m3
   Visualization has always been an effective tool in environmental studies, and many classic visualization methods exist, such as contour line (Watson 1992; Johansson et al. 2010), standard coloration (Li et al. 2014a, b), etc. These methods can only show analysis results, lacking interactions and the abilities for discovering the potential knowledge from the huge amounts of meteorological data. Johansson et al. (2010) used a 3D GIS platform to show the meteorological data. Both Yannier et al. (2008) and Janicke et al. (2009) have performed the studies of weather variation visualization, similar to our previous work. Their studies, however, focus on the use of touch screens in enhancing the perception effects.
The most related idea was found in Qu et al. (2007), which analyses the air pollution problem in Hong Kong. Several visualization tools have been proposed, such as S-shape parallel coordinates, polar system with circular pixel bar and weighted complete graph. However, small scale analysis cannot prove the effectiveness of the method. Our method focuses on the smog problem at a large scale. We also analyze different city clusters in multiple latitudes.
3 Approach overview
3.1 Data source
China’s Ministry of Environmental Protection and Environmental Monitoring Centre Station has operated a national air quality observation network containing about thousands of observation stations throughout the country. These stations can continuously observe and record air qualities at a fixed temporal interval and automatically transfer these data to the specified data centers. Through an online data service, we could obtain air quality observation data of 846 stations in each hour. The attributes of air quality data are summarized in Table 1.
To analyse the correlations between the smog of a city and the meteorological attributes in the sur- rounding areas of the city, we also need to collect meteorological data containing multiple parameters. The ECMWF re-analysis data with the spatial resolution of 1° 9 1° is used in our approach.
An inversion means that the air temperature at the lower altitude is lower than that at the higher altitude. An inversion layer forms a stable air structure and makes it difficult for smog to dissipate. Wind field and inversion layer are two major factors to the formation of smog, therefore wind and air temperature are the two parameters in the downloaded grid data.
3.2 Objectives
Among all the tasks, understanding the overall dynamics in different spatiotemporal scales is the most important. For example, ‘‘which is the heaviest smog area?’’, and ‘‘whether the smog problem is a regional or a nationwide phenomenon?’’ Apart from the overall dynamics, environmental experts also wish to know the exact pollution sources in major cities to guide the government in developing economic and industrial programs. Based on the ‘‘Beijing–Tianjin–Hebei integration strategy’’, all the high-polluting enterprises in Beijing should be transferred to other places and the new locations should be fully evaluated to minimize the impacts on Beijing resulting from air movement. Understanding the temporal trend of smog at different spatiotemporal scales becomes an important task for the above strategy. Experts wish to know not only the overall smog changes in a year or a month, but also a city’s air components in each hour to accurately detect the pollutant’s origins. Ideally, they wish to be able to forecast the air quality, which is closely related to the
464 J. Li et al.
 public life. To better design the visualization framework, we classify the most important tasks of smog studies into three major categories:
• Overall distribution: identifying the air pollution situation at different spatial scales.
• Correlation analysis: discovering the impacts of meteorological attributes on air quality, and even
predicting the future change based on the patterns observed today.
• Temporal trend: determining the air pollution change in any temporal interval, and the variations of air
pollution components at different temporal scales. 3.3 Visualization views
Based on the previous system architecture (Li et al. 2014a, b), we add a new visual analysis layer, in which four views, specialized in the different analysis objectives, are integrated.
• Global Distribution View—a radial layout plot including a map that shows the global spatial distribution, a sector-based band for displaying temporal trend, and several clustering rings, in which the K-Means algorithm is adapted to cluster all the cities into groups of similar smog change trends.
• Correlation Detection View—visualizing air quality and meteorological data in one framework. Users can simultaneously explore two types of data via interactions, and analyze the impacts of different meteorological parameters on cities’ air quality changes.
• Component Trend View—depicting the daily and hourly air component changes of all the cities. The cities are projected onto a radial scatterplot (Astrolabium) containing multiple axes, each representing one type of pollutant, such as SO2, NO2, etc. When a city is selected by clicking on a point in the Astrolabium, the detailed hourly component changes in different temporal intervals for the city are shown on an Air Component River.
• Aggregation View—supporting exploration and comparison of the air quality in different cities or regions within a 2D/3D combined context. The aggregation functions often used in DBMS are included in this view, which allows the user to select interested attributes to be visually explored.
All the four views are interrelated. As the main visualization, the Global Distribution View displays both the spatial, temporal data, from which other views’ input parameters can be selected by through mouse interaction. In addition, the cities and temporal intervals analysed in other views are automatically high- lighted in the Global Distribution View.
4 Multi-view visualization
We first introduce the Correlation Detection View specialized in analysing the fluctuations of smog in a city with the variations of different meteorological attributes.
4.1 Global Distribution View
To visually encode air quality data, we first use the radial map (Li et al. 2014a, b) containing three visual elements: (1) amap in the center conveying the spatial information; (2) a ring band outside the map, encoding temporal trend changes in the user-defined interval, and (3) multiple concentric rings outside the ring band representing the different clusters generated by a clustering algorithm using the air quality changes rates as the clustering reference, as illustrated in Fig. 1. Using such three components, the spatial, temporal and the clustering information of all the cities can be jointly visualized in a concise and intuitive manner.
All the cities are drawn in the innermost map to illustrate the spatial distribution. Furthermore, each city takes up a sector. To keep the symmetry of the view, the angle interval of each city’s sector is equal. For N cities, the angle of each sector is 360/N. A colored line is drawn along the radial direction, through a sector- based band, and finally locates on a concentric ring. Each small bins in the sector-based band represents the attribute value of the corresponding city at a time point, and the entire band can reflect the change trend in the defined temporal interval, as illustrated in Fig. 2. Outside the sector-based band, each concentric ring indicates a cluster of cities that share similar temporal change rates. We introduce the concrete design of the view in three aspects:
Visual analytics of smogs in China 465
  Fig. 1 Global Distribution View consists of a map, a ring band outside the map, and multiple concentric rings, showing the clustering information
  Fig. 2 Temporal mapping represented by a sector-based ring-band
4.1.1 Spatial mapping
By aligning the cities (with 3D coordinates, i.e. longitude, latitude, altitude) to a 1D angular coordinate system, we can create an extra space for visualizing time and clustering information. To minimize the loss of spatial information when computing the angular coordinate for each city, we first divide China into 7 subareas according to the traditional geographic convention (see the different colors in Fig. 1), and assign an angular span to each subarea in proportion to the number of the cities in that area. The cities in one subarea
466 J. Li et al.
 are placed in descending order of their latitudes. Our method does not limit the sorting reference. Other parameters, such as longitude, altitude, etc., can also be selected upon application needs.
The cities are arranged radially on different concentric rings along the circumference of a circle at equal angular intervals. This arrangement aims to effectively condense a large number of widely distributed cities in a single view, while emphasizing the map as the core of the visualization.
To keep the map style intuitive and minimize the viewer’s cognitive effort, we use the online tool ColorBrewer (Brewer and Harrower 2009) to assign a color in the recommended color solution to each geographical subareas, while the cities are colored the same as the geographical subarea they belong to.
4.1.2 Temporal mapping
The radial direction represents the time axis. Each sector indicates the time series of a city, while a radial bin is colored to represent the concrete AQI (Air Quality Index) value at an instance of time. Figure 2 shows an example of time series visualization of at six cities in 4 years.
4.1.3 Clustering
To represent the clustering information, several concentric rings are used. Each ring represents one cluster, and a ring’s thickness represents the absolute value of the average slope of linear regression line between time and the AQI. The cities in a cluster are drawn on the corresponding ring and are highlighted when the mouse moves on the ring.
To support clustering, we first compute the slope to represent the climate change condition of a station. Let X = {x1, x2,..., xN} be the set of time instances in the selected time interval, and Y = {y1, y2,..., yN} be the set of AQI values at the corresponding instances of time. The slope is calculated as follows:
PNi1⁄41 ððxi   xÞðyi   yÞÞ
slope 1⁄4 PNi1⁄41 ðxi   xÞ2 ð1Þ
We adapt the K-Means algorithm to generate clusters of slopes and map each cluster onto a ring. Of course, our framework does not depend on any specific clustering algorithm. An extra merit of using the K-Means algorithm is that average slope of each cluster can also be obtained when performing the algorithm.
4.2 Correlation Detection View
The air quality of a city has always been changing and affected by its surrounding areas due to air movement. We introduce a Correlation Detection View specialized in analyzing the fluctuations of smog in a city with the variations of different meteorological attributes. The Correlation Detection View is con- structed based on our previous oceanographic data visualization system (Li et al. 2014a, b). We add a smog correlation detection component which is similar to a radar plot, as shown in Fig. 3.
4.2.1 Meteorological factor visualization
We use the ECMWF re-analysis data as the input. Because the format of such data has been supported in our previous system, we can conveniently integrate the downloaded meteorological data into our framework. The Correlation Detection View visualizes the wind field and the inversion layer, which are used as the context of analyzing the correlation between meteorological factors and smog.
A wind record at a grid point consists of a horizontal component and a vertical component. By calcu- lating the vector sum of such two components, we can get the speed and the direction of wind at that grid point. We use a long equilateral triangle to point to the wind direction, and the width of the triangle’s bottom encodes the wind speed. To determine whether there exits an inversion layer at a grid point, we compare the temperatures of different altitudes at the grid point. If an inversion layer exists, the triangle at that point is highlighted in a different color, as shown in Fig. 3.
Through the case study (see Sect. 5), we discover that inversion layers and wind fields are highly related to serious smog phenomena.

Visual analytics of smogs in China 467
  Fig. 3 Correlation Detection View
 Fig. 4 a Computing a city’s meteorological data based on its four closest grid points. The yellow arrowed line is the wind at the target city. b Decomposing the wind at neighboring cities to obtain wind portions directed at the target city
4.2.2 Correlation analysis
Correlation analysis aims at identifying the air quality of a city affected by its neighboring cities. Because the cities with air quality observation records may not be exactly on the grid points, we first calculate the wind field (speed and direction) and air temperature of each city using an interpolation algorithm. The four nearest grid points around the city are used as the inputs to the interpolation algorithm, while the weighting of each point is inversely proportional to the distance between the city and the point, as shown in Fig. 4a.
Having obtained the meteorological data and air quality observation data for every city, we select a city as the target city (see Fig. 4b) to analyze the relationship of the smog movement between the city and its surrounding cities. For each neighboring city located within the spatial range of this city, we compute the wind portion (see Fig. 4b) that originates from a neighboring city and points to the target city (or away from the target city), by decomposing the wind using the parallelogram law.
468 J. Li et al.
  Fig. 5 Component Astrolabium
Let D = {d1, d2,..., dN} be the set of distances between the target city and its N neighboring cities, and V = {v1, v2,..., vN} be the set of the wind portions of the neighboring cities. The correlation ci between the ith city and the target city can be simply represented as:
ci 1⁄4 di ð2Þ jvi j
The correlation is drawn as an arrowed line. The solid line indicates that the air quality of the target city is affected by the other cities, while the dotted line reflects that the target city affects its neighboring cities (the wind blew from the target city to these cities). The thicker, the arrowed line is, the stronger the correlation between two cities.
The color of each city (see the small circles in Fig. 3) represents the air quality, with the legend shown in Fig. 5. If there exists an inversion layer above a city, the city’s outline is highlighted (Fig. 3). The neighboring cities that have both a strong correlation with the target city and poor air quality should attract our attention during analysis. Our approach also provides the filtering control to help the user to view only the cities that satisfy the defined correlation and air quality conditions.
4.3 Component Trend View
Identifying the primary pollutants and variation patterns of different air pollutants is the prerequisite for preventing and controlling smog. We therefore design a Component Trend View consisting of a Compo- nents Astrolabium and an Air Component River, based on an Astrolabe method (Wang et al. 2013), RadViz (Sharko et al. 2008) and ThemeRiver (Havre et al. 2002).
4.3.1 Component Astrolabium
A Component Astrolabium contains several axes, each representing a type of pollutant contained in the air quality observation data. We first normalize all the pollutant values into the range between 0 and 1. A Cartesian coordinate system is then constructed, using the center of the Astrolabium as the origin. The air quality of a city at a time can be viewed as an set Q = {pollutant1, pollutant2, ..., pollutantN}, (N is the number of pollutant types), and each component in the set Q has a 2D coordinate. Let (x, y) be the coordinate of a city, and (ui, vi) be the component on the ith axis. We can calculate the coordinate of each

Visual analytics of smogs in China 469
  Fig. 6 Air Component River
city by simply computing the vector sum of 6 pollutants (the attribute value on each axis) of the city using
Eq. 3:
8<pollutanti 1⁄4ðui;viÞ
PN
i1⁄40
: ðx;yÞ1⁄4
ðui;viÞ
ð3Þ
If a point with deep color is mapped near the outer vertex of a pollutant axis, then such pollutant is the major air pollutant of that city. Figure 5 shows that smog is a national problem, because the AQIs of most of the cities are above 100. Furthermore, PM2.5 is the primary pollutant of the cities having a poorer air quality, due to the fact that most of the purple points (AQI [ 300) are close to the PM2.5 axis.
In extreme cases, a point may be positioned outside of the Astrolabium. For example, assume the radius of the Astrolabium is 1, if the values of CO2, PM2.5 and NO2 of a city are the maximum 1 and the other three attribute values are all 0, then the city’s coordinate is (2, 0). Therefore, we introduce a parameter ki for each component coordinate, and Eq. 3 is changed as follows:
8< :
Pn i1⁄40
ðx; yÞ 1⁄4
ðui; viÞ   ki
ð4Þ
ki 1⁄4M  cos jui;vij p2 þ1 ð0\M\1Þ
 where|ui,vi| represents the length of the ith component, ranging from 0 to 1, and ki is monotonically decreasing. As |ui,vi| increases, ki decreases more rapidly, and finally reaches M (when |ui,vi| = 1). The user can manually adjust M to generate a desirable distribution of the small circles in the Astrolabium.
4.3.2 Air Component River
Since an Astrolabium can only show the component distribution at an instance of time, we design an Air Component River to represent the component variations of a city over a period. An Air Component River consists of multiple branches, each representing the content of a pollutant. As in Fig. 6, the width of the river indicates the sum of all the pollutants, and the width of each branch changes over time, visualizing the variation trend of each pollutant. Component Astrolabium and Air Component River can jointly show the trends and the air component distribution characteristics of different cities over time (see Fig. 11).
4.4 Aggregation View
We overlay bar charts on the map, making an Aggregation View for the interactive exploration and comparison of the air qualities in different cities.
The view in Fig. 7 is constructed based on a GIS platform. When using this view, the user first needs to select the pollutant, temporal interval and an aggregation function, then a 3D bar chart and a 2D heatmap are generated. These two plots are simply two different visual representations of the same data, and one of them can be easily selected according to the personal preference. Four aggregation functions (SUM, AVERAGE, MAX, MIN) used in DBMS are provided in this view. For example, if the user wishes to compare the max AQIs in a temporal interval of different cities, by selecting the MAX aggregation function, he/she obtains the view that reflects maximal AQI values in the two plots. This view also supports interactive operations often used in GIS platforms.
470 J. Li et al.
  Fig. 7 Aggregation View. a 3D bar chart. b 2D heatmap
 Fig. 8 Daily-average AQI of four stations in Beijing
5 Case study
This section reports the evaluation of our method using China Air Quality Observation Data and ECMWF re-Analysis data.
5.1 Data pre-processing
In a preliminary data analysis phase, we find the stations in one city have almost the same changing trend, as in Fig. 8. Therefore, we aggregate the data of all the stations in a city, and perform visual analysis using a city as the smallest unit. Because most cities are not exactly located in grid points, a linear interpolation algorithm is used to compute the meteorological attributes at the location of a city, weighted on the distances between the city and its four surrounded grid points.
We divide China into seven areas using the customary geographical division method in climate studies, and assign a color to each area. Each area consists of several provinces, each containing several cities. We thus construct a hierarchical structure to organize the geographic information. The ECMWF re-Analysis data are in NetCDF format. Through the NetCDF data operation interfaces of the oceanographic application platform, we conveniently obtain the meteorological attributes of the cities.
5.2 Overall distribution
We first analyze the overall smog distribution in China. Because December is considered to be the month when smog happened frequently, we download the smog data and meteorological data in December 2013.
Visual analytics of smogs in China 471
  Fig. 9 The smog conditions in three important regions in China. Red circles indicate the capitals of the provinces in the regions. a Beijing, Tianjin and Hebei region. b Jiangsu, Zhejiang and Shanghai region. c Guangzhou and Shenzhen region
Using the example in Fig. 2a, we divided the cities into nine clusters based on the average AQI. We found that the numbers of cities in different clusters are almost identical and the average AQIs of all the clusters are higher than 50, implying that smog in China is a national phenomenon. By selecting the outmost ring that represents the cluster of the most serious smog, we find most of the cities in that cluster are in the west of Hebei province, and those cities are smogged during the entire month. East China is another area that suffered from smog, while the smog in Southwest, South and Northwest China are relatively moderate (see Fig. 9c). Inspecting the time series of all the cities in East China, we find the extreme smog occurred only at the beginning of the month.
The Aggregation View also proves our findings. Figure 9 visualizes the smog situations of three important areas in China. As the regional centers, Beijing, Shanghai, and Guangzhou all have high popu- lation densities and industrial production activities. Smogs in such cities are, however, not as serious as their surrounding cities. One reason may be that the small cities excessively aim to improve their GDPs and indiscriminately build new enterprises which may cause serious pollution. To the contrary, big cities have better urban planning and invest more funds on environment protection.
5.3 Correlation analysis
To reveal the correlation between meteorological attributes and smog phenomenon, we compare the air quality data of Beijing and Tianjin. These two cities are geographically close and have similar economic developments. As shown in Fig. 10e, the air qualities in the two cities are almost the same, but on 3 December, smog in Tianjin was very serious, while Beijing’s air quality was normal. By querying the historical weather forecast, we found neither city experienced heavy rainfall or snowfall. Therefore, we tentatively hypothesized that the extreme smog in Tianjin came from other cities.
We first use the Correlation Analysis View to detect the smog sources of Tianjin. By visualizing the wind field, we found the winds in Beijing and Tangshan both blew to Tianjin (see Fig. 10a). Therefore, we suspected Tangshan as the pollution source of Tianjin’s smog on that day. To prove our hypothesis, we continued to analyze the relationship between wind and smog on other days. We select the Tanshan as the target city, and view the influences of it on its neighboring cities. The radius of the observation range is set to 100KM to clearly view the air movement among such three cities (Beijing, Tianjin, and Tangshan).
We found when the arrowed lines from Tangshan to Tianjin and Beijing are dotted, which indicates the air movement is from Tangshan to other two cities, the air quality in Beijing and Tianjin are worse (see Fig. 10b, d). Instead, when the wind was east to west (the arrowed line from Tangshan to Tianjin are solid), Beijing and Tianjin both had relatively better air quality (see Fig. 10c). These findings are consistent with our hypothesis.
On 23th December, the wind was from Tangshan to Beijing and Tianjin (see Fig. 10d), but smog was not as serious as on 8th December. This may be related to inversion layer. On 8th December, there was an inversion layer around Beijing and Tianjin (see the highlighted points on the wind field plot), which hindered air movement and retained the smog. On the contrary, there was no inversion layer on 23th December. Smog was not so serious, although the wind direction was the same. Apart from the above cases, we also discovered many similar cases in other cities. The literature records that in 2008, many heavy
472 J. Li et al.
  Fig. 10 The correlation between AQI and wind and inversion layer
 Fig. 11 Air pollution contents change in 24 h
industrial enterprises were moved to Tangshan from Beijing in order to clear air for the Beijing Olympic Games, making Tangshan a current air pollution source of Beijing.
From these case studies, we learned extreme smog cases are not necessarily caused locally. It is highly related to the pollution levels of the surrounding cities, and the meteorological conditions.
5.4 Air components change trend
Figure 11 shows the daily change of air components for all the 189 cities on 3rd December, 2013. Each Astrolabium represents the air components over 4 h. The point distribution in each Astrolabium is obviously close to the axis of PM2.5 and CO, implying that the problems of air pollution in most cities are associated with these two components. To view the component variation in a single day, we select Beijing as an example. We found that all the components except O3 maintained a low level during the daytime, while the amount of O3 increased obviously during the daytime. By consulting the related literatures, we know the
Visual analytics of smogs in China
473
 Table 2 Questionnaire results Factor
visual design Interaction Learnability Performance Functionality Scalability
Highest
8 9 9 9 9 9
Lowest Average
6 8.67 7 8.20 7 8.38 8 8.22 5 7.61 7 7.67
Variance
0.22 0.48 0.37 0.84 1.21 0.44
   harmful gases, such as NO, NO2, SO2, CO and PM2.5 will convert to O3 under the influence of ultraviolet rays, which may cause the decrease of all the harmful gases except O3.
6 Evaluation
To evaluate the usability of our visualization approach, we have conducted an experiment with nine people volunteered as the experimental subjects. Four subjects are from National Ocean Technology Center with the education background of atmospheric science, while the other five are from the School of Computer Science and Technology, Tianjin University. The subjects are aged between 20 and 35, including 4 females and 5 males.
We first explained our approach and the prototype system to the subjects. After getting familiar with the system, they were asked to complete a questionnaire by providing scores between 0 and 10 on six aspects: visual design, interaction, learnability, performance, functionality, and scalability. We did not impose a time limit, yet every questionnaire was completed within 2 min. Table 2 shows the statistical results of the questionnaire.
The results show that the visual design scored the highest with the smallest variance, since all the subjects liked the design of our approach and believed that the four views were suitable for analyzing domain tasks. Interaction, learnability and the performance also received high scores, indicating that our approach is easy to learn and use. However, a subject pointed out that the four views could only satisfy specific tasks. This may account for the low scores of the functionality. In fact, our approach focuses on the primary tasks in air quality analysis, and we believe that no analysis tool can satisfy all kinds of the analysis tasks in environment science studies. Scalability also receives a relative low score. This may be because that the Global Distribution View might not be able to accommodate a great number of cities due to the small angle space of each sector. However, such effects are limited since the number of the cities having air quality observation data is less than 300. Furthermore, with several simple modifications of the metaphor, such as using a small circle on a clustering ring to represent the number of the cities in a region, etc., the Global Distribution View can accommodate a large number of cities.
An important limitation of our approach, pointed out by three subjects with atmosphere science back- ground, is its correctness and accuracy. They thought our approach was too simple, lacking the complicated mathematical computing capability often used in traditional numeric models. Furthermore, although what we discovered have been verified, they remained doubtful about the correctness of our findings. In fact, all the existing models output results with inherent uncertainties, because of our limited understanding of many physical processes and the fact that not all the meteorological physics have been scientifically modeled. We hope that our approach capable of objectively visualizing actual observation data can be an effective alternative for experts to verify the conclusions generated by other methods. At the same time, our approach may also guide experts in more in-depth studies based on the patterns found in our approach, as a mech- anism for hypotheses generation.
7 Conclusion
This paper has presented a comprehensive visualization approach to smog analysis. New visualization techniques have been integrated and used to analyze the smog problem in China using air quality data and ECMWF data. Having analyzed the smog variation patterns in Beijing, we consider our approach to be
474 J. Li et al.
 effective and useful in real-world scenarios. However, due to the lack of fine-grained data and the partic- ipation of domain experts, our method can only be viewed as a qualitative analysis manner. Our approach could be improved in two aspects. First, visualization of the relationships between meteorological attributes and smog could be improved in a more intuitive manner. Second, cooperating with domain experts can makes our approach more scientific and professional.
Acknowledgments The authors wish to thank Zhuang Cai, Meng-Yao Chen, and Chao Wen for their discussions. We are also grateful to the anonymous reviewers for their insightful comments that have helped us in improving the final presentation. The work was partially supported by Seed Foundation of Tianjin University (under Grant Number 2014XJJ-0003).
References
Brewer CA, Harrower MA (2009) ColorBrewer 2.0: color advice for cartography. Penn State University. http://colorbrewer2. org/
Chen R, Zhao Z, Kan H (2013) Heavy smog and hospital visits in Beijing, China. Am J Respir Crit Care Med 188(9):1170–1171
Chen J, Chen H, Zhang G, Pan JZ, Wu H, Zhang N (2014) Big smog meets web science: smog disaster analysis based on social media and device data on the web. In: Proceedings of the companion publication of the 23rd international conference on world wide web, pp 505–510
Havre S, Hetzler B, Nowell L (2000) Themeriver: Visualizing theme changes over time. In: Proceedings of the IEEE Symposium on Information Visualization (InfoVis’00), pp 115–123
Havre S, Hetzler E, Whitney P, Nowell L (2002) ThemeRiver: visualizing thematic changes in large document collections. IEEE Trans Vis Comput Graph 8(1):9–20
Hulek R, Jarkovsky J, Boruvkova J, Kalina J, Gregor J., Sebkova K, Schwarz D, Klanova J, Dusek L (2013) Global monitoring plan of the Stockholm convention on persistent organic pollutants: visualization and on-line analysis of data from the monitoring reports. http://www.pops-gmp.org/visualization/
Janicke H, Bottinger M, Mikolajewicz U, Mikolajewicz U (2009) Visual exploration of climate variability changes using wavelet analysis. IEEE Trans Vis Comput Graph 15(6):1375–1382
Johansson S and Jern M (2007) GeoAnalytics visual inquiry and filtering tools in parallel coordinates plots. In: Proceedings of the 15th annual ACM international symposium on advances in geographic information systems
Johansson J, Neset TS, Linner BO (2010) Evaluating climate visualization: an information visualization approach. In Proceedings of the international conference on information visualization (IV’10), pp 156–161
Landesberge TV, Bremm S, Andrienko N, Andrienko G, Tekusov M (2012) Visual analytics methods for categoric spatio- temporal data. In: Proceedings of the IEEE conference on visual analytics science and technology (VAST’12), pp 183–192 Lee B, Riche NH, Karlson AK, Carpendale S (2010) Sparkclouds: visualizing trends in tag clouds. IEEE Trans Vis Comput
Graph 16(6):1182–1189
Li J, Meng ZP, Zhang K (2014) Visualization of oceanographic applications using a common data model. In: Proceedings of
the 2014 ACM symposium on applied computing (SAC’14), pp 933–938
Li J, Zhang K, Meng ZP (2014) Vismate: interactive visual analysis of station-based observation data on climate changes. In:
Proceedings of the IEEE conference on visual analytics science and technology (VAST’14), pp 133–142
Ma J, Xu X, Zhao C, Yan P (2012) A review of atmospheric chemistry research in China: photochemical smog, haze pollution,
and gas–aerosol interactions. Adv Atmos Sci 29(5):1006–1026
Malik A, Maciejewskiet R, Jang Y, Huang W (2012) A correlative analysis process in a visual analytics environment. In:
Proceedings of the IEEE conference on visual analytics science and technology (VAST’12), pp 33–42
Nguyen DQ, Schumann H (2010) Taggram: exploring geo-data on maps through a tag cloud-based visualization. In
Proceedings of the international conference on information visualization (IV’10), pp 322–328
Nocke T, Schumann H, Bohm U (2004) Methods for the visualization of clustered climate data. Comput Stat 19(1):75–94 Qu H, Chan WY, Xu A, Chung KL, Guo P (2007) Visual analysis of the air pollution problem in Hong Kong. IEEE Trans Vis
Comput Graph 13(6):1408–1415
Sharko J, Grinstein G, Marx KA (2008) Vectorized radviz and its application to multiple cluster datasets. IEEE Trans Vis
Comput Graph 14(6):1427–1444
Slocum TA, Mcmaster RB, Kessler FC, Howard HH (2009) Thematic cartography and geographic visualization, 3rd edn.
Pearson Education, London
Tominski C, Abello J, Schumann H (2004) Axes-based visualizations with radial layouts. In: Proceedings of the 2004 ACM
symposium on applied computing (SAC’04), pp 1242–1247
Wang C, Xiao Z, Liu Y, Xu Y, Zhou A, Zhang K (2013) SentiView: sentiment analysis and visualization for internet popular
topics. IEEE Trans Hum Mach Syst 43(6):620–630
Watson DF (1992) Contouring. Pergamon Press, New York
Yannier N, Basdogan C, Tasiran S, Sen OL (2008) Using haptics to convey cause-and-effect relations in climate visualization.
IEEE Trans Vis Comput Graph 1(2):130–141
Zhou W, Cohan DS, Henderson BH (2013) Slower ozone production in Houston, Texas following emission reductions:
evidence from Texas air quality studies in 2000 and 2006. Atmos Chem Phys 13(7):19085–19120


