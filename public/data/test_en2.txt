A Visual Analytics Approach for Word Relevances in Multiple Texts
Abstract—We investigate the problem of analyzing word fre- quencies in multiple text sources with the aim to give an overview of word-based similarities in several texts as a starting point for further analysis. To reach this goal, we designed a visual analytics approach composed of typical stages and processes, combining algorithmic analysis, visualization techniques, the human users with their perceptual abilities, as well as interaction methods for both the data analysis and the visualization component. By our algorithmic analysis, we first generate a multivariate dataset where words build the cases and the individual text sources the attributes. Real-valued relevances express the significances of each word in each of the text sources. From the visualization perspective, we describe how this multivariate dataset can be visualized to generate, confirm, rebuild, refine, or reject hypothe- ses with the goal to derive meaning, knowledge, and insights from several text sources. We discuss benefits and drawbacks of the visualization approaches when analyzing word relevances in multiple texts.
INTRODUCTION
Getting an overview of the word-based content of a text corpus can be a challenging task, in particular, when a text contains many words. Finding similarities between multiple text corpora based on occurring word frequencies is even more difficult, however, can give valuable insights for further text explorations, e.g., with a more semantics-based approach. An analyst might, for example, be interested in the similar word- based content of politicians’ speeches or scientific publications in order to naively judge what those texts have in common.
Visual analytics [1], [2] is an emerging discipline, combin- ing several high-level concepts such as data analysis, visual- ization, the human user, and interaction techniques, targeting the discovery of insights into large data sets and data bases. Algorithmic analyses are required to first reduce the amount of data to meaningful rules and clusters, i.e., to compute patterns from the data. These derived patterns are typically still too large to be understood directly by an analyst. Consequently, an interactive visualization technique is needed to further support an analyst in rapidly getting insights and knowledge and, based on the gained hypotheses about the data, to reapply algorithmic analyses for a more fine-granular parameter setting. In this work, we focus on this ‘user-in-the-loop’ strategy to explore text data by providing a combined overview for multiple texts.
Our approach is able to process the raw text sources by tokenizing and sentence splitting, followed by recognition of named entities. Then, a filter by stop word elimination and the distribution of characters is applied. Finally, a relevance metric value for each word in each text corpus is computed by using, for example, the term frequency, the term frequency inverse document frequency, and also the result of combining them with a support vector machine. This extraction and mapping of words to relevance values generates a multivariate dataset that can later be used to visually encode the resulting tabular data to a visualization technique from a larger repertoire such as scatter plot matrices [3], [4], parallel coordinates plots [5], [6], [7], glyph-based techniques [8], [9], [10], or more words-in-focus representations based on word clouds like RadClouds [11] or ConcentriClouds [12].
Finding insights into the text data by visualization only is challenging when the algorithmic pre-analyses cannot be applied, however, it is also challenging when only the analyzed data with its textual output is available, without further graph- ically representing it to a human viewer. These two analysis stages benefit from each other, i.e., their combination leads to synergy effects, where the human observer is faster and more accurate when performing typical tasks on the similarities of multiple texts. However, not all visualization techniques are equally well suitable, hence, we discuss the benefits and drawbacks of the existing approaches based on the task at hand, i.e., if analysts are more interested in data correlations or in the word representation itself. In this respect, we also discuss algorithmic, visual, as well as perceptual scalability issues to find the most appropriate visualization technique for word relevances in multiple text corpora.
RELATED WORK
Analyzing multiple text corpora for commonalities is a challenging task since they typically become very large, con- taining a huge number of words. Getting an overview of the word-based content cannot be provided by visualization alone. A data preprocessing step has to be applied in order to compute word relevances expressing how significant the individual words are in each text source. This is important to get a condensed overview of all contents in combination.
Generating such a relevance dataset finally leads to data in a tabular form consisting of rows and columns that model a multivariate dataset. There are several existing visualization techniques to deal with this data type. Scatter plot matrices (SPLOMs) [3], [4] easily allow one to see pairwise cor- relations between words occurring in two text corpora by using a pixel-based two-dimensional representation. Parallel coordinates plots (PCPs) [5], [13], [14], [6], [7] use a polyline for each word connecting coordinates on parallel axes that describe the text corpora. The heights of the intersection points depend on the calculated word relevances. Although PCPs visually scale to many text corpora, they are not useful to compare all pairs of attributes. Glyph-based techniques like Chernoff faces [9], software feathers [8], star plots [15], or leaf glyphs [10] do not allow one to easily detect correlations nor do they scale to very many words and text corpora. They benefit from the ability to give a direct view on the words themselves in their textual form. This again is challenging in PCPs or SPLOMs because visual clutter is produced [16] by overplotted texts. Zhou et al. [17] illustrate a method that plots text labels on the polylines of parallel coordinates that might be easily adaptable to our dataset but suffers from visual clutter, making it hard to explore the data for correlations.
Recently developed techniques are based on the concept of showing all of the most relevant words in focus in their textual representation while indicating the word relevances in each of the text corpora by additional visual attributes. For example, the RadCloud visualization [11] uses a radial layout to place the text corpora representatives on the circle circumference. Vectors are computed that take into account the word relevances to place each word inside the circle. The vector-based strategy gives the impression in which category (text corpus) a word is most likely to be used. However, this layout strategy is not unique nor does it guarantee that words do not overlap unless a more advanced layout technique for word groups is used. Another similar concept makes use of Venn diagrams and is denoted by ConcentriCloud [12]. In this technique, the most relevant words occurring in corresponding categories are accumulated and shown closer to the center of the view. This technique comes with the drawback of too many subregions produced by the 2n possible subsets of n text corpora.
Our experience shows that, although many advanced tech- niques exist, a visualization for word relevances in multiple text corpora strongly depends on the user task, i.e., if the user is more interested in data correlations or in word-in-focus approaches showing each word in its textual representation. The intention of this paper is to illustrate how we can apply the visual analytics approach [1], [2] to find insights into such multivariate word relevance data to offer opportunities for future research in this direction. Moreover, these insights serve as intermediate steps in the visual analytics process, that means analysts can refine their hypotheses, adjust data preprocessing parameters that lead to a possibly different visual appearance of the data. Consequently, we argue that a combination of several techniques is required which will be discussed in Section V.
VISUAL ANALYTICS APPROACH
Visual analytics is a promising field of research [1], [2] supporting the combination of several processes and sub- processes with the goal to derive knowledge from data. It combines algorithmic analyses exploiting concepts from data mining and data retrieval, visualization techniques supporting pattern recognition by human users with their perceptual abilities, as well as interaction techniques, making this a useful strategy to bring large data into an understandable form.
Multiple Texts Data Model
In this work, we deal with multiple text sources. From a user’s perspective, there just need to be visualizations of this text. However, to find out which visual representation fits the data at hand, we develop a common data model that serves as input for multiple visualizations.
As a first step, we extract words, which provide the basis for computing metric values expressing their occurrence rele- vances in each of the text sources. We model a single text T asafinitesequenceSTof1n2Nwords,i.e., S :=(w ,...,w ), T1n
where each word is composed of characters from a finite alphabet S, i.e.
wi := (s1,...,sk)
with 1k2N and si 2S. Multiple texts are modeled as a
set
M := {T1,...,Tm}
containing 1  m 2 N single texts Ti, where the characters in all of the texts are elements of the same alphabet S. It may be noted that the texts Ti do not necessarily have to be of the same length.
From the set of texts M, we compute relevances of all occurring words wi. These relevance values are modeled as rwi,Tj 2 R and treated as multivariate data.
B. Visualization Techniques
This multivariate data is full of patterns, but without an overview in the form of a visualization, it is difficult to explore the data for correlations among the words, their relevances, and their belonging to text sources, i.e., categories.
Parallel coordinates plots (PCPs) are the most straightfor- ward way to represent the data, but only axis pairs can be compared directly, i.e., it is not easy to explore the data for correlations between more than two text sources. This can only be achieved by interaction techniques such as brushing and linking. Moreover, PCPs are not word-in-focus techniques but they scale to very many words, which can hardly be achieved for word-in-focus techniques like word clouds, i.e., RadClouds or ConcentriClouds, originally designed for showing word frequencies in several text sources.
Scatter plot matrices can show all pairwise comparisons and scale to many words since they use a pixel-based approach to indicate the correlations, but it is also not that easy to find correlations among words in more than two text sources without suitable interaction techniques.
In addition, glyph-based techniques have some drawbacks because they do not scale to very many words, but as a benefit, they allow us to place the words on top of each glyph, i.e., they can be transformed into a word-in-focus technique. Unfortunately, correlations are hard to find with glyph-based techniques.
One way to display the multivariate data. Word relevances are visually encoded in font sizes and distance from center. Each text source is displayed separately making comparison and correlation exploration tasks hard to solve (even with brushing and linking).
Word-in-focus techniques, such as word clouds, give a visually appealing overview of a text and are easy to un- derstand. However, there are some visualization challenges, for example, words require considerable space and quickly become unreadable if they overlap or the positioning of a word according to its relevance in the different text categories is ambiguous. Furthermore, comparisons of single words are difficult, since different relevance values for the words in each of the categories lead to different font sizes and layouts, as depicted in Figure 1.
Consequently, in our work we would like to take the benefits from all of the techniques and, hence, provide views that allow analyzing the word relevances in the light of several visualization techniques.
Table I illustrates visual encodings, scalability issues, and correlation detection tasks as are supported by the different visualization techniques for multivariate data. Please note that the specified values for the number of words and text corpora are to be understood as rough estimates. These numbers are based on literature research and our experiences with the visualization techniques.
C. Interaction
All presented visualization techniques result in plots that can easily be read by users in order to get an overview of the data. Interaction through brushing and linking can be used with all different representations in order to make use of their specific advantages and maintain a coordinated view on the data. However, visual analytics is not just about overview, but also detailed information. When the user detects a word wi with an interesting relevance rwi,Tj, it is helpful to find out how it relates not just to the different categories, but also in which context it appears in each text source T.
Massively scaling visualizations, such as PCPs and SPLOMs, offer overviews of large data, but their interaction techniques are not well suited for the selection of individual words. This is due to their pixel-based layout approach and rendering overlaps. Word-in-focus techniques provide a more user-friendly way of selecting individual words for detailed inspection. Such details are added on user request using simple interaction techniques such as mouse hover (see tooltip in Figure 2).
A RadCloud visualization where detailed word relevances are shown as tooltips.
We extend our previous data model by storing not only relevances, but also the exact positions of word occurrences in every text source. Users can therefore read the contexts of all word occurrences in all source texts. This gives them the ability of starting with a high-level overview and drilling down to the actual source documents for further analysis and comparison, as shown in Figure 3 a.
Analyzing text also includes the discovery of relations between multiple important words. This could for instance be the case when trying to get a common view of the chain of events by analyzing multiple reports of the same incident, but from different perspectives. Therefore, we also provide an additional level of interactivity by searching for the most relevant entities in the word’s source text neighborhoods (see Figure 3 b). These neighborhoods are defined either by sentence boundaries or a constant word count radius.
D. Algorithmic Analyses
Our approach facilitates analysts in exploring and under- standing word relevances in multiple text sources. It provides an overview of word-based similarities in a text corpus and serves as a starting point for further text analyses. It supports multiple source formats as well as different languages. Further- more, our approach supports several common text input file formats, such as plain text (TXT) or named entity recognition files (NER or TXP), for different languages.
Since we are dealing with textual data and multivariate data based on various categories, it is necessary to distinguish documents from different categories. Without the information about the belonging to categories, there would only be a need for one static individual representation that does not allow exploring the data for correlations among the categories (text sources). There are several possibilities to achieve this.
For example, a support vector machine (SVM) could be trained to classify the documents. This would require a training set, and manual annotations for an initial classification. There are already robust tools [18], [19] available that could be used. A further possibility could be to set manual categorization annotation. For small data sets, a manual categorization might be faster because of the lack of subsequent steps such as the training and application of software tools. However, that is not our focus, therefore, the source data is required to already be categorized and separated into different directories with the category names.
Interactive drill-down and analysis: a concordance view showing multiple occurrences of a selected word in their original source document contexts (a) and the result of a search for recognized entities in the neighborhood of a selected word (b).
Named entity recognition: Once a categorized and sepa- rated text corpus is loaded into the system, it is processed in a linguistic analysis pipeline, consisting of tokeniza- tion, sentence splitting, and named-entity recognition. We use the Stanford CoreNLP tools1 for this purpose with standard English, German, and Italian models to detect the names of persons and locations. This information can be used for further text analyses.
• Data extraction: To obtain a relevance metric for vi- sualizing the data, we perform further analysis steps to get a set of words with their respective weight in every
category.
• Filtering: The aim of filtering the text corpus is to
remove irrelevant words to adapt it to different analysis contexts. For this purpose, our approach provides several lists of stop words for different corpus languages, such as word lists for English, German, Italian, or Italian legal documents. As a next step, we filter out words with a minimum and maximum word length, since a word that is shorter or longer than the allowed length is in many cases not plausible. Many search algorithms for internet forums also use this technique to ignore search patterns that would yield too many results. We also filter out words with a maximum number of equal consecutive characters. Many languages have words with double letters or like the German language, also allow triple letters. If a word has even more equal consecutive characters, it is probably an error in the input, such as it can happen when optical character recognition (OCR) is used to digitize documents.
• Relevance metrics document: After all words are pre- processed, our approach provides several methods to cal- culate a metric that determines the relevances of the words to the different categories. One of these methods is term frequency (TF). It determines that the most relevant words are also the most frequently used ones. This works well for single documents, however, if we are working with multiple documents, TF has its shortcomings. Assuming that there are two documents A and B, and A is much longer than B, and A contains a relatively unimportant word, it can appear more often than an important one in B. To resolve the shortcoming of TF, the analysts can select the term frequency inverse document frequency (TF-IDF), where TF-IDF is the product of TF and the inverse document frequency. TF-IDF considers in how many documents a word appears and the more documents contain it, the less is the weighting. As a result, words that make a document different to many other documents in the corpus get a high weighting.
The user interface shows how we analyze words for the occurrence probabilities in different catefories, i.e., text corpora. First, we choose a metric (a) and set its parameters (b). Then we calculate the relevances (c) and check the metric details (d). We can repeat steps (a) to (d) until we achieve a satisfactory result and generate an export (e) for visualization techniques that are capable to deal with multivariate data.
Support vector machine: Furthermore, we implemented a third relevance metric that can either be based on TF or TF-IDF. It uses an SVM to determine the relevances of the words. For this purpose, we use a .NET imple- mentation of libsvm [20]. Normally, an SVM is used to determine the document’s belonging to categories [21]. However, if the words are the features, the SVM could determine the most relevant ones in a category as part of its support vector. Typically, the SVM would first be trained and later the results could be used to categorize documents, however, to get the mentioned support vector the last step is not needed.
As aforementioned, the input documents are already cat- egorized by their file location. Typically, an SVM works with exactly two categories; however, Sebastiani [21] describes how to address this issue. For n categories, the SVM is trained n times by labeling all documents from the targeted category ci as belonging and all documents from other categories cj as not belonging.
An SVM expects vectors that are labeled as belonging to a category. Our approach creates a vector for each document and the dimensions are set by getting the entire list of all extracted words. The TF or TF-IDF value of a word in a document is then the value of the respective dimension. If a document is part of the category to be examined, it is labeled with a value of +1, all others are labeled  1. The importance of each word in each category is then calculated as the sum of values that the SVM yielded for the vectors.
After the training, the quality of the SVM’s model is assessed by performing a five-fold cross validation on the training data itself. The result is then displayed, so the user can select other settings in case of an unsatisfactory accuracy. This is an important feature, because the more accurate the SVM classifies the documents, the better it discovered the words that distinguish them.
• Result format: After the relevance metric is calculated, analysts can explore the results in a metric view as depicted in Figure 4. The words are listed in a table with their corresponding relevances to the different cate- gories (c). Rows represent the words and columns the different categories. Since in this example the analyst selected an SVM-based relevance metric (a and b), the used settings and the resulting accuracy of each class as well as the average are shown in an information area (d). This facilitates analysts to evaluate the quality of the results and as reference for new settings. Furthermore, the calculated data can be exported as soon as the first metric has been calculated, either in a custom file format or as a plain or annotated CSV file (e). The analysts can choose a maximum count of words to save from each category. This setting is important to keep the export file size within reasonable limits and to restrict the amount of information. In addition, if the analysts desire to compare results between different types of metrics, it is easily possible by updating the weights with another calculation method and exporting the results again. The generated data can then be used as input for visualization techniques that are capable to deal with multivariate data.
A splatted parallel coordinates plot, showing the relevances of all words (depicted as polylines) in relation to different document categories (axes). The axes scale from 0 at the bottom to 100 percent at the top. The sum of every category specific relevance for each word results in a total of 100 %. Since the individual lines have been splatted, only a density field is visible. This density is mapped to a color scale: (green corresponds to low, yellow to medium and red to high density).
APPLICATION EXAMPLE
We applied our visual analytics system to multiple text corpora from Wikipedia2 focusing on invertebrate animal species. In this scenario, we work with the eight text cate- gories Annelid, Arthropod, Cnidaria, Ctenophora, Flatworm, Mollusca, Nematode, and Sponge. We use English for natural language processing with the goal of named entity recognition. Words and entities with a minimum length of three and not more than 30 characters are extracted from the texts. Numbers are not allowed in the extraction process and are hence filtered out right from the beginning.
After the words have been extracted and the TF-IDF rel- evance metric has been applied (see Figure 4 a), we store the resulting data set (e). We then train an SVM with the previous metric values and test the accuracy with which the individual documents are categorized (a, b, and d). The graphical user interface displays a scrollable table of words with relevance values for each category (c). We adjust the SVM parameters (b), either manually or with a grid search, and recalculate the metric values iteratively until we reach a satisfying accuracy (d). We then store the generated SVM values as a new data set (e).
The generated metric values are multivariate data, which can be loaded into different kinds of visualizations. A text data analyst might be interested in correlations of words in at least a pair of categories, which is difficult to do in a textual representation or also in pure visualization concepts like in the RadCloud approach [11]. Consequently, we pro- vide the analysts with several views that support them with large multivariate relevance datasets. With the user-in-the-loop approach, analysts can build hypotheses about the data and then go back to refine parameters in the data extraction and relevance computation. This leads to an updated multivariate data visualization that helps rapidly detect visual patterns that can be mapped to data patterns in order to analyze the data.
Figure 5 shows a parallel coordinates plot for all contained words and all categories (11,563 unique words). Flatworm, Cteneophora, and Sponge are only represented by a small amount of words, leading to clearly distinguishable green lines. Anthropod and Cnidaria, however, contain many with different relevances, generating yellow-green areas, with some orange-red lines around 100, 50, 30, and 0 percent. While this visualization scales well, correlations between words and categories are hard to detect: many words are only relevant in a single category, leading to red areas with high line densities at the bottom of the plot.
To get more detailed information about the relations of specific words, the analyst can switch to a word-in-focus visualization, as in Figures 1 and 2. Here, we can select single words and check whether they play an important role in any other category. Using brushing and linking, the parallel coordinates plot could highlight the single line, belonging to the selected word. In representations like RadClouds, a word’s position itself represents its relevance. This can however lead to misinterpretations, as for instance, in Figure 2, the word eggs is located in the center but does not seem relevant for the categories Annelid and Sponge (as they do not lay eggs). Having a tooltip appear with the relevance values helps against such misunderstandings, as well as showing SPLOMs and PCPs synchronously.
If the analyst wants to explore the word’s neighborhoods or contexts in the source texts, our tool provides detailed views (see Figure 3). After having interactively experimented with the data analysis components and the visualization techniques, the analyst can go back to the first extraction and metric calculation stage. They can then generate relevance data with a different focus and return to the visualization stage for further exploration and discovery of correlations.
DISCUSSION AND LIMITATIONS
To design the visual analytics approach we also have to consider drawbacks worth investigating and worth discussing.
• Algorithmic scalability: We have to apply several data preprocessing algorithms that sometimes have high run- time requirements. The runtime depends on the number of different text sources but also on the number of different extracted words that have an influence on the visualization methods and their interaction techniques.
• Visual scalability: With sufficient time for preprocessing the data (e.g., by overnight runs), the generated multivari- ate data can be visualized. However, the question can be raised how many words and categories are displayable at the same time? This is also one reason why we support several visualization techniques for the multivariate data. In the future, visual scalability could be enhanced by better linking these techniques with each other.
• Perceptual scalability: Visual features are typically ap- plied to produce useful visual encodings. However, de- pending on the number of different data entities, we might need several different colors or shapes. However, there are perceptual limits to the number of colors or shapes that can be distinguished. This is a particularly important aspect for glyph-based techniques or word clouds (word- in-focus techniques).
VI. CONCLUSION AND FUTURE WORK
In this paper, we investigated the problem of analyzing multiple text corpora algorithmically as well as visually by means of a visual analytics approach. Word relevances are computed for each word and in each of the text corpora. Based on these computations, we generate a multivariate dataset consisting of rows and columns indicating the words and their text corpora. Apart from changing parameters in the algorithmic processes, a user can also have a different look on algorithm outcomes by means of different visualization techniques for multivariate data such as scatter plot matrices, parallel coordinates, glyph-based techniques, or more word- in-focus techniques like the recently published RadCloud or ConcentriCloud approaches. The focus of this paper is to explore the design space from an algorithmic but also from a visualization perspective and to give further suggestions for future directions.
For future work, we plan to design an approach that is able to combine all the visualization techniques in a multiple coordinated views system that allows having perspectives on the same multivariate data using different visualization techniques. Moreover, further interaction techniques could be provided for both the algorithmic as well as the visualization part of the approach. Finally, a user study should be conducted in order to find out which visualization technique works best for a certain kind of task and if the system as a whole is usable to navigate, browse, and explore the multiple text data.
ACKNOWLEDGMENTS
This work was supported by the German Research Foun- dation (DFG) within project B01 of SFB/Transregio 161 and under grant WE 2836/6-1.
