Gaze-Based Annotations for Reading Comprehension
ABSTRACT
We study eye gaze movement behavior during paper reading and generate a series of annotations from a user’s reading features: gray shading to indicate reading speed, borders to indicate frequency of re-reading, and lines to indicate transitions between sections of a document. Through a user study, we validate that our SocialReading system that shares teachers’ gaze data for an academic paper can improve students’ reading comprehension of that paper.
Author Keywords
Eye Tracking; Fixation; Social Computing.
ACM Classification Keywords
H.5.m. Information interfaces and presentation (e.g., HCI): Miscellaneous.
INTRODUCTION
People often create manual annotations when reading physical books and documents. The annotations not only help them with identifying or remembering some important, interesting or hard to read passages, but can also supply them who re-read, or others who read, the same book or document with deeper understanding [4,9]. Digital annotations for a document can also be useful, e.g., highlights of important passages that are shared with other readers who may share interests. Hence there is the potential for a reader to take advantage of previous readers’ experiences and understand a document through a more focused reading process and with greater efficiency. Markitup [4] is a crowdsourced reading tool that projects digital annotations on artifacts, such as physical books. This enables individuals to read and annotate in a collaborative environment and benefit from the knowledge in a social context. However, such a tool uses specialized hardware, limiting deployment and usage.
Traditional manual annotations also have limitations. For example, highlighting a sentence or making a comment takes some manual physical and cognitive effort and cannot reveal readers’ visual attention behaviors (e.g., whether the reader just scanned the page or did a focused deep read). In contrast, gaze data can indicate people’s visual attention, and visualizations of gaze data can be used as a new kind of annotation. Text 2.0 framework [1] creates annotationsfrom a user’s gaze data in real time, such as displaying or highlighting figures when the user gazes at related text descriptions. Others have used eye tracking to monitor what and how many words users read and to detect the type of document being read [6, 7]. Eye tracker and OCR (optical character recognition) techniques have been used to determine which lines users read or skim, to generate document-level annotations (e.g., reading duration) automatically [3]. However, leveraging gaze-based annotations to improve reading comprehension has not been explored.
In this context, we contribute an exploration of gaze-based annotations that visualize visual attention features such as reading speed. We conducted a study of our SocialReading system in which we share teachers’ (experts) gaze-based annotations with students (non-experts). Although teachers sometimes have different reading patterns due to their expertise and experience (e.g., skip sections), we validated that viewing the teacher’s annotations resulted in increased similarities in the reading patterns of students, and improved students’ comprehension of the reading material.
GAZE-BASED ANNOTATIONS
Reading-related measures
For this work, we explored visual attention with academic papers and defined each title (or subtitle), paragraph (including references), and figure as an area of interest (AOI). The remaining areas were mostly blank and only infrequently viewed, and therefore we did not code them in our study. Then we record raw gaze data located in each AOI, and analyze them to obtain different measures. Reading-related measures, such as reading speed and reading count, are the most discriminative features for distinguishing very comprehensible from barely comprehensible text passages [2]. Inspired by this research, we define two reading-related measures: (1) Intra-AOI measures: reading speed for a single AOI (a specific part of a document); reading count (frequency) for each AOI. (2) Inter-AOI measures: switching frequency between two AOIs. It can indicate tight semantic relationships among different parts of a document, and is a useful navigation aid for active reading [9].
Reading speed on each AOI
Reading speed can reflect reading interest; for example, users may glance at some boring or well-understood parts of a document and go through them quickly. We denote reading speed for each AOI as the ratio of saccade length and dwell time. Here saccade length is the sum of thelengths of all saccades on the current AOI including both regression (i.e., back tracking) and skimming saccades (i.e., forward tracking) that can help a reader achieve a better understanding of the information [2, 10]. If the dwell time on the current AOI is longer than a predefined Intra-AOI threshold (e.g., 1s), we record the reading speed over this period of dwell time, because a short dwell time often means these are spurious glances and not helpful for reading. For example, users will often scroll up and down in a document browser. During this process, their gaze will be located in several AOIs for very short durations, with little specific reading activity occurring. We not only use the threshold to exclude this “noisy” data, but also use it to adjust the visualization results. For example, reducing the Intra-AOI threshold will visualize more AOIs to highlight low reading speed, and it may be distracting, causing readers to focus on more parts of the paper than necessary.
Reading count for each AOI
We denote the reading count for each AOI as the number of times a user’s gaze returns to that AOI after a period of time of gazing elsewhere (longer than the Intra-AOI threshold). It can indicate difficult passages that need to be re-read [8].
Switching frequency
We denote switching frequency as the number of times a user’s gaze switches between two AOIs. Here we define “Source AOI” and “Target AOI”. If the user’s gaze is located in one AOI for longer than the Intra-AOI threshold, the AOI is the source AOI; then if the gaze moves to another AOI and remains there for longer than the threshold, that AOI is the target AOI. This helps us filter the data, in this case, for spurious switching. For example, when a user focuses on an AOI, her gaze will unconsciously shift to other AOIs (visual distractors that may attract bottom-up attention [5], e.g., a salient title located nearby) for a short while and then switch back. In addition, two AOIs have tight relationships will be close to each other. So, if the duration time between the gazes departing the source AOI and arriving at the target AOI is lower than a predefined threshold (Inter-AOI threshold, e.g., 10s), we indicate that these AOIs are related and increase the switching frequency between these two AOIs by one. Note that we also filter out natural transitions from one paragraph to the next paragraph in the paper, except for cycles where the reader goes back and forth between adjacent paragraphs.
Visual Encodings
We propose the following gaze visualizations to encode the reading-related measures: (1) Gray shading for reading speed: indicates how quickly a user reads a specific AOI (darker indicates faster). (2) Border for reading count: indicates how many times a user focuses on an AOI. Thicker borders around an AOI, indicates a higher reading count for that AOI. (3) Transition line for switching frequency: indicates how many times the user switched between reading two different AOIs. A thicker line indicates more switches between the two connected AOIs. Color indicates links between different pairs of AOIs.
PROTOTYPE SYSTEM TM We built our SocialReading system using an SMI iViewX RED eye tracker and its SDK to record users’ gaze data when reading the paper. We auto-calculated our gaze measures, and projected the gaze data onto the corresponding AOIs as annotations (Figure 1).Raw gaze data is shown in Figure 1(a): the red solid circles represent fixation points, and the red lines connecting them represent the saccades. Figure 1(b) shows the corresponding gaze annotations: gray shade of each paragraph indicates reading speed (lighter means slower), border width indicates reading count (thicker is higher), and colored semi-transparent lines that link two paragraphs indicate gaze switching frequency (thicker is higher). Parameters of the visualization can be configured by the viewer, such as thresholds, transparency, hiding/displaying different visualization layers, to enable the best visual effects. For example, we hide the fixation points and saccades (raw data), and allow users to view our 3 visualization types.
USER STUDY
Participants
Our study has two kinds of participants, two faculty members (one female and one male, ages between 35 and 45, with similar academic backgrounds and expertise) and 30 students (11 females, 19 males, aged between 18 and 37 years, M=24.8, SD=4.6) all from our local participant pool. None of them had any visual impairment. All participants were native English speakers, and had similar backgrounds in HCI. Each student read at least one academic paper every week. We conducted a reading pre-test to screen out student participants who read a 2-page academic paper very quickly(e.g., < 6 min) or slowly (e.g., > 30 min). These checks were performed to identify any obvious differences in reading experiences and general reading abilities.
Apparatus
We used the SMITM iViewX RED desk-mounted eye tracker (gaze sampling rate was configured to 60Hz) to record gaze data. Its operating distance (between user’s gaze and monitor screen) is about 40 to 50cm. In this context, we measured an average accuracy of 0.5 degrees. We selected one academic 4-page paper from the SIGCHI archives for participants to read, because it had clear and compact logic structure and layout, which made it easy to measure and visualize gaze data. It was shown on a 21-inch LCD screen with a resolution of 1680×1050.
Procedure
Faculty member participants read the paper first with no time limit. After reading, each wrote a brief summary of the paper, and answered a questionnaire to assess their reading comprehension. We also recorded their gaze data during their reading process, and visualize them as annotations. A between-subject user study design was chosen to investigate the effect of gaze-based annotations on students’ reading comprehension. Hence, student participants were divided into two groups randomly, and then some group members were swapped to balance age and gender. All students read the same paper that the faculty read. There was no time limit, so that each student could read at her own natural reading speed. The students in the control group did so without any annotation. Those in the experimental group received the annotations based on faculty members’ gaze data. After paper reading, we used both objective and subjective measures to evaluate students’ reading comprehension. Students were asked to answer the comprehension questions about the paper by recalling what they read (objective measure). Each student was also asked to write a brief summary of the paper, and the summary was evaluated by faculty members for reading comprehension quality (subjective measure).
Results
Expert gaze annotations help non-experts improve their comprehension performance
In the questionnaire, there were 5 fact-finding (multiple- choice) comprehension questions, whose answers could be found at related sections of the paper. For example, one question was “what is the paper’s main contribution”, and the answer was in the Introduction. Correct answers were given a score of 1, for a total possible score of 5. A Mann- Whitney U test showed a significant difference in the scores for the control group (M=3.0, SD=1.3) and experimental group (M=3.9, SD=0.8); U=61.0, p<0.05.
We asked 2 faculty participants to independently evaluate student summary quality with a 5-point rating (1 is the worst, and 5 is the best). They evaluated the students’ summaries similarly, and we averaged their ratings. A Mann-Whitney U test showed a significant difference in theratings for the control (M=2.2, SD=1.3) and experimental groups (M=3.4, SD=1.2); U=54.0, p<0.05.
Expert gaze annotations influence non-experts
Our 4-page test paper had 77 AOIs (AOI1 to AOI77), but 23 were very tiny, in which little gaze data was captured (e.g.,titles/subtitles), or were less important for reading comprehension (e.g., references). Accordingly, an analysis comparing the reading similarity (e.g., reading speed) across all AOIs for our two conditions (control and experimental group), did not reveal any significant results. To better understand why reading comprehension likely was higher for the experimental group, we focus in on a more fine-grained analysis of particular AOIs. Due to the limited paper length, we just look at a subset of gaze annotation on the same page of the read paper from one faculty participant (see Figure 2). AOI48, AOI50 and AOI54 had salient annotations, such as light gray shadings and thick borders. They indicate that the faculty member read slowly and re-read these AOIs, and also switched gaze between AOI48 and AOI50, and AOI50 and AOI54.
We considered 4 metrics to explore participants’ gaze patterns for each AOI: average fixation duration (ms), average saccade length (px), re-reading frequency and switching frequency. We conducted a statistical analysis with Mann-Whitney U tests: (1) For AOI48, there was a significant difference in average fixation duration for the control (M=102.5, SD=77.8) and experimental groups (M=156.8, SD=49.4); U=53.5, p<0.05. (2) For AOI50, there was a significant difference in average saccade length for the control (M=62.7, SD=41.3) and experimental groups (M=91.6, SD=30.4); U=58.0, p<0.05. (3) For AOI54, there was a significant difference in re-reading frequency for the control (M=0.7, SD=0.5) and experimental groups (M=1.0, SD=0.4); U=77.5, p<0.05. There was also a significant difference in switching frequency between AOI54 and AOI50 for the control (M=1.3, SD=1.0) and experimental groups (M=2.0, SD=0.8); U=77.5, p<0.05. Accordingly, the metrics of the faculty member were: average fixation duration in AOI48 was 171.4; average saccade length in AOI50 was 110.8; re-reading and switching frequency for AOI54 (to/from AOI50) was 1 and 2, respectively. We also found that gaze patterns of the experimental group in these AOIs were similar to (and likely influenced by) the faculty members, but different from the control group.
Participants find gaze annotations are helpful
From the questionnaire, participants in the experimental group (15 participants) rated (5-point scale, 1= not helpful at all, 2=slightly not helpful, 3=neutral, 4=very helpful, 5=extremely helpful) each type of visualization: gray shading (1, 4, 5, 2, 3 participants rated 1-5 respectively), border (1, 6, 4, 3, 1 participants rated 1-5 respectively) and transition line (3, 3, 4, 3, 2 participants rated 1-5 respectively). 8 participants rated the gray shading (reading speed) to be the most helpful for reading comprehension. 9 participants rated the gaze annotations as being “very helpful” for improving the quality of their summary.
After the questionnaire, we interviewed the experimental group participants. When speaking about the gray shading , some of them said, “the reading speed visualization is the most visually salient”, “gray shading distinguishes importance of paragraphs”, and “professor’s reading speed is a good proxy for what is/isn’t important, fast reading means content s/he disregards, and slow reading means content s/he deems important”. When speaking about the summary writing using the gaze annotations, some said, “[gaze annotation] shows you the paper’s focal points”, and “If I and my professor agree that a paragraph is important, then I will know to include it in a summary”.
DISCUSSION
Our approach for visualizing expert readers’ gaze data can help non-experts to achieve improved reading comprehension. Here we discuss future modifications to our approach that we will explore. One participant said: “Do people re-read a paragraph because it is important, or it is poorly written? Also, some people can read an important paragraph quickly because they are familiar with the content”. To address this ambiguity, we will integrate our approach with semantic analysis of documents. This analysis can determine which passages may be difficult to understand (e.g., have professional terms or equations) or are important (e.g., include key words of the paper).
Reading behavior differences between expert and novice readers may reduce the usefulness of our approach. As a participant said: “Professors have expertise, so they may skip sections. This sometimes makes their visualizations less helpful for students”. We will explore the sharing other students’ gaze annotations with student readers as well.
When faculty members evaluating the students’ summaries for reading comprehension quality, they judged the quality based on their own criteria and experience. Hence, there may be some subjective judging. For example, if one student captured the main point of the paper, but this point was not valued by the faculty member, the student may have low ratings for the reading performance. We will refine our reading comprehension measure. Finally, in our study, the reading styles of two faculty members were similar to each other, so we merged two sets of gaze data before showing the annotations to students. We will continue to explore how to merge multiple annotations when experts’ reading styles are different, or visualize these differences for students.
CONCLUSION
We record users’ gaze data during their reading process, and then visualize them using reading-related features. We share the gaze-based annotations from faculty reading a paper with students who were asked to read the paper. We found that the gaze-based annotations resulted in greater reading comprehension and increased the similarity between the reading process for the students and faculty.
ACKNOWLEDGMENTS
This work was supported by NSF grant SES-0968566 and NSFC grant 61272308.
REFERENCES
1. Biedert,R.,Buscher,G.,Schwarz,S.,etal.Text2.0. Ext. Abstracts CHI 2010, 4003-8.
2. Biedert, R., Dengel, A., Elshamy, M.,et al. Towards robust gaze-based objective quality measures for text. ETRA 2012, 201-204.
3. Buscher G., Dengel A., Elst, L., et al. Generating and using gaze-based document annotations. Ext. Abstracts CHI 2008, 3045-3050.
4. Chircop,L.,Radhakrishnan,J.,Selener,L.,etal. Markitup: crowdsourced collaborative reading. Ext. Abstracts CHI 2013, 2567-2572.
5. Connor,C.E.,Egeth,H.E.,andYantis,S.Visual attention: bottom-up versus top-down. Current Biology 14, 19 (2004), 850-852.
6. Kunze,K.,Iwamura,M.,Kise,K.,etal.Activity recognition for the mind: toward a cognitive “quantified self”. Computer 46, 10 (2013), 105-108.
7. Kunze,K.,Utsumi,Y.,ShigaY.KiseK.,etal.Iknow what you are reading: recognition of document types using mobile eye tracking. ISWC 2013, 113-116.
8. Lenzner,T.,Kaczmirek,L.,andGalesic,M.Seeing through the eyes of the respondent: an eye-tracking study on survey question comprehension. Int'l Journal of Public Opinion Research 23, 3 (2011), 361–373.
9. Tashman,C.andEdwards,W.Activereadingandits discontents: the situations, problems and ideas of readers. CHI 2011, 2927-2936.
10.Vo, T., Mendis, B. and Gedeon, T. Gaze pattern and reading comprehension. ICONIP2010, 124-131.