Candidates of the national college entrance exam review lessons on a special train which will take them to exam site in Alihe Township from Dayangshu Township of Oroqen Autonomous Banner, north China's Inner Mongolia Autonomous Region, June 5, 2017. Over 600 students took the special train to attend this year's national college entrance examination, known as the Gaokao, due to kick off on June 7. (Xinhua/Wang Song)
BEIJING, June 6 (Xinhua) -- When China's national college entrance exam, gaokao, resumed in 1977, Huang Liang, breadwinner of his family, quit his job.
He was in his dormitory in a factory in southwest China's Chongqing when he heard the news through a loudspeaker.
The 32-year-old father took the exam in the bitter winter of that year and was admitted to Chongqing Normal University, one of 273,000 lucky people to get letters of college acceptance. With 5.7 million sitting the exam that year, the enrollment rate was less than 5 percent.
For the past 40 years, the exam has been decisive in determining who would find a well-paid job and successful career.
Gaokao was disrupted by the Cultural Revolution (1966-76) and its reintroduction by then leader Deng Xiaoping was a clear signal that times had changed. For over 40 years, the fierce but fair competition has been almost the only way for students from poor, rural areas to change their lives.
On Wednesday, 9.4 million students will take the world's largest exam in pursuit of their own dreams and join the drive for national rejuvenation, the Chinese Dream.
The restoration of gaokao changed the lives of tens of millions of people like Huang. And the country soon began reform and opening-up, which led directly to decades of growth.
From 1977 to 2016, 120 million Chinese enrolled in universities. "The average number of years in education for our labor force increased from 5.7 to 11.9," said Dai Jiagan, deputy director of the Chinese Society of Education.
"Without gaokao, we could not have reached our current phase of development. The fate of gaokao has been closely related to that of the country and the people."
In the first three years after the restoration of gaokao, more than 900,000 university students became backbone professionals in different walks of life in the country.
"This group of people were a key driving force in reform and opening-up," said Liu Haifeng, head of the school of education at Xiamen University in east China's Fujian Province.
"Despite its weak points such as the heavy burden on students, gaokao is a test that fits China's overall conditions and is a fair start for all."
"The successes of the past 30-odd years have a lot to do with the gaokao system," Liu added.
Cheng Fangping, a professor in history of education at Renmin University of China, agrees. He regards the normalization of China's higher education system as epoch-making, with an inspirational impact on reform, culture, science, thinking and law.
The building of discipline, the fostering of talent and the vision that has driven economic development, were all strengthened through gaokao, said Cheng.
Cooperation between colleges and industry and the emergence of university-centered development zones have all played their parts in the country's economic rise.
From 2011 to 2015, universities churned out nearly 20 million professionals, firing up high-tech and emerging industries. Vocational schools send nearly 10 million technical graduates out into the job market each year.
China has almost 3,000 higher-learning institutions. Of 9.4 million Chinese who sit the exam on Wednesday, around 4 million of them will be given the chance to continue their education. By 2020, 50 percent of gaokao entrants will find places at college.
Special dispensation is now guaranteed for under-privileged students and those with special needs, including those with disabilities. The system now better serves innovation-driven development and encourages creativity.
The exam, however, is not without its critics, mostly for overemphasis on grades and students being obliged to choose between science and liberal arts at an early stage.
China plans to overhaul its college entrance exam and university enrollment system by 2020, to improve fairness and transparency. In east China's Shanghai and Zhejiang Province, pilot reforms are under way in this regard.
Students from central and western parts of the country are now guaranteed more places in college, and there is a push to enroll more students from rural areas.
Gaokao continues to play its key role in enabling the nation to eradicate poverty by 2020 and pursue sustainable growth driven by innovation and consumption.
Sun Pishu, chairman of China's cloud computing and big data provider Inspur Group, went to university in 1978. After graduation, he worked in an electronics plant, the predecessor of Inspur. Ten years later, he developed the country's first small server. Currently, Inspur, based in east China's Shandong Province, supplies IT products and services for 108 countries and regions
"I was part of the second batch of students after the gaokao resumption. I was driven to serve the country and full of energy," said Sun.
 2015 13th International Conference on Document Analysis and Recognition (ICDAR)
The Eye as the Window of the Language Ability: Estimation of English Skills by Analyzing Eye Movement While Reading Documents
Kazuyo Yoshimura, Koichi Kise
Graduate School of Engineering,
Osaka Prefecture University, Japan yoshimura@m.cs.osakafu-u.ac.jp, kise@cs.osakafu-u.ac.jp
Kai Kunze
Graduate School of Media Design Keio University, Japan kai.kunze@gmail.com
Abs act-Reading-Iife log is a research  eld of analyzing our activities of reading documents to know more about readers and documents. In this paper we propose an implementation of reading-life log which is to estimate the English language skill by analyzing the activities of reading English documents. As input for the analysis, we employ eye movement information, because we consider the eye movement of skillful readers is far different from that of novices. From the experiments, we have found that the following two features are informative: (1) the sum of  xation duration, and (2) the sum of the velocity of saccades. By using these features the proposed method is to estimate the class of English skill from among low, middle and high, which are de ned based on the scores of English standardized test called TOEIC. From the experimental results with 11 subjects and 10 documents, we have been successful to estimate the class with the accuracy of 90.9%.
I. INTRODUCTION
People acquire a lot of information daily through reading. Thus by recording and analyzing reading activities, we are able to obtain various types of information about the reader including his / her interests and preferences. In addition, once the reading activities of many readers are recorded in relation to the contents of documents, we are able to consider the analysis of documents based on reading activities such as  nding interesting and di cult parts for many readers. In other words we can open up a new research  eld of document analysis through the mutual analysis of reading behavior and document contents.
Our research on reading-life log has started with the mo­ tivation mentioned above. In our research the main source of information about the reading activities is the eye movement, because we think it contains rich information about reading activities. By analyzing eye movement, we can obtain various types of information about his/her reading. The simplest is the amount of reading: how many words are read by a reader in a speci c time frame such as a day [17]. In addition, the analysis of eye movement allows us to segment reading activities  om others, to estimate a type of documents the reader is working on such as novels, magazines, newspapers and mangas [15]. The contents of read documents can also be recorded as a log of reading [13].
In this paper we are concerned with a higher level of reading-life log as compared to the previous approaches. As such a high-level log we have selected the estimation of a level of language ability by analyzing reading activities.
A simplest and traditional method for obtaining the lan­ guage abilities is based on paper tests. For the case of English, for example, standardized tests such as TOEFL and TOEIC are well accepted for this purpose. However, such tests require time to take. Another problem would be that the test is available not so often, say once a month, and thus it is not possible to have a "real-time" monitoring of the ability.
Our research attempts to build a method for easy and real­ time monitoring of the language ability with a special focus on English. The proposed method analyzes eye movement obtained through an eye tracker to learn/classify the English ability of readers. To be precise features about the  xations and saccades of eye movement while reading documents are employed for the analysis. The main contribution of this paper is that, though the experiments, we have found that the two features, the sum of duration of  xations and the sum of angular velocity of an eye, enable us to estimate the class (low, middle, high) of English ability with a high accuracy of more than 90%. All of the work described here was done under the permission of the research ethics committee of the graduate school of engineering, Osaka Prefecture University.
II. RELATED WORK
The strong relationship between reading and eye move­ ments is well explored in cognitive science and psychology [21], [11]. For example, Kligel et al. investigate correlations of eye  xations with cognitive tasks related to reading [14]. Most of the reading research in psychology however emphasizes on older adults or disabled [8], [7].There are only a few research publications centering around reading detection in mobile and stationary settings [5], [4]. As such detecting reading can be used as a very simple word counting mechanism, as there's a relation between time read and the read volume. Biedert et al. look into how people read text. They presented a method to discriminate skimming from reading using a novel set of eye movement features [3]. Their algorithm works in real­ time, deals with distorted eye tracking data and provides robust classi cation with 86% accuracy. They also showed a method to recognize text comprehensibility with an accuracy of 62% from gaze data recorded from multiple readers [2].
In a series of works, Biedert et al. studied ways to enhance the reading experience of the user. They presented Text 2.0 [1] as a reading interface that observes which part of the text is currently being read by the user and that generate appropriate
978-1-4799-1805-8/15/$31.00 ©2015 IEEE
251
 2015 13th International Conference on Document Analysis and Recognition (ICDAR)
 e ects (e.g. playing sounds). However, they do not evaluate what suitable interventions are to increase users enjoyment, comprehension or attention. Xu et al. apply eye movement analysis for document su aries, yet the enviroment is very controlled, e.g. the users need to rest their chin on a support when performing the reading task[22].
Concerning reading habits, there are some questionnaires based evaluations giving advice about effective reading tech­ niques to second language learners, as well as for children with reading disabilities [9], [l9].Hansen [10] reports on a series of studies on reading comprehension with rapid readers trained in the Evelyn Wood method. Several mention rigorous practice and steady increase in reading volume as one of the key factors to success [12].
There are also some efforts to infer the users expertise, language skill and other higher level cognitive activities using eye tracking [16], [18], [6].
III. PROPOSED METHOD A. Basic concepts
It is o en said that the eye is the window of the mind. We consider that this holds for our speci c purpose of estimation of English ability. Eye movement of persons with a high skill of English should be different from that of low skill persons. Thus we consider in this paper to establish a method of estimating the English skill by analyzing eye movement.
As measurement of English skill, we employ a standardized test called TOEIe. TOEIC is one of the well-known English tests, which consists of two parts: hearing and reading. The score ranges  om 10 to 990 as a result of the process of standardization of the raw scores. Thus the score is considered as a general measure of the English ability of a person.
One may ask the reason why we need to "estimate" the TOEIC score, because the easiest way is to take the paper test and receive the score. The answer is quite simple. Taking the test requires long time (more than two hours). Thus it is not possible for us to employ the test for  equent monitoring the English ability of a person. In other words, if we are able to estimate the score by the analysis of eye movement, it is possible for us to obtain the information about the English skill in real time.
Our ultimate goal is to estimate the score itself by cal­ culating the function  om features of eye movement to the score. However this is not so easy due mainly to the lack of the number of samples for learning the function. Thus in this paper we attempt to learn a classi er that takes as input features of eye movement to estimate the subject's class as a three class classi cation problem: low (less than 600), middle (between 600 and 800) and high (higher than 800).
B. Outline of p cessing
Figure 1 illustrates processing steps of the proposed method. First, eye movement data are obtained. For this pur­ pose, we utilize a stationary eye tracker when the subject reads a document on the display. Then these raw data are analyzed to segment eye activities into three states: blink,  xation and saccade, where the  xation and the saccade indicate a
I acquisition of eye movement I
 Fig. 1.
I

I
Ilearning I classificationI Overview of the processing.
I

analysis of eye movement
feature extraction
I
short stop of the eye and a rapid movement between short stops, respectively. Human reads text by repeating the  xation and saccade. At the third step, various features are obtained from these two states. At the last step, extracted features are employed for learning or classifying reader's English ability.
In the following, we explain the details of steps 2-4.
C. Analysis of eye movement
The raw data of eye movement is a sequence of eye gazes represented as the 2D coordinates in the coordinate system of the display. The  rst task is to analyze the data to convert them to a sequence of states: blink,  xation and saccade. The eye gazes located close with each other are combined into one  xation with the coordinates as the average of its members. Once the  xations are recognized, saccades are identi ed as the eye movement that connects two consecutive  xations. Blinks are recognized based, for example, on irregular eye movements. In our system, the above states are identi ed by the so ware called BeGaze, which is a product of SMI.
D. Feature extraction
Features are extracted  om the sequence of states. Table I shows the list of features we employed in our method. These features are employed to select the best combination to classify the subject's English ability at the next step.
E. Lea ing and class cation
This step consists of two phases: learning of the classi er and its application. As a classi er, we employ the support vector machine (SVM).
At the phase of learning, we  rst apply feature selection from a pool of features in Table I. To be more precise, we apply the backward stepwise selection. We start  om learning with all the features. Then the feature with the lowest correlation to the TOEIC score is removed if learning without it gives us a better or the same result. Otherwise the feature is kept and the feature with the next lowest correlation is tested. This process continues until no more features are removed. Note that we employ multiple documents read by a single subject to obtain enough number of blinks, saccades and  xations to calculate features.
At the phase of classi cation, we apply the learned SVM to the features in order to estimate the class of subject's English ability. Note again that the features are extracted from the eye movement for multiple documents for a better estimation.
252
I feature
End Time[ms]
Blink Count
Blink Frequency[coun s]
2015 13th International Conference on Document Analysis and Recognition (ICDAR)
TABLE I.
FEATURES.
explanation
elapsed time of reading
the number of blinks
the number of blinks per second
sum, average, maximum and minimum of the time of blinks
the number of  xations
the number of  xations per second sum, average, maximum and minimum of duration of  xations
sum, average, maximum and minimum dispersion of  xations
sum of the distance of saccades
the number of saccades
the number of saccades per second sum, average, maximum and minimum of duration of saccades
sum, average, maximum and minimum of the rotation angle of an eye ball during saccades
sum, average, maximum and minimum of the angular velocity of an eye ball during saccades
    Blink_Duration [ms]
_Total Average Maximum
-Minimum Fixation Frequency[coun s]
Fixation Count
 Fixation_Duration [ms]
Fixation_Dispersion [px]
_Total Average Maximum
_Minimum Total
Average Maximum Minimum
Fig. 2.
Procedure of experiments.
Scanpath Length[px] Saccade Count Saccade_Frequency[coun s]
TABLE II.
CLASSES OF TOEIC SCORES.
# subjects 3
6
2
TABLE III.
RESULTS OF ESTIMATION.
  Saccade_Duration [ms]
Saccade_Amplitude [0]
Saccade_Velocity [0Is]
Total _Average
Maximum Minimum Total Average
_Maximum Minimum
_Total Average Maximum Minimum
Using the above data of eye movement the following experiments were done: The  rst is to estimate the English abilities of subjects using all the data. At the learning phase, leave-one-subject-out was applied to make the learning result independent from subjects. The second experiment is to eval­ uate the dependency on documents. Different documents were employed to verify whether the method is in uenced by doc­ uments. The third and the fourth are to know the relationships with the number of documents and classes, respectively.
B. Estimation of the English abili
The  rst task here was the feature selection. Thanks to the backward stepwise selection, we selected the fol­ lowing two features: Fixation_Duration_Total and Sac­ cade_Velocity_Total. The former is the sum of the duration of  xation and the latter is the sum of angular velocity of an eye ball. Note that these sums are not only for a single document but for all documents employed in the learning.
=
 IV. EXPERIMENTS A. Conditions and goals
We employed the dataset of eye movement obtained from 11 subjects who are students of graduate schools. All of them are Japanese and thus non-native of English. The distribution of TOEIC scores is shown in Table II.
As the eye tracker, SMI RED250 was used. This is a stationary eye tracker with a sampling rate of 250 Hz, which is high enough to catch eye movement while reading.
In the experiments, we asked the subjects to perform the task of reading parts (PART7) of TOEIC tests. Each part consists of a document with several paragraphs and four questions about it. Each question is to select from among four choices.
Figure 2 shows the procedure of experiments. First, the calibration of the eye tracker was done every time just before measuring eye movement. Next, a subject was asked to start reading the text shown on the display. At the time of reading, the subject was not allowed to read the questions. A er reading, the text disappears  om the display and the subject was asked to answer to the questions. Finally the subject also answered to some questionnaires. This process was repeated until  rst  ve sheets of text are read. After 15 min. break, the subject repeated the process for the remaining  ve sheets.
Table III shows the results of test. Estimated and correct mean the estimated and correct classes, and the error is the di erence between them. The average indicates the average of all errors. As shown in the table, only one subject K was with
the error. The accuracy of estimation is 90.9% (
10/11).
Figure 3 shows the distribution of features. One outlier with a high value of Saccade_Velocity-Total corresponds to the subject K. Figures 4 and 5 show the distributions of Fixa­ tion_Duration_Total and Saccade_Velocity_Total, respectively.
The distribution in Fig. 4 shows that the class 1 (low) can be distinguished by the feature Fixation_Duration_Totai. This is understandable since it has been known that the feature Fixation_Duration_Total is relevant to the di culty of a document for a subject [20]. However it is also shown that the feature is not e ective to separate the class 2 (middle) from the class 3 (high).
As shown in Fig. 5 the feature Saccade_Velocity_Total plays this role. For the class 2 subjects, their saccade velocity is smaller than that of the class 1 and class 3, because it is necessary for them to read carefully to understand the contents. For the class 1 subjects, they read faster simply because of a lot of local re-reading; this could be due to the di erence of grammatical structure between English and Japanese. Low skill subjects are not able to understand English in its word order
253
2015 13th International Conference on Document Analysis and Recognition (ICDAR)
  Fig. 3. Distribution of features.
Fig. 4. Distribution of Fixation_Duration_Total.
and thus must go back and forth, which makes the velocity
higher. The reason of high velocity for the class 3 subjects is that they require shorter time to understand the contents as compared to the class 2 subjects.
  Dependency on documents
The above result should be independent of the documents we employed for the experiment. In order to verify this point, we changed all of the ten documents for  ve subjects. The new documents were as dif cult as the originals  om the viewpoint of subjective di culties.
The result is shown in Table IV. Only one additional error, which was with the new documents, was observed for the subject E. From this result, we have con rmed that the estimation is well independent of documents. Figure 6 illustrates the distribution of features for this experiment. The new error was caused by the position of the subject E closer to a class 3 subject. However the distribution is similar to that of the previous experiment in Fig 3, from which we can also con rm the independence.
D. The number of documents
The next is about the number of documents necessary for the estimation. It is better to employ a smaller number of documents if the same accuracy of estimation is obtained. Generally speaking, the accuracy can drop as the number of documents is reduced. Thus in this experiment we clarify the relationship between the number and the accuracy. In the experiment, reduced sets of documents were prepared by a random sampling without replacement for 30 times and the accuracy was obtained as their average.
The result is shown in Fig. 7, where the horizontal axis represents the number of documents employed for learning and the vertical axis is the accuracy of classi cation. It is
Fig. 5. Distribution of Saccade_Velocity_Total.
Fig. 6. Feature distribution with new documents.
observed that the accuracy improves as the number of doc­ uments increases. This means that if the user of this method is interested in accuracy he/she should use documents as many as possible. For those who would like to minimize the efforts at the subject side, at least two documents should be used. The use of a single document cannot be recommended.
E. The number of classes
The  nal question here is to verify the setting of classes. The accuracy of classi cation could be di erent depending on the classes. We de ned the different classes as shown in Table V. In addition to the original setting of three classes, we employed four and  ve classes. Table VI shows the accuracy obtained by using di erent numbers of classes. As shown in this  gure, the accuracy drops when the number of classes increase  om three. We consider this is mainly because of the lack of the number of subjects in the same class.
F Discussions
From the above experimental results, we have con rmed that the proposed method is capable of estimating the subject's English ability from among the basic three classes: low, middle and high. It has also been shown that the method is user­ and document-independent. If the number of documents for evaluation increases, we can obtain a more accurate estimation. Note that even with ten documents, the time required for the estimation is much less than taking the full TOEIC test, which requires more than two hours.
On the other hand, we consider that the experimental results have posed some limitations. An important one is about the "resolution" of classes. The accuracy of estimation dropped when we increased the number of classes. However this should not be considered as the limitation of the proposed method but caused by a small number of learning samples.
  254
Fig. 7.
The number of documents and accuracy.
V. CONCLUSION AND FUTURE WORK
[6] [7]
[8]
[9]
[10] [II]
[12] [13]
[14]
[15] [16] [17]
[18]
[19] [20]
[21] [22]
Andreas Bulling and Thorsten 0 Zander. Cognition-aware computing. Pervasive Computing, IEEE, 13(3):80-83,2014.
Adam R. Clarke, Robert J. Barry, Rory McCarthy, and Mark Selikowitz. Eeg analysis of children with attention-de cit/hyperactivity disorder and comorbid reading disabilities. Jou al of Lea ing Disabilities, pages 276-285,2002.
Evelyn C. Ferstl, Jane Neumann, Carsten Bogler, and D. Y ves von Cra­ mono The extended language network: A meta-analysis of neuroimaging studies on text comprehension. Human B in Mapping, pages 581-593, 2008.
Fred Genesee, Kathryn Lindholm-Leary, William Saunders, and Donna Christian. English language learners in us schools: An overview of research  ndings. Jou al of Education for Students Placed at Risk, 10(4):363-385,2005.
Dorothy Morrison Hansen. A discourse structure analysis of the comprehension of rapid readers. 1975.
David E Irwin. Fixation location and  xation duration as indices of cognitive processing. The intetface of language, vision, and action: Eye movements and the visual world, pages 105-134,2004.
Marcel Adam Just and Patricia Ann Carpenter. The psychology of reading and language comprehension. Allyn & Bacon, 1987.
Takashi Kimura, Rong Huang, Seiichi Uchida, Masakazu Iwamura, Shinichiro Omachi, and Koichi Kise. The reading-life log - technolo­ gies to recognize texts that we read. In Proceedings of ICDAR2013, pages 91-95. IEEE, 2013.
Reinhold Kliegl, Antje Nuthmann, and Ralf Engbert. Tracking the mind during reading: the in uence of past, present, and future words on  xation durations. Jou al of Experimental Psychology: Gene l; fou al of Experimental Psychology: Gene l, 135(1):12,2006.
K. Kunze, A. BuUing,   Utsumi, S. Yuki, and K. Kise. I know what you are reading - recognition of document types using mobile eye tracking. In ISWC 2013, 2013.
K. Kunze, H. Kawaichi, K. Yoshimura, and K. Kise. Towards inferring language expertise using eye tracking. In Ext. Abs. CHI 2013, pages 4015-4021,2013.
K. Kunze, H. Kawaichi, K. Yoshimura, and K. Kise. The wordmeter - estimating the number of words read using document image retrieval and mobile eye tracking. In P c. of ICDAR 2013, pages 25-29. IEEE, 2013.
Pascual Martinez-G6mez and Akiko Aizawa. Recognition of un­ derstanding level and language skill using measurements of reading behavior. In Proceedings of the 19th inte ational co erence on Intelligent User Intetfaces, pages 95-104. ACM, 2014.
Carol A Rashotte and Joseph K Torgesen. Repeated reading and reading  uency in learning disabled children. Reading Research Quarterly, pages 180-188,1985.
Keith Rayner. Eye movements and attention in reading, scene percep­ tion, and visual search. The quarterly jou al of experimental psycology, 62(8):1457-1506,2009.
George D Spache. Is this a breakthrough in reading? The Reading Teacher, 15(4):258-263,1962.
Songhua Xu, Hao Jiang, and Francis CM. Lau. User-oriented document summarization through vision-based eye-tracking. In P c of lUI, lUI '09,pages 7-16,2009.
error
0 0 0 0 I 0 0 0 0 0 1 0.18
2015 13th International Conference on Document Analysis and Recognition (ICDAR)
TABLE IV. DEPENDENCY ON DOCUMENTS. IN THE "DOCUMENT SET" ROW, "0" AND "n" INDICATES THE ORIGINAL AND THE NEW DOCUMENTS, RESPECTIVELY.
TABLE V. TOEIC SCORES AND CLASSES.
subJec,s l C I J B I E I K I FIA 0 I I I D I I I TOEIC 465 _ m ® � W � � ill � �
    subject
A B C 0 E F G H
K ave
score 3 classes
classes classes
documentset 0 0 0 0 n 0 n 0 n n n estimated 211332232I1 correct 21132223212
It is often said that the eye is a window of the mind. We claim that this holds for the language ability. The subject's language ability can be estimated by analyzing his/her eye movement. Based on this concept we have proposed a method of estimating a class (low, middle, high) of English ability de ned based on the TOEIC score. The key contribution of this paper is that in addition to the known feature of the sum of  x­ ation duration, the sum of saccade velocity plays an important role to distinguish the classes. From the experimental results with 11 subjects and 10 documents, we have con rmed that the proposed method is capable of estimating the class with the accuracy of 90.9% in a user- and document-independent way.
The future work includes experiments with a larger number of subjects and documents, as well as building attractive services based upon the functionality of the estimation of language abilities.
ACKNOWLEDGMENT
This work was supported in part by JST CREST and JSPS KAKENHI (25240028).
REFERENCES
Ralf Biedert, Georg Buscher, Sven Schwarz, Jorn Hees, and Andreas
Dengel. Text 2.0. In Ext. Abs. CHI 2010, pages 4003-4008,2010.
Ralf Biedert, Andreas Dengel, Mostafa Elshamy, and Georg Buscher. Towards robust gaze-based objective quality measures for text. In P c. ETRA 2012, pages 201-204,2012.
Ralf Biedert, Jorn Hees, Andreas Dengel, and Georg Buscher. A robust realtime reading-skimming classi er. In Proc. ETRA 2012, pages 123- 130,2012.
Andreas Bulling, Jamie A. Ward, and Hans Gellersen. Multimodal Recognition of Reading Activity in Transit Using Body-Worn Sensors. ACM T ns. on Applied Perception, 9(1):2:1-2:21,2012.
Andreas Bulling, Jamie A. Ward, Hans GeUersen, and Gerhard  oster. Eye Movement Analysis for Activity Recognition Using Electroocu­ lography. IEEE T ns. on Patte  Analysis and Machine Intelligence, 33(4):741-753,April 2011.
TABLE VI.
CLASSES OF ENGLISH ABILITIES AND ACCURACY.
 Journal of ReadingBehavior 1979,Vol.XI, No. 4
EYE MOVEMENT DYNAMICS OF GOOD AND POOR READERS: THEN AND NOW
Lester A. Lefton, Richard J. Nagle, Gwendolyn Johnson
University of South Carolina
Dennis F. Fisher
U.S. Army Human EngineeringLaboratory Aberdeen Proving Ground, Maryland
Abstract.
When presented with English text readers move their eyes from left to right across the page in a fairly systematic manner. Adult readers can extract about 1.2 words during each fixation and with each fixation lasting approximately 250 msec. Like adults, children behave in a similar systematic manner; but they make more fix- ations per line, take in fewer characters of text, and sometimes spend more time per fixation. Both adults and children make regressions, those eye movements that are from right to left, although adults generally make fewer regressions than children (cf. Tinker, 1939; Taylor, Frackenpohl &Pettee, 1960; Spragins, Lefton, & Fisher, 1976). Unsystematic patterns of disabled readers have been reported by Zangwell and Blakemore (1972) and Pavlidis (1978) and Ciuffreda, Bahill, Kenyon, and Stark, (1976) in reading and by Lahey and Lefton (1976) in a match-to-sample task.
Systematic investigations of oculomotor control began in the early part of the century (Buswell, 1937; Tinker, 1939, 1951, 1958) followed by a more recent resurgence and interest in the study of eye movements with more recent advances in the state-of-the-art by psychologists who have proposed that the study of eye movements would be helpful in providing data to further our understanding of reading. Eye movements provide a direct behavioral index of what a reader is doing
While reading text, the eye movements of good and poor reading fifth graders, third graders and adults were assessed. Subjects were tested in two sessions one year apart. Dependent variables included the duration and frequency of forward going fixations and regressions; an analysis of individual differences was also made. Results showed that poor reading fifth graders have relatively unsystematic eye movement behavior with many more fixations of longer duration than other fifth graders and adults. The eye movements of poor readers are quantitatively and qualitatively different than those of normal readers.
 320 Journal of Reading Behavior
during reading (e.g., Just &Carpenter, 1978). Whether or not eye movements can ex- plain reading is yet to be determined—indeed, few have argued that eye movement dynamics offer the answer to the nature of reading because of the idiosyncratic nature of the patterns and the lack of an appropriate index to quantitatively describe fixation patterns. Such limitations have restricted models of reading that incorporate eye movements to focus on the global aspects of eye movements and whether it is the text or the visual system that determines oculomotor sequencing (Rayner, 1978).
Reading disabled children are not only inexperienced readers but also people with special reading problems. In spite of normal intelligence and pedagogical ex- perience, they are unable to deal effectively with text and only a few improve significantly over time with remediation. Some of the reading disabled exhibit specific kinds of reading errors; for example, letter and word reversals, or poor word attack skills, while others exhibit deficits that are tied more to comprehension than to decoding. Because of the highly idiosyncratic nature of the behavior of reading disabled children, eye movement characteristics can only be generally summarized as exhibiting fixations of long duration that are close together and an inordinate amount of regressive movements (cf. Zangwell and Blakemore, 1972; and Pavlidis, 1978). Consequently, it is very difficult, except perhaps in single subject case studies, to get meaningful group data because of the limited nature of the state-of-the-art of oculometry and specific idiosyncracies of these nonreaders.
Poor readers, on the other hand, may also suffer motivational problems or slow processing of sensory and cognitive input. These problems may result from a slightly below average intellectual ability or may be due to faulty processing mechanisms which may interfere with the speed with which semantic and syntactic integration occurs in visual verbal material. Whatever the cause, poor readers do read; however, they do so more slowly than normal readers and are set apart both quantitatively and qualitatively from disabled readers. Little is known about poor readers and changes in their reading attack skills over time. The present study, using eye movement monitoring assesses those changes. Particular emphasis is placed on the magnitude of change in eye movement dynamics between measurements of the function of pro- ficiency with particular attention paid to individual variations within groups (cf. Rothkopf, 1978).
METHOD
Subjects
During the first year of the study, 60 elementary school children and 20 undergraduate psychology students served as subjects. These subjects were initially classified into four groups of 20 each; however, attrition in the second year of testing reduced the size of the groups: adults [N = 18), third graders [N = 13, age mean 8.8, range 8.5-9.3 years), fifth graders [N = 16, age mean 10.9, range 10.5-11.5 years),
children attended a public elementary school in suburban Columbia, South Carolina, and the adult subjects were enrolled in an introductory psychology course at the University of South Carolina. All third and fifth graders at the elementary school were tested on the reading comprehension subtests of the Stanford Achievement Test (SAT) to verify teacher reports of reading achievement. Based on these scores,
and poor reading fifth graders [N = 17, age mean 11.2, range 10.7-12.0 years). The 1
 EyeMovements inPoorReaders 321
subjects were selected and assigned to each group based on grade placement and reading level. Scores on SAT obtained before each test session, separated by one year, are shown in Table 1. The poor reading group was matched for chronological age with a normal fifth grade group (f < 1}and SAT reading grade level with third graders (f < 1). It should be noted that at the time of the second testing third graders were in the fourth grade and fifth graders were in the sixth grade. The adult subjects were selected randomly from class lists of students who wished to participate in the study as an optional course assignment.
Materials
TABLE 1
Mean SAT Scores for Good and Poor Readers
Group First
Third 3.55 Fifth 5.72 PR 3.44
Second
4.55 7.5 4.5
The stimuli were paragraphs published by the Educational Development Laboratories, Huntington, New York (No. 367002). The paragraphs were typed with an IBM Selectric typewriter (Artisan) and mounted on 3 x 5 inch reading cards. Three different paragraphs from each grade level (i.e., third, fifth, and adult) were used, with each subject receiving paragraphs appropriate to his/her grade placement. Thus, paragraphs differed in length from 62, 118 and 200 words, respectively by grade. Each paragraph was accompanied by a series of 10 yes/no comprehension questions.
Apparatus
Eye movements during reading were monitored by an Eye-trac (Standard Model 106, Biometrics, Inc.) 14 inches from the reader's eyes; each paragraph subtended 19.5 horizontally by 7.5 or less vertically. The Eye-trac detects optical movements by aiming photocells at an image of the reader's eyes. The photocells respond to the dif- ference in reflectivity between the iris and sciera and generate signals proportional to the eye movement. The signal is amplified electronically and is used to draw the pen of a two-channel strip chart recorder, each channel recording the horizontal movements of one eye. A mark was judged a fixation if the pen traveled .5 to the right or left and provided a duration of 50 msec or longer.
Calibration of the Eye-trac is accomplished quickly. The subject is instructed to sit in a chair and place his or her chin in the chin rest and head against the rest. The subject is asked next to fixate on an "X" in the center of the calibration card located
Session
 322 Journal of Reading Behavior
on the easel while the centering control is aligned. The subject is then asked to fixate on an "L" and then on "R," delineating the extreme left and right of the textual material he/she will read, as the respective amplitudes are calculated.
Procedures
All subjects were tested individually in a room at their school. Subjects were told that their eye movements would be recorded while they were reading. The sub- jects were instructed to read each paragraph silently and then close their eyes to signal they had completed reading. Following these brief instructions and calibration of the Eye-trac, subjects received their stimuli. The first paragraph presented to the subject served as a practice trial to insure that the subject understood the procedure. Upon completion of the practice passage, the subject was told to move out of the Eye-trac in order to answer 10 yes/no questions about the paragraph.
Subjects then returned to the Eye-trac and recalibration was carried out. Each subject then received the remaining two paragraphs, following the same procedure for each passage as in the practice trial. Order of paragraph presentation was counterbalanced across subjects within grade levels.
RESULTS AND DISCUSSION
Eye movements of subjects were monitored while they read two paragraphs in each of two sessions. The rated reading grade level of the paragraphs remained the same between the sessions in order to better assess change due to development. There were four groups of subjects (adults, fifth grade good and poor readers, and good third grade readers) tested individually with one year separating test sessions (Year 1 vs. Year 2). The dependent variables were frequency and duration of forward and regressive fixations. Comprehension scores and variability within groups were also assessed. Data were entered into separate unequal N analyses of variance with groups as the between-subjects variable and sessions as the within-subject variable. The data were then entered again separately for fifth grade good and poor readers. These data and those for other derived descriptive measures are shown in Table 2.
Forward Fixations
Duration. From Table 2 it can be seen that fixation duration is quite similar for normal reading fifth graders and adults and also for third graders and fifth grade poor readers. The overall analysis of variance showed a significant main effect of groups, F(3, 60) = 35.28, p < .001 and similarly the fixation duration for the fifth grade nor- mal and poor readers proved highly significantly different F{1, 31) = 26.29, p < .001. No effect was found in either analyses for test sessions F < 1). In each case, however, the interaction of Groups x Year proved significant in the overall analyses F(3, 60) = 5.888, p < .001 and for the fifth graders, F(l, 31) = 12.15, p < .01.Here it was found that the fifth grade poor readers and adults improved while the third grade and fifth grade normal readers showed longer fixation durations. This effect might be partially attributed to the increase of perceptual span of these groups; any other explanation for these fluctuations must remain speculative at best and certainly contradicts notions of efficiency based on learning.
 co eg CO
-a ta <u Cr".
q
§
Third Fifth Fifth PR Adults
TABLE2
Eye Movement Dynamics for Good and Poor Readers
Grade
Year 12121212
Words Read
Speed (wpm)
No. of Fixations
Fixation Duration (msec)*
Fixations/Une* 9.8 Characters/fixation 4.8
No. of Regressions Regression Duration (msec)* Regressions/line*
•Analyses provided
18.7 11.2 423 452
2.67 1.5
17.8 25.2 197 301
1.37 1.94
62 62 118 118 87.4 108.2 146.9 166.0 68.6 50.4 108.8 95.3
458 481 249 307 7.2 8.37 7.33
6.5 5.9 6.8
118 118 95.0 102.S 130.8 119.1
497 418 10.06 9.16
4.9 5.5 51.1 41.1 521 446.
3.93 3.16
200 200 184.2 197.6 167.4 155.3 272 233
6.2 5.75
7.2 7.8 35.1 29.7
260 219 1.3 1.1
oon
 324 Journal of Reading Behavior
Frequency. Examination of Table 2 reveals a very definite development trend among normal readers. That is, with increasing age or developmental level, fewer fixations were made. This trend directly reflects the size of the perceptual span or characters per fixation. The exception once again is with the fifth grade poor reading group. Fifth grade poor readers tend to resemble more closely the third graders than their fifth grade normal reading counterparts. For these analyses, most trends proved significant. Both the main effect of groups for the overall analyses F(3, 60) = 22.23, p < .001, and for the fifth graders alone F(l, 31) = 9.15, p < .01 were significant. Both the effects for Year 1 vs. Year 2 for the overall analyses F(l, 60) = 27.97, p < .001 and for the fifth graders alone F(l, 31) = 10.07, p < .01 were significant. Fur- ther the Groups X Year interaction for the overall analyses F(3, 60) = 4.15, p < .01 was also significant. However, this interaction for the analysis of the fifth grade data alone failed to reach statistical significance [F < 1.0). In essence, these data indicate that although a developmental trend was evidenced in the overall analyses both good and poor reading fifth graders reduced the number of fixations per line and hence in- creased perceptual span by similar magnitude. Improvement per se does not guarantee accomplished reading nor can the poor readers be classified as "doing the
same thing" as third graders even though the eye movement dynamics appear similar.
Regressive Fixations
Duration. Review of Table 2 illustrates that regression durations for the four groups are quite similar to the fixation durations previously described. The main ef- fect of groups for the overall analyses F(3, 60) = 37.08, p < .001 and for the fifth grade analyses F(l, 31) = 41.24, p < .001 confirms the trends previously found for the forward going fixation durations. Likewise, there was an interaction of Group x Year in the overall analyses F(3, 60) = 4.56, p < .01 and for the fifth grade analyses F(l, 31) = 7.0, p < .05. No statistically significant effect was found for the main ef- fect of session (F < 1.0). Thus, although improvement over time takes place, the longer the regressive fixations the more inefficient the reader.
Frequency. The main effect of groups in the overall analyses F(3, 60) = 9.98, p < .001, and for the fifth grade analyses, F(l, 31) = 9.42, p < .01, proved to be the only statistically significant effect for this variable. In essence, groups is the primary determiner of regression frequency with the third graders and poor reading fifth
graders making the most regressions.
Summary. In general, these eye movement data can be interpreted as a further substantiation of developmental trends in reading performance. One point of con- cern is that although those trends proved significant the test-retest reliability proved somewhat less than optimal in many cases even though the same materials were us- ed. It was expected that improvement in reading dynamics would have occurred in this situation but all paragraphs were approached as though they were new. That the poor readers improved over time was consistent with the notion that they can only get better because no processing disability or dysfunction interferes. The poor reading fifth graders were found to be substantially different from normal reading fifth graders but also different from normal reading third graders on some measures while similar on others. These trends support the notion that poor readers behave similarly to normal reading fifth graders but in a slower manner, as though a dampen- ing of the processing system has taken place.
 EyeMovements inPoorReaders 325
Comprehension. The subjects were required to answer a series of true/false questions about the content of each paragraph they read. The task was initiated to in- sure that subjects would actually read the paragraph. Analyses of these scores were completed to gain this assurance. While there was no effect of year {1 vs. 2) or significant interaction of Group x Year, the main effect of groups proved significant F(3, 60) = 18.11, p < .001 with scores of 78.2, 78.6, and 70.0 and 55 for the third graders through adults respectively. These scores can by no means be taken as a comparison between the groups because of the varying level of paragraphs. Our primary concern was to assure that these subjects were reading for comprehension and these scores provide us that assurance.
Individual differences-variability. Eye movements while reading text are fairly predictable and stable within a normal adult subject on a particular type or level of text. An analysis was undertaken to assess further potential individual differences. The number of fixations and regressions made on each line of text was calculated for each subject. These data yielded the mean number of fixations and regressions reported in Table 2; however, in addition, they allowed for the assessment of each subject's variability. These variability scores for each subject were calculated by tak- ing the standard deviation of each subject's performance across the lines of text. Thus the subject who read each line in a similar manner with the same number of fix- ations and regressions per line would show a low degree of variability while a subject who made fewer fixations on one line than another would show greater variability as indicated by a larger standard deviation.
The individual subject variability scores were summed within reading groups and then a mean individual variability score was calculated and the standard devia- tion of this mean was derived. Figure 1 shows the results from these analyses. The individual subject's standard deviations were entered into the analysis of variance for frequency of both fixations and regressions with group and year once again as ef- fect of concern.
For fixation there was an age related developmental trend showing highest variability with younger subjects, F(3, 60) = 12.27, p < .001. Variability decreased from Year 1 to Year 2, F[l, 60) = 4.58, p < .05 and irregularly between groups with the most substantial decrease found for the third graders and a slight (2.45 vs. 2.52) increase for the poor readers as indicated by the significant interaction of Group x Year F(3, 60) = 4.22, p < .01. Overall, adults obtained standard deviations of 1.33; this shows that on the average the subjects deviated less than 1.33 fixations from their average number of fixations per line (5.9). In total contrast, the poor readers not only made more fixations per line (9.61) but they varied more from line to line with a group standard deviation of 2.47. Furthermore, not only were the poor readers more variable as a group but the variability within this group of subjects was quite con- siderable.
Similar trends are also apparent with the data on regression; there were age related developmental trends, F(3, 60) = 9.24, p < .001, but there were no statistically significant effects of year or the interaction of Grade X Year. The adults once again were least variable (1.06) and the poor readers were most variable (1.83).
 326 JournalofReadingBehavior
7
at Q
2-
I1i] I1I
Adult 5th 3rd PR Adult 5th 3rd PR READING LEVEL
Z 5 - O
4-
u3-
Fixations
5Û
0-
25- O
5
Q
Regressions
r1 1-
4-
3-
2-
1-
Yeari
Year 2
Yeari
Year 2
I0-
iiI
Adult 5th 3rd PR Adult 5th 3rd PR READING LEVEL
Figure 1. Standard deviation of mean individual variability score. Data points are the mean individual variabililty scores whereas brackets indicate onestan- dard deviation on either side of this mean.
CONCLUSIONS
Apart from thepoor reading group, theagerelated development findings are not startling, particularly thedecreased number of fixations andregressions from Year1 to Year 2.Furthermore, theageeffects corroborate existing literature (e.g., Spragins, et al.,1976).Themost interesting aspect of ourresults lies inthepoor reading group
 EyeMovements inPoorReaders 327
and in the analysis of subject variability. The poor readers made the greatest number of fixations for longer durations. Similarly, they made more regressions for longer durations than those of any other group. This is important because, 1) the poor readers performed worse than their age-matched counterparts—this is not surprising since they are poor readers, however, 2) they performed even worse than the third graders in many cases. Fifth and third graders improved in some cases markedly; im- provements for the poor readers were minimal.
Rothkopf (1978) has recently argued that individual subject's eye movement strategies are very different. He has shown that within a group of college subjects several individual eye movement strategies are adopted by the subjects. These strategies may not be evident by examining the group scores usually reported in the psychological literature, thus Rothkopf urged that researchers consider the im- portance of individual differences. Similarly, Lefton et al. (1978) have argued that the eye movement patterns of reading disabled subjects are nonsystematic. The present study supports these findings with results showing that adults are far less variable than children. More important, variability within poor readers is high and variability among poor readers is high. Adults adopt a conservative system, left-to-right, se- quences of eye movements with little more than one regression per line; further- more, most adults adopt this strategy. In total contrast, the poor readers were making more fixations, many more regressions, for long durations, and were thus examining fewer character ..spaces per fixation. Most important, the poor readers were par- ticularly variable—on some lines they made many fixations or regressions, on other
lines they didn't. Most important, this varied dramatically among subjects.
Whereas adults and normal children were fairly predictable in terms of the number and duration of eye movements, the poor readers were chaotic in their eye movements. Since eye movements allow a reader to gather information from text, an erratic, unsystematic fixation strategy only adds to the burdens of a new reader. Most contemporary models of eye movement dynamics acknowledge the role of the periphery in providing information to direct subsequent eye movements. If such in- formation is picked up randomly and/or chaotically, a meaningful flow of informa- tion extraction is nearly impossible.
Although our results provide some indications for the dampening or slowing down of the processing characteristics of the poor readers, it has also been shown that imprecise sequencing and an irregularity in the processing characteristics of the poor reader far exceeds that observed among normal reader populations. In sum- mary, the results of the present study show that when compared to normals, the eye movements of poor reading children are chaotic, frequent, of longer duration, and generally unsystematic. Normal developmental gains made by children are not shown in the eye movement dynamics of poor readers. The potential information picked up by the poor reader is therefore severely hampered by the chaotic ocular- motor control.
FOOTNOTES
1
University of South Carolina, Columbia, SC 29208. This paper may be reproduced in full or part
for any purpose of the United States Government.
Reprint requests should be sent to Lester A. Lefton, c/o Department of Psychology,
2
Congaree Elementary School, as well as other school personnel in conducting this study.
We gratefully acknowledge the helpful assistance of Mr. H. Steve Patterson, Principal of
 328 Journal of Reading Behavior
REFERENCES
BUSWELL, G. T. How adults read. Supplementary Educational Monograph, No. 45. Chicago, IL: University of Chicago Press, 1937.
CIUFFREDA, K. J., BAHILL, A. T., KENYON, R.V., &STARK, L. Eye movements during reading: Case reports. American Journal of Optometry and Physiological Optics, 1976, 53, 389-395.
JUST, M. A., &CARPENTER. P. A. Inference processes during reading: Reflections from eye fixations. In J. W. Senders, D. F. Fisher, & R. A. Monty (Eds.), Eye movements and the higher psychological functions, Hillsdale, NJ: Lawrence Erlbaum Associates, 1978.
LAHEY, B. B., & LEFTON, L. A. Discrimination of letter combinations in good and poor readers. The Journal of Special Education, 1976, 10, 205-210.
LEFTON, L. A., LAHEY, B. B., & STAGG, D. I. Eye movements in reading disabled and normal children: A study of systems and strategies. Journal of Learning Disabilities, 1978, 11, 549-559.
PAVLIDIS, G. The dyslexies' erratic eye movements: Case studies. Dyslexia Review, 1978, 1, 22-28.
RAYNER, K. Eye movements in reading and information processing. PsychologièalBulletin, 1978, 85,618-660.
ROTHKOPF, E. Z. Analyzing eye movements to infer processing styles during learning text. In J. W. Senders, D. F. Fisher, & R. A. Monty (Eds.), Eye movements and the higher psychological functions. Hillsdale, NJ: Lawrence Erlbaum Associates, 1978.
SPRAGINS. A. B., LEFTON, L. A., & FISHER, D. F. Eye movements while reading and sear- ching spatially transformed text: A developmental examination. Memory and Cognition, 1976, 4, 36-42.
TAYLOR, S. E., FRACKENPOHL. H., & PATTEE, J. L. Grade level norms for the components of the fundamental reading skill. Huntington, NJ: Education Development Laboratories, 1960.
TINKER, M. A. Fixation pause during reading. Journalof EducationalResearch, 1951, 44, 471- 479.
TINKER, M. A. Recent studies of eye movements in reading. Psychological Bulletin, 1958, 55, 215-231.
TINKER, M. A. Reliability and validity of eye-movement measures of reading. Journalof Experimental Psychology, 1939, 19,732-746.
ZANGWELL, O. L., & BLAKEMORE, C. Dyslexia: Reversal of eye movements during reading. Neuropsychologia, 1972, 10, 371-373.
Interactivity: Research
CHI 2013: Changing Perspectives, Paris, France
 Marco Marchesi
University of Bologna, Faculty of Engineering
Viale del Risorgimento, 2 Bologna, 40122 Italy marco.marchesi@unibo.it
Bruno Ricco`
University of Bologna, Faculty of Engineering
Viale del Risorgimento, 2 Bologna, 40122 Italy bruno.ricco@unibo.it
BRAVO: A BRAin Virtual Operator For Education Exploiting Brain-Computer Interfaces
Abstract
This paper introduces a new e-learning system that works with a Brain-Computer Interface to customize the educational experience, according to user’s reactions and preferences.
Author Keywords
BCI; Education; mobile
ACM Classification Keywords
H.5.2 [Information Interfaces and Presentation]: User InterfacesUser-centered design; K.3.1 [Computers and Education]: Computer Uses in EducationDistance learning.
Introduction
At present, education is undergoing a large transformation due to the fast evolution of digital technologies, providing new tools for both teachers and students. For instance, Learning Managing Systems have been introduced in primary schools as well as in university courses, while the dramatic increase in Internet connections allows advanced solutions, some of which are supported by interactive and social networking technologies.
In this scenario, Brain-Computer Interfaces (BCI) can play an interesting role [2], as they can provide useful information about students’ attention and motivation [3].
 Copyright is held by the author/owner(s).
CHI 2013 Extended Abstracts, April 27–May 2, 2013, Paris, France.
ACM 978-1-4503-1952-2/13/04.
3091
Interactivity: Research
CHI 2013: Changing Perspectives, Paris, France
On the other hand, mobile devices, such as tablets and smartphones, are widely recognized as suitable complements of conventional learning tools, because of their widespread utilization, particularly by young people, and their extremely powerful interfaces.
In this context, this paper describes a new system, called BRAVO (BRAin Virtual Operator), for content visualization in a mobile e-learning context. BRAVO makes use of a commercial BCI to detect the user’s brain activity (in particular attention and meditation levels) while learning, worked out from brainwaves detected in real time. This allows to understand which parts of the lecture content are most difficult for the user, so as to propose them in the most appropriate form, in a different way or with a reduced or deeper level of explanation. Although primarily dedicated to educational purposes, the system presented in this paper can be easily adapted to other applications, such as interactive visual or audio experiences.
Overview
BRAVO has been designed primarily for e-learning applications, and for this reason it has been developed as a client application of Moodle, a popular open-source e-learning tool to create courses, lectures and assignments through customizable modules.
BRAVO extends Moodle features and allows working with any kind of contents, though particularly suitable for those with interactive and navigable presentations, because of their higher level of engagement. A critical point for the effectiveness of the system is the correct estimate of the students interest and motivation. To this purpose, touchable interfaces and mobile devices can help in better tracking the user activity. For this reason BRAVO features many elements of gamification, such as, for instance, progress bars, flags or scores.
In practice, with BRAVO the whole learning process, or part of it, can be monitored in real-time by means of the BCI (that acquires EEG waveforms), in order to customize the lectures to the student needs.
Setup
For the realization of a prototype of the system, the Mindwave Mobile headset by Neurosky Inc. has been used. This BCI device is a one channel EEG headset, cheap, of almost immediate use, that connects via Bluetooth to tablets and smartphones. After very short calibration (a few seconds), it starts detecting EEG brainwaves with a frequency up to 512Hz, from which every second it calculates attention and meditation levels, that are in relationship with alpha and beta waves, particularly.
Exploiting this information, a Computerized Adaptive Testing (CAT) algorithm has been implemented. In this way the difficulty level of the new proposed contents can be guessed intelligently without degrading the resulting ability estimates [4].
For example, a detected low attention level while studying may suggest the presence of difficulties with previous parts of the learning program or a temporary lack of interest in it. Thus, such parts could be repeated or an easier approach could be proposed to go ahead and get back the attention. On the contrary, well-focused students could be stimulated to work harder to reach higher results at the end.
In general, by detecting and showing the levels of attention and meditation, people are recursively encouraged to improve their performance. This process is generally called (neurofeedback).
To demonstrate BRAVO, a course of Art History has been realized featuring the 3D models of significant Italian Monuments. The models are navigable and present
3092
Interactivity: Research
CHI 2013: Changing Perspectives, Paris, France
different hot spots localized in specific points.
Once the student selects one of the hot spots, a related content appears in the form of HTML text or multimedia. If the student shows interest while reading, BRAVO provides further studying material, while a status bar indicates the progress made. If, instead, the students attention decreases, the system can repeat the presentation or suggests new topics as alternatives.
Figure 1: A lecture in BRAVO: the brain activity is shown at the top.
Application in groups and classrooms
In addition to the features briefly described above, BRAVO supports collective studying by a group of students, particularly if they are connected remotely. In this case, each user, with a tablet, transmits his own brain activity to other group members so as to facilitate collaboration through the mutual comprehension of the difficulties that are encountered. Moreover, teachers can see in real time the trend of the classroom and adapt their teaching accordingly. Group mode is currently simulated and under test.
Figure 2: A group of students can improve the learning curve helping each other. A significant low level of attention of a member can be notified to the other ones that can help him in real time.
BRAVO is a new project that expands on the results
  3093
Interactivity: Research
CHI 2013: Changing Perspectives, Paris, France
achieved with Mobie, a software for creating interactive movies under brain control [1]. Accordingly to Mobie, in BRAVO the attention and meditation levels are optimized to take in account only the trend observed during a task. In fact, cognitive processes like the attention change every second.
Of course, BRAVO concept and technology can be applied to a many different application fields. Just as an example, the explanations in museums could depend on visitors’ interest.
Conclusions
This paper introduces a new tool (BRAVO) to help students in better learning different kind of content. Such a system makes use of Brain-Computer Interfaces to adapt explanations and learning to the actual student’s learning ability and conditions. In this way it improves performance by means of neurofeedback loops and CAT tests.
BRAVO is specifically designed for use with mobile devices and social networks; it is a client application of a popular e-learning platform and allows a much greater degrees of interactivity in e-learning than conventional tools.
References
[1] M. Marchesi, E. Farella, B. Ricc`o, and A. Guidazzoli. Mobie: a movie brain interactive editor. In SIGGRAPH Asia 2011 Emerging Technologies, SA ’11, pages 16:1–16:1, New York, NY, USA, 2011. ACM.
[2] A. Nijholt, D. Tan, B. Allison, J. del R. Milan, and B. Graimann. Brain-computer interfaces for hci and games. In CHI ’08 Extended Abstracts on Human Factors in Computing Systems, CHI EA ’08, pages 3925–3928, New York, NY, USA, 2008. ACM.
[3] G. Rebolledo-Mendez, S. de Freitas,
J. Rojano-Caceres, and A. Garcia-Gaona. An empirical examination of the relation between attention and motivation in computer-based education: a modeling approach, 2010.
[4] D. J. Weiss and G. G. Kingsbury. Application of computerized adaptive testing to educational problems. Journal of Educational Measurement, 21(4):361–375, 1984.
3094
Carnegie Mellon University
From the SelectedWorks of Marcel Adam Just
1980
A theory of reading: From eye  xations to comprehension
Marcel Adam Just, Carnegie Mellon University Patricia A. Carpenter, Carnegie Mellon University
    Available at: h p://works.bepress.com/marcel_just_cmu/64/
 Psychological Review VOLUME87 NUMBER4 JULY1980
A Theory of Reading: From Eye Fixations to Comprehension
Marcel Adam Just and Patricia A. Carpenter Carnegie-Mellon University
This article presents a model of reading comprehension that accounts for the allocation of eye fixations of college students reading scientific passages. The model deals with processing at the level of words, clauses, and text units. Readers make longer pauses at points where processing loads are greater. Greater loads occur while readers are accessing infrequent words, integrating information from important clauses, and making inferences at the ends of sen- tences. The model accounts forthe gaze duration on each word of text as a func- tion of the involvement of the various levels of processing. The model is em- bedded in a theoretical framework capable of accommodating the flexibility of reading.
Although readers go through many of the same processes as listeners, there is one striking difference between reading and listening comprehension—a reader can control the rate of input. Unlike a listener, a reader can skip over portions of the text, re- read sections, or pause on a particular word. A reader can take in information at a pace that matches the internal comprehension processes. By examining where a reader pauses, it is possible to learn about the comprehension processes themselves. Us- ing this approach, a process model of read- ing comprehension is developed that ac- counts for the gaze durations of college students reading scientific passages.
The research was supported in part by Grant G-79- 0119 from the National Institute of Education and Grant MH-29617 from the National Institute of Mental Health.
We thank Allen Newell and Robert Thibadeau for their very helpful discussions.
The order of authorship was decided by the toss of a coin.
Requests for reprints should be sent to Marcel Adam Just, Department of Psychology, Carnegie-Mellon University, Pittsburgh, Pennsylvania 15213.
The following display presents an excerpt from the data to illustrate some characteris- tics of eye fixations that motivate the model. This display presents a protocol of a col- lege student reading the first two sentences of a passage about the properties of fly- wheels. The reader averages about 200 words per minute on the scientific texts. In this study, the reader was told to read a para- graph with understandingand then recall its content. Consecutive fixations on the same word have been aggregated into units called
gazes. The gazes withineach sentence have been sequentially numbered above the fixated word with the gaze durations (in msec) indicated below the sequence number.
One important aspect of the protocol is that almost every content word is fixated at least once. There is a common misconcep- tion that readers do not fixate every word, but only some small proportion of the text, perhaps one out of every two or three words. However, the data to be presented in this article (and most of our other data collected in reading experiments) show that during ordinary reading, almost all content
Copyright 1980 by the American Psychological Association, Inc. 0033-295X/80/8704-0329$00.75
329
 330 MARCEL ADAM JUST AND PATRICIA A. CARPENTER
Eye fixations of a college student reading a scientific passage. Gazes within each sentence are sequentially
numbered above the fixated words with the durations (in msec) indicated below the sequence number.
12345678912 1566 267 400 83 267 617 767 450 450 400 616 Flywheels are one of the oldest mechanical devices known to man. Every internal-
354678 9 1011 12 13 517 684 250 317 617 1116 367 467 483 450 383 combustion engine contains a small flywheel that converts the jerky motion of the pistons into the
14 15 16 17 18 19 20 21 284 383 317 283 533 50 366 566 smooth flow of energy that powers the drive shaft.
words are fixated. This applies not only to scientific text but also to narratives written for adult readers. The current data are not novel in this regard. The eye fixation studies from the first part of the century point to the same conclusion (Buswell, 1937, chap. 4; Dearborn, 1906, chap. 4; Judd & Buswell, 1922, chap. 2). When readers are given a text that is appropriate for their age level, they average 1.2 words per fixation. The words that are not always fixated tend to be short function words, such as the, of, and a. The number of words per fixation is even lower if the text is especially difficult or if the reader is poorly educated. Of course, this is not the case when adults are given simple texts, such as children's stories; under such circumstances, these same studies show an increase to an average of two words per fixation. Similarly, readers skip more words if they are speed-reading or skimming (Taylor, 1962). These old results and the current results are consistent with the report of McConkie and Rayner (1975; Rayner, 1978) that readers generally cannot deter- mine the meaning of a word that is in periph- eral vision. These results have important implications for the present model; since most words of a text are fixated, we can try to account for the total duration of com- prehension in terms of the gaze duration on each word.
The protocol also shows that the gaze duration varies considerably from word to word. There is a misconception that in- dividual fixations are all about 250 msec in duration. But this is not true; there is a large variation in the duration of individual fixations as well as the total gaze duration on individual words. As the preceding dis- play shows, some gaze durations are very long, such as the gaze on the word Fly-
wheels. The model proposes that gaze dura- tions reflect the time to execute comprehen- sion processes. In this case the longer fixations are attributed to longer processing caused by the word's infrequency and its thematic importance. Also, the fixations at the end of each sentence tend to be long. For example, this reader had gaze durations of 450 and 566 msec on each of the last words of the first two sentences. The sentence- terminal pauses will be shown to reflect an integrative process that is evoked at the ends of sentences.
The link between eye fixation data and the theory rests on two assumptions. The first, called the immediacy assumption, is that a reader tries to interpret each content word of a text as it is encountered, even at the expense of making guesses that sometimes turn out to be wrong. Interpretation refers to processing at several levels such as en- coding the word, choosing one meaning of it, assigning it to its referent, and deter- mining its status in the sentence and in the discourse. The immediacy assumption posits that the interpretations at all levels of processing are not deferred; they occur as soon as possible, a qualification that will be clarified later.
The second assumption, the eye-mind assumption, is that the eye remains fixated on a word as long as the word is being processed. So the time it takes to process a newly fixated word is directly indicated by the gaze duration. Of course, compre- hending that word often involves the use of information from preceding parts of the text, without any backward fixations. So the con- cepts corresponding to two different words may be compared to each other, for exam- ple, whereas only the more recently en-
 countered word is fixated. The eye-mind assumption can be contrasted with an alter- native view that data acquired from several successive eye fixations are internally buf- fered before being semantically processed (Bouma & deV oogd, 1974). This alternative view was proposed to explain a reading task in which the phrases of a text were suc- cessively presented in the same location. However, the situation was unusual in two ways. First, there were no eye movements involved, so the normal reading processes may not have been used. Second, and more telling, readers could not perform a simple comprehension test after seeing the text this way. By contrast, several studies of more natural situations support the eye-mind as- sumption that readers pause on words that require more processing (Just & Carpenter, 1978; Carpenter & Daneman, Note 1). The eye-mind assumption posits that there is no appreciable lag between what is being fixated and what is being processed. This assumption has also been explored in spa- tial problem-solving tasks and has been supported in that domain as well as in read- ing (Just & Carpenter, 1976). The im- mediacy and eye-mind assumptions are used to interpret gaze duration data in the development of the reading model.
~-l
Get Next Input:
Move Eyes
Extract ^Physical Features
i
Encode Word and Access Lexicon
Assign Case Roles
The article has four major sections. The first briefly describes a theoretical frame- work for the processes and structures in reading. The second section describes the reading task and eye fixation results ac- counted for by the model. The third sec- tion presents the model itself, with subsec- tions describing each component process of the model. The fourth section discusses some implications of the theory for language comprehension and relates this theory of reading to other approaches.
Theoretical Framework
Reading can be construed as the coor- dinated execution of a number of process- ing stages such as word encoding, lexical access, assigning semantic roles, and relat- ing the information in a given sentence to previous sentences and previous knowl- edge. Some of the major stages of the proposed model are depicted schematically in Figure 1. The diagram depicts both pro- cesses and structures. The stages of reading in the left-hand column are shown in their usual sequence of execution. The long-term memory on the right-hand side is the store- house of knowledge, including the pro- cedural knowledge used in executing the
K \ >
WORKING MEMORY
physical features words
meanings
clauses
LONG TERM
Productions that represent
orthography phonology
semantics pragmatics discoursestucture scheme of
domain episodic knowledge
^J
THEORY OF READING 331
-
Representation /I domainofdiscourse
• Integrate with
text units
variable- binding memory
of Previous Text t
M
Figure 1. A schematic diagram of the major processes and structures in reading comprehension. (Solid lines denote data-flow paths, and dashed lines indicate canonical flow of control.)
 332 MARCEL ADAM JUST AND PATRICIA A. CARPENTER
stages on the left. The working memory in the middle mediates the long-term memory and the comprehension processes. Al- though it is easy to informally agree on the general involvement of these processes in reading, it is more difficult to specify the characteristics of the processes, their inter- relations, and their effects on reading per- formance.
The nature of comprehension processes depends on a larger issue, namely the ar- chitecture of the processing system in which they are embedded. Although the human architecture is very far from being known, production systems have been suggested as a possible framework because they have several properties that might plausibly be shared by the human system. Detailed dis- cussions of production systems as models of the human architecture are presented else- where (Anderson, 1976; Newell, 1973, 1980). The following three major properties are of particular relevance here.
1. Structural and procedural knowledge is stored in the form of condition-action rules, such that a given stimulus condition produces a given action. The productions "fire" one after the other (serially), and it is this serial processing that consumes time in comprehension and other forms of thought. In addition to the serial produc- tions, there are also fast, automatic pro- ductions that produce spreading activation among associated concepts (Anderson, 1976; Collins & Loftus, 1975). These auto- matic productions operate in parallel to the serial productions and in parallel to each other (Newell, 1980). These productions are fast and automatic because they operate only on constants; that is, they directly as- sociate an action with a particular condition (such as activating the concept dog on de- tecting cat). By contrast, serial productions are slow because they operate on variables as well as constants; they associate an action with a class of conditions. A serial production can fire only after the particular condition instance is bound to the variable specified in the production. It may be the binding of variables that consumes time and capacity (Newell, 1980). This architectural feature of two kinds of productions permits serial comprehension processes to operate in the foreground, whereas in the back-
ground, automatic productions activate relevant semantic and episodic knowledge.
2. Productions operate on the symbolsin a limited-capacity working memory. The symbols are the activated concepts that are the inputs and outputs of productions. Items are inserted into working memory as a result of being encoded from the text or being inserted by a production. Retrieval from long-term memory occurs when a produc- tion fires and activates a concept, causing it to be inserted into working memory. Long-term memory is a collection of pro- ductions that are the repositories of both procedural and declarative knowledge. In the case of reading, this knowledge includes orthography, phonology, syntax, and seman- tics of the language, as well as schemas for particular topics and discourse types (Schank & Abelson, 1977). A new knowl- edge structure is acquired in long-term memory if a new production is created to encode that structure (Newell, 1980). This occurs if the structure participates in a large number of processing episodes.
One important property of working memory is that its capacity is limited, so that information is sometimes lost. One way in which capacity can be exceeded (causing forgetting) is that the level of ac- tivation of an item may decay to some sub- threshold level through disuse over time (Collins & Loftus, 1975; Hitch, 1978; Reit- man, 1974). A second forgetting mech- anism allows for processes and structures to displace each other, within some limits (Case, 1978). Heavy processing require- ments in a given task may decrease the amount of information that can be main- tained, perhaps by generating too many competing structures or by actively inhibit- ing the maintenance of preceding informa- tion. There is recent evidence to suggest that working memory capacity (as opposed to passive memory span) is strongly cor- related with individual differences in read- ing comprehension performance, pre- sumably because readers with greater ca- pacity can integrate more elements of the text at a given time (Daneman & Carpenter, in press).
3. Production systems have a mechanism for adaptive sequencing of processes. The items in working memory at a given time
 enable a given production to fire and insert new items, which in turn enable another production, and so on. In this way, the inter- mediate results of the comprehension pro- cess that are placed in working memory can influence or sequence subsequent pro- cessing. There is no need for a super- ordinate controlling program to sequence the mental actions.
The self-sequencing nature of produc- tions is compatible with the model depicted in Figure 1. The composition of each stage is simply a collection of productions that share a common higher level goal. The pro- ductions within a stage have similar enabling conditions and produce actions that serve as conditions for other productions in the same stage. The productions within a stage need not be bound to each other in any other way. Thus the ordering of stages with a production system is accomplished not by direct control transfer mechanisms but an indirect self-sequencing accomplished by one production helping to create the con- ditions that enable the "next" production to fire.
This architecture permits stages to be exe- cuted not only in canonical orders but also in noncanonical orders. There are occa- sions when some stages of reading seem to be partially or entirely skipped; some stages seem to be executed out of sequence, and some "later" stages sometimes seem to be able to influence "earlier" stages (Levy, in press). Stages can be executed earlier than normal if their enabling conditions exist earlier than normal. For example, if a con- text strongly primes a case role, then the case assignment could precede the lexical access of a word. Having read John pounded the nail with a , a reader can assign the last word to the instrumental case on the basis of cues provided by the words pound and nail, before encoding hammer. This or- ganization can permit "context effects" in comprehension, where a strong preceding context shortens reading time on a given word or clause. This might occur if a pro- cessing stage that is normally intermediate between two others is partially or entirely eliminated. It could be eliminated if the pre- ceding stage plus the context provided sufficient enabling conditions for the later stage. Analogously, a misleading context
could lengthen comprehension time by pro- viding elements that enable conflicting processes.
The production system organization can also explain how "later" stages can in- fluence "earlier" stages, so that higher level schemas can affect word encoding, for example. If the productions of the normally later stage are enabled earlier than usual, then their outputs can serve as inputs to the normally earlier stage. The ordering of stages does not have to be entirely reversed to obtain this top-down influence. It may be sufficient for just a portion of the produc- tions of the "later" stage to fire in order to influence the "earlier" stage.
In this view of processing stages, several stages can be executed cotemporaneously in the sense that firings of productions of two or more stages may be interleaved. Conse- quently, data and control can be transferred back and forth among different stages, somewhat similarly to computer programs organized into coroutines. Coroutines are two or more subprograms that have equal status (i.e., there is no master-slave rela- tionship). When one coroutine obtains con- trol, it executes until it detects a condition indicating it should relinquish control, and then another coroutine executes, and so on. One interesting difference between co- routines and the production system model
is that coroutines generally transfer data between each other only along specified paths, used especially for this purpose. By contrast, productions "transfer" data by placing it in the working memory, so that all processes have access to it. In this sense, the working memory serves as a message center, and communication among stages is by means of the items in working memory. This is distinct from one stage feeding its output directly to another stage.
Research
Texts
This section describes the texts that were used in the reading research because their properties, both local and global, have a large influence on the processing. The global organization of a narrative text has been shown to influence how a reader recalls
THEORY OF READING 333
 334 MARCEL ADAM JUST AND PATRICIA A. CARPENTER
Definition Setting Come Contiqueneg \
Definition /Selling Couae C
The content of the passages was analyzed by segmenting the text into idea units and categorizing these units by means of a sim- ple text grammar. First, all of the 15 pas- sages were segmented into text units called sectors, producing 274 sectors. The average sector length was seven words. Each sector was judged to be a single meaningful piece of information, whether it consisted of a word, phrase, clause, or sentence. The general cri- teria for segmentation into sectors were similar to those used by Meyer and Mc- Conkie (1973), who related such text units to recall performance.
A simplified grammar was developed to categorize the sectors of the texts. The grammar (shown schematically in Figure 2) classifies the text units into a structure that is quasi-hierarchical. This abbreviated grammar captures most of the regularities in our short passages (see Vesonder, 1979, for a more complete grammar for longer scientific passages). The initial sentences generally introduced a topic—a scientific development or event. The beginnings of the passage sometimes gave details of the time, place, and people involved with the discovery. Familiar concepts were simply named, whereas unusual concepts were ac- companied by an explicit definition. The main topic itself was developed through specific examples or through subtopics that were then expanded with further descrip- tions, explanations, and concrete examples. Consequences, usually toward the end of the passage, stated the importance of the event for other applications. Table 1 shows how each text unit or sector in the "Fly- wheel" passage was classified according to these categories. Each of the 274 sectors was assigned to one of the five levels of the grammar by one of the authors. The levels of the grammar were further confirmed by a pretest involving 16 subjects who rated the importance of each sector in its pas- sage on a 7-point scale. The mean im- portance ratings differed reliably among the five levels F(4, 270) = 40.04, p <.01. Specifically, the means decreased mono- tonically through the five postulated levels. Hence, the grammar potentially has some psychological reality, and its relevance to
Figure 2. A schematic diagram of the major text- grammatical categories of information in the scientific paragraphs.
the text (Kintsch & van Dijk, 1978; Mandler & Johnson, 1977; Meyer, 1975; Rumelhart, 1977b; Thorndyke, 1977). The experiment reported next shows that the organization has at least part of its effect when the text is being read. Scientific texts were selected from Newsweek and Time because their content and style is typical of what students read to learn about technical topics. The passages discussed a variety of topics that were generally unfamiliar to the readers in the study. When readers were asked to rate their familiarity with the topic of each passage on a 5-point scale, the modal rating was at the "entirely unfamiliar" end of the scale. There were 15 passages, averaging 132 words each. Although the texts are moderately well written, they are on the borderline between ' 'fairly difficult'' and "difficult" on Flesch's (1951) reada- bility scale, with 17 words per sentence and 1.6 syllables per word. The following is an example of one of the passages:
Flywheels are one of the oldest mechanical devices known to man. Every internal-combustionenginecon- tains a small flywheel that converts the jerky motion of the pistons into the smooth flow of energy that powers the drive shaft. The greater the mass of a fly- wheel and the faster it spins, the more energy can be stored in it. But its maximum spinning speed is limited by the strength of the material it is made from. If it spins too fast for its mass, any flywheel will fly apart. One type of flywheel consists of round sand- wiches of fiberglass and rubber providing the maxi- mum possible storage of energy when the wheel is confined in a small space as in an automobile. Another type, the "superflywheel," consists of a series of rim- less spokes. This flywheel stores the maximum energy when space isunlimited.
 THEORY OF READING 335
Table 1
A Classification of the "Flywheel" Passage Into Text-Grammatical Categories
Category
Topic
Topic Expansion Expansion Expansion Cause Consequence Subtopic Subtopic Expansion Expansion Definition Expansion Expansion Detail Definition Expansion Detail
Sector
Flywheels are one of the oldest mechanical devices
known to man
Every internal-combustion engine contains a small flywheel
that converts the jerky motion of the pistons into the smooth flow of energy that powers the drive shaft
The greater the mass of a flywheel and the faster it spins,
the more energy can be stored in it.
But its maximum spinning speed is limited by the strength of the material
it is made from.
If it spins too fast for its mass,
any flywheel will fly apart.
One type of flywheel consists of round sandwiches of fiberglas and rubber providing the maximum possible storage of energy
when the wheel is confined in a small space
as in an automobile.
Another type, the "superflywheel," consists of a series of rimless spokes. This flywheel stores the maximum energy
when space isunlimited.
reading will be demonstrated with the eye fixation data. The next section presents the data collection and analysis procedures, followed by the model and results.
Method and Data Analysis
The readers were 14 undergraduates who read 2 practice texts followed by the 15 scientific texts in random order. Although the readers were asked to recall each passage immediately after reading it, they also were told to read naturally without memoriz- ing. They were also asked not to reread the passage or parts of it. The texts were presented on a television monitor using uppercase and lowercase letters and a conventional paragraph layout. To initiate the read- ing of a passage, the reader had to look at a fixation point (located where the first word of the paragraph would later appear) and press a "ready" button. If the reader's point of regard (as measured by the eye tracker) was within 1° of the fixation point, then exactly 500 msec later the passage appeared in its entirety on the screen. The passage appeared instantaneously (i.e., within one video frame) and remained there until the reader signaled that he had finished reading by push- ing a response button.
The reader's pupil and corneal reflections were monitored relatively unobtrusively by a television camera that was 75 cm away. The monitoring sys- tem, manufactured by Applied Science Laboratories, computed the reader's point of regard (as opposed to eye or head position) every 16.7 msec. The accuracy of the tracker was verified before and after each pas- sage was read by having the reader look at a fixation point and determining whether the obtained point of regard was within 1° of that point. This procedure
indicated that accuracy was maintained during the reading of 195 of the 210 experimental passages in the entire experiment; the data from the 15 inaccurate trials were discarded.
Data reduction procedures converted the 60 ob- servations per sec into fixations and then into gazes on each word. While the data were being acquired, a new "fixation" was scored as having occurred if the point of regard changed by more than 1°(the size of a three-letter syllable). The durations of blinks that were preceded and followed by fixations on the same loca- tion were attributed to the reading time on that loca- tion. Another program aggregated consecutive fixa- tions on the same word into gazes and computed the duration of gaze on each of the 1,936 words in the 15 passages. Fixations on interword spaces were at- tributed to the word on the right because the perceptual span is centered to the right of the point of regard, at least for readers of left-to-right languages (McConkie & Rayner, 1976; Schiepers, 1980). The durations of saccades, blinks that occurred between words, regres- sions, and rereading were not included in the data analysis. Because of the instructions not to reread, these categories account for relatively little of the total reading time, approximately 12%in all. The mean duration of gaze on each word was computed by averaging over readers; these 1,936 mean gaze dura- tions constitute the main dependent measure of interest.
The model presents a number of factors that in- fluence various reading processes; some factors have their effect on individual words and some on larger units, such as clauses. The data were fit to the model with a multiple linear regression in which the in- dependent variables were the factors postulated to af- fect reading time and the dependent variable was the mean gaze duration on each word. Since the model also applies at the level of clauses and phrases, a
 336 MARCEL ADAM JUST AND PATRICIA A. CARPENTER
second regression analysis was done at the phrase/ clause level. The independent variables for the latter analysis were the factors postulated to affect read- ing time at the clause level, and the independent variable was the mean gaze duration on each of the 274 sectors described previously.
The psychological interpretation of the independent variables in the two regression analyses will be described in detail in the sections that follow. The equation for the analysis of the gaze duration on in- dividual words was
GW,=£amXim +fi,
where GW( is the gaze duration on a word /, am is the regression weight in msec for independent variable Xm, Xlm are the independent variables that code the following seven properties of word i:
(a) length, (b) the logarithm of its normative fre- quency, (c) whether the word occurs at thebeginning of a line of text, (d) whether it is a novel word to the reader, (e) its case grammatical role (one of 11 pos- sibilities), (f) whether it is the last word in a sentence, (g) whether it is the last word in a paragraph.
The equation for the analysis of the gaze duration on individual sectors was
GSj = b0 + 5>» Zln + 6,
where GSj is the gaze duration on sector j, and bn is the regression weight in msec for independent variable Z n . The ZJn are the independent variables that code the following eight properties of sector j :
(a) its text grammatical level, multiplied by the num- ber of content words; (b) length; (c) the sum of the logarithms of the frequencies of its component words; (d) the number of line-initial words it contains; (e) the number of novel words it contains; (f) the sum of the case role regression weights of its component words; (g) whether it is the last sector in a sentence; (h) whether it is the last sector in a paragraph.
Results
The mean gaze duration on each word (239 msec) indicated reading rates that are typical for texts of this difficulty. If the 239 msec per word is incremented by 12% to allow for saccades, blinks, and occasional rereading, the reading rate is 225 words per min. The standard deviation of the 239- msec gaze mean was 168 msec, indicating considerable variability in gaze duration from word to word. The results of the re- gression analyses are shown in Table 2. The table is divided into three sections, cor- responding to the three major processing stages postulated by the model, encoding and lexical access, case role assignment, and interclause integration. The regression weights shown in Table 2 for the word-by- word analysis (above the double line) are de-
rived from a regression equation involving 17 independent variables (11 of which are the case role indicator variables). The stand-
ard error of estimate of this model was 88 2
msec, and the ^? value was .72. The results of the interclause integration stage make use of both the word-by-word analysis and the sector-by-sector analysis. (The latter analy- sis will be explained in more detail in the section on interclause integration). Since the gaze durations on successive words and phrases are time-series data, it is interesting to note that there was no reliable positive serial correlation among the residuals in the word-by-word regression or the sector-by- sector regression.
The Reading Model
The next five subsections describe the major stages shown in Figure 1: get next in- put, encoding and lexical access, case role assignment, interclause integration, and sentence wrap-up. Each subsection de- scribes the processes in that stage together with the factors that affect the duration of those processes, and hence the gaze durations.
Get Next Input
This is the first stage of a cycle that finds information, encodes it, and processes it. When the perceptual and semantic stages have done all of the requisite processing on a particular word, the eye is directed to land in a new place where it continues to rest until the requisite processing is done, and so forth. The specification of what con- stitutes "all of the requisite processing" is contained in a list of conditions that must be satisfied before the reader terminates the gaze on the current word and fixates the next one.These conditions include a speci- fication of the goals of normal reading. For instance, one condition may be that a mean- ing of the word be accessed and another condition may be that a case role be as- signed. These conditions can also reflect more specific reading goals. A reader who is trying to memorize a text may have as a condition that the word or phrase be trans- ferred to long-term memory. By setting the conditions appropriately, the reader can
 adjust his processes to the situation at hand. When the goal conditions for processing a word are satisfied, the resulting action is get next input.
The command to get next input usually results in a saccade to the next part of the text, one or two words forward. The process that selects the placement of the next for- ward fixation does not have to be very com- plex or intelligent. The choice of where to place the next forward fixation appears to depend primarily on the length of the next word or two to the right of the current fixation (McConkie & Rayner, 1975). The length information, which is encoded para- foveally, is then used to program a right- ward saccade. However, if only the right margin is visible in the parafovea, then the eye is directed to the first word of the next
Table 2
line, producing a return sweep. In this case the information in peripheral vision is not adequate for accurate targeting. The return sweep is typically too short; the eye often lands on the second word of the new line for a brief amount of time (50 or 75 msec) and then makes a corrective saccade leftward to the first word of the line (Bayle, 1942). On occasion, a comprehension stage may re- quire a review of previously read text to reencode it or process it to deeper levels. In those cases, the get next input stage results in a regressive saccade to the rele- vant portion of the text.
The duration of the get next input stage is short, consisting of the time for a neural sig- nal to be transmitted to the eye muscles. In monkeys, this takes about 30 msec (Robinson, 1972). This duration must not be
THEORY OF READING 337
Application of the Regression Model to the Gaze Duration on Each Word (Above Double Line) and to Each Sector (Below Double Line)
Processing stage Encoding and lexical access
Case role assignment
Interclause integration
Regression weight Factor (msec)
no. of syllables 52** log frequency 53** beginning of line 30** novel word 802**
agent (86) 51** instrument (110) 53** direct or indirect object (174) 25* adverb/manner (35) 29 place or time (64) 23 possessive (genitive) (39) 16
verb (368)
state/adjective (451) 44** rhetorical word (15) 70** determiner (243) 26** connective (351) 9
last word in sentence 71** last word in paragraph 157**
Integration time per content word from regression analysis of data aggregated into sectors
topic (22) 72* definition/cause/consequence (23) 94* subtopic (48) 78* expansion (68) 73* detail (113) 60'
Note. Frequency of occurrence of case roles is in parentheses. * t = p < 0.05; ** ( = p <.01.
33**
 338 MARCEL ADAM JUST AND PATRICIA A. CARPENTER
confused with the typical 150- to 200-msec latency of a saccade to a visual stimulus that has spatial or temporal uncertainty (Westheimer, 1954). These latencies include stimulus detection, interpretation, and selection of the next fixation target. In nor- mal reading, there is very little uncertainty about direction of the next saccade (it is almost always rightward for forward fixa- tions, except for the return sweeps), nor is there much uncertainty about distance. On the average, the saccade distance may be simply the mean center-to-center distance between words, a distance that does not vary much, relative to the physically pos- sible variation in eye movements. Thus it is reasonable to suppose that the preprogram- ming time is very short here, consisting usually of a "go" signal and the time it takes that signal to be translated into a motor movement, about 30 msec (Robinson, 1972). The actual movements, the saccades, con- stitute about 5%-10% of the total reading time. Recent analyses suggest that the sac- cade itself may destroy the visual per- sistence of the information from the preced- ing fixation so that it does not mask the in- put from the new fixation (Breitmeyer, 1980). Consequently, it is reasonable to as- sume that stimulus encoding can commence soon after the eye arrives at a new location.
Word Encoding and Lexical Access
The reading process involves encoding a word into an internal semantic format. It is assumed that prior to this encoding, the transduction from the printed word to the visual features has already taken place, and that the features have been deposited into the working memory. Perceptual encoding productions use the visual features as condi- tions; their action is to activate the repre- sentation of the word. Once the representa- tion of the word has been sufficiently activated, its corresponding concept is ac- cessed and inserted into working memory. The concept serves as a pointer to a more complete representation of the meaning, which consists of a small semantic network realized as a set of productions. The major nodes of the network are the possible mean- ings of the word, the semantic and syntactic
properties of the meanings, and informa- tion about the contexts in which they usually occur (see Rieger, 1979, for a related pro- posal).The word meanings are represented as abstract predicates, defined by their rela- tions to other predicates.
The productions that encode a word generally trigger on orthographically based subword units such as syllables (Mewhort & Beal, 1977; Spoehr & Smith, 1973; Taft,
1979). However, there are times when al- ternative codes, including orthographic, phonological, and whole-word codes, are used (Baron, 1977; Kleiman, 1975; LaBerge & Samuels, 1974). Since the syllablelike encoding is believed to be the dominant mode, the data were analyzed in terms of the number of syllables in each word. Encoding time increased by 52 msec for each syllable, as shown in Table 2.
The mechanism underlyinglexical access is the activation of a word's meaning repre- sentation by various sources. There are three ways that a concept's level of ac- tivation can be temporarily increased above its base level. One activation mechanism is perceptual encoding; the encoded repre- sentation of a word can activate its meaning. A second source is the parallel productions that produce spreading activation through the semantic and episodic knowledge base of the reader. The third source is activa- tion by the serial productions that do the major computations in all of the stages of processing. When a concept has been ac- tivated above some threshold by one or more of these sources, a pointer to its mean- ing is inserted into working memory. The activation level gradually decays to a sub- threshold level unless some process reac- tivates it. If the word soon reoccurs in the text while the concept is still activated, lexical access will be facilitated because the activation level will still be close to thresh- old. When the activation level does de- crease, it decreases to an asymptote slightly higher than the old base level. In this way, the system can learn from both local and long-term word repetitions. Frequently used words will have a high base level of activation, and consequently will require relatively less additional activation to re- trieve them. Thus, frequent words should
 take less time to access than infrequent words (Morton, 1969). Similarly, the various possible interpretations of each word will have different base activation levels, such that the more common inter- pretations have higher base activation levels. For example, although the word does has at least two very different meanings, the "third-person-singular verb" interpretation would have a higher base activation because it is more common than the ' 'female deer'' interpretation (Carpenter & Daneman, Note 1). The more common interpretation would then be accessed faster, since less additional activation would be required to bring the activation level to threshold. This model of lexical access can account for word fre- quency effects, priming effects, and repeti- tion effects in reading.
The gaze duration showed both frequency and repetition effects. Frequency was ana- lyzed by relating gaze duration to the logarithm of the normative frequency of each word, based on the Kucera and Francis (1967) norms. It was expected that gaze duration would decrease with the logarithm of the word's frequency; that is, small differences among infrequent words would be as important as much larger differences among frequent words (Mitchell & Green, 1978). For algebraic convenience, the normative frequencies were increased by one (to eliminate the problem of taking the logarithm of zero), and the logarithm was computed and then subtracted from 4.85, the logarithm of the frequency of the most frequent English word. The analysis in- dicated a clear relation between this mea- sure of frequency and gaze duration. As shown in Table 2, gaze duration increased by 53 msec for each log unit of decrease in word frequency. A moderately frequent word like water (with a frequency of 442) was accessed 140 msec faster than a word
that did not appear in the norms.
At one extreme of the frequency dimen- sion are words that a reader has never en- countered before. In scientificpassages, the novel words tend to be technical terms. To read these words, a reader cannot depend on contacting some prior perceptual and se- mantic representation; neither exists. The reader must construct some perceptual
representation (perhaps phonological as well as orthographic), associate this with the semantic and syntactic properties of the concept that can be inferred from the pas- sage, and then possibly construct a lexical entry. These processes seem to take a great deal more time than ordinary encoding and access processes. Two judges identified seven words in the texts (that had zero fre- quency) as probably entirely novel to the readers. Novelty was coded as an indicator variable, and it was found that these words took an additional 802 msec on average to process, as shown in Table 2. However, there was considerable variability among the words; their gaze durations ranged from 913 msec (for staphyllococci) to 2,431 msec (for thermoluminescence).
Once a word has been encoded and ac- cessed once, it should be easier to access it when it occurs again. Other research has suggested that frequency and repetition have their primary effect on lexical access rather than encoding (Dixon & Rothkopf, 1979; Glanzer & Ehrenreich, 1979; Scar- borough, Cortese, & Scarborough, 1977), although the possibility of some small ef- fects on the encoding process does exist. According to the model, repetition effects should occur in reading because the first time a word meaning is accessed, it should temporarily achieve a higher activation level similar to the level of a more frequent word. This mechanism particularly predicts repetition effects for infrequent words, whose activation levels are low to start with, but not for the highly frequent words that occur in natural text. Generally, repeti- tion effects are larger for low-frequency words (Scarborough et al., 1977). "Low
frequency" in the Scarborough study was defined as less than 28 occurrences per mil- lion, the boundary of 28 emerging from a median split of the frequencies of their stimuli. So the analysis of repetition ef- fects was limited to words with frequencies of 25 occurrences per million or less. There were 346 such instances in the text; 251 were initial occurrences and 95 were repeti- tions. The repetitions were words with the same morphological stem, disregarding af- fixes. An analysis of covariance on this sub- set of the data examined the effects of repe-
THEORY OF READING 339
 340 MARCEL ADAM JUST AND P A TRICIA A. CARPENTER
titions covarying out the number of syl- lables. The adjusted mean gaze durations were 49 msec longer on the initial appear- ance of these words than on the subsequent appearances, f(343) = 2.21, p < .03. Most of this effect (43 msec) was obtained on the second appearance of a word. These results indicate that once an infrequent word ap- pears in a text, processing time on that word is decreased on subsequent appearances.
allows the activation of the unselected inter- pretations to decay, preventing them from activating their associates. Thus, the con- textual effects would remain focused in the appropriate semantic domain. This permits a limited-capacity working memory to cope with the information flow in a spreading ac- tivation environment that may activate many interpretations and associations for any lexical item. This method of processing also avoids the combinatorial explosion that results from entertaining more than
Lexical access is complicated by the fact
that some words have more than one mean-
ing, so the appropriate interpretation must one interpretation for several succes- be selected, or at least guessed at. When a
polysemous word is accessed, the word
representation that is retrieved is a pointer
to a semantic network that includes the mul-
tiple representations. The interpretation word are initially activated, only one that is selected is the one with the highest meaning remains activated after a few hun- activation level, and several factors can af- dred milliseconds. In one experiment, the fect the activation. First, some interpreta- subjects simultaneously listened to a tions start off with a higher activation level; sentence and pronounced a visually pre- for instance, the "third-person-singular" sented word. When an ambiguous word interpretation of does has a higher base (rose) was presented auditorally in a activation level than the "deer" interpreta-
tion. Second, the automatic productions
that produce spreading activation can con-
tribute selectively to the activation level of
one particular interpretation. The spreading
activation can emanate from the preceding
semantic and syntactic context, from the
reader's knowledge of the domain, and from
knowledge of the discourse style. Third, the
output of other stages operating on the same
word may activate a particular interpreta-
tion. For example, although hammer can be
interpreted as a noun or a verb, a sentence
context that suggests an instrument to the
case role assignment stage (e.g., John hit the
nail with a ) may help activate the noun interpretation. Fourth, when a word with many highly related meanings occurs in an impoverished context, there may be no single interpretation with higher activation than the others, and the superordinate con- cept may be the selected interpretation of the word. This probably occurs for words that have many closely related interpreta- tions, such as get and take.
The selection of only one interpretation of each word, posited by the immediacy as- sumption, provides a measure of cognitive economy. Selecting just one interpretation
syntactic context (e.g., They all rose), the speed of pronouncing a simultaneous visual probe related to either meaning (stood or flower) was faster than in a control condi- tion (Tanenhaus, Leiman, & Seidenberg, 1979). In another experiment, the subjects listened to a sentence and performed a lexi- cal decision task on visually presented stimuli. When an ambiguous word (bug) was presented in a semantic context (John saw several spiders, roaches, and bugs), the speed of a simultaneous lexical decision related to either meaning (insect or spy) was faster than a control (Swinney, 1979). In both studies, the facilitation of the inap- propriate meaning was obtained only within a few hundred milliseconds of the occur- rence of the ambiguous word. If the probe was delayed longer, the inappropriate inter- pretation was no faster than the control. These results suggest that both meanings are available when an ambiguous word is being accessed, but the inappropriate meaning is lost from workingmemory after a short time.
As the interpretation of the text is con- structed, a corresponding representation of the extensive meaning—the things being talked about—is also being built. If the referents of the words in a passage cannot
sive words.
This aspect of the model is consistent with
some recent results on lexical access that in- dicate that although multiple meanings of a
 be determined, the text will be more dif- ficult to understand. One example of this problem is highlighted in a passage from Bransford and Johnson (1973) concerning a procedure that involved arranging "things into groups. Of course, one may be suf- ficient depending on how much there is. . . ." (p. 400). Subjects who were not given the title washing clothes though the story was incomprehensible. The referential representation helps the reader disam- biguate referents, infer relations, and inte- grate the text.
The immediacy assumption posits that an attempt to relate each content word to its referent occurs as soon as possible. Some- times this can be done when the word is first fixated, but sometimes more information is required. For example, although the se- mantic interpretation of a relative adjective like large can be computed immediately, the extensive meaning depends on the word it modifies (e.g., large insect vs. large build-
ing). The referent of the entire noun phrase can be computed only after both words are processed. The immediacy assumption does not state that the relating is done im- mediately on each content word, but rather that it occurs as soon as possible. This is an important distinction that will be made again in the discussion on integrative processes.
Assigning Case Roles
role can be an important contributor to the assignment process. But this normative in- formation generally is not sufficient to as- sign its case role in a particular clause. Con- sequently, the assignment process relies on heuristics that use the word meaning to- gether with information about the prior semantic and syntactic context, as well as language-based inferences. The output of the process is a representation of the word's semantic role with respect to the other constituents in its clause.
Just as certain meanings suggest par- ticular case roles, so, too, can the context prime a particular case role. Consider the sentence John was interrogated by the
The semantic and syntactic cues sug- gest that the missing word will be an agent, such as detective. The strength of the con- text becomes evident if the primed case does not occur, for example, John was interrogated by the window. The prior semantic context can precede the affected case assignment by more than a few words. In the sentences The lawyer wanted to know where in the room John had been interro-
gated and Mary told him that John was interrogated by the window, the thematic focus of the first sentence on a location alters the interpretation of by and facilitates a locative case role assignment for window.
The specific heuristics that are used in case role assignment have received some attention (see Clark & Clark, 1977, for some examples). Many proposals contain the suggestion that readers use the verb as a pivotal source of information to establish the necessary and possible case roles and then fit the noun phrases into those slots (Schank, 1972). But the immediacyassump- tion posits that the case role assignment for an item preceding the verb is not post- poned in anticipation of the verb. Similar to the lexical access stage, the case assign- ment stage makes a best guess about a word's case when the word is fixated, rather than making the decision contingent on sub- sequent words. So, the model would not ac- cord any special status to verbs. Another suggested heuristic (that children appear to use) is to assign a sequence consist-
THEORY OF READING 341
Comprehension involves determining
the relations among words, the relations
among clauses, and the relations among
whole units of text. This section describes
the first of these processes, that of deter-
mining the relations among the words in a
clause (or in Schank's, 1972, terms, deter-
mining the dependencies among the con-
cepts). These relations can be categorized
into semantic cases, such as agent, recipi-
ent, location, time, manner, instrument,
action, or state (Chafe, 1970; Fillmore,
1968). The case role assignment process
usually takes as input a representation of the
fixated word, including information about
its possible case roles and syntactic proper-
ties. For example, hammers tend to be instru-
ments rather than locations or recipients, ing of animate noun-verb-noun to the and information about a word's usual case case roles of agent-action-object (Bever,
 342 MARCEL ADAM JUST AND PATRICIA A. CARPENTER
1970). Like all heuristics, this one some- times fails, so young children sometimes misinterpret passive sentences (Eraser, Bellugi, & Brown, 1963). This heuristic may be employed by adults, but in a modified version that conforms to the immediacy as- sumption. Rather than waiting for the three major constituents before assigning case roles, the reader should assign an animate noun to the agent role as soon as it is en- countered, in the absence of contrary prior context.
The immediate assignment of a case role implies that readers will sometimes make errors and have to revise previous deci- sions. For example, an adult who assigns the role of agent to an animate noun and then encounters a passive verb will have to revise the agent assignment. (Presumably, young children do not make this revision.) The im- mediacy of the case assignment process is evident in the reading of sentences such as Mary loves Jonathan. . . . The im- mediacy assumption suggests that a reader would assign to Jonathan the role of recipient; this would in turn result in an in- correct assignment if the sentence con- tinued Mary loves Jonathan apples.
Because case roles are assigned within clauses, the assignment process must in- clude a segmentation procedure to deter- mine clause boundaries within sentences. Sentences can sometimes be segmented into clauses on the basis of explicit markers, such as a subordinating conjunction (e.g., because, when). More often, the reader can- not tell with certainty where one clause ends and another starts until beyond the clause boundary (or potential boundary). A general strategy for dealing with such cases has been suggested, namely to assign a word to the clause being processed, if possible (Frazier & Fodor, 1978). For example, the word soil in the sentence When farmers are
plowing the soil . . . can continue the initial clause (When farmers are plowing the soil, it is most fertile) or start a new one (When farmers are plowing the soil is most fertile). The suggested strategy is to continue the initial clause until contrary in- formation is encountered. Interestingly, the strategy discussed by Frazier and Fodor (1978) presupposes the immediacy assump- tion; the segmentation decision arises be-
cause case roles are assigned as soon as the words are encountered.
There is no direct mapping between par- ticular case roles and the duration of the assignment process. For example, there is no a priori reason to expect that assignment of instruments takes more or less time than locations. The time for a particular assign- ment might depend more on the context and properties of the word than on the particular case role being assigned. Detailed specifica- tion of the process is not within the scope of this article; it probably requires a large-scale simulation model to examine the complex interactions of different levels of process- ing. Nevertheless, we examined whether, all things being equal, different case role assignments tend to take different amounts of time.
The analysis included the usual case roles just noted (Fillmore, 1968), as well as other categories such as determiners and adjec- tives that are not cases but still play a part in the parsing and assignment process. Each word was classified into 1of 11categories: verb, agent, instrument, indirect or direct object, location or time, adverb, adjective or state, connective (preposition or conjunc- tion), possessive, determiner, and rhetorical word (such as well). Some cases were pooled (such as location and time) because they were relatively infrequent in the text and because they have some concep- tual similarity. The case roles were coded as indicator variables and were all entered into the regression with the intercept forced
to zero.
The results of the case role assignment
analysis, shown in Table 2, indicate that there are some variations among the cases. As expected, verbs did not take particularly long (33 msec), and in fact, although the time was significantly different from 0, it was not greater than the agent orinstrument cases (51 msec and 53 msec, respectively). Four cases had parameters that were not significantly different from 0, connectives (9 msec), adverb/manner (29 msec), place or time (23 msec), and possessives (16 msec). These parameters could reflect some properties of particular word classes, in addition to parsing and case role assign- ment processes. For example, if a connec- tive (e.g., and or but) simply takes less
 THEORY OF READING 343
time to access than other words, the ad-
vantage should appear in this parameter.
However, the parameters are not due solely
to length or frequency, since these variables
make a separate contribution to the regres-
sion equation. Although this analysis does
not examine any of the contextual ef-
fects thought to be of some importance in representation for potential points of at-
the case assignmentprocess, it does indicate roughly the relative amount of time spent assigning various categories of words to their case roles in a clause. Later theories will have to account for the precise pat- tern of case assignment durations in terms of specific operations that use prior con- text and word meanings to assign the various cases.
Interclause Integration
Clauses and sentences must be related to each other by the reader to capture the co- herence in the text. As each new clause or sentence is encountered, it must be in- tegrated with the previous information ac- quired from the text or with the knowledge retrieved from the reader's long-term mem- ory. Integrating the new sentence with the old information consists of representing the relations between the new and the old structures.
Several search strategies may be used to locate old information that is related to the new information. One strategy is to check if the new information is related to the other information that is already in working mem- ory either because it has been repeatedly referred to or because it is recent (Car- penter & Just, 1977a; Kintsch & van Dijk, 1978). Using this strategy implies that ad-
jacency between clauses and sentences will cause a search for a possible relation. For instance, the adjacent sentences Mary hurt herself 'and John laughed seem related (John must be a cad) even though there is no ex- plicit mention of the relation. This strategy also entails trying to relate new informa- tion to a topic that is active in working memory. This is a good strategy, since information in a passage should be related to the topic.
A second strategy is to search for specific connections based on cues in the new sen- tence itself. Sentences often contain old in-
tachment between the new information and the old (Haviland & Clark, 1974). This second strategy may take more time than the first. In fact, it takes longer to read a sentence that refers to information intro- duced several sentences earlier than one that refers to recently introduced informa- tion (Carpenter & Just, 1977a).
There are two main points at which inte- gration can occur. First, as each ensuing word of the text is encountered, there is an attempt to relate it to previous informa- tion (Just & Carpenter, 1978). Second, a running representation of the clause is maintained, with an updating as each word of the clause is read. This running clause representation consists of the configuration of clause elements arranged according to their case relations. This second type of integration involves an attempt to relate the running clause representation to pre- vious information at each update. Integra- tion occurs whenever a linking relation can be computed. Consider the sentence Al- though he spoke softly, yesterday's speaker could hear the little boy's question. The point of this example is not so much that the initial integration of he and speaker is incor- rect, but that the integration is attempted at the earliest opportunity. This model im- plies that integration time may be dis-
tributed over fixations on different parts of a clause. Moreover, the duration of the process may depend on the number of con- cepts in the clause; as these increase, the number of potential points of contact be- tween the new clause and previous informa- tion will increase. There is also evidence for integration triggered by the end of the sentence; this process is discussed next in more detail.
Integration results in the creation of a new structure. The symbol representing that structure is a pointer to the integrated con- cepts, and this superordinate symbol is then available for further processing. In this
formation as well as new. Sometimes the old information is explicitly marked (as in cleft constructions and relative clauses), but often it is simply some argument repeated from the prior text. The reader can use this old information to search his or her long- term text representation and referential
 344 MARCEL ADAM JUST AND P A TRICIA A.CARPENTER
way, integration can chunk the incoming text and allows a limited working memory to deal with large segments of prose. The macrorules proposed by Kintsch and van Dijk (1978) can be construed as productions that integrate.
Integration can also lead to forgetting in working memory. As each new chunk is formed, there is a possibility that it will dis- place some previous information from working memory. Particularly vulnerable are items that are only marginally activated, usually because they were processed much earlier and have not recently participated in a production. For instance, the repre- sentation of a clause will decay if it was processed early in a text and was not re- lated to subsequent information. This mech- anism can also clear working memory of "lower level" representations that are no longer necessary. For example, the ver- batim representation of a previously read sentence may be displaced by the processes that integrated the sentence with other in- formation (Jarvella, 1971). By contrast, the semantic elements that participate in an in- tegration production obtain an increased activation level. This increases the proba- bility that they will become a permanent part of long-term memory.
The main types of interclause relations in the scientific passages correspond to the text-grammatical categories described previously, such as definitions, causes, con- sequences, examples, and so forth. Text roles that are usually more important to the text and to the reader's goals, such as topics or definitions, are integrated dif- ferently than less important units, such as details. The more central units will initiate more retrievals of relevant previous knowl- edge of the domain (schematic knowledge) and retrievals of information acquired from the text but no longer resident in the work- ing memory. In addition, more relations will be computed between the semantically central propositions and previous informa- tion because centrality inherently entails relations with many other units. By con- trast, details are often less important to the reader's goals and to the text. Moreover, when a detail is to be integrated, the process is simpler because details are often con-
crete instantiations of an immediately preceding statement (at least in these scientific texts), so they can be quickly ap- pended to information still present in the working memory. Thus, higher level units will take more time to integrate because their integration is usually essential to the reader's goals, and because integration of higher units involves more relations to be computed and more retrievals to be made.
The nature of the link relating two struc- tures may be explicitly denoted either in the text (with connectives like because, there-
fore, and for instance) or it may have to be inferred on the basis of schematic knowl- edge of the domain. For example, the causal relation between the sentences Cynthia fell off the rocking horse and She cried bitter tears is inferred from the reader's knowl- edge about the temporal and causal relation between falling and hurting oneself (Char- niak, Note 2).
The model predicts that the gaze duration on a sector depends on its text-grammatical role and on the number of concepts it con- tains. Because integration can occur at many points in a sector, the gaze duration associated with integration cannot be localized to a particular word. Thus, to do the clause level of analysis, the gaze dura- tions on the individual words of a sector were cumulated, producing a total of 274 sector gaze durations as the dependent variable. The independent variables were the aggregates of the word-level variables, except for case roles. The independent variable that coded the case-role effect for a sector was the sum of the case-coefficients (obtained from the word-by-word regres- sion analysis) for each of the words in the sector. A new independent variable coded the text-grammatical role of a sector and its number of content words; it was the inter- action of the indicator variables that repre- sented the five text-grammatical levels and the number of content words in the sector, with content words defined as in Hock- ett (1958).
The results indicate that the integration time for a given sector depends on its text- grammatical role. The portion of Table 2 below the double line shows the integration time per content word for each type of sec-
 tor. Generally, more important or central sectors take longer to integrate. The model describes this effect in terms of the inte- grative processes initiated by the seman- tics of the different types of information and their relevance to the reader's goals. An analysis of covariance examined the effect of text roles covarying out the number of syllables. The adjusted mean gaze durations differed reliably, F(4, 268) = 8.82,p < .01; paired comparisons indicated that details took significantly less time than all other roles, and expansions took significantly less time than topics and definitions/causes/ consequences (all ps < .01). These results quantitatively and qualitatively replicate those reported previously for a slightlydif- ferent paradigm (Carpenter & Just, in press). The previously obtained coefficients for the five text-grammatical categories were 65, 106, 81, 76, and 47 msec per con- tent word, respectively, corresponding to the newly obtained 72, 94, 78, 73, and 60.
The model accounts very well for the sector-
2
level data. The R value was .94, and the
standard error of estimate was 234. The mean gaze duration on a sector was 1,690 msec, with a standard deviation of 902msec, and the mean sector length was 4.9 words.1
One cost of immediate interpretation, case role assignment, and integration is that some decisions will prove to be incorrect. There must be mechanisms to detect and recover from such errors. The detection of a misinterpretation often occurs when new information to be integrated is incon- sistent with previous information. Thus, misinterpretation detection may be con- strued as inconsistency detection. For example, the sentence There were tears in her brown dress causes errors initially be- cause the most frequent interpretation of tears is not the appropriate one here, and the initial interpretation is incompatible with dress. The eye fixations of subjects reading such garden path sentences clearly indicate that readers do detect inconsistencies, typically at the point at which the incon- sistency is first evident (Carpenter & Dane- man, Note 1). At that point, they use a num- ber of error-recovery heuristics that enable them to reinterpret the text. They do not start reinterpreting the sentence from
its beginning. The heuristics point them to the locus of the probable error. Readers start the backtracking with the word that first reveals the inconsistency, in this case, dress. If that word cannot be reinterpreted, they make regressions to the site of other words that were initially difficult to inter- pret, such as ambiguous words on which a best guess about word meaning had to be made. The ability to return directly to the locus of the misinterpretation and to recover from an error makes the immediacy strategy feasible.
Sentence Wrap-Up
A special computational episode occurs when a reader reaches the end of a sentence. This episode, called sentence wrap-up, is not a stage of processing defined by its func- tion, but rather by virtue of being executed when the reader reaches the end of a sen- tence. The processes that occur during sentence wrap-up involve a search for ref- erents that have not been assigned, the con- struction of interclause relations (with the aid of inferences, if necessary), and an at- tempt to handle any inconsistencies that could not be resolved within the sentence.
The ends of sentences have two important properties that make them especially good places for integration. First, within-sen- tence ambiguitiesare usuallyclarified by the end of the sentence. For example, if a sen- tence introduces a new object or person whose identity cannot be inferred from the preceding context, some cue to their identity is generally given by the end of the sentence. For that reason, if readers can-
1
It might be argued that the variables coding the text-grammatical roles ought to be independent of the number of content words. One might argue that a definition, for example, takes a fixed amount of time to integrate, regardless of the number of content words it contains. Although the model predicts a length- sensitive duration, the analysis can also be done with five simple indicator variables to encode the five levels of the grammar. This analysis produced a fit that was almost as good (R* = .93). The weights (assuming a zero intercept) were 250, 341, 257, 214, and 118 msec for the five categories, from topics to details. Although this alternative is not ruled out by the data, we will continue to retain the view that integration time depends on the number of content words involved.
THEORY OF READING 345
 346 MARCEL ADAM JUST AND PATRICIA A. CARPENTER
not immediately determine the referent of a particular word, then they can expect to be told the referent or given enough informa- tion to infer it by the end of the sentence. Indeed, readers do use the ends of sentences to process inconsistencies that they cannot resolve within the sentence (Carpenter & Daneman, Note 1). The second property is that the end of a sentence unambiguously signals the end of one thought and the be- ginning of a new one. It can be contrasted with weaker cues that signal within-sen- tence clause boundaries such as commas, relative pronouns, and conjunctions that can signal other things besides the end of a clause. Since ends of sentences are unam- biguous, they have the same role across sentences, and they may be processed more uniformly than the cues to within-sentence clause boundaries.
There is ample empirical support for the integrative processing at the ends of sen- tences. Previous eye fixation studies show that when a lexically based inference must be made to relate a new sentence to some previous portion of the text, there is a strong tendency to pause at the lexical item in question and at the end of sentence that contains it (Just & Carpenter, 1978). Read- res were given paragraphs containing pairs of related sentences; the first noun in the second sentence was the agent or instru- ment of the verb in the first sentence:
(la) It was dark and stormy the night the millionaire was murdered.
(Ib) The killer left no clues for the police to trace, In another condition, the integrating in-
ference was less direct:
(2a) It was dark and stormy the night the millionaire died.
(2b) The killer left no clues for the police to trace.
It took about 500 msec longer to process Sentence 2b than Ib, presumably due to the more difficult inference linking killer to die. There were two main places in which the readers paused for those 500 msec, in- dicating the points at which the inference was being computed. One point was on the word killer, and the other was on the end of the sentence containing killer. Another eye fixation study showed that integration link- ing a pronoun to its antecedent can occur
either when the pronoun is first encountered or at the end of the sentence containing the pronoun (Carpenter & Just, 1977b).
Reading-time studies also have shown that there is extra processing at the end of a sentence. When subjects self-pace the word-by-word or phrase-by-phrase pre- sentation of a text, they tend to pause longer at the word or phrase that terminates a sentence (Aaronson & Scarborough, 1976; Mitchell & Green, 1978). The pause has been attributed to contextual integration processes, similar to the proposed inter- clause integration process here. Y et another source of evidence for sentence wrap-up processes is that verbatim memory for re- cently comprehended text declines after a sentence boundary (Jarvella, 1971; Perfetti & Lesgold, 1977). The model attributes the decline to the interference between sen- tence wrap-up processes and the main- tenance of verbatim information in working memory. Finally, another reason to expect sentence wrap-up processes is that we have observed pauses at sentence terminations in an eye fixation study similar to the one re- ported here (Carpenter & Just, in press). However, the current study provides stronger evidence because the text was presented all at once.
The results indicate that readers did pause longer on the last word in a sentence. As Table 2 shows, the duration of the sentence wrap-up period is 71 msec.
It is possible that wrap-up episodes could occur at the ends of text units smaller or larger than a sentence. For example, the data of Aaronson and Scarborough (1976) suggest that there are sometimes wrap-up processes at the ends of clauses. It is also possible that wrap-up could occur under some circumstances at the ends of para- graphs. The decision of when and if to do a wrap-up may be controlled by the desired depth of processing. For example, skim- ming may require wrap-up only at paragraph terminations, whereas understanding a legal contract may require wrap-up at clause boundaries. In fact, the clause-boundary ef- fects obtained by Aaronson and Scar- borough are sensitive to the subjects' read- ing goals. The current analysis indicated that the final word in the paragraph might
 also be a wrap-up point; it received an ad- ditional 157 msec of fixation. However, since readers also pressed a button to in- dicate that they had finished reading the passage, this parameter might be influenced by their motor response.
Finally, the model included one other fac- tor that involves a physical property of reading, namely the return sweep of the eyes from the right-hand side of one line of text to the left-hand side of the next line. Return sweeps are often inaccurate, landing to the right of the first word in a line. The inac- curacy is often corrected by a leftward saccade to the first word. As a result of this error and recovery, the first word on a line eventually receives an increased gaze duration, relative to a line-medial word. Al- most all readers we have studied display the undershoot, but there are considerable individual differences in whether they com- pensate for it by making an extra leftward fixation to the first word. In fact, some re- searchers have associated these corrective leftward movements with poor readers (Bayle, 1942). To test for increased gaze
durations on line-initial words, an indicator variable coded whether a word was the first one on aline. As Table 2 shows, these words received an additional 30 msec of fixation.
Fit of the Model
To see how well the model accounts for the data, one can informally compare how closely the estimated gaze durations match the observed gaze durations. The dis- play that follows shows the estimated (in italics) and observed (in msec) gaze dura- tions for two sentences from the "Fly- wheel" passage. The estimated durations can be computed by an appropriate com- bination of the weights given in Table 2. These estimates take into account the processes of encoding, lexical access, case- role assignment, sentence wrap-up, and the beginning of the line effect; they do not include integration time for text roles, since there is no way to distribute this time on a word-by-word basis. In spite of this, the match is satisfactory, and as mentioned earlier, the standard error of estimate was 88 msec overall.
THEORY OF READING 347
Observed mean gaze durations (msec) on each word of a text sample and estimates (italicized, from the word-by-word regression analysis.
169 215 165 295 290 73 196 504 29 482 0 328 431 51
165 236 75 409 304 75 249 438 75 413 80 338 349 78 . . . One type of flywheel consists of round sandwiches of fiberglas and rubber providing the
369 326 308 22 272 253 128 199 69 336 32 41 267 197 70 164 195
354 318 297 75 378 138 77 239 128 326 87 102 206 209 112 87 127
maximum possible storage of energy when the wheel is confined in a small space as in an
340 323 182 72 626 276 46 21 346 60 467 519
465 334 236 77 513 304 75 102 289 75 361 319
automobile. Another type, the "superflywheel," consists of a series of rimless spokes . . .
Table 3 presents an analogous compari- son from the sector-by-sector analysis; this includes integration time. Again, the esti- mates from the model match the observed data quite well. The standard error of estimate was 234 msec overall.
Another way to evaluate the goodness of fit is to compare the regression results to those of another model that lacks most of the theoretically interesting independent variables and contains only the variable that codes the number of syllables. For the word-by-word analysis, this rudimentary
2
model produces an R of .46, compared to
.72 for the complete model. For the sector- by-sector analysis, the rudimentary model accounts for a large portion of the variance between the gaze durations on sectors (R2 = .87). This is not surprising, since there is considerable variation in their lengths. The complete sector-by-sector model accounts for 94% of the variance, or 54% of the variance unaccounted for by the re- duced model.
The regression equations were also fit to
 348 MARCEL ADAM JUST AND PATRICIA A. CARPENTER
Table 3
Observed and Estimated Gaze Durations (msec) on Each Sector of the ''Flywheel'' Passage, According to the Sector-By-Sector Regression Analysis of the Group Data
the gaze durations of each of the 14 readers individually. The subjects varied in their reading skill, with self-reported Scholastic Aptitude Test scores ranging from 410 to 660, which were correlated with their read- ing speeds in the experiment, ranging from
186 words per min. to 377 words per min. 2
r(12) = .54, p < .05. The mean R of the 14 readers was .36 on the word-by-word analysis and .75 on the sector-by-sector analysis. This indicates substantial noise in each reader's word-by-word data. Some of the regression weights of the readers in- dicated considerable individual differences with respect to certain processes. For example, 4 of the 14 readers spent no extra time on the last word of a sentence. Another parameter of great variability among readers was the extra time spent on novel words, which ranged from 94 msec to 1,490 msec.
Although the sector-by-sector regression analysis uses an independent variable (the sum of the case role coefficients) that is estimated from the same data, this pro- cedure does not do violence to the results. To estimate the effect of this procedure, the 14 subjects were divided randomly into two subgroups, and the case-role coefficients were obtained for each subgroup in a word- by-word analysis. Then these coefficients were aggregated and used as independent
variables in a sector-by-sector analysis, such that one subgroup's coefficients were used in the analysis of the other subgroup's sector gaze durations. The results indicated no difference of any importance between the two subanalyses, and generally con- firmed that using the case role coefficients from the word analysis in the sector analysis was an acceptable procedure.
Some of the variables that were reliable in the word-by-word analysis were not reliable in the sector analysis. For example, sectors that included a line-initial word did not have reliably longer durations, and sec- tors that included the end of a sentence took 57 msec longer, but the reliability of the effect was marginal (p < .08). The sum of the logarithms of the frequencies of the words in a sector did not reliably affect gaze duration on the sector. These differ- ences between the two levels of analysis indicate that some effects that are word specific are not reliable or large enough to be detected when the data are aggregated over groups of words. Nevertheless, some of these effects can be detected at the sec- tor level if the appropriate analysis is done. For example, the reason that the frequency effect was not reliable is that the aggrega- tion of the logarithms smooths over the differences between infrequent words and
Sector
Observed
1,921 478 2,316 2,477 1,056 2,143 1,270 2,440 615 1,414 1,200 2,746 1,799 1,522 769 2,938 1,416 1,289
Estimated
1,999 680 2,398 2,807 1,264 2,304 1,536 2,553 780 1,502 1,304 3,064 1,870 1,448 718 2,830 1,596 1,252
Flywheels are one of the oldest mechanical devices
known to man.
Every internal-combustion engine contains a small
that converts the jerky motion of the pistons into the smooth flow of energy that powers the drive shaft.
The greater the mass of a flywheel and the faster it spins,
the more energy can be stored in it.
But its maximum spinning speed is limited by the strength of the material it is made from.
If it spins too fast for its mass,
any flywheel will fly apart.
One type of flywheel consists of round sandwiches of fiberglas and rubber providing the maximum possible storage of energy
when the wheel is confined in a small space
as in an automobile.
Another type, the "superflywheel," consists of a series of rimless spokes. This flywheel stores the maximum energy
when space is unlimited.
flywheel
 frequent words. A regression analysis of the sector data shows a reliable word fre- quency effect if the independent variable encodes the numberof infrequent words (ar- bitrarily defined as less than 25 in Kucera & Francis, 1967) occurring for the first time. This latter analysis indicates 82-msec
tial agreement (i.e., within 50%) on 94% of the judgments; disagreements were re- solved by a third judge.
Text units that were higher in the text grammar were generally recalled better, F(4, 269) = 5.67, p < .01. There was a monotonic increase in the probability of re- call as a function of a sector's level in the text grammar. Recall probabilities were lowest for details (.31), then increased for expansions (.34), subtopics (.39), defini- tions/causes/consequences (.41), and topics (.53). This replicates previous text-role ef- fects observed with other types of texts (Meyer, 1975; Thorndyke, 1977). The model partially explains this result in terms of the processes that occur during com- prehension. In addition, retrieval pro- cesses may play a role in this effect. For example, there may be many retrieval paths from less important concepts that lead to topics, but not vice versa. Also, a complete model of recall will have to consider how the recall of particular facts is affected by the reader's previous knowledge. Although the passages were generally unfamiliar, par- ticular facts surely differed in their familiar- ity, and this could have a powerful ef- fect on recall (Spilich, Vesonder, Chiesi, & Voss, 1979). Finally, there could be re- sponse output effects in recall. In summary, the results show that a model of the com- prehension processes can be used to par- tially account for recall performance. To totally explain recall will require a precise account of the role of prior long-term knowledge and the role of retrieval and
reconstruction processes in recall.
Discussion
This section discusses three aspects of the theory: first, the implications of the im- mediacy assumption for language process- ing in general; second, how variation in reading modes can be handled by the theory; and third, the relation of the current theory to other theories of reading.
The Immediacy Assumption
The model's ability to account for fixa- tion durations in terms of the processes that operate on words provides some valida-
extra spent for each infrequent word, and 2
has an R of .94. (Carpenter & Just, in press, reported a 51-msec effect for this variable).
Recall Performance
The recall of a given part of a text should depend in part on what happens to the infor- mation as it is read. A clause that is thoroughly integrated with the representation of the text should tend to be stored in long-term mem- ory, and therefore should be recalled better. There are two factors that determine how well a clause will be integrated. First, those sectors on which more integration time has been spent, like topics and definitions, should be recalled better. As predicted the integration parameter for a text role (i.e., the five weights at the bottom of Table 2) reliably affected the probability that a sec- tor would be recalled, ?(271) = 2.01, p < .05. A second factor affecting integration is the number of times an argument of a clause is referred to in the text; each repeti- tion involving that argument may initiate another integration episode that increases its chances of being recalled (Kintsch & van Dijk, 1978). A rough index of this kind of repetition was obtained by counting the number of times the arguments of each sec- tor were repeated in subsequent sectors. The frequency of reference to the argu- ments did increase the probability of re- calling a sector, f(271) = 5.90, p < .01.
The recall measure just reported was the proportion of the 14 subjects that recalled each of the 274 sectors. Two independent
judges assigned 100%, 50%, or 0% credit for the recall of each sector, depending on whether it had been fully, partially, or not at all correctly recalled. Synonyms and paraphrases were given full credit if they were close to the gist of the sector. If only a part of a sector was recalled, then partial credit was given. The two judges were in full agreement about 80% of the time and in par-
THEORY OF READING 349
 350 MARCEL ADAM JUST AND PATRICIA A. CARPENTER
tion for the immediacy and eye-mind as- sumptions. Readers interpret a word while they are fixating it, and they continue to fixate it until they have processed it as far as they can. As mentioned before, this kind of processing eliminates the difficulties caused by the potential ambiguity in lan- guage. It avoids the memory load and com- putational explosion that would result if a reader kept track of several possible mean- ings, case roles, and referents for each word and computed the final interpretation at the end of a clause or a sentence. This architectural feature also allows a limited- capacity processor to operate on a large semantic network without being bom- barded by irrelevant associations. After a single interpretation has been selected, the activation of the unselected meanings can be dampened to their base levels so that they will not activate their semantic associates any further. This minimizes the chances that the reader will be conceptually driven in many directions at the same time.
The cost of this kind of processing is fairly low because the early decisions usually are correct. This is accomplished by taking a large amount of information into account in reaching a decision. The pro- cesses have specific heuristics to combine semantic, syntactic, and discourse informa- tion. Equally important, the processes operate on a data base that is strongly biased in favor of the common uses of words and phrases, but one that also reflects the ef- fects of local context. The cost is also low because the reader can recover from errors. It would be devastating if there were no way to modify an incorrect interpretation at some later point. However, there are error- recovery heuristics that seem fairly ef- ficient, although the precise mechanisms are only now being explored (Carpenter & Daneman, Note 1).
The fact that a reader's heuristics for interpreting the text are good explains why the garden path phenomenon is not the predominant experience in comprehension; it only happens occasionally. Perhaps the most common, everyday garden path ex- periences occur when reading newspaper headlines; for example, Carter Views Dis-
cussed andJudge Admits TwoReporters. The incorrect initial interpretations occur be- cause headlines are stripped of the syntac- tic and contextual cues that guide the pro- cessing of normal text. Similarly, many jokes and puns explicitly rely on the con- trast between two interpretations of an am- biguous word or phrase (Schultz & Horibe, 1974). Even garden path sentences some- times seem funny. The humor in all of these cases resides in the incongruity between the initial interpretation and the ultimate one. Garden path sentences are also infrequent because writers usually try to avoid am- biguities that might encourage or allow in- correct interpretations. These kinds of sentences are useful tools for studying com- prehension because they indicate where the usual comprehension strategies fail. But the fact that they are not frequent in- dicates that a reader's heuristics usually are sufficient.
Variation in Reading
There is no single mode of reading. Read- ing varies as a function of who is reading, what they are reading, and why they are reading it. The proposed model for the read- ing of scientific texts in this task is only one point in a multidimensional space of reading models. However, such variation can be accommodated within the framework presented in this article.
The reader's goals are perhaps the most important determinant of the reading pro- cess. A reader who skims a passage for the main point reads differently than some- one who is trying to memorize a passage, or another person who is reading for entertain- ment. Goals can be represented in several aspects of the theory, but the main way is to require that each goal is satisfied or at least attempted before proceeding on to the next word, clause, or sentence. These goals cor- respond to the major products of each stage of comprehension and to the specific de- mands of a particular task. For example, an obvious goal associated with lexical access might be that one interpretation is selected. An added goal associated with the task of memorizing a passage may require
 rehearsing phrases or constructing explicit mnemonics before going on to the next phrase or sentence. But goals can be deleted as well as added. A speed-reader may well eliminate goals for syntactic coherence, be- cause the strategy of skipping over many words will destroy the syntax. Variations in goals can be detected with the current theory and analytic techniques. For exam- ple, it is possible to determine how much time is spent integrating different kinds of text roles in different tasks. When readers anticipate a recognition comprehension test, rather than recall, they spend less time integrating details (Carpenter & Just, in press).
Reading also depends on the text, the topic, and the reader's familiarity with both. A well-written paragraph on a familiar topic will be easier to process at all stages of comprehension. The lexical items will be easier to encode, the concepts will be more easily accessed, the case and text roles will be easier to infer, and the interrelations will be easier to represent. All of these dimen- sions of variation can be accommodated, measured, and evaluated within the theo- retical framework. Moreover, any adequate theory must be sufficiently flexible to en- compass such variation.
Even reading of the same text under the same circumstances will vary from person to person. There are several plausible sources of individual differences in the theory. One interesting source is the opera- tional capacity of the working memory. Readers with a large working memory should be able to retain more of the text in the memory while processing new text, so their integration of the information may be more thorough. A promising first ex- ploration of this hypothesis has found a very strong correlation between working memory capacity and various aspects of reading comprehension tests (Daneman & Carpenter, in press). By contrast, tradi- tional measures of passive short-term memory capacity do not have a strong cor- relation with reading comprehension. Operational capacity may depend on the automaticity of basic reading processes such as encoding and lexical access. Poor
readers may devote more time and atten- tion to these processes (Hunt, Lunneborg, & Lewis, 1975; Perfetti & Lesgold, 1977) and consequently have less capacity for maintaining previous information and inte- grating the new information (Case, 1978).
Theories of Reading
Previous theories of reading have varied in their choice of dependent measures, the levels of information represented in the theory, and the implementationoftop-down effects. It is useful to consider how the current theory compares to these alterna- tive proposals along these three dimensions.
One important feature of the current theory is its attempt to account for reading time on individual words, clauses, and sen- tences. This approach can be distinguished from research that is more centrally con- cerned with recall, question answering, and summarizing (e.g., Rumelhart, 1977b). The dependent measure is not an incidental as- pect of a theory; it has important implica- tions for which issues the theory addresses. The present focus on processing time has resulted in a theory that accounts for the moment-by-moment, real-time characteris- tics of reading. By contrast, the theory pays less attention to retrieval and recon- struction, two later occurring processes that are important to an account of summarization.
Another feature of the theory is the at- tempt to account for performance at several levels of processing. Previous theories have tended to neglect certain stages. For example, the reading models of LaBerge and Samuels (1974) and Gough (1972) focus on the word-encoding pro- cesses, whereas the model of Kintsch and van Dijk (1978) focuses on integration. This is not to say that these models do not ac- knowledge other aspects of processing, but simply that they describe detailed mech- anisms for one aspect of reading and no comparable mechanisms for other stages. The current theory has attempted to span the stages of reading by describing mech- anisms for the word-encoding and lexical- access stages, as well as the parsing and text integration stages. Moreover, it has
THEORY OF READING 351
 352 MARCEL ADAM JUST AND PATRICIA A. CARPENTER
attempted to describe some formal similari- ties by placing them all within the architec- ture of a production system.
A final but important distinction among reading theories is the manner in which they accommodate top-down and bottom-up fac- tors in reading (see Rumelhart, 1977a). Some reading theories, particularly those addressed to word encoding, omit mech- anisms to account for top-down or contex- tual effects (e.g., Gough, 1972). At the other extreme, there have been some theories that appear to place a major burden of compre- hension on contextual effects. Some of these are recent schema-based theories of language comprehension (Schank & Abel- son, 1977). Others are the older top-down models, developed out of analysis-by- synthesis theory; these models suggested that readers form explicit predictions about the next word and fixate it merely to con- firm the hypothesis (Goodman & Niles, 1970). The current model falls somewhere between the extremes. It allows for con- textual influences and for the interaction among comprehension processes. Knowl- edge about a topic, syntactic constraints, and semantic associates can all play a role in activating and selecting the appropriate concepts. However, the printed words themselves are usually the best information source that the reader has, and they can seldom be entirely replaced by guesses from the preceding context. Thus the top-down processes can influence the bottom-up ones, but their role is to participate in selecting interpretations rather than to dominate the bottom-up processes. Finally, the produc- tion system architecture permits a degree of coordination among different processes, so that any stage can be influenced by any cotemporaneously or previously executed stage.
Future Directions
The current theory suggests two major avenues of reading research. One direction is to construct computer simulations that are driven by reading performance data. The postulated human heuristics can be imple- mented in a computer program to examine the resulting complex interactions among
knowledge sources. Reading-time data may be sufficiently constraining to select among various alternative heuristics. We are cur- rently implementing aspects of the model presented here as a production system in collaboration with a colleague, Robert Thibadeau, to develop greater specification and more stringent tests of the model.
Although the production system frame- work is not essential for the interpreta- tion of the empirical results in the present study, it has other benefits. First, it provides an architecture that can accommodate the flexibility and interaction that has been ob- served among the processes in reading and still express typical or canonical process- ing. Even though this theoretical framework is minimally specified, it seems sensible to start at this point and allow successive generations of data to constrain it, as Newell (1980) suggests. Finally, when expressed as a computer simulation, the model retains correspondence to postulated human pro- cesses and structures. Collections of serial productions may correspond to heuristic processes employed in comprehension. The firing of parallel productions can be identi- fied with spreading activation in long-term memory. The production system's working memory can be identified with the reader's working memory. Thus, the production sys- tem can be viewed as a useful theoretical vehicle, or excess baggage, depending on one's intended destination.
The second avenue includes further em- pirical research on the real-time character- istics of reading. Eye movement and read- ing-time methodologies can reveal reading characteristics with other types of texts, tasks, and readers. The useful property of these methodologies is that they can mea- sure reading time on successive units of text. One method is to present the successive words of a sentence one at a time, allowing the reader to control the interword interval ( A a r o n s o n & S c a r b o r o u g h , 1976). T h i s procedure is only one end of a continuum defined by what units are presented. Rather than single words, they could be phrases, clauses, sentences, or entire passages (Car- penter & Just, 1977a; Mitchell & Green, 1978; Kieras, Note 3). In this way, it will be possible to gain more information about
 human performance characteristics and then use these data to develop a more complete theory of reading.
Reference Notes
1. Carpenter, P. A., & Daneman, M. Lexical access and error recovery in reading: A model based on eye fixations. Unpublished manuscript. Carnegie- Mellon University, 1980.
2. Charniak, E. Toward a model of children's story comprehension (Tech. Rep. 266). Cambridge, Mass.: MIT Artificial Intelligence Laboratory, 1972.
3. Kieras, D. E. Modelling reading times in different reading tasks with a simulation model of compre- hension (Tech. Rep. 2). Tucson: University of Arizona, 1979.
References
Aaronson, D., & Scarborough, H. S. Performance theories for sentence coding: Some quantitative evidence. Journal of Experimental Psychology; Human Perception and Performance, 1976, 2, 56-70.
Anderson, J. R. Language, memory, and thought. Hillsdale, N.J.: Erlbaum, 1976.
Bayle, E. The nature and causes of regressive move- ments in reading. Journal of Experimental Educa- tion, 1942, //, 16-36.
Baron, J. Mechanisms for pronouncing printed words: Use and acquisition. In D. LaBerge & S. J. Samuels (Eds.), Basic processes in reading: Perception and comprehension. Hillsdale, N.J.: Erlbaum, 1977.
Bever, T. G. The cognitive basis for linguistic struc- tures. In J. R. Hayes (Ed.), Cognition and the development of language. New York: Wiley, 1970.
Bouma, H., & deVoogd, A. H. On the control of eye saccades in reading. Vision Research, 1974, 14, 273-284.
Bransford, J. D., & Johnson, M. K. Considerations of some problems of comprehension. In W. G. Chase (Ed.), Visual information processing. New York: Academic Press, 1973.
Breitmeyer, B. G. Unmasking visual masking: A look at the "why" behind the veil of "how." Psycho- logical Review, 1980, 87, 52-69.
Buswell, G. T. How adults read. Supplementary educational monographs (45). Chicago, 111.: Uni- versity of Chicago Press, 1937.
Carpenter, P. A., & Just, M. A. Integrative processes in comprehension. In D. LaBerge & S. J. Samuels (Eds.), Basic processes in reading: Perception and comprehension. Hillsdale, N.J.: Erlbaum, 1977. (a)
Carpenter, P. A., & Just, M. A. Reading compre- hension as eyes see it. In M. A. Just & P. A. Car- penter (Eds.), Cognitive processes in comprehen- sion. Hillsdale, N.J.: Erlbaum, 1977. (b)
Carpenter, P. A., & Just, M. A. Cognitive processes in reading: Models based on readers' eye fixations. In A. M. Lesgold & C. A. Perfetti (Eds.), Interactive processes in reading. Hillsdale, N.J.: Erlbaum, in press.
Case, R. Intellectual development from birth to adult- hood: A neo-Piagetian interpretation. In R. Siegler (Ed.), Children's thinking: What develops? Hills- dale, N.J.: Erlbaum, 1978.
Chafe, W. L. Meaning and the structure of language. Chicago, 111.: University of Chicago Press, 1970. Clark, H. H., & Clark, E. V. Psychology and language.
New York: Harcourt Brace Jovanovich, 1977. Collins, A. M., & Loftus, E. F. A spreading activation theory of semantic processing. Psychological Re-
view, 1975, 82, 407-428.
Daneman, M., & Carpenter, P. A. Individual differ-
ences in working memory and reading. Journal of
Verbal Learning and Verbal Behavior, in press. Dearborn, W. The psychology of reading (Columbia University contributions to philosophy and psy-
chology). New York: Science Press, 1906.
Dixon, P ., & Rothkopf, E. Z. Word repetition, lexical access, and the process of searching words and sentences. Journal of Verbal Learning and Verbal
Behavior, 1979, 18, 629-644.
Fillmore, C. J. The case for case. In E. Bach & R. T.
Harms (Eds.), Universals in linguistic theory.
New York: Holt, Rinehart & Winston, 1968. Flesch, R. F. How to test readability. New York:
Harper, 1951.
Fraser, C., Bellugi, U., & Brown, R. Control of
grammar in imitation, comprehension, and produc- tion. Journal of Verbal Learning and Verbal Behavior, 1963,2, 121-135.
Frazier, L., & Fodor, J. The sausage machine: A new two-stage parsing model. Cognition, 1978, 6, 291- 325.
Glanzer, M., & Ehrenreich, S. L. Structure and search for the internal lexicon. Journal of Verbal Learning and Verbal Behavior, 1979, 18, 381-398.
Goodman, K. S., & Niles, O. S. Reading process and program. Urbana, 111.: National Council of Teachers of English, 1970.
Gough, P. B. One second of reading. In J. F. Kavanagh & I. G. Mattingly (Eds.), Language by eye and ear. Cambridge, Mass.: MIT Press, 1972.
Haviland, S. E., & Clark, H. H. What's new? Acquir- ing new information as a process in comprehension. Journal of Verbal Learning and Verbal Behavior, 1974, 13, 512-521.
Hitch, G. J. The role of short-term working memory in mental arithmetic. Cognitive Psychology, 1978, 10, 302-323.
Hockett, C. F. A course in modern linguistics. New York: Macmillan, 1958.
Hunt, E., Lunneborg, C., & Lewis, J. What does it mean to be high verbal? Cognitive Psychology, 1975,2, 194-227.
Jarvella, R. J. Syntactic processing of connected speech. Journal of Verbal Learning and Verbal Behavior, 1971, 10, 409-416.
Judd, C. H., & Buswell, G. T. Silent reading: A study of the various types. Supplementary Educa- tional Monographs (23). Chicago, 111.: University of Chicago Press, 1922.
Just, M. A., & Carpenter, P. A. Eye fixations and cognitive processes. Cognitive Psychology, 1976, 8, 441-480.
THEORY OF READING 353
 354 MARCEL ADAMJUST AND
PATRICIA A. CARPENTER
Just, M. A., & Carpenter, P. A. Inference processes during reading: Reflections from eye fixations. In J. W. Senders, D. F. Fisher, & R. A. Monty (Eds.), Eye movements and the higher psychological func- tions. Hillsdale, N.J.: Erlbaum, 1978.
Kintsch, W., & van Dijk, T. A. Toward a model of text comprehension and production. Psychological Review, 1978,55, 363-394.
Kleiman, G. M. Speech receding in reading. Journal of Verbal Learning and Verbal Behavior, 1975, 14, 323-339.
Kucera, H., & Francis, W. N. Computational analysis of present-day American English. Providence, R.I.: Brown UniversityPress, 1967.
LaBerge, D., & Samuels, S. J. Toward a theory of automatic information processing in reading. Cogni- tive Psychology, 1974, 6, 293-323.
Levy, B. A. Interactive processes during reading. In A. M. Lesgold & C. A. Perfetti (Eds.), Interactive processes in reading. Hillsdale, N.J.: Erlbaum, in press.
Mandler, J. M., & Johnson, N. S. Remembrance of things parsed: Story structure and recall. Cognitive Psychology, 1977,9, 111-151.
McConkie, G. W., & Rayner, K. The span of the effective stimulus during a fixation in reading. Perception & Psychophysics, 1975, 17, 578-586.
McConkie, G. W., & Rayner, K. Asymmetry of the perceptual span in reading. Bulletin of the Psycho- nomic Society, 1976, 8, 365-368.
Mewhort, D., & Beal, A. L. Mechanisms of word identification. Journal of Experimental Psychology: Human Perception and Performance, 1977, 3, 629-640.
prehension model. In N. V. Findler (Ed.), Associa-
tive networks. New York: Academic Press, 1979. Robinson, D. A. Eye movements evoked by collicular stimulation in the alert monkey. Vision Research,
1972,72, 1795-1808.
Rumelhart, D. E. Toward an interactive model of
reading. In S. Dornic (Ed.), Attention and per- formance VI. Hillsdale, N.J.: Erlbaum, 1977. (a)
Rumelhart, D. E., Understanding and summarizing brief stories. In D. LaBerge & S. J. Samuels (Eds.), Basic processes in reading: Perception and com- prehension. Hillsdale, N.J.: Erlbaum, 1977. (b)
Scarborough, D. L., Cortese, C., & Scarborough, H. S. Frequency and repetition effects in lexical memory. Journal of Experimental Psychology: Human Per-
ception and Performance, 1977,5, 1-17.
Schank, R. C. Conceptual dependency: A theory of natural language understanding. Cognitive Psy-
chology, 1972,3, 552-631.
Schank, R. C., & Abelson, R. P. Scripts, plans, goals
and understanding: An inquiry into human knowl-
edge structures. Hillsdale, N.J.: Erlbaum, 1977. Schiepers, C. Response latency and accuracy in visual word recognition. Perception & Psychophysics,
1980,27, 71-81.
Shultz, T ., & Horibe, F. Development of the apprecia-
tion of verbal jokes. Developmental Psychology, 1974, 10, 13-20.
Spilich, G. J., Vesonder, G. T., Chiesi, H. L., & Voss, J. F. Text processing of domain-related information for individuals with high and low domain knowledge.Journal of Verbal Learning and Verbal Behavior, 1979, 18, 275-290.
Spoehr, K. T., & Smith, E. The role of syllablesin perceptual processing. Cognitive Psychology, 1973, 5, 71-89.
Swinney, D. A. Lexical access during sentence comprehension: (Re)consideration of context ef- fects. Journal of Verbal Learning and Verbal Behavior, 1979, 18, 645-659.
Taft, M. Recognition of affixed words and the word frequency effect. Memory & Cognition, 1979, 7, 263-272.
Tanenhaus,M. K.,Leiman, J. M., & Seidenberg, M. S. Evidence for multiple stages in the processing of ambiguous words in syntactic contexts. Journal of Verbal Learning and Verbal Behavior, 1979, 18, 427-440.
Taylor, S. E. An evaluation of forty-one trainees who had recently completed the "reading dynamics" program. Eleventh yearbook of the national reading conference, 1962, 41-55.
Thorndyke, P. W. Cognitive structures in comprehen- sion and memory of narrative discourse. Cognitive Psychology, 1977, 9, 77-110.
Vesonder, G. T. The role of knowledge in the process- ing of experimental reports. Unpublished doctoral dissertation, University of Pittsburgh, 1979.
Westheimer, G. H. Eye movement responses to a horizontally moving visual stimulus. Archives of Ophthalmology, 1954,52, 932-943.
Received May 14, 1979 Revision received February 29, 1980 •
Meyer, B. The organization of prose and its effect recall. Amsterdam: North-Holland, 1975.
on
Meyer, B., & McConkie, G. W. What is recalled after hearing a passage? Journal of Educational Psy- chology, 1973,65, 109-117.
Mitchell, D. C., & Green, D. W. The effects of context and content on immediate processing in reading. Quarterly Journal of Experimental Psychology, 1978, 30, 609-636.
Morton, J. Interaction of information in word recogni- tion. Psychological Review, 1969,76, 165-178.
Newell, A. Production systems: Models of control structures. In W. G. Chase (Ed.), Visual information processing. New York: Academic Press, 1973.
Newell, A. Harpy, production systems and human cognition. In R. Cole (Ed.), Perception and produc- tion affluent speech. Hillsdale, N.J.: Erlbaum, 1980.
Perfetti, C. A., & Lesgold, A. M. Discourse compre- hension and sources of individual differences. In M. A. Just & P. A. Carpenter (Eds.), Cognitive processes in comprehension. Hillsdale, N.J.: Erl- baum, 1977.
Rayner, K. Eye movements in reading and information processing. Psychological Bulletin, 1978, 85, 618- 660.
Reitman, J. S. Without surreptitious rehearsal, in- formation in short-term memory decays. Journal of Verbal Learning and Verbal Behavior, 1974, 13, 365-377.
Rieger, C. J. Five aspects of a full-scale story com-
Comic visualization on smartphones based on eye tracking
Olivier Augereau Osaka Prefecture University Osaka , JAPAN augereau.o@gmail.com
ABSTRACT
The visualization of comic images on a small screen is a difficult problem as the image is too large to be displayed on the screen and we do not know which areas and in which order the users want to see the image. The basic solution for the user is to look at the image in full screen without being able to see the details, or to zoom and scroll through the image, which can be quite inconvenient if the interactions have to often be repeated. Our idea is to use an eye tracker to record where the users reading a comic on paper books or large screens are looking at, to reproduce their reading behaviors with a comic visualization system and guide the users using a smaller screen through the comic.
CCS Concepts
•Human-centered computing → Interaction techniques; Visual-
ization techniques; Ubiquitous and mobile devices; Keywords
Eye tracking; Comic image; Visualization; Smartphone.
1. INTRODUCTION
Reading Japanese mangas, American comics or French bande dessinees on a smartphone is getting more and more popular in the recent years as the smartphones are getting used by more and more people and the sizes of the screen are getting larger. If the size of the screen is large enough, such as tablet screens, displaying the full image is a straightforward solution. But, as we can see in Table 1, the difference between standard comic sizes and a typical smartphone screen is still very large (roughly, from 2 to 4 times).
Reading large images on a small screen is problematic as the user can hardly read the text and see some details of the images. In this case, the user needs to do many manipulations such as zooming and translation in order to read and go through the image.
Until now, the main idea to solve this problem was to segment the images in frames and to detect the reading order [16], [14], [1], [6]. Unfortunately, even the newest proposed methods to segment the comic images fail on complex layouts such as overlapping or
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
MANPU ’16, December 04 2016, Cancun, Mexico
© 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4784-6/16/12. . . $15.00
DOI: http://dx.doi.org/10.1145/3011549.3011553
Mizuki Matsubara Osaka Prefecture University Osaka , JAPAN
Koichi Kise
Osaka Prefecture University Osaka , JAPAN
matsubara@m.cs.osakafu- kise@cs.osakafu-u.ac.jp u.ac.jp
borderless frames [13], [10] [15], [6]. Furthermore, even if the frames are segmented correctly, it often happens that the frame width is the same as the page, or to have a full or double full page containing no frames. In these cases, the image is still too large to be displayed on a small screen and no solutions have been proposed. The last problem to solve is the reading order which is not necessarily easy to predict and is still an open problem for the complex cases[7], [5].
Jain and. al. showed [8] that the artist is leading viewer attention through his drawing and the intended visual route can be found by recording the viewer attention. More recently, the same researchers [9] used the eye gaze information to animate still comic images. The considered comic images are not full pages of comics but just single frames. The proposed solution consists in clustering the eye gaze positions of the readers in two groups and to compute an animation for going from the first group to the second one by analyzing the direction of the saccades.
We propose in this paper a new alternative for visualizing the comic images which is different than the classical way based on segmenting each page frame by frame. When reading a comic, the reader essentially focuses on the text balloons and face of the characters [12] so we think the visualization system should be driven by this information, rather by the frames themselves. As illustrated in Fig. 1, we focus on a solution based on reproducing the real reader behaviors.
We consider that some readers will read the same comic on large screens or on paper. The idea is to use an eye tracker to record the position where the users are looking at, detecting the Regions of Interest (RoIs) and then, to create an ordered sequence of these RoIs that we call a “reading path”. The reading path is obtained from different readers in order to avoid noise or atypical reading behavior of a single user. This path is used to guide a user using a small screen through the comic. To simplify, our visualization system could be seen as a “record and play” system.
This system presents several benefits. It can be used to “play” a comic for people who are no familiar with the reading order. Depending on the reader profile (genre, age, expertise, 1st reading,
Table 1: Approximate typical sizes of different comic types com- pare to the size of a smartphone screen. The width of a smart- phone screen size is around 2 to 4 times smaller.
  Type
  Width
 Height
 Comics
  17 cm
 26 cm
 Mangas
  13 cm
 18 cm
 Bande dessinees
  22 cm
 29 cm
 IPhone 5 screen
  5.2 cm
 9.0 cm

 Figure 1: Principle of our visualization method. The reading behavior is recorded by an eye tracker and then used to guide another reader using a small screen. The red circles represent the positions where the readers using a large screen looked at. Our visualization method consists in translating the comic im- age on the smartphone screen from one red point to another.
etc.) different reading paths can be obtained and used to personalized the reading experience. As the comic can be played automatically, the system can be used for people who cannot interact easily with the smartphone.
The remaining sections of the paper are organized as follow. First we will present how to record the reading behavior of a user. Then a method for combining several reading behaviors and creating a reading path will be presented. After, we will show how the comic can be visualized on the smartphone. In the next section, the experiments will be detailed. Finally, we will conclude and explore some future works.
2. RECORDING THE READING BEHAVIOR
The reading behavior is recorded by an eye tracker. The output of the recording is a sequence of eye gaze positions of the reader associated with a timestamp. If the eye tracker is stationary, the positions are directly obtained. If a mobile eye tracker is used,
the read page is retrieved from the front camera and the eye gaze coordinates are projected from the perspective plan of the camera to the 2D plan of the comic image. More details about the process based on mobile eye tracker can be found in [3] and [2].
The reading behavior is composed of fixations (the position where the reader is looking for around 250-350 ms) and saccades (the quick movements of the eye between two fixations) [11]. We process the raw eye gaze positions in a sequence of fixations and saccades by using the Buscher et al. algorithm [4]. In the next section we explain how to create a reading path by combining the reading behaviors of several users.
3. CREATING A READING PATH
In order to be robust to some eye tracking errors or atypical reading behaviors (for example, only one participant looks at a specific position), we aggregate the reading behaviors of multiple readers. Then, we identify the RoIs and order them to create the reading path that will be used for visualizing the comic on a small screen.
3.1 Finding the RoI
In a first step, we create a heat map based on the reading behavior of multiple readers. The heat map is useful to find the main RoIs and filter the noise. The filter can be seen as a Gaussian filter. Fig. 2 shows a heat map and the corresponding RoIs.
For each reader, the normal distribution (σ, μ) centered on each fixations is computed. The range of the distributions differs for each reader, so each distribution is normalized by dividing each value by the maximal value of the distribution. The heat map is obtained by summing up the normalized distribution of all the readers.
Then, a RoI is detected for each local maximal value of the heat map. We experimentally fixed a threshold Ts in order to ignore the small local maximal values.
3.2 Ordering the RoIs
The heat map is useful for summing up the reading behaviors but the time information is lost, so we need to find the order of the RoIs. While visualizing the comic on a smartphone, the user can navigate through the different RoIs by interacting with the screen (basically, going to the next or to the previous RoI). So each RoI should appear only one time and the reading path should not contain any rereading behavior.
Figure 2: The heat map (on the left) and the corresponding RoIs (on the right). On the heat map, the yellow and red col- ors show a high density of fixations. The local maximum are selected as ROIs (the blues circles on the image on the right).

     Page
Frame
  RoI
 First
  1
8
  1
 Second
  3
0
  5
 Third
  6
2
  4
    Figure 3: Processing of the RoI sequences. The longest consec- utive sequences of each RoI are selected to create the “filtered sequences”. Then, these sequences are used for voting and cre- ating the reading path.
After obtaining the RoIs, we process each individual reading by associating each fixation to the nearest RoI. A threshold Tf is used in order to remove the fixations which are too far from any RoI. The fixation sequence is transformed in a string where each character of the string represents a RoI. As illustrated in Fig. 3 the string is processed in order to remove multiple occurrences of a RoI: only the longest consecutive sequence of each RoI is kept. In case of equality, the first RoI is kept.
After this process, each string participate to a vote to find the order of the RoIs. We analyze the first character of each string. The one which appear the most often is selected and removed from each string. The same process is repeated until all characters have been used. The reading path is made of the selected characters.
Table 2: Ranking of the favorite visualization methods by the users.
heat map. The threshold Tf used to remove the fixations which are too far from any RoI has been set to 316 pixels. The screen size used for the experiment is: 336.2 mm * 597.6 mm (1080 pixels * 1920 pixels).
5.1 Eye gaze recording
We displayed the right and left pages at the same time on a large screen. The comic contains one double pages illustration. We asked 23 participants to read the comic. The age of the participants is early 20s and they are all familiar with reading comics. One of the participants is a Japanese female, one is a Chinese male, all others are Japanese males. The comic is written in Japanese and all participants have enough Japanese ability to read it.
The eye tracker used in the experiment is a low cost eye tracker, similar to Eye Tribe1.
Before starting the experiment, each participant has been asked to read a simple summary of the comic story and the eye tracker is calibrated. After that, we asked the participants to read the comic displayed on the screen in a natural way while recording their eye gazes.
5.2 Comic visualization
We asked 10 participants to read the same comic (Kingdom episode 175) on a smartphone with three different visualizations: page by page, frame by frame and RoI by RoI (the proposed method). The segmentation frame by frame have been done manually.
After reading the comic, the participants were asked to answer a questionnaire to evaluate the visualization methods. As a first implementation, we did not use a timer associated to the reading time and the zoom level was empirically fixed. The device used for the experiment is an iPhone5.
5.3 Results and comments from the users
The users have been asked to rank and to give a comment about the three visualization methods. The result of the ranking is dis- played in Table 2: the users prefer the visualization frame by frame, then RoI by RoI and finally page by page. The comments of the users are quite informative about how to improve the visualization system.
Firstly, the experiment confirmed that visualizing the comic in full page is not enjoyable for the reader.
The advantage of frame by frame visualization is to have, most of time, a zoom rate allowing to read easily the text. Still, some frames have a large width and cannot be zoomed in. Another drawback is that some small frames are usually skimmed on large screen, but they have to be read one by one with this system. Reading all frames one by one causes monotony.
The advantage of RoI by RoI visualization is to have dynamic transitions (translations) which make the users feeling that his read- ing experience is more lively. If a frame is very large, it can contain more than one RoI, so the user can see the details more easily. If the frame contains little information and no RoI, the user will saw
1 https://theeyetribe.com/
4.
VISUALIZING THE COMIC ON A
SMARTHPONE
Thanks to the ordered sequence of RoIs (the reading path), the comic can be displayed on the smartphone. The screen is centered on the first RoI and if the reader interacts with the screen, the comic translates to the next RoI. The user can also go back, if so, the comic is translated to the previous RoI.
The zoom level should be proportional to the time spent by the readers while they were fixating a RoI. Indeed, if the readers spent long time they might were reading text, so the image can be zoomed a bit more to make the reading easier. But if the readers were just looking briefly to an area (such as the face of a character for example), the zoom level do not need to be high.
A timer can be set to navigate automatically from one RoI to another based on the reading time of the other readers and the usual reading time of the current user.
5. EXPERIMENT
We conducted an experiment to evaluate the usability of the pro- posed method. The experiment is divided in two parts: recording the eye gaze from users reading a comic on a large screen and then asking other users to visualize the same comic on a smartphone. The comic used in this experiment is Kingdom episode 175. The number of pages is 20. The threshold Ts used to ignore the small local min- ima has been set as 10% of the maximal value of the corresponding

it just during the translation such as in Fig. 1. One problem is that the face of the character and the speech balloon are sometimes not shown at the same time. Also, if two RoIs are close to each other, the images around both RoIs will be highly overlapped.
5.4 Discussion
Processing the heat map is a sensitive step. The parameters and threshold have an influence on the number of RoIs found. If there are too many RoI, the user will feel uncomfortable especially if the RoIs are highly overlapped. If there are not enough RoI, the user will feel strange as some parts of the comic might never appear. So the heat map should be processed in order to merge the RoIs which are too close, based on the size of the smartphone screen.
The automatic segmentation of comic images is still an open problem, so the comics have to be segmented by hand which is time consuming especially for complex layout comics. Furthermore, as pointed out in the introduction, the frame segmentation does not solve the problem of double page illustration, or frame with the same width as the page. So, even if our proposal was not ranked 1st we think we can improve it as it will be more easily usable than the segmentation.
6. CONCLUSION AND FUTURE WORK
We presented a comic visualization system based on the eye gaze analysis. Recording the reading behaviors helps smartphone users to navigate easily through the comic. For now, image segmentation and finding the reading order of comics is still an open problem. The experiment shows that visualizing the comic page by page is the less appreciated method for the users. This two reasons show that our new proposed visualization method is promising.
Furthermore, this method is just a first step towards dynamic comic visualization. We plan to use more information from the eye tracking such as the fixation time or the pupil diameter to estimate the reading time and changing the zoom level. We will consider to merge these information with screen interactions such as pinching and scrolling. This will help to deal with large dataset.
Another future work is to augment the digital comic reading experience. Because the software controls which specific portion of image is displayed at which time on the screen, it is possible to know what the reader is focusing on. Then, we can make the content more interactive by adding audio, visual or vibration effects based on the content of the comic.
7. ACKNOWLEDGMENTS
This work is supported in part by JST CREST and JSPS KAK- ENHI Grant Numbers 25240028, 15K12172, and 1681207700.
8. REFERENCES
[1] Kohei Arai and Herman Tolle. 2010. Automatic e-comic content adaptation. International Journal of Ubiquitous Computing 1, 1 (2010), 1–11.
[2] Olivier Augereau, Hiroki Fujiyoshi, Kai Kunze, and Koichi Kise. 2016. Estimation of English Skill with a Mobile Eye Tracker. In Adjunct Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous
Computing and Proceedings of the 2016 ACM International
Symposium on Wearable Computers. ACM.
[3] Olivier Augereau, Koichi Kise, and Kensuke Hoshika. 2015.
A proposal of a document image reading-life log based on document image retrieval and eyetracking. In Document Analysis and Recognition (ICDAR), 2015 13th International Conference on. IEEE, 246–250.
[4] Georg Buscher, Andreas Dengel, and Ludger van Elst. 2008. Eye movements as implicit relevance feedback. In CHI’08 extended abstracts on Human factors in computing systems. ACM, 2991–2996.
[5] Ying Cao, Rynson WH Lau, and Antoni B Chan. 2014. Look over here: Attention-directing composition of manga elements. ACM Transactions on Graphics (TOG) 33, 4 (2014), 94.
[6] Ying Cao, Xufang Pang, Antoni B Chan, and Rynson WH Lau. DynamicManga: Animating Still Manga via Camera Movement. IEEE Transactions on Multimedia (????).
[7] Neil Cohn. 2013. Navigating comics: an empirical and theoretical approach to strategies of reading comic page layouts. Frontiers in psychology 4 (2013), 186.
[8] Eakta Jain, Yaser Sheikh, and Jessica Hodgins. 2012. Inferring artistic intention in comic art through viewer gaze. In Proceedings of the ACM Symposium on Applied Perception. ACM, 55–62.
[9] Eakta Jain, Yaser Sheikh, and Jessica Hodgins. 2016. Predicting Moves-on-Stills for Comic Art Using Viewer Gaze Data. IEEE Computer Graphics and Applications 36, 4 (2016), 34–45.
[10] Xufang Pang, Ying Cao, Rynson WH Lau, and Antoni B Chan. 2014. A robust panel extraction method for manga. In Proceedings of the 22nd ACM international conference on Multimedia. ACM, 1125–1128.
[11] Keith Rayner. 2009. Eye movements and attention in reading, scene perception, and visual search. The quarterly journal of experimental psychology 62, 8 (2009), 1457–1506.
[12] Christophe Rigaud, Thanh-Nam Le, J-C Burie, Jean-Marc Ogier, Shoya Ishimaru, Motoi Iwata, and Koichi Kise. 2016. Semi-automatic Text and Graphics Extraction of Manga Using Eye Tracking Information. In 2016 12th IAPR Workshop on Document Analysis Systems (DAS). IEEE, 120–125.
[13] Christophe Rigaud, Norbert Tsopze, Jean-Christophe Burie, and Jean-Marc Ogier. 2013. Robust frame and text extraction from comic books. In Graphics Recognition. New Trends and Challenges. Springer, 129–138.
[14] Takamasa Tanaka, Kenji Shoji, Fubito Toyama, and Juichi Miyamichi. 2007. Layout Analysis of Tree-Structured Scene Frames in Comic Images.. In IJCAI, Vol. 7. 2885–2890.
[15] Yongtao Wang, Yafeng Zhou, and Zhi Tang. 2015. Comic frame extraction via line segments combination. In Document Analysis and Recognition (ICDAR), 2015 13th International Conference on. IEEE, 856–860.
[16] Masashi Yamada, Rahmat Budiarto, ENDO Mamoru, and Shinya Miyazaki. 2004. Comic image decomposition for reading comics on cellular phones. IEICE transactions on information and systems 87, 6 (2004), 1370–1376.
Comic visualization on smartphones based on eye tracking
Olivier Augereau Osaka Prefecture University Osaka , JAPAN augereau.o@gmail.com
ABSTRACT
The visualization of comic images on a small screen is a difficult problem as the image is too large to be displayed on the screen and we do not know which areas and in which order the users want to see the image. The basic solution for the user is to look at the image in full screen without being able to see the details, or to zoom and scroll through the image, which can be quite inconvenient if the interactions have to often be repeated. Our idea is to use an eye tracker to record where the users reading a comic on paper books or large screens are looking at, to reproduce their reading behaviors with a comic visualization system and guide the users using a smaller screen through the comic.
CCS Concepts
•Human-centered computing → Interaction techniques; Visual-
ization techniques; Ubiquitous and mobile devices; Keywords
Eye tracking; Comic image; Visualization; Smartphone.
1. INTRODUCTION
Reading Japanese mangas, American comics or French bande dessinees on a smartphone is getting more and more popular in the recent years as the smartphones are getting used by more and more people and the sizes of the screen are getting larger. If the size of the screen is large enough, such as tablet screens, displaying the full image is a straightforward solution. But, as we can see in Table 1, the difference between standard comic sizes and a typical smartphone screen is still very large (roughly, from 2 to 4 times).
Reading large images on a small screen is problematic as the user can hardly read the text and see some details of the images. In this case, the user needs to do many manipulations such as zooming and translation in order to read and go through the image.
Until now, the main idea to solve this problem was to segment the images in frames and to detect the reading order [16], [14], [1], [6]. Unfortunately, even the newest proposed methods to segment the comic images fail on complex layouts such as overlapping or
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
MANPU ’16, December 04 2016, Cancun, Mexico
© 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4784-6/16/12. . . $15.00
DOI: http://dx.doi.org/10.1145/3011549.3011553
Mizuki Matsubara Osaka Prefecture University Osaka , JAPAN
Koichi Kise
Osaka Prefecture University Osaka , JAPAN
matsubara@m.cs.osakafu- kise@cs.osakafu-u.ac.jp u.ac.jp
borderless frames [13], [10] [15], [6]. Furthermore, even if the frames are segmented correctly, it often happens that the frame width is the same as the page, or to have a full or double full page containing no frames. In these cases, the image is still too large to be displayed on a small screen and no solutions have been proposed. The last problem to solve is the reading order which is not necessarily easy to predict and is still an open problem for the complex cases[7], [5].
Jain and. al. showed [8] that the artist is leading viewer attention through his drawing and the intended visual route can be found by recording the viewer attention. More recently, the same researchers [9] used the eye gaze information to animate still comic images. The considered comic images are not full pages of comics but just single frames. The proposed solution consists in clustering the eye gaze positions of the readers in two groups and to compute an animation for going from the first group to the second one by analyzing the direction of the saccades.
We propose in this paper a new alternative for visualizing the comic images which is different than the classical way based on segmenting each page frame by frame. When reading a comic, the reader essentially focuses on the text balloons and face of the characters [12] so we think the visualization system should be driven by this information, rather by the frames themselves. As illustrated in Fig. 1, we focus on a solution based on reproducing the real reader behaviors.
We consider that some readers will read the same comic on large screens or on paper. The idea is to use an eye tracker to record the position where the users are looking at, detecting the Regions of Interest (RoIs) and then, to create an ordered sequence of these RoIs that we call a “reading path”. The reading path is obtained from different readers in order to avoid noise or atypical reading behavior of a single user. This path is used to guide a user using a small screen through the comic. To simplify, our visualization system could be seen as a “record and play” system.
This system presents several benefits. It can be used to “play” a comic for people who are no familiar with the reading order. Depending on the reader profile (genre, age, expertise, 1st reading,
Table 1: Approximate typical sizes of different comic types com- pare to the size of a smartphone screen. The width of a smart- phone screen size is around 2 to 4 times smaller.
  Type
  Width
 Height
 Comics
  17 cm
 26 cm
 Mangas
  13 cm
 18 cm
 Bande dessinees
  22 cm
 29 cm
 IPhone 5 screen
  5.2 cm
 9.0 cm

 Figure 1: Principle of our visualization method. The reading behavior is recorded by an eye tracker and then used to guide another reader using a small screen. The red circles represent the positions where the readers using a large screen looked at. Our visualization method consists in translating the comic im- age on the smartphone screen from one red point to another.
etc.) different reading paths can be obtained and used to personalized the reading experience. As the comic can be played automatically, the system can be used for people who cannot interact easily with the smartphone.
The remaining sections of the paper are organized as follow. First we will present how to record the reading behavior of a user. Then a method for combining several reading behaviors and creating a reading path will be presented. After, we will show how the comic can be visualized on the smartphone. In the next section, the experiments will be detailed. Finally, we will conclude and explore some future works.
2. RECORDING THE READING BEHAVIOR
The reading behavior is recorded by an eye tracker. The output of the recording is a sequence of eye gaze positions of the reader associated with a timestamp. If the eye tracker is stationary, the positions are directly obtained. If a mobile eye tracker is used,
the read page is retrieved from the front camera and the eye gaze coordinates are projected from the perspective plan of the camera to the 2D plan of the comic image. More details about the process based on mobile eye tracker can be found in [3] and [2].
The reading behavior is composed of fixations (the position where the reader is looking for around 250-350 ms) and saccades (the quick movements of the eye between two fixations) [11]. We process the raw eye gaze positions in a sequence of fixations and saccades by using the Buscher et al. algorithm [4]. In the next section we explain how to create a reading path by combining the reading behaviors of several users.
3. CREATING A READING PATH
In order to be robust to some eye tracking errors or atypical reading behaviors (for example, only one participant looks at a specific position), we aggregate the reading behaviors of multiple readers. Then, we identify the RoIs and order them to create the reading path that will be used for visualizing the comic on a small screen.
3.1 Finding the RoI
In a first step, we create a heat map based on the reading behavior of multiple readers. The heat map is useful to find the main RoIs and filter the noise. The filter can be seen as a Gaussian filter. Fig. 2 shows a heat map and the corresponding RoIs.
For each reader, the normal distribution (σ, μ) centered on each fixations is computed. The range of the distributions differs for each reader, so each distribution is normalized by dividing each value by the maximal value of the distribution. The heat map is obtained by summing up the normalized distribution of all the readers.
Then, a RoI is detected for each local maximal value of the heat map. We experimentally fixed a threshold Ts in order to ignore the small local maximal values.
3.2 Ordering the RoIs
The heat map is useful for summing up the reading behaviors but the time information is lost, so we need to find the order of the RoIs. While visualizing the comic on a smartphone, the user can navigate through the different RoIs by interacting with the screen (basically, going to the next or to the previous RoI). So each RoI should appear only one time and the reading path should not contain any rereading behavior.
Figure 2: The heat map (on the left) and the corresponding RoIs (on the right). On the heat map, the yellow and red col- ors show a high density of fixations. The local maximum are selected as ROIs (the blues circles on the image on the right).

     Page
Frame
  RoI
 First
  1
8
  1
 Second
  3
0
  5
 Third
  6
2
  4
    Figure 3: Processing of the RoI sequences. The longest consec- utive sequences of each RoI are selected to create the “filtered sequences”. Then, these sequences are used for voting and cre- ating the reading path.
After obtaining the RoIs, we process each individual reading by associating each fixation to the nearest RoI. A threshold Tf is used in order to remove the fixations which are too far from any RoI. The fixation sequence is transformed in a string where each character of the string represents a RoI. As illustrated in Fig. 3 the string is processed in order to remove multiple occurrences of a RoI: only the longest consecutive sequence of each RoI is kept. In case of equality, the first RoI is kept.
After this process, each string participate to a vote to find the order of the RoIs. We analyze the first character of each string. The one which appear the most often is selected and removed from each string. The same process is repeated until all characters have been used. The reading path is made of the selected characters.
Table 2: Ranking of the favorite visualization methods by the users.
heat map. The threshold Tf used to remove the fixations which are too far from any RoI has been set to 316 pixels. The screen size used for the experiment is: 336.2 mm * 597.6 mm (1080 pixels * 1920 pixels).
5.1 Eye gaze recording
We displayed the right and left pages at the same time on a large screen. The comic contains one double pages illustration. We asked 23 participants to read the comic. The age of the participants is early 20s and they are all familiar with reading comics. One of the participants is a Japanese female, one is a Chinese male, all others are Japanese males. The comic is written in Japanese and all participants have enough Japanese ability to read it.
The eye tracker used in the experiment is a low cost eye tracker, similar to Eye Tribe1.
Before starting the experiment, each participant has been asked to read a simple summary of the comic story and the eye tracker is calibrated. After that, we asked the participants to read the comic displayed on the screen in a natural way while recording their eye gazes.
5.2 Comic visualization
We asked 10 participants to read the same comic (Kingdom episode 175) on a smartphone with three different visualizations: page by page, frame by frame and RoI by RoI (the proposed method). The segmentation frame by frame have been done manually.
After reading the comic, the participants were asked to answer a questionnaire to evaluate the visualization methods. As a first implementation, we did not use a timer associated to the reading time and the zoom level was empirically fixed. The device used for the experiment is an iPhone5.
5.3 Results and comments from the users
The users have been asked to rank and to give a comment about the three visualization methods. The result of the ranking is dis- played in Table 2: the users prefer the visualization frame by frame, then RoI by RoI and finally page by page. The comments of the users are quite informative about how to improve the visualization system.
Firstly, the experiment confirmed that visualizing the comic in full page is not enjoyable for the reader.
The advantage of frame by frame visualization is to have, most of time, a zoom rate allowing to read easily the text. Still, some frames have a large width and cannot be zoomed in. Another drawback is that some small frames are usually skimmed on large screen, but they have to be read one by one with this system. Reading all frames one by one causes monotony.
The advantage of RoI by RoI visualization is to have dynamic transitions (translations) which make the users feeling that his read- ing experience is more lively. If a frame is very large, it can contain more than one RoI, so the user can see the details more easily. If the frame contains little information and no RoI, the user will saw
1 https://theeyetribe.com/
4.
VISUALIZING THE COMIC ON A
SMARTHPONE
Thanks to the ordered sequence of RoIs (the reading path), the comic can be displayed on the smartphone. The screen is centered on the first RoI and if the reader interacts with the screen, the comic translates to the next RoI. The user can also go back, if so, the comic is translated to the previous RoI.
The zoom level should be proportional to the time spent by the readers while they were fixating a RoI. Indeed, if the readers spent long time they might were reading text, so the image can be zoomed a bit more to make the reading easier. But if the readers were just looking briefly to an area (such as the face of a character for example), the zoom level do not need to be high.
A timer can be set to navigate automatically from one RoI to another based on the reading time of the other readers and the usual reading time of the current user.
5. EXPERIMENT
We conducted an experiment to evaluate the usability of the pro- posed method. The experiment is divided in two parts: recording the eye gaze from users reading a comic on a large screen and then asking other users to visualize the same comic on a smartphone. The comic used in this experiment is Kingdom episode 175. The number of pages is 20. The threshold Ts used to ignore the small local min- ima has been set as 10% of the maximal value of the corresponding

it just during the translation such as in Fig. 1. One problem is that the face of the character and the speech balloon are sometimes not shown at the same time. Also, if two RoIs are close to each other, the images around both RoIs will be highly overlapped.
5.4 Discussion
Processing the heat map is a sensitive step. The parameters and threshold have an influence on the number of RoIs found. If there are too many RoI, the user will feel uncomfortable especially if the RoIs are highly overlapped. If there are not enough RoI, the user will feel strange as some parts of the comic might never appear. So the heat map should be processed in order to merge the RoIs which are too close, based on the size of the smartphone screen.
The automatic segmentation of comic images is still an open problem, so the comics have to be segmented by hand which is time consuming especially for complex layout comics. Furthermore, as pointed out in the introduction, the frame segmentation does not solve the problem of double page illustration, or frame with the same width as the page. So, even if our proposal was not ranked 1st we think we can improve it as it will be more easily usable than the segmentation.
6. CONCLUSION AND FUTURE WORK
We presented a comic visualization system based on the eye gaze analysis. Recording the reading behaviors helps smartphone users to navigate easily through the comic. For now, image segmentation and finding the reading order of comics is still an open problem. The experiment shows that visualizing the comic page by page is the less appreciated method for the users. This two reasons show that our new proposed visualization method is promising.
Furthermore, this method is just a first step towards dynamic comic visualization. We plan to use more information from the eye tracking such as the fixation time or the pupil diameter to estimate the reading time and changing the zoom level. We will consider to merge these information with screen interactions such as pinching and scrolling. This will help to deal with large dataset.
Another future work is to augment the digital comic reading experience. Because the software controls which specific portion of image is displayed at which time on the screen, it is possible to know what the reader is focusing on. Then, we can make the content more interactive by adding audio, visual or vibration effects based on the content of the comic.
7. ACKNOWLEDGMENTS
This work is supported in part by JST CREST and JSPS KAK- ENHI Grant Numbers 25240028, 15K12172, and 1681207700.
8. REFERENCES
[1] Kohei Arai and Herman Tolle. 2010. Automatic e-comic content adaptation. International Journal of Ubiquitous Computing 1, 1 (2010), 1–11.
[2] Olivier Augereau, Hiroki Fujiyoshi, Kai Kunze, and Koichi Kise. 2016. Estimation of English Skill with a Mobile Eye Tracker. In Adjunct Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous
Computing and Proceedings of the 2016 ACM International
Symposium on Wearable Computers. ACM.
[3] Olivier Augereau, Koichi Kise, and Kensuke Hoshika. 2015.
A proposal of a document image reading-life log based on document image retrieval and eyetracking. In Document Analysis and Recognition (ICDAR), 2015 13th International Conference on. IEEE, 246–250.
[4] Georg Buscher, Andreas Dengel, and Ludger van Elst. 2008. Eye movements as implicit relevance feedback. In CHI’08 extended abstracts on Human factors in computing systems. ACM, 2991–2996.
[5] Ying Cao, Rynson WH Lau, and Antoni B Chan. 2014. Look over here: Attention-directing composition of manga elements. ACM Transactions on Graphics (TOG) 33, 4 (2014), 94.
[6] Ying Cao, Xufang Pang, Antoni B Chan, and Rynson WH Lau. DynamicManga: Animating Still Manga via Camera Movement. IEEE Transactions on Multimedia (????).
[7] Neil Cohn. 2013. Navigating comics: an empirical and theoretical approach to strategies of reading comic page layouts. Frontiers in psychology 4 (2013), 186.
[8] Eakta Jain, Yaser Sheikh, and Jessica Hodgins. 2012. Inferring artistic intention in comic art through viewer gaze. In Proceedings of the ACM Symposium on Applied Perception. ACM, 55–62.
[9] Eakta Jain, Yaser Sheikh, and Jessica Hodgins. 2016. Predicting Moves-on-Stills for Comic Art Using Viewer Gaze Data. IEEE Computer Graphics and Applications 36, 4 (2016), 34–45.
[10] Xufang Pang, Ying Cao, Rynson WH Lau, and Antoni B Chan. 2014. A robust panel extraction method for manga. In Proceedings of the 22nd ACM international conference on Multimedia. ACM, 1125–1128.
[11] Keith Rayner. 2009. Eye movements and attention in reading, scene perception, and visual search. The quarterly journal of experimental psychology 62, 8 (2009), 1457–1506.
[12] Christophe Rigaud, Thanh-Nam Le, J-C Burie, Jean-Marc Ogier, Shoya Ishimaru, Motoi Iwata, and Koichi Kise. 2016. Semi-automatic Text and Graphics Extraction of Manga Using Eye Tracking Information. In 2016 12th IAPR Workshop on Document Analysis Systems (DAS). IEEE, 120–125.
[13] Christophe Rigaud, Norbert Tsopze, Jean-Christophe Burie, and Jean-Marc Ogier. 2013. Robust frame and text extraction from comic books. In Graphics Recognition. New Trends and Challenges. Springer, 129–138.
[14] Takamasa Tanaka, Kenji Shoji, Fubito Toyama, and Juichi Miyamichi. 2007. Layout Analysis of Tree-Structured Scene Frames in Comic Images.. In IJCAI, Vol. 7. 2885–2890.
[15] Yongtao Wang, Yafeng Zhou, and Zhi Tang. 2015. Comic frame extraction via line segments combination. In Document Analysis and Recognition (ICDAR), 2015 13th International Conference on. IEEE, 856–860.
[16] Masashi Yamada, Rahmat Budiarto, ENDO Mamoru, and Shinya Miyazaki. 2004. Comic image decomposition for reading comics on cellular phones. IEICE transactions on information and systems 87, 6 (2004), 1370–1376.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities from Eye Gaze Data
BEN STEICHEN, CRISTINA CONATI, and GIUSEPPE CARENINI, University of British Columbia
Information visualization systems have traditionally followed a one-size-fits-all model, typically ignoring an individual user’s needs, abilities, and preferences. However, recent research has indicated that visualization performance could be improved by adapting aspects of the visualization to the individual user. To this end,
this article presents research aimed at supporting the design of novel user-adaptive visualization systems.
In particular, we discuss results on using information on user eye gaze patterns while interacting with a
given visualization to predict properties of the user’s visualization task; the user’s performance (in terms
of predicted task completion time); and the user’s individual cognitive abilities, such as perceptual speed,
visual working memory, and verbal working memory. We provide a detailed analysis of different eye gaze
feature sets, as well as over-time accuracies. We show that these predictions are significantly better than
a baseline classifier even during the early stages of visualization usage. These findings are then discussed 11 with a view to designing visualization systems that can adapt to the individual user in real time.
Categories and Subject Descriptors: H.5.m [Miscellaneous] General Terms: Human Factors, Experimentation
Additional Key Words and Phrases: Adaptive information visualization, eye tracking, adaptation, machine learning
ACM Reference Format:
Ben Steichen, Cristina Conati, and Giuseppe Carenini. 2014. Inferring visualization task properties, user performance, and user cognitive abilities from eye gaze data. ACM Trans. Interact. Intell. Syst. 4, 2, Article 11 (July 2014), 29 pages.
DOI: http://dx.doi.org/10.1145/2633043
1. INTRODUCTION
Information visualization is a thriving area of human-computer interaction that aims to help users in managing and understanding increasing amounts of information. Although visualization systems have gained in terms of general usage and usabil- ity, they have traditionally been designed using a one-size-fits-all approach, typically ignoring an individual user’s needs, abilities, and preferences. To better assist each user during visualization tasks, recent research has started to investigate novel user- adaptive visualizations that can dynamically infer relevant user characteristics and provide appropriate interventions tailored to these characteristics. Initial research on user-adaptive visualizations has already provided evidence for improved user perfor- mance (e.g., time on task, task accuracy), such as by using click behavior to infer and
The reviewing of this article was managed by associate editor Judy Kay.
Authors’ addresses: B. Steichen, C. Conati, and G. Carenini, Department of Computer Science, Univer- sity of British Columbia, Department of Computer Science, Vancouver, Canada; emails: steichen@cs.ubc.ca, conati@cs.ubc.ca, carenini@cs.ubc.ca.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212) 869-0481, or permissions@acm.org.
⃝c 2014 ACM 2160-6455/2014/07-ART11 $15.00
DOI: http://dx.doi.org/10.1145/2633043
  ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
11:2 B. Steichen et al.
adapt to suboptimal usage patterns [Gotz and Wen 2009], or by considering a user’s visualization selections to infer and adapt to a user’s visualization expertise and prefer- ences [Grawemeyer 2006]. In terms of intervention mechanisms, these initial systems have typically investigated recommending visualizations that are most suitable for the current task and/or appropriate for a particular user’s preference and expertise.
Our long-term goal is to extend this research on user-adaptive visualization in a number of aspects. First, we aim to expand the set of features that the system can adapt to by including visualization task properties (e.g., task type, task difficulty), as well as a user’s properties beyond expertise and performance, such as cognitive abilities that have been shown to influence visualization performance. Second, although existing research has looked at improving visualization performance solely using information on a user’s direct interaction (e.g., mouse clicks), we aim to provide assistance exploiting additional, potentially complementary data sources (e.g., eye tracking). Third, whereas existing work has focused on interventions that recommend alternative visualizations, we envision to also deliver interventions that can dynamically help the user with the current visualization (e.g., through highlighting relevant visualization elements).
In this article, we address the first two aspects by investigating to what extent a va- riety of visualization task properties (task type, complexity, and difficulty), the user’s performance (in terms of task completion time), the user’s visualization expertise, and three different cognitive abilities (perceptual speed, visual working memory, and verbal working memory) can be inferred from a user’s eye gaze behavior. For all of these dimen- sions, we found statistically significant results, except for user visualization expertise.
We focus on gaze behavior because visual scanning and processing are fundamental components of working with any visualization (and the only components for noninter- active visualizations). Specifically, we ask the following two research questions:
Q1. To what extent can a user’s current task, performance, and/or long-term cognitive abilities and visualization expertise be inferred from eye gaze data?
Q2. Which gaze features are the most informative?
The motivation of this work is twofold. First, to provide appropriate support, an adaptive system needs to know about the user’s current task characteristics, as well as her expected performance. For example, if the system knows that the user is cur- rently performing a “filter” task (i.e., trying to find data cases that satisfy a particular condition [Amar et al. 2005]) and appears to be slow (i.e., the user is predicted to have a high completion time), the system could adaptively de-emphasize nonrelevant data to reduce the user’s cognitive load. Correspondingly, if a system knows individual user characteristics, it will be able to provide user-specific support. For example, since cer- tain cognitive ability levels have already been shown to lead to lower performance (e.g., low perceptual speed leads to decreases in speed and accuracy [Conati and Maclaren 2008; Velez et al. 2005; Toker et al. 2012], low-ability users might benefit most from adaptive support. Furthermore, Carenini et al. [2014] have shown that the effect of visualization interventions can depend on such characteristics—for example, showing that a user’s subjective rating of different highlighting mechanisms is affected by visual working memory. Therefore, adaptive support not only requires identifying users who are currently “struggling” with the task but also consists of predicting and tailoring to each user’s individual characteristics.
Second, we are interested in determining which eye gaze features are most informa- tive for classification. As will be shown in this article, different task and user classifica- tions rely on different features, suggesting that adaptive applications need to monitor specific features depending on the intended adaptation purpose. Moreover, the discov- ery of the most discriminatory features might also provide new suggestions with regard
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities 11:3
to how the system can adapt to support the different tasks and/or user characteristics. For example, if the number of gaze transitions to a certain area of interest (AOI) (e.g., a graph’s legend) is found to be very high for users with low cognitive abilities, and knowing that these users are typically less efficient and/or effective on their tasks, we may want to provide help that focuses particularly on reducing the need for visits to this area. Similarly, by finding gaze behaviors that are often exhibited by high-ability users, we might devise adaptive interventions that can encourage low-ability users to also change to such behavior.
This research was first presented in Steichen et al. [2013]. Here, we expand on that work with additional experiments on predicting the task difficulty, as well as the users’ expected performance (in terms of completion time). Second, to better evaluate the feasibility of real-time classification, we calculated results at absolute time intervals for each of our experiments, such as the level of accuracy after seeing 5s of gaze data (see Section 5.1). Third, for each of our experiments, we also provide results for a dataset that only includes information on AOIs. Last, in addition to classification accuracy, we also calculated area under receiver operating characteristic (ROC) results—a measure commonly used in machine learning to measure the discriminatory power of models [Egan 1975]—to further strengthen the validity of our findings.
The remainder of this article is structured as follows. First, we provide an overview of related research in adaptive visualization and eye tracking, as well as the most recent findings on the impact of individual user differences in visualization. Next, we present the user study that provided the gaze data for our research. This is followed by a series of classification experiments that we ran on this gaze data to answer the research questions outlined previously. Finally, we conclude with a discussion of the overall findings and outline several directions for future work.
2. RELATED WORK
Adaptation and personalization have long been established as effective techniques to support individual users in a variety of tasks and applications, including personal- ized search and adaptive hypermedia [Steichen et al. 2012], and desktop assistance and e-learning [Jameson 2008]. By contrast, information visualization research has traditionally maintained a static, one-size-fits-all approach by ignoring an individual user’s needs, abilities, and preferences. In particular, early automatic visualization systems have focused only on adapting the visualization to task or data properties that are known a priori [Casner 1991; Mackinlay 1986] rather than dynamically infer- ring individual properties during visualization usage. An exception to this nonadaptive paradigm is presented in Grawemeyer [2006], where users’ visualization expertise and preferences are dynamically inferred through monitoring visualization selections (e.g., how long it takes a user to decide which visualization to choose). Using this inferred level of user expertise and preferences, the system then attempts to recommend the most suitable visualizations for subsequent tasks. Results from the user studies in Grawemeyer [2006] show that the recommendations indeed lead to better user per- formance in terms of task effectiveness (i.e., accuracy), as well as user efficiency (i.e., time on task). However, this work does not actively monitor a user during a task and thus cannot adapt in real time to help the user with the current task. In contrast, the system developed by Gotz and Wen [2009] actively monitors real-time user behavior during visualization usage to infer needs for intervention. In their work, interaction data (i.e., mouse clicks) are constantly tracked to detect suboptimal usage patterns— that is, activities of users that are of a repetitive (and hence inefficient) nature. Each of these suboptimal patterns indicates that an alternative visualization may be more suitable to the current user activity. The patterns used in their paper include scan- ning (a user is iteratively inspecting over similar visual objects), flipping (iteratively
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
11:4 B. Steichen et al.
changing filter constraints), swapping (repeatedly rearranging the order of data dimen- sions), and drilling (repeatedly filtering down along orthogonal dimensions). Once these patterns are detected, the system triggers adaptation interventions similar to those in Grawemeyer [2006], namely they recommend alternative visualizations that may be more suitable for the current activity (e.g., the location of a set of hotels may be best viewed using a map visualization rather than a user having to repeatedly drill down to this information for each result). However, there are a number of shortcomings of this work. First of all, the usage patterns, as well as the respective visualization recommen- dations, are determined by experts a priori rather than being based on experimental findings. Second, their system is only able to provide adaptations for visualizations that allow users to interact directly with the visualizations either through mouse clicks or other forms of direct user input. This approach therefore does not work if a user is simply “looking” at a visualization without manipulating its controls/data. Third, their patterns do not try to infer general (low-level) visualization tasks (e.g., filter, compute derived value). Last, their approach does not attempt to adapt to any individual user characteristics.
As mentioned previously, since visual scanning and processing are fundamental com- ponents of working with any visualization (they are in fact the only components for noninteractive visualizations), it is important to consider eye tracking as a source of real-time information on user behavior. Although this technology is currently confined to research environments (mostly due to the high cost of eye-tracking devices), the rapid development in affordable, mainstream eye-tracking solutions (e.g., using stan- dard Web cams) will enable the widespread application of these techniques in the near future [Sesma et al. 2012]. In the field of cognitive and perceptual psychology, the use of eye tracking has long been established as a suitable means for analyzing user at- tention patterns in information processing tasks [Rayner 1998]. Similarly, research in this field has investigated the impact of individual user differences on basic reading and search tasks [Rayner 1995]. More recently, the fields of human-computer inter- action and information visualization have also started to use eye-tracking technology to investigate trends and differences in user attention patterns and cognitive/decision processing. This research has typically focused on either identifying pattern differences for different visualizations [Goldberg and Helfman 2011] or task types (e.g., reading vs. mathematical reasoning) [Iqbal and Bailey 2004], or on explaining differences in user accuracy between alternative interfaces [Plumlee and Ware 2006]. However, these studies have generally only attempted to gain insights into differences in gaze behav- iors for different tasks and/or interfaces rather than providing a means for directly driving adaptive systems. In particular, these analyses have typically consisted of of- fline processes that require further human analysis (e.g., manually analyzing eye gaze coordinate plots [Iqbal and Bailey 2004]). In terms of actually using raw eye-tracking data for real-time prediction, most research has so far focused on identifying the user’s cognitive processes while she is performing nonvisualization activities, such as during exploratory e-learning [Kardan and Conati 2012; Conati and Merten 2007], quizzes [Courtemanche et al. 2011], simple puzzle games [Eivazi and Bednarik 2011], or infor- mation search tasks (e.g., word search) [Simola et al. 2008]. By contrast, our gaze-based work focuses on information visualization, where a user’s main activity is to perform simple visualization lookup and comparison tasks.
It is also important to note that none of the preceding approaches has attempted to adapt to user differences other than expertise. However, recent research has shown that other user traits can in fact significantly influence task performance, espe- cially in the field of information visualization. For example, a user’s spatial abilities have been shown to influence a user’s performance in visual navigation [Chen and Czerwinski 1997] and information search tasks [Westerman and Cribbin 2000].
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities 11:5
Similarly, Ziemkiewicz et al. [2011] and Green and Fisher [2010] have looked at the influence of a user’s personality traits, showing that locus of control (internal vs. ex- ternal) can impact visualization performance. Cognitive measures such as perceptual speed and visual working memory have particularly been shown to influence a user’s ability to complete basic visualization tasks effectively [Conati and Maclaren 2008; Velez et al. 2005]. For example, it has been shown that users with high perceptual speed have significantly faster completion times and accuracy on certain tasks. These results have been confirmed and extended in a recent study by Toker et al. [2012], where perceptual speed, visual and verbal working memory, and user expertise were shown to influence not only a user’s task performance but also satisfaction regarding different visualization types. Most recently, it was found that these individual user differences have an impact on different user eye gaze measures Toker et al. [2013], which directly serves as the motivation for the work in this article on using gaze data to dynamically identify and adapt to user cognitive abilities.
3. USER STUDY
As mentioned in the Introduction, this article is part of our ongoing work on designing user-adaptive information visualizations. In particular, our research studies both the effect that different user characteristics have on visualization performance and the real-time detection of task and user characteristics to be able provide appropriate interventions (the focus of this article). For these purposes, we conducted a user study during which users had to perform a battery of visualization tasks using two alternative basic visualization techniques, namely bar graphs (Figure 1, top) and radar graphs (Figure 1, bottom). By choosing two different types of visualizations, we aimed to investigate the generality of our results.
Bar graphs were chosen because they are one of the most popular and effective visualization techniques. We chose radar graphs because although they are often con- sidered inferior to bar graphs on common information seeking tasks [Few 2005], they are still widely used for multivariate data. Furthermore, there are indications that radar graphs may be just as effective as bar graphs for more complex tasks [Toker et al. 2012].
3.1. Study Tasks
The task domain used in the study required users to evaluate the performance of one or two students in eight different academic courses (using an artificial dataset). We chose this domain to avoid an effect of participant’s domain expertise on our results. In the context of this domain, we developed a set of tasks that varied both in task type and task complexity. In terms of different task types, we based our questions on a set of general visualization tasks that had been identified by Amar et al. [2005] to be “representative of the kinds of specific questions that a person may ask when working with a data set.” To keep the study conditions manageable, we chose a selection of five task types: retrieve value (RV), filter (FI), compute derived value (CDV), find extremum (FE), and sort (SO). The types were chosen so that each of our two target visualizations would be suitable to support them. Example questions for each of these task types are shown in Table I.
To vary the task complexity, we differentiated between single and double tasks. Single tasks required participants to compare one student’s performance with the class aver- age for the eight academic courses (e.g., “In how many courses is Alice below the class average?”), whereas double tasks required participants to compare the performance of two students with the class average (e.g., “Find the courses in which Andrea is below the class average and Diana is above it.”). In total, our study comprised five single tasks, one for each task type (i.e., RV1, FI1, CDV1, FE1, SO1) and four double tasks (RV2,
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
11:6 B. Steichen et al.
 Fig. 1. Sample bar (top) and radar graph (bottom). Table I. Example Task Questions
CDV2, FI2a, FI2b), meaning that the most fine-grained task type/complexity classifi- cation could consist of nine classes (see classification experiments in Section 5.2).
3.2. Cognitive Abilities
The long-term user traits that we investigated in this study consisted of the following three cognitive abilities: perceptual speed (a measure of speed when performing simple perceptual tasks), verbal working memory (a measure of storage and manipulation
  RV
 Did Alice receive a higher mark in Marine Biology or Painting?
  FI
 In which course(s) is Mary above the class average? (Select all that apply).
  CDV
 In how many courses is Alice below the class average?
  FE
 In which course does Alice deviate most from the class average?
  SO
 What are Mary’s two strongest courses?
      ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities 11:7
capacity of verbal information), and visual working memory (a measure of storage and manipulation capacity of visual and spatial information). Perceptual speed and visual working memory were selected because they were among the perceptual abilities explored by Velez et al. [2005], as well as among the set that Conati and Maclaren [2008] found to impact user performance with radar graphs and a multiscale dimension visualizer (MDV). We also chose verbal working memory because we hypothesized that it may affect a user’s performance with a visualization in terms of how the user processes its textual components (e.g., legends).
3.3. Study Procedure
Thirty-five subjects (18 female) participated in the experiment, ranging in age from 19 to 35 years. Participants were recruited via advertising at our university, with the aim of collecting a heterogeneous pool with suitable variability in their cognitive abilities. Ten participants were computer science students, whereas the rest came from a variety of backgrounds, including microbiology, economics, classical archaeology, and film production. The experiment was designed and pilot tested to fit in a single session lasting at most 1 hour. Participants began by completing tests for three cognitive measures: a computer-based OSPAN test for verbal working memory [Turner and Engle 1989] (lasting between 7 and 12 minutes), a computer-based test for visual working memory [Fukuda and Vogel 2009] (10 minutes long), and a paper-based P-3 test for perceptual speed [Ekstrom and U.S. Office of Naval Research 1996] (3 minutes long). The experiment was conducted on a Pentium 4, 3.2GHz, with 2GB of RAM and a Tobii T120 eye tracker as the main display. Tobii T120 is a remote eye tracker embedded in a 17” display, providing unobtrusive eye tracking. After a short calibration of the eye tracker, participants underwent a training phase to familiarize themselves with the two visualizations and study tasks. Participants then performed 14 tasks per visualization—that is, 2 × 5 single and 1 × 4 double (note that each user saw the exact same 28 graphs). The presentation order with respect to visualization type was fully counterbalanced across subjects. Although there was still a remaining training effect (as previously reported in Toker et al. [2012] and recently discussed in Toker et al. [2014]), the counterbalancing ensured that the data was balanced for our classification experiments (albeit containing some potential noise that may have slightly decreased some classification accuracies).
For each task, users were presented with a radar/bar graph displaying the relevant data, along with a textual question (Figures 2 and 3). Participants would then select their answer from a drop-down list, along with their confidence in their answer (be- tween 1 and 5), and click OK to advance to the next task. The experimental software was fully automated and coded in Python.
4. EYE TRACKING MEASURES AND FEATURES
An eye tracker captures gaze information through fixations (i.e., maintaining gaze at one point on the screen) and saccades (i.e., a quick movement of gaze from one fixation point to another), which can be analyzed to derive a viewer’s attention patterns. For our experiments, we generated a large set of eye-tracking features by calculating statistics on basic eye-tracking measures (Table II).
Of these basic measures, fixation rate, number of fixations, and fixation Duration are widely used in eye tracking studies. In addition, we included saccade length (e.g., distance d in Figure 4), relative saccade angle (e.g., angle y in Figure 4) and absolute saccade angle (e.g., angle x in Figure 4), as suggested in Goldberg and Helfman [2010], because these measures are potentially useful for summarizing trends in user attention patterns within a specific interaction window, such as if the user’s gaze follows a planned sequence (as opposed to being scattered).
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
11:8
B. Steichen et al.
 Fig. 2.
Sample experimental screen of a single-complexity radar graph.
 Fig. 3.
Sample experimental screen of a double-complexity bar graph.
Table II. Description of Basic Eye-Tracking Measures
   Basic Gaze Measures
 Description
  Number of fixations
 Number of eye fixations detected during an interval of interest
  Fixation rate
 Number of fixations divided by time interval, e.g., fixations per millisecond
  Fixation duration
 Time duration of an individual fixation
  Saccade length
 Distance between the two fixations delimiting the saccade (d in Fig. 4)
  Relative saccade angles
 The angle between the two consecutive saccades (e.g., angle y in Fig. 4)
  Absolute saccade angles
 The angle between a saccade and the horizontal (e.g., angle x in Fig. 4)
       ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities 11:9
 Fig. 4. Saccade-based eye measures.
Fig. 5. The five AOI regions defined over a bar graph.
The raw gaze data from the Tobii eye tracker was processed using our open-source data analysis toolkit EMDAT, which is freely available for download and extension by the research community.1 The toolkit computes statistics such as sum, average, and standard deviation over the eye-tracking measures with respect to (1) the overall screen, to get a sense of the complete interaction with the task (high-level measures from now on) and (2) specific AOIs, identifying parts of the interface relevant for understanding a user’s attention processes during each task (AOI-level measures from now on). A total of five AOIs were defined for each of the two visualizations.
These regions were selected to capture the distinctive and typical components of the two visualizations used in the study. Figures 5 and 6 show how these AOIs map onto bar and radar graph components, respectively.
—High area: Covers the upper half of the data elements of each visualization. This area is the graphical portion of an Infovis that contains the relevant data values. On the bar graph, it corresponds to a rectangle over the top half of the vertical bars (see Figure 5); for the radar graph, it corresponds to the combined area of the eight trapezoidal regions covering the data points (see Figure 6).
1 https://www.cs.ubc.ca/group/iui/EMDAT/index.html.
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.

11:10 B. Steichen et al.
 Fig. 6. The five AOI regions defined over a radar graph.
—Low area: Covers the lower half of the data elements for each visualization. —Labels: Covers all of the data labels in each graph.
—Question text: Covers the text describing the task to be performed.
—Legend: Covers the legend showing the mapping between each student and the color
of the visualization elements that represent her performance.
The selection of these five AOIs is the result of a trade-off between having detailed information on user attention over areas that are salient for task execution and keeping the number of AOIs manageable for real-time computation. Note that the data values used in the experiment generally ranged between 40 and 100, thereby providing an opportunity to have both “High Area” and a “Low Area” AOIs. If the displayed data values were to range between 0 and 100, there would only be room for a single “Data Value” AOI. However, as will be shown in this article, the Low Area AOI did not play a significant role in our classification experiments; therefore, the results are likely to hold in case there is only one such Data Value AOI.
Overall, a total of 74 features were calculated from the gaze data (Table III). For experimental purposes, we differentiated between a feature set that contained all features, including high-level and AOI features (called the Full set from now on), one that did not contain features relating to AOIs—that is, only containing the task-level features (called the No AOI set), and one that only contained features relating to AOIs (called the Only AOI set). This differentiation was chosen to evaluate the relative information gain attained from AOI and non-AOI features—for instance, how much can be inferred from a user’s gaze with and without information on the specific visualization at which the user is looking (similar to what was done in Bondareva et al. [2013] for assessing student learning with an intelligent tutoring system).
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities 11:11 Table III. Eye-Tracking Features
   HIGH-LEVEL FEATURES
   Fixations (2): Number of fixations, fixation rate
   Fixation durations (3): Sum, mean, std. deviation
   Saccade length (3): Sum, mean, std. deviation
   Relative saccade angles (3): Sum, mean, std. deviation
   Absolute saccade angles (3): Sum, mean, std. deviation
   AOI-LEVEL FEATURES (for each AOI)
   Number of fixations in AOI (5)
   Sum and mean of fixation durations in AOI (10)
   Time to first fixation in AOI (5)
   Longest fixation in AOI (5)
   Proportion of total number of fixations in AOI (5)
   Proportion of total fixation durations in AOI (5)
   Proportion of total number of transitions from this AOI to every other AOI (including self-transitions) (25)
 5. CLASSIFICATION EXPERIMENTS
The classification experiments described in this section use the previously mentioned features to infer a number of task properties, user performance, and user cognitive traits. In particular, we investigate the extent to which these factors can be inferred from gaze data (research question Q1 in the Introduction), as well as what gaze features are most important for classification (research question Q2).
First, we provide a quick overview of the experimental process used for classification. This is followed by a detailed analysis of each of the classification results, which in- cludes classification accuracy for task type (at different granularities); task complexity; task difficulty; user performance; and accuracy on classifying the three user cognitive abilities of perceptual speed, visual working memory, and verbal working memory. In addition, we ran a classification experiment for predicting the currently active visu- alization type (i.e., bar graph vs. radar graph) to evaluate the extent to which this information can be inferred when it is not available to the system (i.e., if the visual- ization system and the eye-tracking component are independent). We conclude with a summary of the overall results, as well as a discussion regarding the extent to which these results could be used for providing adaptive visualizations.
5.1. Experimental Process
Using the gaze features described in the previous section, we generated a number of datasets to simulate partial observation of gaze data during each task. We used two different processes for generating this “over-time” data, each serving a distinct analysis purpose. First, we generated partial observation datasets based on relative length, such as the first 10%, 20%, 30%, and so forth, of each trial. The goal of this analysis was to determine if there are observable eye gaze patterns regardless of the actual time it took users to finish the task—for example, if the first 20% of a user’s interaction is particularly good at classifying user characteristics. Although this approach can give valuable insights into generalizable trends and patterns, it requires a task to be fully completed to determine what constitutes 100% of the interaction. For this reason, we also generated a second batch of partial observation datasets based on absolute length— that is, the first 1,000ms, 2,000s, 3,000ms, and so forth, of each trial. These datasets are more accurate in simulating classification accuracies while a user is interacting with (i.e., looking at) the visualization. The goal of this analysis is hence to investigate the feasibility of real-time interventions when integrating the classification component into a live user-adaptive visualization system.
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
11:12 B. Steichen et al.
Each of the datasets has a total number of 725 instances, which is a result of pruning the complete set of 980 trials (i.e., 35 subjects × 14 tasks × 2 visitations) to only contain trials with 90% of valid gaze samples.2
We used the WEKA data mining toolkit [Hall et al. 2009] for model learning and eval- uation. For model learning, we tried a number of different classifier types (Decision Trees, Support Vector Machines (SVMs), Neural Networks, and Logistic Regression) with feature selection and 10-fold cross-validation for model evaluation. We used the standard evaluation metrics of Accuracy and Area under ROC. In all of our experiments, Logistic Regression (LR from now on) was the classifier with the highest accuracy and ROC. In the following sections, the performance of this classifier is evaluated on the Full, No AOI, and Only AOI datasets. As a baseline for comparison, we use a clas- sifier that always selects the most likely class—for example, for task complexity, the baseline classifier would always predict a task to be single, since there are more single tasks overall (thus failing in all cases of double tasks). Results are generated using the WEKA experiment API with the default 10(repetition) ∗ 10(cross-validation) set- ting, and statistical significance is tested using t-tests with Bonferroni adjustment on pairwise comparisons between the different classifiers. All reported results are sta- tistically significant (at p < 0.05), unless mentioned otherwise. In cases of two-class classifications, we also present the strongest features generated by feature selection. For simplicity, in the case of multiclass classifications, we do not present feature se- lection in detail, given that this involves presenting n-1 feature selection results (with n = number of classes). Instead, we discuss the impact of features only with respect to the performance of the Full versus No AOI versus Only AOI datasets. In addition, note that in some cases, the y-axis has been readjusted to better show the relative classification performances over time (particularly for the cognitive abilities).
5.2. Classification Results for Task Types and Task Complexity
As explained in Section 3.3, users performed tasks of varying type and complexity. In this section, we first show that task type can be predicted with reasonable accuracy even when tasks are defined at a very fine granularity (in our case comprising nine different task types). Our analysis also reveals that some tasks are frequently con- fused with one other, suggesting that some fine-grained task types may actually entail similar user strategies. We then present classification results for more coarse-grained task type classifications (five task types, three task types), as well as a two-class clas- sification for task complexity (single vs. double scenario tasks). We show that we can achieve high accuracies for each of these predictions; that a classifier using all eye gaze information generally performs best; and that classification accuracy generally increases after seeing more data.
Task type—nine tasks. The most fine-grained analysis splits tasks into nine different classes, one for each separate question type-complexity combination used in the study, such as Single Retrieve Value (RV1) and Double Retrieve Value (RV2). Because of the high number of classes, this case represents a difficult multiclass classification challenge, with a baseline classification accuracy of only 15.45% (i.e., always predicting RV1, since this task is, after data pruning, the most common task with 112 out of 725 instances). Nonetheless, when looking at the over-time performance of the LR classifier using the full feature dataset (LR-Full from now on), a classification accuracy of 56.60% can be achieved after seeing all available data (i.e., after 100%) (Figure 7).
2Note that gaze data during a trial can be lost due to the subject looking off the screen; due to loss of calibration from rapid movement, blinking, or other such events; or due to blocking of the infrared beam to the user’s eyes (e.g., by the user’s hands).
 ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities 11:13
 Fig. 7. Task type—classification accuracy (nine tasks).
Table IV. Overall Average Task Classification (Nine Tasks) Accuracy and ROC for Percentage
and Absolute Timings
As shown in Figure 7, classification accuracy grows continuously as more gaze data becomes available, going higher than 50% after seeing 60% of the data. As shown in Table IV, the average classification accuracy over time for LR-Full is 44.81% and for LR-OnlyAOI is 43.57% (with the difference not being statistically significant). Results are not as good for the LR classifier using the No AOI dataset (LR-NoAOI from now on). The average accuracy over time for this classifier is 29.69%, and its maximum accuracy after seeing all of the data is 30.61%, both statistically significantly lower than the corresponding accuracies for LR-Full. Moreover, the accuracy of the LR-NoAOI classifier is not statistically significantly better than the baseline until after seeing 60% of the data. These differences in performance for the Full versus No AOI versus Only AOI datasets indicate that AOI-related features have a strong impact on classification accuracy for task type at this granularity.
As shown in Figure 8, we found a similar trend when comparing classifiers using the Area under ROC measure, with LR-Full and LR-Only AOI again performing best from 20% of the interaction onward (albeit with slightly different margins compared to the accuracy results). In fact, for almost all of the other experiments described in the following sections, we found similar results for both accuracy and Area under ROC. We will therefore only focus on accuracy results from here onward, except for instances where there was indeed a noticeable difference between the two measures.
In addition to these experiments regarding the percentage of observed interactions, we also test classification performance when using absolute time cut-offs. Both the LR- Full and the LR-OnlyAOI classifiers significantly outperform the baseline classifier from the outset (Figure 9), with accuracies close to 40% after only 5,000ms. However, the LR-NoAOI classifier is only statistically significantly better than baseline after 8,000ms.
   Baseline
  Full
No AOI
  Only AOI
 Percentage (Accuracy)
 15.45
  44.81
29.69
  43.57
 Percentage (ROC)
 0.50
  0.74
0.68
  0.72
 Absolute (Accuracy)
 15.45
  41.65
25.03
  40.71
 Absolute (ROC)
 0.50
  0.75
0.67
  0.71
     ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
11:14 B. Steichen et al.
 Fig. 8. Task type—Area under ROC results (nine tasks).
Fig. 9. Task type—classification accuracy at absolute time cut-offs (nine tasks).
When analyzing sources of errors in the confusion matrix, the most commonly con- fused task pairs were single filter (FI1) and double filter (FI2) (which is intuitive given the common task type), as well as single find extremum (FE1) and single sort (SO1) (further discussed in the next section).
Last, as mentioned in Section 5.1, when comparing different types of classifier models (e.g., SVMs, Decision Trees, Neural Networks), we always found LR to yield the highest results (in fact, statistically significantly higher than other models). As an example of this comparison, Figure 10 shows how the various classifiers performed for the nine- task-type classification. As can be seen in this figure, all of the classifiers outperform the baseline from the outset. However, LR performs best across all time intervals. This trend was found across all of our experiments, and we will therefore only discuss the LR results from here onward.
Task type—five tasks. In addition to the fine-grained task analysis involving nine different tasks, we also investigated classifying task type from gaze data when type is defined at a coarser level of granularity that ignores the complexity difference be- tween single and double tasks—for example, ignoring the difference between retrieve value when one student is mentioned in the question text (RV1) as opposed to when two students are involved (RV2). Ignoring this difference leaves us with five different
 ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities 11:15
 Fig. 10. Comparison of classifier models, each using the Full dataset.
Table V. Overall Average Task Classification (Five Tasks) Accuracy and ROC for Percentage
and Absolute Timings
classes, corresponding to five different task types from Amar’s taxonomy (i.e., retrieve value (RV), filter (FI), compute derived value (CDV), find extremum (FE), and sort (SO)). From the point of view of inferring task type with the goal of providing adaptive interventions specific to tasks types, this five-class classification task is very meaning- ful, because the classes represent general task types recognized as being common for information visualization.
Similar to the nine-class classification, the trends of the relative and absolute time datasets (i.e., percentage of total observation vs. time in milliseconds) are very com- parable. Since the absolute time dataset is more interesting for integration into a live adaptive system (as discussed in Section 5.1.), we only discuss the results of this analysis from here onward. LR-Full reaches an average accuracy of 53.36% over time (Table V), an accuracy of 50% after 8,000ms, and a maximum accuracy of 63.32% after seeing all data. LR-Full statistically significantly outperforms the baseline’s accuracy (27.86%, i.e., always choosing filter, the most common task with 202 instances) from the start. As was the case with nine tasks, LR-OnlyAOI is not statistically significantly different from the LR-Full classifier. Similarly, removing AOI-related features statis- tically significantly reduces accuracy, as shown by the performance of LR-NoAOI in Figure 11. Moreover, this classifier only starts to be statistically significantly better than baseline after 3,000ms.
Although the accuracies are clearly improved over the nine-task classification (as to be expected due to the reduction of the classification complexity), it is still arguable if they are acceptable for real-time fully adaptive interventions. However, one could certainly consider using such a classifier in a mixed-initiative system, where a range of task-appropriate interventions could be recommended to a user rather than applied automatically. This would still potentially reduce a user’s workload while not inter- rupting a task with inappropriate interventions. In addition, it is worth noting that
    Baseline
 Full
 No AOI
 Only AOI
 Percentage (Accuracy)
  27.86
 53.36
 41.98
 51.77
 Percentage (ROC)
  0.5
 0.74
 0.67
 0.72
 Absolute (Accuracy)
  27.86
 50.00
 35.35
 48.29
 Absolute (ROC)
  0.5
 0.75
 0.66
 0.72
     ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
11:16 B. Steichen et al.
 Fig. 11. Task type—classification accuracy (five tasks).
the predictions are solely based on eye gaze data, and that the integration of other data sources (e.g., interaction data) could probably complement and improve results.
When analyzing sources of errors in the confusion matrix, we found two pairs of tasks that are most often confused with one other. The first pair involves the tasks compute derived value (CDV) and filter (FI). For example, in 57% of the cases where CDV was misclassified, the predicted class was FI. This result is not surprising, since both of these tasks essentially involve applying a filter to all data values (e.g., finding values above a given threshold), with the difference being that CDV requires an additional computation (e.g., “In how many courses is student X above the class average?”). Thus, FI can be regarded as a subtask of CDV for the questions used in our study. In fact, as noted by Amar et al. [2005], the filter task “is used as a subtask in many other questions.” Adaptations that particularly support this FI task may therefore also be of use to CDV tasks if they contain such a subcomponent. The second pair of tasks often confused with each other involves find extremum (FE) and sort (SO). For example, in 38% of the cases where FE was misclassified, the predicted class was SO. This result is again not surprising given the nature of these two tasks. FE involves going through all values to find the highest value(s) from a set of values, whereas SO involves sorting all values from highest to lowest. Thus, FE essentially involves a subpart of the steps necessary to perform an SO task. This finding confirms the observation in the taxonomy of Amar et al. [2005] that “sorting is generally a substrate for extreme value finding.”
The aforementioned relations between the two pairs of frequently confused tasks suggest that combining each pair into one new task type and building a classifier that can recognize this combined type is still valuable for adaptation, since adaptations could be provided to support the common subtask. Thus, in the next section, we evaluate the accuracy of a classifier for task type that involves three classes: FI-CDV (combined), SO-FE (combined), and RV.
Task type—three tasks. When considering only three different task types, LR-Full reaches an average accuracy of 68.42% over time and an accuracy of 70% after only 8,000ms. LR-Full statistically significantly outperforms the baseline’s accuracy (48.14%) from the start.
As was the case with nine and five tasks, the performance of LR-OnlyAOI is very similar to the performance of LR-Full. Similarly, removing AOI-related features once again statistically significantly reduces accuracy, as shown by the performance of
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities 11:17
 Fig. 12. Task type—classification accuracy (three tasks).
Table VI. Overall Average Task Classification (Three Tasks) Accuracy and ROC for Percentage
and Absolute Timings
LR-NoAOI in Figure 12 and Table VI. This classifier only reaches an average accu- racy of 56.75% over time and only statistically significantly outperforms the baseline after 10,000ms.
With accuracies reaching 70% after only 8s, one can certainly envision the use of such a classifier to assist a user in an adaptive visualization system. Interventions could consist of recommendations, as well as direct automatic visualization adaptations. However, to validate the practicality of this approach, we will need to run further user studies with a fully implemented adaptive system.
Task types—summary of results. In summary, we found that across all task type granularities, LR with the Full dataset outperformed both the baseline and LR with the No AOI dataset, but not the LR-OnlyAOI classifier, showing the importance of having AOI-related features for task-type classification. Figure 13 summarizes the results in terms of average accuracy over time. As expected, accuracy for all of the classifiers increases as task granularity gets coarser. Although only the classification of three tasks with the LR-Full classifier reaches accuracies that may be suitable for providing reliable task-based interventions, we see these results as being very important for two reasons. First, as we argued earlier, suitable interventions can be provided even if task type is recognized at this coarser level. Second, our results have been obtained by using relatively simple eye gaze features that do not capture gaze patterns beyond simple transitions between two AOIs. Using more complex gaze patterns or additional sources of information to guide classification (see discussion in Section 5.6), it is likely that we can increase accuracy on all of our classification tasks.
Task complexity. The classifier in this experiment predicts if the user is attending to a task of the single or double scenario. As discussed in the Section 3, this dis- tinction provides a measure for task complexity. LR-Full and LR-OnlyAOI are still the most accurate classifiers, with statistically significantly higher average accuracy (80.39%/80.22%) over time than both LR-NoAOI (74.76%) and the baseline classifier
  Percentage (Accuracy)
  48.14
 68.42
56.75
  67.00
 Percentage (ROC)
  0.5
 0.81
0.65
  0.80
 Absolute (Accuracy)
  48.13
 67.10
52.56
  65.91
 Absolute (ROC)
  0.5
 0.80
0.58
  0.80
    ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
11:18 B. Steichen et al.
 Fig. 13. Task type—average classification accuracy over time for different task granularities.
Fig. 14. Task complexity—classification accuracy.
Table VII. Overall Average Complexity Classification Accuracy and ROC for Percentage
and Absolute Timings
(72.69%, i.e., always choosing single, since this is the dominant class with 527 out of the 725 instances). The top classifiers, LR-Full and LR-OnlyAOI, are once again not statistically significantly different from each other. It should be noted that at 72.69%, the baseline accuracy is relatively high in this experiment, since users performed more than twice as many single tasks than double ones. Nevertheless, all three LR classifiers performed higher, with accuracies reaching up to 84.45% for the Full dataset (Figure 14 and Table VII). Accuracy again improved with more data being observed, and each of the feature sets outperformed the baseline after relatively low amounts of observed data.
   Baseline
  Full
No AOI
  Only AOI
 Percentage (Accuracy)
 72.69
  80.39
74.76
  80.22
 Percentage (ROC)
 0.50
  0.81
0.73
  0.80
 Absolute (Accuracy)
 72.69
  79.96
75.02
  79.74
 Absolute (ROC)
 0.50
  0.81
0.69
  0.80
      ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities 11:19
Since task complexity consists of a simple two-class classification, we also investi- gated which specific features from the feature selection process are contributing the most to the classification (note that for multiclass logistic regression involving n classes, this analysis would have been too cumbersome because it would need to consider n-1 feature selection results). The most predictive features were “proportion of total fixation durations in legend AOI,” “sum of fixation durations in legend AOI,” and “proportion- ate number of transitions from/to legend AOI to/from high AOI.” With increased task complexity, we found that the use of the graph legend increased considerably, both in terms of proportion of total fixation durations (compared to all other AOIs), as well as in terms of transitions (i.e., there were more transitions to and from the legend). This result shows that an increase in data series has an effect on how much users may need to refer back to the legend during a visualization task, as to be expected. Nevertheless, it is an interesting finding that such an increase in complexity can be captured in real time using simple eye gaze measures, which may in turn allow a user-adaptive system to provide adaptations for more complex tasks (e.g., provide support for better legend access and processing).
5.3. Classification Results for Task Difficulty
In addition to “task complexity,” which we defined based on the number of data series, we also tried to predict the overall “difficulty” of a task, which we defined based on a combination of subjective and objective measures. For this measure, we again found similar relative performances of the different classifiers, and that accuracy generally improves with more data. We will now first describe in detail how we generated a difficulty value for each task, followed by the detailed results of the classification experiments.
Definition of task difficulty. Defining tasks as being easy or difficult a priori is chal- lenging, since difficulty depends on user expertise and perceptual abilities, which were varied on purpose in our study. We therefore defined task difficulty a posteriori, based on four different measures (two objective and two subjective) aggregated using a prin- cipal component analysis (PCA). Because there was a ceiling effect on task correctness, our first objective measure of task difficulty is task completion time (assuming that, in general, more time is needed for more difficult tasks). However, longer completion times may also simply be an indication of a task being longer while not necessarily be- ing more difficult. Therefore, our second objective measure of difficulty is the standard deviation of completion time for each task across all users. A high value of this metric indicates a high variability among users’ completion times, an indicator that the task may be difficult or confusing for some users.
Our two chosen subjective measures of task difficulty are based on the users’ reported confidence of their performance, which was elicited after each task. The first subjective measure is the average confidence reported by users on each task. Intuitively, less difficult tasks would have higher values for this average. However, we also want to take into account that some users may tend to be more confident overall than other users. Therefore, our second subjective measure is the average deviation of confidence for each task across all users and is computed as follows. For each user, we look at their average confidence across their tasks. Then, for each task, we compute the deviation of confidence as the difference between the user’s reported confidence for that task and the user’s average confidence across tasks. Finally, for each task, we average the deviation of confidence across all users. This average indicates for which tasks users were giving confidence ratings that were above or below their typical rating.
To combine the preceding four variables, we performed a PCA, which is a form of dimensionality reduction that allows one to identify and combine groups of interrelated
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
11:20 B. Steichen et al.
 Fig. 15. Task difficulty—classification accuracy.
Fig. 16. Task difficulty—classification results using the Area under ROC measure.
variables into components more suitable for data analysis. A PCA on our four measures of task difficulty resulted in one output component. Bartlett’s test of sphericity (x2 = 73.35, df = 6, p < .001) indicated that the PCA was appropriate. Kaiser’s sampling adequacy was 0.55, and all variables showed a communality >0.52, which was above the acceptable limit of 0.5. The component that we generated had an eigenvalue over Kaiser’s criterion of 1 and explained 62.22% of the variance. In summary, we used the output component generated by this PCA (i.e., dimensional reduction) as the measure of task difficulty, and for classification purposes, we labeled tasks with a negative component as easy and tasks with a positive component as difficult (resulting in 497 easy and 228 difficult trials). Each of the easy/difficult classes included both bar and radar graph trials, as well as both single and double task trials, thereby showing that difficulty was not solely confined to radar graphs and/or double complexity tasks.
Classification results. Our classification experiments on this task difficulty property showed slightly different patterns (Figures 15 and 16) compared to task complexity— that is, a nonsmooth curve between 5,000ms and 10,000ms. However, as noted in Provost et al. [1998], for datasets that are imbalanced (as was the case for task dif- ficulty), accuracy is less reliable. When analyzing the Area under ROC results, we
 ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities 11:21
Table VIII. Overall Average Difficulty Classification Accuracy and ROC for Percentage and Absolute Timings
found that the curve for this measure was indeed relatively smooth (see Figure 16) and that the LR-Full and LR-OnlyAOI datasets once again statistically significantly outperformed LR-NoAOI, as well as the baseline (Table VIII). Relatively high accu- racies/Area under ROC are reached after only 5,000ms (72%/0.68) and reach up to 78%/0.86 for LR-Full and LR-OnlyAOI. This classifier could hence be used in an adap- tive system to support users during “hard” tasks. However, there is still significant room for improvement in terms of the accuracies, especially compared to the baseline (which could potentially come from different data sources, such as input devices).
In terms of “how” to provide this support, it is again worth investigating the feature selection results to see which particular eye gaze behaviors are most indicative of a user facing a difficult task. The three most predictive features for task difficulty were “proportionate number of transitions from labels to high AOI,” “sum of fixation duration in label AOI,” and “proportion of total fixation durations in high AOI.” These findings indicate that difficult tasks might require users to do more repeated visits of the actual graph values (and their associated labels), perhaps due to a task being ambiguous or simply requiring more cognitive effort. Therefore, if in addition to determining the task type our classifier flags the current task as difficult (e.g., due to the user performing repeated visits to the high AOI area, and/or spending a high proportionate amount of time in this area), the system should try to provide assistive support to the user through an adaptive intervention that reduces this effort.
5.4. Classification Results for User Performance
In addition to predicting what particular task a user is performing (as well as the task’s complexity/difficulty), we also aimed at predicting “how well” a user is/will be performing on this current task. The reasoning for this experiment is that while there might be situations where we could provide adaptations purely based on a predicted task type and/or task characteristics, an adaptive visualization system is arguably most appropriate when a user is currently performing “suboptimally.” In particular, since adaptive interventions could introduce slight disruptions in a user’s workflow, it might be best to only provide interventions when we detect a “slow” user (while ignoring users who are already performing well). To estimate a user’s current performance, we hence are trying to predict if a user’s completion time will be above or below the median time (based on all users) for this particular task. Overall, our results showed that performance can be predicted well within the first stages of the prediction and that more general features (i.e., not specific to any AOIs) are particularly discriminative.
Figure 17 shows the overall classification results based on the 725 valid trials, of which 357 were labeled as “slow” and 368 were labeled as “fast” (note that we did not run separate classification experiments per task). As can be seen in this figure, LR-Full achieves the highest overall accuracies, reaching 65% after only 5,000ms. Overall, all three classifiers are closely matched, with an average accuracy of 65% for LR-Full and 62% for both LR-NoAOI and LR-OnlyAOI (baseline 51%) (Table IX). Nonetheless, the differences observed in the early stages (at 5,000ms) are statistically significant, as are all performances compared to the baseline (from 2,000ms onward).
   Baseline
  Full
No AOI
  Only AOI
 Percentage (Accuracy)
 68.41
  76.02
74.46
  75.78
 Percentage (ROC)
 0.50
  0.79
0.75
  0.78
 Absolute (Accuracy)
 68.41
  72.12
70.36
  71.56
 Absolute (ROC)
 0.50
  0.73
0.65
  0.72
     ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
11:22 B. Steichen et al.
 Fig. 17. User performance—classification accuracy.
Table IX. Overall Average Performance Classification Accuracy and ROC for Percentage
and Absolute Timings
Interestingly, LR-NoAOI outperforms LR-OnlyAOI in the early stages of a user’s task, showing that a user’s overall performance may be predicted using features that are independent of any AOIs. When analyzing feature selection results for LR-Full at this early stage, we found that “mean saccade length,” “standard deviation of absolute saccade angles,” and “standard deviation of saccade length” were most predictive (each being non-AOI features). When implementing an adaptive intervention system that targets the early phases of a user’s interaction, it may therefore be sufficient to base the classification on non-AOI related features.
However, after 10,000ms, it is once again the LR-OnlyAOI classifier that closely matches the LR-Full classifier, showing that AOI-related information positively con- tributes to classification accuracy in the later stages. The most predictive features in these stages were “proportion of total number of fixations in text AOI,” “proportion of total fixation durations in text AOI,” and “proportionate number of transitions from text to low AOI,” indicating that users with lower performances (in terms of time) refer more often to the textual information associated with the graph (which could consist of the graph’s caption).
5.5. Classification Results for Cognitive Abilities
In this section, we discuss classification results relating to inferring a user’s level of visual working memory, verbal working memory, and perceptual speed. The specific task of each of the three classifiers is to infer if a user belongs to either the High or Low category for that measure (based on a median split).
In general, we found similar results across each of these three classification exper- iments. First, we found that AOI features are again very useful for predictions and that most classifiers outperform the baseline from the outset. Although average accu- racies for even the best classifier were rather low (between 56% and 60%; Tables X, XI, and XII), it has to be noted again that these experiments are solely based on simple
  Baseline
  Full
No AOI
  Only AOI
 Percentage (Accuracy)
 50.75
  65.20
62.45
  62.23
 Percentage (ROC)
 0.50
  0.73
0.69
  0.69
 Absolute (Accuracy)
 50.75
  63.20
61.27
  61.09
 Absolute (ROC)
 0.50
  0.68
0.66
  0.66
      ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities
11:23
Table X. Overall Average Visual Working Memory Classification Accuracy and ROC for Percentage and Absolute Timings
Table XI. Overall Average Verbal Working Memory Classification Accuracy and ROC for Percentage and Absolute Timings
Table XII. Overall Average Perceptual Speed Classification Accuracy and ROC for Percentage and Absolute Timings
eye-tracking measures, which may be improved using additional sources of informa- tion (see overall result discussion in Section 5.6). Second, we made several interesting observations when analyzing the accuracies at different data cut-off points. In partic- ular, for each of the experiments, the peak accuracies were actually found during the early stages of each trial, as opposed to after all data had been observed (as found in most of the other classification experiments described earlier). This pattern suggests that a user’s cognitive abilities most strongly affect a user’s gaze patterns during the initial phase of a visualization task (as shown in Figures 18, 19, and 20) and that these patterns are increasingly “diluted” by other factors (e.g., task type) as the task goes on. Although this goes against the intuition that more data generally helps classification, the analysis of feature selection actually provided some sensible explanations for this finding (discussed next).
For visual working memory, the peak accuracy of 60% occurred after 6,000ms (see Figure 18). When analyzing the features that received the highest coefficient during feature selection, we found that the time to first fixation for text, label, and high AOIs played an important role in classifying users. We found that high visual working memory users had lower times to first fixation (indicated by a negative coefficient), meaning that they were very quick at scanning the various AOIs of the visualization.
Similarly, for verbal working memory, the highest classification accuracy for both LR-Full (64%) and LR-OnlyAOI (61%) was found after observing only 3,000ms (see Figure 19).
When analyzing the feature selection results for LR-Full, we found that features related to the text and label AOI most strongly contributed to the classification accuracy. In particular, high verbal working memory users spent less time in the text AOI, both overall and in proportion to other AOIs. Since users are most likely to read the question text at the beginning of each task, it therefore seems intuitive that the highest accuracies were found after only a few seconds of the data had been observed.
   Baseline
  Full
No AOI
  Only AOI
 Percentage (Accuracy)
 54.90
  57.47
56.78
  56.95
 Percentage (ROC)
 0.50
  0.60
0.57
  0.59
 Absolute (Accuracy)
 54.73
  56.93
54.89
  57.21
 Absolute (ROC)
 0.50
  0.59
0.52
  0.59
        Baseline
  Full
No AOI
  Only AOI
 Percentage (Accuracy)
 52.14
  60.75
55.40
  59.83
 Percentage (ROC)
 0.50
  0.65
0.58
  0.64
 Absolute (Accuracy)
 52.14
  60.33
55.84
  59.82
 Absolute (ROC)
 0.50
  0.65
0.57
  0.64
        Baseline
  Full
No AOI
  Only AOI
 Percentage (Accuracy)
 50.07
  57.07
53.19
  57.12
 Percentage (ROC)
 0.50
  0.59
0.54
  0.59
 Absolute (Accuracy)
 50.07
  56.29
52.45
  56.35
 Absolute (ROC)
 0.50
  0.58
0.53
  0.58
     ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
11:24 B. Steichen et al.
 Fig. 18. Visual working memory—classification accuracy for relative time slices.
 Fig. 19. Verbal working memory—classification accuracy.
 Fig. 20. Perceptual speed—classification accuracy for absolute time slices.
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities 11:25
Table XIII. Overall Average Visualization Type Classification Accuracy and ROC for Percentage and Absolute Timings
A similar pattern was observed for the perceptual speed classification experiments, where the highest accuracy for LR-Full (59%) was found after only 2,000ms of data had been observed (see Figure 20). Interestingly, the LR-NoAOI classifier outperformed LR-Full and LR-OnlyAOI after 20,000ms.
When analyzing the feature selection results during the early stages, we found that features related to the label and legend AOIs had the strongest coefficients. In par- ticular, we found that high perceptual speed users had a “lower number of fixations in the legend AOI” and “lower proportion of total number of fixations in the legend AOI.” This finding may indicate that low perceptual speed users would benefit from adaptations relating to this particular AOI (through highlighting, facilitating easier access, etc.). In addition, low perceptual speed users had a longer “longest fixation in the label AOI” and a lower “fixation rate” compared to high perceptual speed users. Again, this may indicate that we can provide adaptations particularly tailored toward the label AOI—for example, by temporarily increasing the size of relevant labels to support low perceptual speed users.
However, during the later stages, we found that in addition to fixation rate, the No AOI features of “mean fixation duration” and “sum of absolute saccade angles” were the most predictive (hence explaining the better performance of the LR-NoAOI classifier).
Visualization type. As shown in almost all of the preceding classification results, the inclusion of AOI-related features is critical toward generating predictions for task properties, user performance, and user cognitive measures. However, having these AOI- related features requires knowing which visualization is currently active. Although there are many scenarios in which this information is indeed available to an adap- tive component (i.e., if the adaptation component is part of the visualization system), this is not always the case. For example, if an adaptive component were to run as a standalone system in parallel to a separate visualization system (in the context of an information retrieval task when the user gets back a visualization, or in case the adaptive component acts as a complement to a third-party analysis tools, etc.), it would first be necessary to infer the currently active visualization type to utilize the right AOIs for accurate task/user classifications. Thus, in this section, we present results on whether visualization type can be inferred from gaze data. Since AOI information would not be available for this task, LR-Full and LR-OnlyAOI are not applicable in a realistic scenario. For the LR-NoAOI classifier, the average accuracy is 66.58%, which is statistically significantly higher than the baseline (52.69%) (Table XIII). As shown in Figure 21 (note that LR-Full and LR-OnlyAOI are included for completeness), the accuracy of LR-NoAOI continuously grows as more gaze data is observed, reaching 66% after 5,000ms and leveling off at around 70% after 2,0000ms. All accuracies are statistically significantly higher than baseline after only 3,000ms. Although these re- sults are encouraging, further research needs to be conducted in terms of improving accuracies to employ these techniques in a live system (see discussion in Section 5.6).
Regarding the feature selection for this NoAOI classifier, we found that users have different viewing patterns in terms of path angles. Specifically (and not surprisingly), users have more horizontal viewing patterns in the bar graph (lower mean absolute
   Baseline
  Full
No AOI
  Only AOI
 Percentage (Accuracy)
 52.69
  83.45
66.58
  80.39
 Percentage (ROC)
 0.50
  0.91
0.73
  0.88
 Absolute (Accuracy)
 52.69
  84.31
64.63
  81.67
 Absolute (ROC)
 0.50
  0.91
0.69
  0.88
     ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
11:26 B. Steichen et al.
 Fig. 21. Visualization type—classification accuracy.
path angles) and more “erratic” saccades (higher standard deviation of absolute path angles), whereas in the radar graph users follow a circular trajectory to view the various data points (indicated by a higher mean of absolute path angles) and have more uniform saccades due to the proximity of the labels to the respective data points (indicated by a lower standard deviation of absolute path angles).
5.6. Summary of Results and Discussion
As outlined in the Introduction, the specific goals of our experiments were to investigate the extent to which a user’s current visualization task properties, a user’s performance, and a user’s long-term cognitive abilities could be inferred solely based on eye gaze data (Q1), as well as which gaze features would be the most informative (Q2). By running a number of classification experiments and analyzing in detail the effects of different feature sets, we have found several interesting results regarding these research questions.
We found that a user’s eye gaze behavior provides evidence about each of the visual- ization task types and characteristics, user performance, and user cognitive abilities. In particular, we showed that for each classification task, gaze-behavior–based pre- dictions outperform a baseline classifier (except for user expertise, which we hence did not discuss further) (Q1). Moreover, we show that for most of the predictions, the classification accuracy is statistically significantly higher even after only partial data observations. We have shown that for some experiments, accuracy is actually highest at the beginning of each task, indicating that a user’s eye gaze at this time may con- tain the most relevant information regarding the target characteristics. These results provide very encouraging evidence that user eye gaze behavior could indeed be used for driving adaptive systems, particularly given that the experiments used a relatively simple set of features. Interestingly, LR consistently achieved the highest accuracies compared to other machine learning models, such as SVM or Decision Trees. Although we do not have an intuitive explanation for this finding, several other works have simi- larly found LR to perform well with eye gaze data [Kardan and Conati 2012; Bondareva et al. 2013].
It may be argued that from a practical point of view, some of the accuracies are not sufficiently high to be exploited in a live system. In particular, the accuracies relating to the cognitive abilities yielded results that were only in the 55% to 60% range. However, depending on the nature of the intervention/guidance that is being provided, it can be envisioned that if the system is unsure about the user’s classification, some
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities 11:27
minimal adjustments can be done, followed by continued tracking to see if performance improves. Nevertheless, further research should be conducted to improve the presented accuracies. On the one hand, we envision that the addition of sequence features (e.g., scan path patterns) could provide even more information about the various tasks and user characteristics. On the other hand, eye-tracking data could be integrated with other sources, such as interaction data, if this information is available. Similarly, there are further sources of information that could potentially be added to such a system. For instance, it may be possible to infer the user’s task through automatic graph analysis (e.g., based on computer vision techniques [Elzer et al. 2011] or natural language processing (e.g., by processing a visualization’s caption).
We also obtained very interesting results regarding the more fine-grained details of each classification experiment. In particular, we found that depending on the goal of the classification, different features are most informative for different task/user characteristics (Q2). For example, we found that the legend usage increases for more complex tasks (i.e., tasks that have more data series) and label usage for generally more difficult tasks, suggesting that users could benefit from interventions relating to these particular AOIs. Similarly, we found that low perceptual speed users spend more time in the legend, suggesting that these users may benefit from interventions that particularly relate to this AOI (e.g., giving these elements more emphasis or providing easier access). These detailed analyses thereby not only provide evidence to what extent different characteristics can be inferred but also how a system may adapt to individual differences.
In terms of general trends regarding the most informative features, we found that for each of the classification runs, AOI-related features were crucial toward more accurate predictions. Thus, it may be argued that to build effective adaptive visualizations, a system needs to be aware of the currently active visualization. We therefore also showed that even in the case of the system not knowing this information a priori (e.g., if the adaptive component is not directly attached to the visualization system), it is possible to infer the visualization type solely based on a user’s eye gaze with 70% accuracy. Again, this accuracy may potentially be improved with additional, more sophisticated features such as sequential scan paths.
Our experiments have only investigated two simple visualization techniques; how- ever, there are many results that may be generalized to a wider array of visualization designs. In particular, we have shown that many of the important features are ac- tually based on generic AOIs that are common to most types of visualizations, such as a graph’s labels or legend. Similarly, while the study only focused on an artificial dataset involving student grades, the actual tasks were derived from an established set of general, low-level analysis tasks for information visualization [Amar et al. 2005] and may therefore be generalized to other application domains.
Our analysis also included a novel way of investigating task type/subtype similarity, since our study of confusion matrices (see Section 5.2) revealed common eye gaze patterns for certain types of tasks. This type of analysis may also be used for further research purposes, such as to determine which type of adaptation to provide to best support different task types/subtypes or common user strategies.
Last, although our experiments have shown results regarding the classification of different task and user characteristics—for instance, what to adapt to and to a certain extent how to adapt—more work needs to be carried out in terms of predicting when adaptive assistance is required. In particular, further research is necessary relating to the identification of potential user confusion or cognitive overload, which is related to the detection of “suboptimal usage patterns” that was discussed in related work by Gotz and Wen [2009].
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
11:28 B. Steichen et al.
6. CONCLUSIONS AND FUTURE WORK
In conclusion, we have presented research results showing that a user’s eye gaze is a valuable source to infer a number of task and user characteristics. In particular, we have shown encouraging results using simple machine learning techniques on simple eye-tracking metrics, even after only partial data has been observed. The study has therefore provided a first step toward our long-term goal of designing user-adaptive information visualizations.
The next step of this research is to design user studies that focus on the effect of differ- ent adaptive interventions (e.g., highlighting, drawing reference lines, recommending alternative visualizations) on a user’s performance, both in general and in relation to different tasks and individual user differences. These studies will also need to focus on different degrees of intervention intrusiveness, such as comparing fully adaptive versus mixed-initiative approaches. Following this investigation, we hope to develop a fully integrated adaptive information visualization system that is able to dynami- cally provide adaptive interventions that are informed by real-time user behavior data. Last, we will investigate the detection of user confusion/cognitive overload, as well as the usage of more complex features such as sequential scan paths, to improve on the results presented in this article.
REFERENCES
R. Amar, J. Eagan, and J. Stasko. 2005. Low-level components of analytic activity in information visualization. In Proceedings of the 2005 IEEE Symposium on Information Visualization. 15–21.
D. Bondareva, C. Conati, R. Feyzi-Behnagh, J. M. Harley, R. Azevedo, and F. Bouchet. 2013. Inferring learning from gaze data during interaction with an environment to support self-regulated learning. In Artificial Intelligence in Education. Lecture Notes in Computer Science, Vol. 7926, 229–238.
G. Carenini, C. Conati, E. Hoque, B. Steichen, D. Toker, and J. T. Enns. 2014. Highlighting interventions and user differences: Informing adaptive information visualization support. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI’14). 1835–1844.
S. M. Casner. 1991. Task-analytic approach to the automated design of graphic presentations. ACM Trans- actions on Graphics 10, 111–151.
C. Chen and M. Czerwinski. 1997. Spatial ability and visual navigation: An empirical study. New Review of Hypermedia and Multimedia 3, 67–89.
C. Conati and H. Maclaren. 2008. Exploring the role of individual differences in information visualization. In Proceedings of the Working Conference on Advanced Visual Interfaces (AVI’08). 199–206.
C. Conati and C. Merten. 2007. Eye-tracking for user modeling in exploratory learning environments: An empirical evaluation. Knowledge-Based Systems 20, 557–574.
F. Courtemanche, E. A ̈ımeur, A. Dufresne, M. Najjar, and F. Mpondo. 2011. Activity recognition using eye-gaze movements and traditional interactions. Interacting with Computers 23, 202–213.
J. P. Egan. 1975. Signal Detection Theory and ROC-Analysis. Academic Press.
S. Eivazi and R. Bednarik. 2011. Predicting problem-solving behavior and performance levels from visual at-
tention data. In Proceedings of the 2nd Workshop on Eye Gaze in Intelligent Human Machine Interaction
(IUI’11). 9–16.
R. B. Ekstrom and U.S. Office of Naval Research. 1996. Manual for Kit of Factor-Referenced Cognitive Tests.
Educational Testing Service.
S. Elzer, S. Carberry, and I. Zukerman. 2011. The automated understanding of simple bar charts. Artificial
Intelligence 175, 526–555.
S. Few. 2005. Keep Radar Graphs Below the Radar—Far Below. Perceptual Edge.
K. Fukuda and E. K. Vogel. 2009. Human variation in overriding attentional capture. Journal of Neuroscience
29, 8726–8733.
J. Goldberg and J. Helfman. 2011. Eye tracking for visualization evaluation: Reading values on linear versus
radial graphs. Information Visualization 10, 182–195.
J. H. Goldberg and J. I. Helfman. 2010. Comparing information graphics: A critical look at eye tracking. In
Proceedings of the 3rd BELIV’10 Workshop: BEyond Time and Errors: Novel evaLuation Methods for
Information Visualization (BELIV’10). 71–78.
D. Gotz and Z. Wen. 2009. Behavior-driven visualization recommendation. In Proceedings of the 14th Inter-
national Conference on Intelligent User Interfaces (IUI’09). 315–324.
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities 11:29
B. Grawemeyer. 2006. Evaluation of ERST: An external representation selection tutor. In Proceedings of the 4th International Conference on Diagrammatic Representation and Inference (Diagrams’06). 154–167.
T. M. Green and B. Fisher. 2010. Towards the personal equation of interaction: The impact of personality factors on visual analytics interface interaction. In Proceedings of the 2010 IEEE Symposium on Visual Analytics Science and Technology (VAST’10). 203–210.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. 2009. The WEKA data mining software: An update. SIGKDD Explorations Newsletter 11, 10–18.
S. T. Iqbal and B. P. Bailey. 2004. Using eye gaze patterns to identify user tasks. Presented at the the Grace Hopper Celebration of Women in Computing.
A. Jameson. 2008. Adaptive interfaces and agents. In A. Sears and J. A. Jacko (Eds.), The Human-Computer Interaction Handbook: Fundamentals, Evolving Technologies and Emerging Applications (2nd ed.). CRC Press, Boca Raton, FL, 433–458.
S. Kardan and C. Conati. 2012. Exploring gaze data for determining user learning with an interactive simulation. In Proceedings of the 20th International Conference on User Modeling, Adaptation, and Personalization (UMAP’12). 126–138.
J. Mackinlay. 1986. Automating the design of graphical presentations of relational information. ACM Trans- actions on Graphics 5, 110–141.
M. D. Plumlee and C. Ware. 2006. Zooming versus multiple window interfaces: Cognitive costs of visual comparisons. ACM Transactions on Computer-Human Interaction 13, 179–209.
F. J. Provost, T. Fawcett, and R. Kohavi. 1998. The case against accuracy estimation for comparing induction algorithms. In Proceedings of the 15th International Conference on Machine Learning (ICML’98). 445– 453.
K. Rayner. 1995. Eye movements and cognitive processes in reading, visual search, and scene perception. Studies in Visual Information Processing, 3–22.
K. Rayner. 1998. Eye movements in reading and information processing: 20 years of research. Psychological Bulletin 124, 372–422.
L. Sesma, A. Villanueva, and R. Cabeza. 2012. Evaluation of pupil center-eye corner vector for gaze esti- mation using a Web cam. In Proceedings of the Symposium on Eye Tracking Research and Applications (ETRA’12). 217–220.
J. Simola, J. Saloja ̈rvi, and I. Kojo. 2008. Using hidden Markov model to uncover processing states from eye movements in information search tasks. Cognitive Systems Research 9, 237–251.
B. Steichen, H. Ashman, and V. Wade. 2012. A comparative survey of personalised information retrieval and adaptive hypermedia techniques. Information Processing and Management 48, 4, 698–724.
B. Steichen, G. Carenini, and C. Conati. 2013. User-adaptive information visualization: Using eye gaze data to infer visualization tasks and user cognitive abilities. In Proceedings of the 2013 International Conference on Intelligent User Interfaces (IUI’13). 317–328.
D. Toker, C. Conati, G. Carenini, and M. Haraty. 2012. Towards adaptive information visualization: On the influence of user characteristics. In Proceedings of the 20th International Conference on User Modeling, Adaptation, and Personalization (UMAP’12). 274–285.
D. Toker, C. Conati, B. Steichen, and G. Carenini. 2013. Individual user characteristics and information visualization: Connecting the dots through eye tracking. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI’13). 295–304.
D. Toker, B. Steichen, M. Gingerich, C. Conati, and G. Carenini. 2014. Towards facilitating user skill ac- quisition: Identifying untrained visualization users through eye tracking. In Proceedings of the 19th International Conference on Intelligent User Interfaces (IUI’14). 105–114.
M. L. Turner and R. W. Engle. 1989. Is working memory capacity task dependent? Journal of Memory and Language 28, 127–154.
M. C. Velez, D. Silver, and M. Tremaine. 2005. Understanding visualization through spatial ability differ- ences. in: IEEE Visualization, 2005. VIS 05. In Proceedings of IEEE Visualization (VIS’05). 511–518.
S. Westerman and T. Cribbin. 2000. Mapping semantic information in virtual space: Dimensions, variance and individual differences. Journal of Human-Computer Studies 53, 765–787.
C. Ziemkiewicz, R. J. Crouser, A. R. Yauilla, S. L. Su, W. Ribarsky, and R. Chang. 2011. How locus of control influences compatibility with visualization style. In Proceedings of the 2011 IEEE Conference on Visual Analytics Science and Technology (VAST’11). 81–90.
Received August 2013; revised February 2014; accepted March 2014
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
 Towards Using Gaze Properties to Detect Language Proficiency
Abstract
Humans are inherently skilled at using subtle physiological cues from other persons, for example gaze direction in a conversation. Personal computers have yet to explore this implicit input modality. In a study with 14 participants, we investigate how a user’s gaze can be leveraged in adaptive computer systems. In particular, we examine the impact of different languages on eye movements by presenting simple questions in multiple languages to our participants. We found that fixation duration is sufficient to ascertain if a user is highly proficient in a given language. We propose how these findings could be used to implement adaptive visualizations that react implicitly on the user’s gaze.
Author Keywords
Eye tracking; pattern recognition; adaptive visualization.
ACM Classification Keywords
H.5.m [Information interfaces and presentation (e.g., HCI)]: Miscellaneous.; H.1.2 [Model and Principles]: User/Machine Systems—Human information processing
Introduction
During conversations, we usually pay attention to the view direction of our conversation partner [4, 5]. This enables us to detect if the other person is bored or not engaged in the
Jakob Karolus
University of Stuttgart Pfaffenwaldring 5a
70569 Stuttgart, Germany jakob.karolus@vis.uni- stuttgart.de
Paweł W. Woźniak
University of Stuttgart Pfaffenwaldring 5a
70569 Stuttgart, Germany pawel.wozniak@vis.uni- stuttgart.de
Lewis L. Chuang
Max Planck Institute for Biological Cybernetics Spemannstr. 38
72076 Tübingen, Germany lewis.chuang@tuebingen.mpg.de
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. NordiCHI ’16, October 23 - 27, 2016, Gothenburg, Sweden
Copyright is held by the owner/author(s).
Publication rights licensed to ACM.
ACM 978-1-4503-4763-1/16/10...$15.00
DOI: http://dx.doi.org/10.1145/2971485.2996753

 Language English German Danish
Dutch
Finnish
French
Greek Romanian   ro
conversation. Changing the topic or requesting attention from the partner is a common countermeasure.
When we interact with personal computer systems communicating on this level is not yet possible. In virtual environments, gaze directional cues have been researched as a predictor of conversational attention [13, 14]. It has been shown that a system that reacts to the user’s gaze, e.g. a virtual agent [8], increases emotional response and attentional allocation by the user. In our work, we examine how gaze characteristics can explain the user’s behavior and their context. We then suggest how these characteristics can be utilized to build adaptive user interfaces.
We propose an approach that leverages the distinctiveness of gaze characteristics as a means to infer whether the user needs assistance or not. In particular, we looked at how the eye movements of users changed when confronted with different languages. In our study, we presented the participants with simple questions in varying languages while recording their gaze with an eye tracking device. While studies regarding people’s language proficiencies are plentiful, they often rely on an extensive analysis given eye movement data from multiple read documents [9, 17]. For a real-time adaptive system, the prospect of having the users read a whole document first to recognize that they did not understand the content is not feasible. Thus, we specifically aim to recognize a user’s understanding as accurately as possible given only a short sentence in the respective language.
Our preliminary results show that a model relying solely on a person’s mean fixation duration is sufficient to ascertain whether the user is highly proficient in the respective language. We contribute results from an eye tracking study with 14 participants that show the feasibility of such an approach.
Study Design
Nineteen participants volunteered for this study, of which the data of 14 participants (7 female, age: 19-36 years) were used for further analysis, based on data quality1. Two participants already had prior experience with eye tracking studies. All of them were native German speakers.
We collected simple questions in 13 different languages (15 questions each). The respective translations were provided by either native or highly proficient users of that language. Table 2 shows a few example questions. Most are part of the Indo-European language family [2], yet we included some outliers such as Finnish or Hungarian to analyze their effect. See Table 1 for a complete overview.
Our setup consisted of a 22 inch LCD display and a remote eye tracker (SMI RED 250) that was attached at the bottom of the screen. Our participants were seated 0.5 to 1 meters away from the display in a small cubicle. Figure 2 shows a picture of the apparatus.
After introducing the participants to our study, they were asked to sign the provided consent form and supply demographic information as well as rate their reading level for specific languages based on the Common European Framework of Reference for Languages [1]. Additionally the participants were asked to provide their proficiency level for languages not listed.
During the experiment, we displayed a total of 150 questions (randomized) one by one to the participants. Each question was visible for exactly ten seconds and could be answered by a single key press, such as one letter or one number (see Table 2). During this time the user was able to provide an answer by pressing the respective key. Eye
1For the other participants the data was not reliable due to glasses or make-up interfering with the recording.
ISO 639-1 [6] en
de
da
     nl fi fr el
    Spanish Turkish Slovenian Arabic Hungarian   hu
Table 1: Languages in this study.
es tr sl ar

Language
English French Danish Finnish
Example questions
 How many days are within a week? Combien de jours y a-t-il dans une semaine? Hvor mange dage er der i en uge?
Kuinka monta päivää on viikossa?
What is the first letter of your first name? Quelle est la première lettre de votre prénom? Hvad er det første bogstav i dit fornavn? Mikä on etunimesi ensimmäinen kirjain?
Table 2: Two example questions in four different languages.
  Figure 2: Apparatus showing LCD monitor with attached eye tracking device.
movements were recorded at a rate of 250Hz. The experiment was conducted in two sessions with an intermediate break in between to allow for relaxation. Figure 1 illustrates an example scanpath of one participant.
Figure 1: Fixations (circles) and saccades (lines) of one participant plotted on top of the respective question.
Results and Discussion
We applied several post processing steps to the obtained eye tracking data including event detection using a velocity based fixation algorithm [11] and blink removal. We discarded any eye movement data after the point in time when the participants provided an answer for the respective question. For the other questions, we decided to limit the examination time to five seconds after the stimulus was presented. Additionally we discarded outliers that were not sensible (lower than 50ms and higher than 600ms) given reported values for fixation duration during reading tasks in literature [10, 12].
In a preliminary analysis, we are interested in how different languages affect the user’s fixations. A simple descriptive statistic is the average fixation duration. Thus, we discarded all saccades and computed the mean fixation duration for each participant for a given language. To compare these results within our participants we normalized the mean fixation duration over all languages per participant. As a normalization factor we used the mean fixation duration of each participant given all fixation data we had collected from the respective participant. Figure 3 shows a violin plot of the distribution of the mean fixation duration (normalized) of each participant per language.
To evaluate whether there is a significant effect of the language proficiency on the average fixation duration, we ran a one-way repeated measures ANOVA. The grand mean of the average fixation duration (normalized) was 1.0. The lowest was 0.80 for proficiency level C22, while the non-proficient level, i.e. no knowledge at all3, showed the highest with 1.04. The main effect of proficiency level on the average fixation duration was statistically significant (F6,175 = 6.583, p < .001). A post-hoc Tukey HSD test revealed that the mean fixation durations of non-proficient readers were significantly longer than C1 (p < 0.05) and C2 (p < 0.001) readers.
2the highest proficiency level according to the CEFR [1] 3less than A1, with regard to the CEFR [1]

     Figure 3: Violin plot showing the distribution of the average fixation duration (normalized) of each participants per language. Language code is given by ISO 639-1 [6] (see Table 1). Color coding refers to the most prominent proficiency level within the language group, such as C-level readers for German.
Conclusion and Future Work
Our results show that even short text snippets are sufficient to infer if a user is highly proficient given the respective language. The questions are not part of a single, long body of text and are randomized in the order that they were displayed. Thus, the independence of each question still holds.
We have shown that by comparing the average fixation duration over an interval of a maximum of five seconds, we can make an adequate statement on whether the user understands the shown content or not. Furthermore, this method does not require a disruptive calibration of the eye tracking device as relative gaze position is sufficient to calculate the fixation duration. Especially low-cost eye tracking devices struggle with accurate calibration and drifts over time, making them a prime target for our approach. In the future, an incorporation of eye tracking
devices into already existing wearable computing interfaces, such as the Google Glasses might very well be possible.
Our vision is to utilize such distinct gaze characteristics in real-world scenarios where people interact with public displays. Gaze-based language switching enables foreigners to switch to a suitable language where explicit interaction might be troublesome or even impossible, e.g. at an airport or other commuting places (see Figure 4). Research already reports that off-the-shelf cameras are sufficient for simple gaze-based interaction in a tablet scenario [15] and in larger setups on wall-sized displays [16].
On a technical level, we will look into further gaze characteristics that might be key indicators of a user’s understanding. Furthermore, we will examine the usefulness of low-level scanpath comparison [3] that utilizes geometrical representations to calculate similarities between different scanpaths [7].
In our future work, we will focus on building adaptive visualizations systems that leverage gaze properties demonstrated here and recognize when the users struggle with their task.
Figure 4: Public display in Japan. There is no way for non Japanese-speaking people to change the language. An adaptive gaze-based system could bridge this gap. © CEphoto, Uwe Aranas.
Acknowledgements
We thank all colleagues and friends that provided us with the necessary language translations. This research is financially supported by the German Research Foundation (DFG) within the project C02 of SFB/Transregio 161 and supported by the SimTech Cluster of Excellence. Ethical approval for this study was obtained from the Ethics Committee at the University of Konstanz.
References
[1] Council of Europe. Common European Framework of Reference for Languages: Learning, Teaching, Assessment. Applied Linguistics Non Series. Cambridge University Press, 2001.
[2] Deutscher, G. The Unfolding of Language: An
Evolutionary Tour of Mankind’s Greatest Invention. Henry Holt and Company, 2006.
[3] Feusner, M., and Lukoff, B. Testing for statistically significant differences between groups of scan patterns. In Proceedings of the 2008 Symposium on Eye Tracking Research & Applications, ETRA ’08, ACM (2008), 43–46.
[4] Foulsham, T., Cheng, J. T., Tracy, J. L., Henrich, J., and Kingstone, A. Gaze allocation in a dynamic situation: Effects of social status and speaking. Cognition 117, 3 (Dec. 2010), 319–331.
[5] Hirvenkari, L., Ruusuvuori, J., Saarinen, V.-M., Kivioja, M., Peräkylä, A., and Hari, R. Influence of Turn-Taking in a Two-Person Conversation on the Gaze of a Viewer. PLoS ONE 8, 8 (Aug. 2013).
[6] International Organization for Standardization. Codes for the representation of names of languages – Part 1: Alpha-2 code. Standard, Geneva, CH, July 2002.
[7] Jarodzka, H., Holmqvist, K., and Nyström, M. A Vector-based, Multidimensional Scanpath Similarity Measure. In Proceedings of the 2010 Symposium on Eye-Tracking Research & Applications, ETRA ’10, ACM (New York, NY, USA, 2010), 211–218.
[8] Marschner, L., Pannasch, S., Schulz, J., and Graupner, S.-T. Social communication with virtual agents: The effects of body and gaze direction on attention and emotional responding in human observers. International Journal of Psychophysiology 97, 2 (Aug. 2015), 85–92.
[9] Martínez-Gómez, P., and Aizawa, A. Recognition of Understanding Level and Language Skill Using Measurements of Reading Behavior. In Proceedings of the 19th International Conference on Intelligent User Interfaces, IUI ’14, ACM (New York, NY, USA, 2014), 95–104.
[10] Rayner, K., Slattery, T. J., and Bélanger, N. N. Eye movements, the perceptual span, and reading speed. Psychonomic bulletin & review 17, 6 (2010), 834–839.

[11] Salvucci, D. D., and Goldberg, J. H. Identifying fixations and saccades in eye-tracking protocols. In Proceedings of the 2000 Symposium on Eye Tracking Research & Applications, ACM (2000), 71–78.
[12] Sereno, S. C., and Rayner, K. Measuring word recognition in reading: Eye movements and event-related potentials. Trends in cognitive sciences 7, 11 (2003), 489–493.
[13] Steptoe, W., Wolff, R., Murgia, A., Guimaraes, E., Rae, J., Sharkey, P., Roberts, D., and Steed, A. Eye-tracking for Avatar Eye-gaze and Interactional Analysis in Immersive Collaborative Virtual Environments. In Proceedings of the 2008 ACM Conference on Computer Supported Cooperative Work, CSCW ’08, ACM (New York, NY, USA, 2008), 197–200.
[14] Vertegaal, R., Slagter, R., van der Veer, G., and Nijholt, A. Eye Gaze Patterns in Conversations: There is More to Conversational Agents Than Meets the Eyes. In Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems, CHI ’01, ACM
(New York, NY, USA, 2001), 301–308.
[15] Wood, E., and Bulling, A. EyeTab: Model-based gaze
estimation on unmodified tablet computers. In
Proceedings of the Symposium on Eye Tracking Research and Applications, ETRA ’14, ACM (2014), 207–210.
[16] Yamazoe, H., Utsumi, A., Yonezawa, T., and Abe, S. Remote gaze estimation with a single camera based on facial-feature tracking without special calibration actions. In Proceedings of the 2008 Symposium on Eye Tracking Research & Applications, ETRA ’08, ACM (2008), 245–250.
[17] Yoshimura, K., Kise, K., and Kunze, K. The eye as the window of the language ability: Estimation of English skills by analyzing eye movement while reading documents. In Document Analysis and Recognition (ICDAR), 2015 13th International Conference on, IEEE (2015), 251–255.
   See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/266149144
An eye movement analysis of highlighting and graphic organizer study aids for learning from expository text
Article in Computers in Human Behavior · December 2014 DOI: 10.1016/j.chb.2014.09.010
CITATIONS READS
6 103
2 authors:
Hector Ponce
University of Santiago, Chile
20 PUBLICATIONS 71 CITATIONS SEE PROFILE
Richard Mayer
University of California, Santa Barbara
369 PUBLICATIONS 33,933 CITATIONS SEE PROFILE
      Some of the authors of this publication are also working on these related projects:
Using Cognitive Science to Guide Video Media Creation in Rural West Africa View project Learning Glass: A New Platform for Promoting STEM Engagement and Learning View project
   All content following this page was uploaded by Hector Ponce on 18 April 2016.
The user has requested enhancement of the downloaded file. All in-text references underlined in blue are added to the original document and are linked to publications on ResearchGate, letting you access and read them immediately.
Computers in Human Behavior 41 (2014) 21–32
     Contents lists available at ScienceDirect Computers in Human Behavior journal homepage: www.elsevier.com/locate/comphumbeh
 An eye movement analysis of highlighting and graphic organizer study aids for learning from expository text
Hector R. Ponce a,⇑, Richard E. Mayer b,1
a Faculty of Management and Economics, University of Santiago of Chile, Av. L. B. O’Higgins 3363, Santiago, Chile
b Department of Psychological and Brain Sciences, University of California, Santa Barbara, CA 93106-9660, USA
   article info
Article history:
Keywords:
Highlighting
Graphic organizers
Study strategies
Eye tracking Computer-based learning
1. Objective
The goal of the present study is to examine how study aids such as highlighting and graphic organizers affect learning from expos- itory text, such as exemplified in Fig. 1, including both the process and outcome of learning. This goal is in line with growing theoret- ical demands for applying the science of learning to study strate- gies (Dunlosky, Rawson, Marsh, Nathan, & Willingham, 2013; Mayer, 2011) as well as growing practical demands of reform efforts such as the Common Core Standards in the U.S. to focus more on expository text (Porter, McMaken, Hwang, & Yang, 2011). In this study, we use eye tracking measures (such as number of fixations and eye fixation transitions, as summarized in the top of Table 1) to provide a picture of the process of learning and we use a multi-leveled posttest (consisting of tests of memory and comprehension, as summarized in the bottom of Table 1) to pro- vide a picture of the outcome of learning.
⇑ Corresponding author. Tel.: +56 2 27180716.
E-mail addresses: hector.ponce@usach.cl (H.R. Ponce), rich.mayer@psych.ucsb.
edu (R.E. Mayer).
1 Tel.: +1 805 8932472.
http://dx.doi.org/10.1016/j.chb.2014.09.010
0747-5632/Ó 2014 Elsevier Ltd. All rights reserved.
abstract
This study uses eye tracking technology to examine how study aids such as highlighting and graphic organizers affect cognitive processing during learning. Participants were 130 college students randomly assigned to one of five experimental conditions. In the control group, students read a plain text; in two behaviorally passive conditions, students read a text with key words colored in red or read the same text along with a filled-in graphic organizer; and in two behaviorally active conditions, students either high- lighted key words in a text or filled in an empty graphic organizer. Students took tests of rote memory (cloze test) and comprehension (summary test). Asking students to fill in a graphic organizer or providing a filled-in graphic organizer resulted in improvements in performance on both tests, whereas asking stu- dents to highlight the text or providing highlighted text improved performance only in the rote memory test compared to students who did not receive any study aids. Eye tracking measures showed that high- lighting (in both conditions) primed the cognitive process of selecting: students spent more time fixating on those words colored in red compared with the control condition. In contrast, eye tracking measures showed that graphic organizers (in both conditions) primed the cognitive processes of selecting, organiz- ing and integrating since the inclusion of an organizer substantially affected both where their eyes fixated and moved (i.e. transitions) within the text.
   Ó 2014 Elsevier Ltd. All rights reserved.
2. Research on highlighting and graphic organizers
Highlighting text is part of a more general technique known as typographical cuing that also includes underlining, boldface, capi- talization, and colored types (Fowler & Barker, 1974). In this article, we refer to this variety of text marking techniques as highlighting. Among the advantages of highlighting text is that it is simple, easy to use, and training is not necessary (Dunlosky et al., 2013). Impor- tantly, several studies have found that highlighting text is among the most common study techniques. For example, in a survey of more than 500 medical students on spontaneous study strategies conducted by Lonka, Lindblom-YlÄnne, and Maury (1994), stu- dents reported the following study techniques as the most com- monly used ones: underlining (88% of participants), note-taking (68% of participants), and defining concepts (49% of participants). Similarly, Wade, Trathen, and Schraw (1990) found that when col- lege students were asked to study a text by using whatever tech- nique they knew, most students employed some form of highlighting followed by verbatim note-taking.
Research on highlighting and related signaling techniques can be divided into (a) studies in which participants are asked to mark words or sentences in a text (reader-generated highlighting), and

22 H.R. Ponce, R.E. Mayer / Computers in Human Behavior 41 (2014) 21–32
(b) instructional design studies in which participants receive learn- ing material with text already highlighted (experimenter-provided highlighting). Research on the effectiveness on both types of research designs has generated mixed results. Some studies have found positive effects while others have found neutral or even det- rimental effects (Bell & Limber, 2010; Dunlosky et al., 2013; Hartley, Bartlett, & Branthwaite, 1980). Notably, one of the most consistent findings is that highlighting can improve memory for the text that has been highlighted (Lorch, 1989). For instance, in a study conducted by Hartley et al. (1980), where sixth grade stu- dents were asked to read either a passage that contained under- lined text (experimenter-provided highlighting) or the same passage but with plain text, they found that the group that read the highlighted passage outperformed the group that read the plain-text passage as measured by a cloze test administered imme- diately and after a one-week delay.
Another study technique recommended to improve reading comprehension is the use of graphic organizers (Fiorella & Mayer, 2015; National Institute of Child Health and Human Development, 2000), although students do not tend to use them as much as the highlighting technique. Graphic organizers are spa- tial layouts that embed different types of text structures, usually associated with expository texts, such as compare-and-contrast (e.g. a matrix), sequence (e.g., flow-chart), and cause-and-effect (e.g., fishbone) (Cook & Mayer, 1988; Ponce, López, & Mayer, 2012). The use of graphic organizers is intended to prime deeper processing of the text in order to identify, first, the text structure (e.g., compare-and-contrast, cause-and-effect, etc.), and, second, the respective components and relationships within the text (e.g., similarities and differences, lists of causes and effects, etc.) (Beyer, 1997).
Similar to the highlighting study technique, research on graphic organizers can be divided into (a) studies in which students are asked to complete an empty organizer along with a text (reader- generated graphic organizers) and (b) studies in which students are provided with filled in graphic organizers along with a text (experimenter-provided graphic organizers). Meta-analyses have concluded that graphic organizers are an effective study technique for improving learning (Marzano, Pickering, & Pollock, 2001; Moore & Readence, 1984), although there are circumstances in which the use of graphic organizers does not lead to better learn- ing. For example, in a study conducted by Stull and Mayer (2007), in which college students read a biology text along with graphic organizers, the graphic organizers were effective in improving learning in cases when a small number of organizers were used (e.g., one or two per paragraph) but not effective when
they were overused (e.g., more than two organizers per paragraph). In a large scale study, Ponce et al. (2012) found that elementary school students significantly improved their reading comprehen- sion skills, measured through a standardized test, after they received scaffolded practice in the use of graphic organizers and when such practice was integrated within the language arts curriculum.
3. Eye movement during reading
In the literature we did not find studies that examine eye move- ment behavior that results from using specific reading comprehen- sion strategies such as highlighting text or filling in a graphic organizer. Most studies on eye movement during reading have focused on cognitive processes directly associated with reading texts (e.g., attention), the influence of textual variables (e.g., con- ceptual density and semantic relationships between words), and discourse factors (e.g., topic structure) (Hyönä, Lorch, & Kaakinen, 2002; Rayner, 1998).
Research on eye movement uses two main measures to study reading patterns: saccade and fixation. A saccade corresponds to a brief and a rapid movement of the eyes from one location to another in the text (principally words). In contrast, a fixation is the action of the eyes coming to rest on part of the text (a word or part of a word) for a very short period of time (Rayner, 1998). Fixation data reveal attention processes and it is only during fixa- tions that information is encoded (Rayner, Chace, Slattery, & Ashby, 2006).
During reading, patterns of fixations and saccades are influ- enced by text difficulty, task demands and use of graphics. For example, eye movement studies demonstrate that reading difficult texts requires more cognitive processing than reading simpler texts. Difficult parts of the text (e.g., uncommon words or syntac- tically complex sentences) demand more attention; which is reflected in readers’ tendency to fixate more and for longer periods of time; with an increase in regressions (i.e., backward saccades) and decrease in saccade length (Rayner, 1995; Rayner et al., 2006).
Task demands also appear to influence attentional processes during reading, as shown by Kaakinen and Hyönä (2010). In their study, a group of students was instructed to read a text for proof- reading purposes and another group to read a text for comprehen- sion purposes. Students who were asked to proofread a text showed an increase in the number of fixations, decrease in saccade length and an increase in fixation duration compared with stu- dents that were instructed to read for comprehension.
The spatial disposition of text and graphics affects cognitive processing during learning as well. Johnson and Mayer (2012) report three experiments that study the effects of the spatial con- tiguity principle (i.e., placing text near the corresponding part of a graphic tends to have a positive effect on learning) by examining eye movement behavior. In one of the experiments, a group was instructed to read and study a separated presentation (i.e., a dia- gram of a braking system separated from a text explaining such system) and another group was instructed to read and study an integrated presentation (i.e., the same diagram but the text was segmented and located next to each mechanism that constituted the braking system). Eye movement data showed that participants in the integrated condition made significantly more saccades between the text and the diagram (i.e., indicating attempts to inte- grate words and picture) and more saccades from the text to the corresponding part of the diagram (i.e., indicating successful inte- gration of words and picture) than participants in the separated condition. No significant differences were observed in total fixation time on diagrams and on text (i.e., indicating attentional focus on words and pictures).
                         Fig. 1. The steamboat passage in the text-only condition.
Table 1
Measures of eye-tracking and learning outcomes.
Name
Measures of eye-tracking
Total fixation time (s) Number of fixations Up–down transitions
Left–right transitions
Measures of learning outcomes
Summary test score Cloze test score
Description
Total duration eyes spend looking at each AOI
Total number of fixations eyes spend on each AOI Number of times eyes move from the top portion of the passage to the bottom portion of the passage or vice versa Number of times eyes move from the passage (top or bottom) to the graphic organizer area or vice versa
Number of key comparisons recalled Number of key verbatim terms filled in
H.R. Ponce, R.E. Mayer / Computers in Human Behavior 41 (2014) 21–32 23
     Our goal in this study is to use eye movement metrics such as number of fixations and number of transitions (as summarized in the top of Table 1) to examine the effect of highlighting and graphic organizers study aids on reading patterns and to use comprehen- sion and memory tests (as summarized in the bottom of Table 1) to examine the effects of highlighting and graphic organizers on learning outcomes.
4. Theoretical framework
The present study extends the growing body of research on the effectiveness of various types of text processing strategies, such as note-taking (Kiewra, 1985; Peper & Mayer, 1978), text-signaling techniques (Lorch, 1989), graphic organizers (Ponce et al., 2012; Robinson et al., 2006) and outlining based on rhetorical structures (Cook & Mayer, 1988; Meyer & Poon, 2001). The main hypothesis (which we call the depth hypothesis) is that highlighting and gra- phic organizer techniques foster qualitatively different cognitive processes during learning and therefore produce qualitatively dif- ferent learning outcomes. In particular, the depth hypothesis pro- poses that some study techniques (e.g., highlighting) promote a superficial processing of the text that improve rote memory of the highlighted material whereas others techniques (e.g., graphic organizer) promote a deeper processing of the text that foster dee- per comprehension of the text.
The depth hypothesis draws on the idea that the quality of a learning outcome depends on the kinds of the cognitive processes that learners engage in during learning, as described in the cogni- tive theory of multimedia learning (Mayer, 2009, 2011). The lear- ner can engage in three different cognitive processes during learning from an expository text such as shown in Fig. 1: the pro- cess of selecting, that is, paying attention to relevant information (e.g., highlighted text); the process of organizing, that is, building a coherent structure of the incoming information (e.g., compare- and-contrast structure); and the process of integrating, that is, relating new information with existing relevant knowledge acti- vated from long-term memory (e.g., text structure).
In reading the steamboat passage shown in Fig. 1, students might adopt a default strategy of memorizing a list of isolated facts or a constructive strategy of building an organized knowledge struc- ture such as a compare-and-contrast matrix in which two elements (eastern-style steamboats and western style steam boats) are com- pared along several dimensions (e.g., rivers, type of hull, type of engine, number of decks, etc.). Alternatively, students might not put much effort into reading the passage regardless of their reading strategy, and thereby not learn much of anything at all.
How can we prime students to engage in deep cognitive pro- cessing during learning, such as structure building? A study aid technique intended to foster structure building is to provide gra- phic organizers to accompany the text (which we call static graphic
Cognitive process
Selecting: attentional focus on each AOI
Selecting: Attentional focus on each AOI
Organizing and integrating: Attempts to relate sentences in the top section with sentences in the bottom section to establish a comparison Organizing and integrating: Attempts to integrate the graphic organize with the text and build a compare-and-contrast structure of the text
Comprehension based on a comparison-and-contrast composition. Relational sentences
Memorization of pieces of information
organizers as shown in Fig. 2) or to ask learners to fill in graphic organizers (which we call interactive graphic organizers as shown in Fig. 3). These kinds of study aids are intended to prime three kinds of cognitive processes during learning: selecting relevant information to put into a structure, organizing the relevant mate- rial into a coherent structure (i.e., a matrix in this case), and inte- grating the new knowledge with relevant prior knowledge (e.g., previous experience with compare-and-contrast structures). These cognitive activities should be reflected in improvements both on memory tests and comprehension tests.
In contrast, a study aid technique aimed mainly at memorizing isolated facts is to highlight key material in red print (which we call static highlighting as shown in Fig. 4) or to ask students to highlight key material in red print (which we call interactive high- lighting as shown in Fig. 5). These kinds of study activities are intended mainly to prime the cognitive process of selecting, which would be reflected in improvements on memory tests but not com- prehension tests.
5. Hypotheses
In this study we primarily examine the depth hypothesis, which holds that not only people learn more deeply from structure-build- ing study aids (such as graphic organizers) than memorization study aids (such as highlighting) but also that the learning process is affected differently when these study aids are adopted, as shown by the eye movement analysis. We define the predictions in terms of eye movements and learning outcomes, as follows.
  Fig. 2. The steamboat passage in the highlighted condition.

24 H.R. Ponce, R.E. Mayer / Computers in Human Behavior 41 (2014) 21–32
  Fig. 3. The steamboat passage and the filled in graphic organizer.
5.1. Eye movements hypotheses
Regarding eye tracking metrics involving number of eye fixa- tions (as an indication of the process of selecting), the depth hypothesis predicts more fixations on the important words colored in red in the static highlighting condition (i.e., guide learner’s selec- tive attention) compared with the text-only condition (Hypothesis 1). Additionally, we expect that participants in the static graphic organizer condition will fixate less on the text compared with the text-only group but will spend more time fixating on the gra- phic organizer area, showing the activation of the cognitive pro- cesses of selecting as well by guiding learner’s selective attention to information found on the organizer (Hypothesis 2).
Regarding eye tracking metrics involving transitions, we con- sider transitions between the top and bottom sections (i.e., up– down transitions) of the steamboat passage as an indication that attempts at making specific comparisons within the text are being made and therefore the activation of the cognitive process of
organizing and integrating (i.e., given the structure of the steam- boat passage it is necessary to relate sentences in the top section with sentences in bottom section). Similarly, left–right transitions also reflect attempts to integrate information. Consequently, the depth hypothesis predicts more transitions per minute (as an indi- cation of integrating) for the graphic organizer groups than the highlighting groups (Hypothesis 3).
As a secondary hypothesis, the interactivity hypothesis is that people learn more when they are primed to engage in hands-on activity during learning through interactive study aids (such as interactive highlighting and interactive graphic organizers) rather than static study aids (such as static highlighting or static graphic organizers). Based on the interactivity hypothesis, we expect to observe more activity in terms of transitions between the top and bottom sections (i.e., up–down transitions) and vice versa in the interactive conditions compared with the static conditions. Additionally, we expect to observe significantly more transitions between the text and the graphic organizer area (i.e., right–left
  Fig. 4. The steamboat passage and the interactive graphic organizer.
transitions) in the IGO group compared with the SGO since partic- ipants in the IGO group need to read the text more often to fill in the graphic organizer (Hypothesis 4).
5.2. Learning outcomes hypotheses
Regarding the memory test (which we consider a reflection of the cognitive process of selecting), the depth hypothesis predicts that the graphic organizer groups (i.e., IGO and SGO groups) and the highlighting groups (i.e., static and interactive highlighting groups) will all outperform the text-only group (Hypothesis 5). In contrast, on the comprehension test (which is intended to reflect the cognitive process of integrating as well as selecting and orga- nizing), the depth hypothesis predicts that the graphic organizer groups will score significantly higher than the text-only group but the highlighting groups will not (Hypothesis 6).
Finally, the interactivity hypothesis predicts that the interactive highlighting group will outperform the static highlighting group (Hypothesis 7) and the IGO group will outperform the SGO group (Hypothesis 8) on the comprehension test and the memory test. It should be noted that the depth hypothesis does not make this prediction because behavioral activity does not guarantee appro- priate cognitive activity.
6. Method
6.1. Participants and design
The participants were 130 college students recruited from the Psychology Subject Pool at the University of California, Santa Bar- bara. The average age of the participants was 19.45years (SD = 1.26) and the proportion of females was 0.83. Forty-five per- cent of the participants were freshmen (first year), 29% were soph- omores (second year), 16% were juniors (third year) and 10% were seniors (fourth year). The mean rating on self-rated prior knowl- edge about steamboats was 1.68 (SD = 0.75) in a scale of 1–5, which is in the very low range.
There were five groups based on a between-subject design, with 26 participants in each group: text-only group, which read a plain text; static highlighting group, which read a text with key words colored in red; interactive highlighting group, which read a text with an editor so learners could highlight portions in red; static graphic organizer (SGO) group, which read a text along with a filled in graphic organizer; and interactive graphic organizer (IGO)
group, which read a text along with an empty graphic organizer for learners to fill in.
The groups did not differ significantly from one another (at p < .05) in mean age, self-rated prior knowledge of steamboats, or proportion of men and women.
6.2. Materials
The instructional materials consisted of five versions of a 123- word expository passage on steamboats adapted from Meyer and Poon (2001), and implemented in PowerPoint as shown in Fig. 1. This text compares two types of steamboats (i.e., eastern-style and western-style) along several dimensions (e.g., river depth, cargo storage engine type, etc.), so the rhetorical structure of this text is compare-and-contrast (Cook & Mayer, 1988). The first 6 sen- tences of the passage describe eastern-style steamboats and the last 7 sentences describe western-style steamboats. The Flesch reading ease index was 0.48 and the Flesch-Kincaid grade level was 10; indicating that the steamboat passage can easily be under- stood by students in 10th grade (i.e., 15–16 years old students).
As shown in Fig. 1, in the text-only version (control group), the steamboat passage was presented as the only window on the screen. As shown in Fig. 2, the static highlighting version was iden- tical to the text-only condition except that 28 words were high- lighted in red. The highlighted words were the elements being compared between eastern-style and western-style steamboats (e.g., Hudson River vs. Missouri, Ohio and Mississippi, deep vs. flat, low-pressure vs. high-pressure, etc.). The objective was to direct readers’ attention to these words in order to make readers more easily aware of the comparisons.
As shown in Fig. 3, the static graphic organizer (SGO) version contained the steamboat passage on the left side and a filled in gra- phic organizer on the right side of the screen. On the top section of the graphic organizer, ‘‘eastern-style steamboat’’ was printed in the left corner and ‘‘western-style steamboat’’ was printed on the right corner. In the middle section, each attribute or dimension was explicitly indicated with the respective value for each type of steamboat. The same values that appear in red in the highlighted condition were presented in the graphic organizer, although the comparison is made more explicit in the graphic organizer by pro- viding additional structural information.
As shown in Fig. 4, the interactive highlighting version presents the steamboat passage within an editor application with function- alities to highlight text. The interactive highlighting editor is an
H.R. Ponce, R.E. Mayer / Computers in Human Behavior 41 (2014) 21–32 25
  Fig. 5. The steamboat passage in the editor for highlighting.

26 H.R. Ponce, R.E. Mayer / Computers in Human Behavior 41 (2014) 21–32
application developed in Adobe Flash. The editor implements func- tions to highlight text in various colors but students were asked to highlight only with the color red. This version also has functional- ity to undo any piece of highlighted text once the participant has already colored part of the text, if required.
As shown in Fig. 5, the interactive graphic organizer (IGO) ver- sion presents the steamboat passage on the left side and an embed- ded graphic organizer on the right side. The graphic organizer was implemented in Adobe Flash so it was simple to integrate into PowerPoint as an embedded object. The interactive graphic orga- nizer was presented with empty boxes so the learner could type in the two elements to be compared, could add information about one dimension and its respective values. Learners could add as many additional dimensions as they wished, and type in the name of each dimension and its values for the eastern-style and western- style steamboats.
The test materials consisted of a cloze test and a summary test. The cloze test was devised to measure verbatim memory of the les- son. The cloze test consisted of a sheet of paper containing the steamboat passage but with 13 words eliminated and replaced with blank spaces for participants to fill in. The following words were eliminated: Hudson, deep (three times), below, low-pressure, shallow, Missouri, Ohio, Mississippi, flat, on, high-pressure (as shown in Appendix A). These are the words that were highlighted in red in the highlighted version of the lesson or used in the cells of the graphic organizer in the graphic organizer version of the lesson. One point was given for each exact word that was filled in cor- rectly, yielding a maximum score of 13 points.
The summary test was devised to measure comprehension of the text and was presented on a computer screen. The summary test consisted of a computer-based blank MS-Word document that participants used to type a summary of the steamboat passage without access to the original version. The exact instruction to par- ticipants was: ‘‘Please write a summary of the steamboat lesson. You have a maximum of 5 min.’’ To score the summary, we created a 9-item rubric consisting of comparisons between the eastern steamboat and the western steamboat on each of nine dimensions (as shown in Appendix B). One point was given for each complete comparison in the summary, either written in one complete sen- tence (e.g., eastern steamboats have deep hulls and western steam- boats have flat hulls) or written in two separate sections (e.g., as in the original text). The maximum score was 9 points.
The informed consent form described the experiment, summa- rized the participant’s rights, and contained a space for the partic- ipant’s signature. The participant pre-questionnaire solicited basic demographic information concerning age, sex, and year in college. The participant post-questionnaire consisted of five statements that asked for ratings on a 5-point scale about previous knowledge of steamboats, difficulty and effort in learning the steamboat les- son, and satisfaction and motivation by asking participants their level of agreement with the following statements ‘‘I like using this computer program’’ and ‘‘I would like more lessons like this one’’.
The apparatus consisted of a Tobii T-60 eye tracking system used to present the passage and record eye movements and a Dell computer where the Tobii Studio software was installed.
6.3. Measures
6.3.1. Measures of eye movements
First, we computed total fixation time and number of fixations in the regions analyzed for each area of the screen (as summarized as the first two measures in Table 1). Second, we established areas of interest (AOIs), which are defined as regions in the stimulus where a researcher focuses his/her attention to examine eye move- ment patterns (e.g. words, sentences, and images), and we counted
the number of transitions (or saccades) between AOIs (Holmqvist et al., 2011).
In our study, the steamboat passage was divided into two AOIs: the top section of the text (i.e., the first 6 sentences) and bottom section of the text (i.e., the last 7 sentences) (see Fig. 1). The reason for this two initial AOIs is that in order to make a comparison between different attributes (e.g., hull types) associated with the steamboats it is necessary to locate the information in the top sec- tion and later to compare it with the respective information in the bottom section. This processing takes place in working memory, which has limited capacity, so numerous fixations and transitions on key parts of the texts may be required. In addition, for the static and interactive graphic organizer groups, the graphic organizer (on the right side of the screen) was defined as a third area of interest. Finally, an orthogonal set of AOIs was constructed to compare highlighted words (i.e., the red words in the static highlighting group) and the non-highlighted words.
We counted two types of transitions: (1) transitions between top and bottom sections and vice versa of the steamboat passage (which we refer to as up–down transitions) and (2) transitions between the steamboat passage and the graphic organizer area and vice versa (which we refer to as left–right transitions). We view up–down transitions as an indication of the cognitive process of organizing and integrating since the learner is attempting to relate specific sentences from the top and bottom sections of the steamboat passage to establish comparisons (as summarized in the third measure in Table 1). Similarly, we consider left–right tran- sitions as attempts to integrate the graphic organizer with the text and construct a compare-and-contrast structure of the steamboat passage (as summarized in the fourth measure in Table 1).
In previous studies, transition measures have been used to reflect attempts at integrating information from two or more AOIs. For instance, Johnson and Mayer (2012) counted the number of transitions between an AOI containing a text and an AOI containing a picture (about a braking system) as attempts at engaging in the cognitive process of integrating during learning. Similarly, Mason, Pluchino, and Tornatora (2013) used second-pass fixation time as a measure of transitions between a text and a picture which repre- sents cognitive efforts at integrating written and pictorial representations.
To facilitate and compare eye tracking measures, the text was formatted exactly in the same way for the text-only, static high- lighting, and static graphic organizer groups in terms of font type and size, space between words and sentences, and location of the text on the screen. In case of the interactive graphic organizer group, the font of the text was slightly smaller than the previous conditions to create more space and better accommodate the func- tionalities of the graphic organizer. Finally, due to the restricted functionalities of the editor application used in the interactive highlighting group, the size and location of the text on the screen were slightly different; so the AOIs were defined separately for this version of text. Data from one participant in the highlighting con- dition was excluded in the eye movement analysis due to the low- quality of eye tracking data.
6.3.2. Measures of learning outcomes
Two measures of learning outcomes were used: rote memory, measured via a cloze test, and reading comprehension, measured by asking the learner to write a summary (as summarized as the fifth and sixth measures, respectively, in Table 1). The cloze test is intended as a test of verbatim memory, and hence is referred to as a memory test; the summary test is intended as a test of how the material is organized, and hence is referred to as a com- prehension test. Cloze tests and summary tests have been used before as a measure of memory and reading comprehension respectively in studies involving the highlighting (Hartley et al.,

1980) and graphic organizer techniques (Kiewra, Kauffman, Robinson, Dubois, & Staley, 1999; Ponce & Mayer, 2014).
6.4. Procedure
Participants were randomly assigned to one of the five groups and were tested individually. As the participant arrived, he or she was seated in the testing room in front of the Tobii’s computer screen. The experimenter explained the aims of the study—to examine eye movement patterns while students read a text. Participants were asked to read and sign the consent form (which explained human subject protections) and to complete the partic- ipant pre-questionnaire (which solicited basic demographic infor- mation concerning age, sex, and year in school).
Next, the experimenter explained the eye tracking technology and proceeded with eye tracking calibration for the participant. Upon completion of calibration, the participant received verbal and written instructions on how to proceed based on his or her treatment group. In the text-only, static highlighting, and static graphic organizer groups, all participants were asked to read and study the text for a maximum of 2 min. In the interactive highlight- ing group and the interactive graphic organizer group, the experi- menter gave each participant a brief explanation of the functionalities of the highlighting editor or graphic organizer appli- cation, respectively. For training purposes, participants were asked to read a brief text that compared Arizona and Rhode Island in terms of population, climate and size. Participants in the highlight- ing conditions were asked to highlight key ideas on the practice text by first selecting a word or group of words and color them in red (using the respective function in the application). Similar to the interactive highlighting group, participants in the interactive graphic organizer group were asked to fill in the graphic organizer by using the same practice text. In both cases, the training sessions lasted about 5 min. After the training session, each participant received verbal and written instructions to read and study the steamboat text for a maximum of 4 min, using the highlighting editor or graphic organizer application corresponding to his or her treatment group. Participants in all five groups were also told that after studying the text they would be tested on the material.
For each condition, immediately after each participant studied the steamboat passage and the eye tracking recording was finished, the participant was asked to write a short summary of the text by typing directly into a MS-Word document on a computer. The experimenter informed the participant that the time limit to write the summary was 4 min. The participants were not allowed to review the original text, their own highlighted text, or their graphic organizer. After writing the summary, the paper-based cloze test was administered, with instructions that participants had 2 min to fill in the blank spaces in the test.
The final stage was to complete the post-questionnaire, in which participants rated their previous knowledge on steamboats, perceived difficulty and effort in performing the task, and satisfac- tion with the application. Finally, participants were debriefed, thanked, and dismissed. We adhered to standards for treatment of human subjects.
7. Results
7.1. Eye tracking results
The primary concern in this study is whether highlighting and graphic organizer study techniques prime different cognitive pro- cessing as shown by eye movement analysis. Our main hypothesis is that highlighting primes the cognitive process of selecting (i.e., focus learner’s attention on memorizing isolated facts) whereas
graphic organizers prime the cognitive processes of selecting, orga- nizing and integrating (i.e., encourage learners to build knowledge structures). We view eye fixations (time and number) as an indica- tion of the cognitive process of selecting and transitions between AOIs as an indication of the cognitive processes of organizing and integrating.
7.1.1. Eye fixations as an indication of the cognitive process of selecting
First, we compared eye fixation time and number between sta- tic highlighting and text-only groups. Highlighting is intended mainly to guide the learner’s selective attention to the highlighted material. Table 2 shows that the mean number of fixations and the mean total fixation time on the highlighted words in the text were greater for students in the static highlighting group than those in the text-only group. A t-test conducted on the fixation data sum- marized in Table 2 shows that the groups differed significantly on number of fixations, t(49) = 2.53, p < .05, d = 0.70, indicating that the static highlighting group outscored the text-only group. Additionally, a t-test conducted on the total fixation time data summarized in Table 2 shows that the groups differed significantly on total fixation time, t(49) = 2.18, p < .05, d = 0.61, indicating that static highlighting group outscored the text-only group. Consistent with the depth hypothesis (Hypothesis 1), these patterns of results provide some validation of the idea that the highlighting technique serves to guide the cognitive process of selecting.
Eye fixation data show that the graphic organizer also primes the cognitive process of selecting, as previously predicted (Hypoth- esis 2). Table 3 shows the mean total fixation time on each AOI for of the text-only and SGO groups. It can be observed that partici- pants in the SGO group fixated in average significantly less on the text compared with the text-only group, with t(50) = 5.64, p < .001. The SGO spent a substantial amount of time fixating on the filled-in graphic organizer, with 58.2% on the text and 41.8% on the graphic organizer. Consequently, these results confirm that the graphic organizer serve to guide learners’ selective attention to the information provided on the organizer.
7.1.2. Transitions as an indication of the cognitive processes of organizing and integrating
We view up–down transitions (and left–right transitions, where applicable) as measures of the deeper cognitive processes of orga- nizing and integrating. The depth hypothesis predicts that graphic organizers will prime more transitions than highlighting (Hypoth- esis 3). Table 4 summarizes the number of transitions and the number of transitions per minute for each group. We computed the mean number of transitions per minute in order make compar- isons between groups that received 2 min of study time (text-only, static highlighting, SGO) and those that received 4 min (interactive highlighting, and IGO).
First, we tested whether reading a highlighted text induces more transitions than reading a plain text. If a larger number of up–down transitions were observed for the highlighted group in comparison with the text-only group it would be indicative of the cognitive process of organizing and integrating, since it would show that the reader is attempting to integrate and organize differ- ent sections of the text by making specific comparisons (e.g., top section: ‘‘cargo was stored...below the main deck’’ vs. bottom
Table 2
Means and standard deviations on time and number of fixations on highlighted text.
H.R. Ponce, R.E. Mayer / Computers in Human Behavior 41 (2014) 21–32 27
    Group
Text-only
Static highlighting
N Time (s) Number of fixations M SD M SD
26 24.27 4.82 90.88 19.87 25 28.06 7.38 108.64 29.59

28 H.R. Ponce, R.E. Mayer / Computers in Human Behavior 41 (2014) 21–32
Table 3
Means and standard deviations on total fixation time in each area of interest (in seconds).
  Group
Text-only
Static graphic organizer
Table 4
N Top Bottom GO Total time
M SD M SD M SD M SD
      26 51.76 13.00 26 33.80 12.28
48.64 11.29 32.09 13.95
N Up–down transitions
M SD M
26 13.08 6.37 –
26 5.46 3.80 12.46 25 16.20 8.42 –
26 24.92 7.10 68.31 26 31.81 14.07 –
– – 47.29 23.46
Total transitions
M SD M SD
100.39 20.61 113.18 16.65
 Means and standard deviations on the transitions between AOIs.
  Group
Text-only
Static graphic organizer Static highlighting Interactive graphic organizer Interactive highlighting
Left–right transitions SD
Transitions per minute
      – 8.68
– 19.48 –
13.08 6.37 17.92 8.89 16.20 8.42 93.23 21.99 31.81 14.07
7.63 3.30 9.35 4.13 9.31 4.44
28.29 6.54 10.23 3.81
 section: ‘‘cargo was carried on the main deck’’). Yet, the depth hypothesis predicts that participants in the static highlighting con- dition would make similar number of up–down transitions com- pared with the text-only group. This hypothesis was confirmed, with t(49) = 1.50, p > .05, indicating that participants in the static highlighting conditions made few attempts to organize and inte- grate different sections of the text.
Second, we tested whether the SGO group induced more transi- tions than the text-only and static highlighting groups. It was observed that the total number of transitions per minutes was not significantly different, with F(2,74)=1.56, p>.05; however, the nature of such transitions was considerably different. Analyz- ing the transitions between the text and graphic organizer (i.e., left–right transitions in Table 4) for the SGO condition; it seems that participants actively used the graphic organizer for organizing and constructing a comparison-and-contrast mental representa- tion of the steamboat passage. The average number of left–right transitions was significantly higher than the average number of up–down transitions in the SGO condition, with t(25) = 3.56, p < .01, which is consistent with the depth hypothesis.
Third, according to the interactivity hypothesis, we expect more cognitive processing when hands-on activities are used to study a lesson (Hypothesis 4). Therefore, we tested whether highlighting text induces more transitions than highlighted text. As shown in the up–down transition column of Table 4, the interactive high- lighting group made considerable more transitions than the static highlighting group. However, if we take time on task into account, it can be observed that the difference between these two groups in terms of transitions per minutes was not statistically significant, t(49) = .80, p > .05.
Fourth, we tested whether asking students to filling in a graphic organizer to analyze a text induce more transitions than given them a filled in graphic organizer. In this case, the proportion of transitions was examined. In relation to the total number of tran- sitions between AOIs, the proportions of up–down transitions were 30% and 27% in the static and interactive graphic organizer condi- tions, respectively; meanwhile, the proportions of left–right transi- tions were 70% and 73%, respectively. These proportions were not statistically different between groups (p > .05). Therefore, the eye movement analysis appears to show that the graphic organizer affected reading patterns in similar fashion in both conditions. However, the number of transitions per minutes shows that the IGO group performed significantly more transitions than the SGO group, with t(50) = 12.48, p < .001 and d = 3.46. In the SGO condi- tion there is no need to make as many transitions between the text and graphic organizer area as in the IGO condition. The tendency in
the SGO condition is to examine both areas (i.e. text and graphic organizer) as independent areas. In contrast, the need to fill in the graphic organizer in the IGO condition makes it necessary to fully integrate the text and graphic organizer area, as shown by the large number of transitions.
7.2. Learning outcomes results
The eye movement analysis is consistent with the idea that the highlighting study aid primes the cognitive process of selecting whereas the graphic organizer study aid primes the cognitive pro- cess of selecting, organizing and integrating. We examine in this section differences on learning outcomes generated by these study aids. Thus, the cloze test was intended to measure rote memory for pieces of information in the lesson (indicating the degree to which the learner engaged in the cognitive process of selecting during learning). The summary test was intended to measure comprehen- sion of the lesson (indicating the degree to which the learner engaged in the cognitive processes of selecting, organizing and integrating during learning).
7.2.1. Depth hypothesis
According to the depth hypothesis, we predicted that all four of the treatments would promote the cognitive process of selecting as compared to the control group and therefore improve performance on the cloze test (Hypothesis 5). The left side of Table 5 shows the mean score (and standard deviation) of each group on the cloze test. An ANOVA showed a significant difference among groups, F(4, 125) = 6.08, p < .001. As predicted, follow-up post hoc analysis using a Dunnett test revealed that all four instructional treatment groups outperformed the text-only group with p < .05. Consistent with predictions, the effect sizes were large for the static graphic
Table 5
Means and standard deviations on cloze test and summary test for each group.
 Group
Text-only
Static graphic organizer Static highlighting Interactive graphic
organizer Interactive highlighting
N Cloze test
M SD d M
Summary test SD d
    26 4.19 2.79 26 6.62* 3.01 26 6.88* 3.39 26 8.27* 2.82
26 6.54* 3.14
– 2.69 .84 4.42* .87 3.92
1.45 5.35*
.79 3.50
2.15 2.58 2.43 1.94
– .89
.49 1.17
2.20
.33
 Note: Effect size (d) computed in relation to the text-only group. * Group scored significantly higher than the text-only group.
organizer (d = 0.84), IGO group (d = 1.45), static highlighting group (d = 0.87), and interactive highlighting group (d = 0.79).
The depth hypothesis predicts that the two graphic organizer groups would outperform the text-only group on the summary test but the two highlighting groups would not outperform the text- only group on the summary test (Hypothesis 6). The right side of Table 5 shows the mean score (and standard deviation) of each group on the summary test. As an initial step, an ANOVA showed a significant difference among groups, F(4, 125) = 5.00, p < .001. As predicted by the depth hypothesis, follow-up post hoc analysis using a Dunnett test showed that the SGO and IGO groups signifi- cantly outperformed the text-only group with p < .05, whereas the other groups did not. Also consistent with predictions of the depth hypothesis large effect sizes were obtained for the SGO group (d = 0.89) and the IGO group (d = 1.17) but smaller effect sizes were obtained for the static highlighting group (d = 0.49) and the inter- active highlighting group (d = 0.33).
Overall, the pattern of results shown in Table 5 is most consis- tent with the depth hypothesis that highlighting primes the cogni- tive process of selecting (reflected in improvements on the cloze test) whereas graphic organizers prime the cognitive processes of selecting, organizing and integrating (reflected in improvements on both the cloze test and the summary test).
7.2.2. Interactivity hypothesis
According to interactivity hypothesis, asking students to high- light a text (i.e., interactive highlighting) should result in better learning than designing a lesson with experimenter-provided high- lighted text (i.e., static highlighting) (Hypothesis 7). We did not find evidence to confirm this hypothesis. A t-test revealed that stu- dents in the static highlighting group did not obtain significantly different scores than students in the interactive highlighting group on the cloze test, t(50) = .38, p > .05, nor on the summary test, t(50) = .66, p > .05. Also in contrast to the interactivity hypothesis, the effect size favoring the static highlighting group was d = 0.10 on the cloze test and d = 0.18 on the summary test, which are regarded as inconsequential.
Furthermore, according to the interactivity hypothesis, asking students to actively fill in a graphic organizer (i.e., IGO) should result in better learning when giving them a lesson with author- provided filled-in graphic organizer (i.e., SGO) (Hypothesis 8). Con- sistent with predictions, on the cloze test, students in the IGO group significantly outperformed students in the SGO group, t(50) = 2.05, p < .05, yielding an effect size of d = 0.57, favoring the IGO group. On the summary test, the difference was not signif- icant, t(50) = 1.46, p > .05, and the effect size favoring the IGO group was d = 0.41.
Overall, based on the results reported in this section, we con- clude that the interactive activity hypothesis was not supported in its most stringent form. In short, learner activity per se does not necessarily cause learning. It appears that interactivity may not be effective for unstructured tasks such as highlighting in which the learner may engage in unproductive activities but may be effective for structured tasks such as filling in a graphic orga- nizer as long as the learner engages in productive activities.
In order to better pinpoint when interactivity might contribute to learning, we examined how well participants used the interac- tive highlighting and graphic organizer learning strategies and its effects on learning outcomes. To assess the highlighting strategy, first, we counted the total number of words highlighted by the par- ticipants using the color red, and second, we counted how many of these words corresponded to those colored in red in the static highlighting condition (i.e. common words). Third, we computed a ratio between total of highlighted words divided by common words (i.e. proportion of corresponding words). Fourth, we com- puted a correlation between this ratio and the cloze and summary
scores. The Pearson correlation between the proportion of corre- sponding words and cloze score was r(26) = .51, p < .05, and with the summary score, it was r(26) = .28, p > .05.
In relation to the IGO condition, we counted the number of cor- rect dimensions and respective values filled in the graphic orga- nizer by each participant. The mean number of dimensions and respective values identified by the participants was 4.15 (SD = 1.57), out of a maximum score of 7.0 (i.e., corresponding to the seven dimensions contained in the static graphic organizer). The Pearson correlation between correctly identified dimensions and values on the graphic organizer and cloze score was r(26) = .71, p < .001, and with the summary score, it was r(26) = .76, p < .001.
Overall, this correlational analysis suggests that the highlight- ing study strategy mostly prime memorization of isolated facts (i.e., as measured by a cloze test) whereas the study strategy of cre- ating a graphic organizer tends to support both building a coherent cognitive structure (i.e., as measured by the summary test) and memorization of the facts that go into it (i.e., as measured by a cloze test).
7.3. Effort, motivation, difficulty and satisfaction
Finally, this section presents the results on the post-question- naire that asked students to rate (on a 5-point scale) their level of effort and difficulty in learning the steamboat lesson and moti- vation and satisfaction with the lesson.
Based on the study aid hypothesis, adding a study aid to a les- son such as a graphic organizer or highlighted text is expected to increase cognitive processing compared with reading a plain text and therefore require more mental effort. On the other hand, study aids are expected to decrease extraneous processing compared with reading a text without aids and consequently reduce the per- ceived difficulty of the lesson. The first two columns of Table 6 show the average results (and standard deviation) for perceived effort and difficulty. Contrary to expectation, we did not find a sig- nificant difference for perceived effort. Regarding difficulty, stu- dents in the text-only condition perceived the highest level of difficult with the lesson. In a pair-wise comparison, using a Dun- nett test, the IGO group was the only one that showed a significant difference in terms of difficulty compared with the text-only group (p < .05) and with an effect size of  0.71.
Another aspect measured was a dimension of motivation (third column in Table 6) with the question ‘‘I would like more lesson like this one,’’ which was devised to measure persistence on a learning task (an aspect of motivation) (third column in Table 6). We did not find significant differences between groups. The last item mea- sured was overall satisfaction with the lesson (see last column in Table 6) with the question ‘‘I like using this computer program.’’ In average, the group that showed the least satisfaction with the lesson was the text-only group. In a pair-wise comparison, using a Dunnett test, the SGO group was the only one that showed a sig- nificant difference in terms of satisfaction compared with the text- only group (p < .05) and with an effect size of d = 0.98. The IGO group resulted with an effect size of 0.58, followed by the static highlighting (d = 0.45) and interactive highlighting (d = 0.34).
8. Discussion
8.1. Empirical contributions
First, we found evidence to support the depth hypothesis, which holds that shallow study aids promote low-level processing (Hypothesis 1) such as selecting relevant information whereas dee- per study aids such as graphic organizers promote both low-level
H.R. Ponce, R.E. Mayer / Computers in Human Behavior 41 (2014) 21–32 29
30 H.R. Ponce, R.E. Mayer / Computers in Human Behavior 41 (2014) 21–32 Table 6
Mean ratings and standard deviations on effort, difficulty, motivation, and satisfaction. Group Effort Difficulty
Motivation Satisfaction
      M SD M
Text-only 3.15 0.88 2.77 Static graphic organizer 3.35 0.80 2.50 Static highlighting 3.38 0.80 2.38 Interactive graphic organizer 3.19 0.85 2.15* Interactive highlighting 3.27 0.67 2.42
Group scored significantly higher than the text-only group.
SD M SD
0.91 3.77 0.71 0.71 3.96 0.66 0.75 3.77 0.65 0.83 3.77 0.59 0.86 3.69 0.79
M SD
3.31 0.62 3.92* 0.63 3.58 0.58 3.65 0.56 3.54 0.71
   *
processing (Hypothesis 2) and high-level processing such as orga- nizing relevant information (Hypothesis 3). Thus, adding study aids such as highlighting and graphic organizer to a lesson improves learning outcomes significantly more compared to a lesson with- out such aids (i.e., text-only) when the test taps rote memory for the presented material (Hypothesis 5), but only graphic organizers (both static and interactive) produce improvements when the test taps comprehension (Hypothesis 6). This pattern is consistent with previous empirical studies that have found that highlighting text tends to improve memory of the text that has been marked (Hartley et al., 1980; Lorch, 1989) but it does not support focusing on connections between ideas in the text (Dunlosky et al., 2013). Our analysis of eye tracking measures further supports this find- ing: students in the highlighting conditions spent significantly more time looking at that part of the text that was highlighted in comparison with the text-only group, with medium effect sizes of 0.70 for number of fixations and 0.61 for fixation time (Hypoth- esis 1).
Second, interactivity per se, that is, adding hands-on activities such as asking students to highlight text or fill in a graphic orga- nizer does not automatically lead to better learning compared with the static (experimenter-provided) versions of the same study aids (Hypotheses 7 and 8). Unstructured tasks such highlighting appear to engage students in unproductive activity, such as extraneous cognitive processing (Mayer, 2009; Sweller, 1999) that is, addi- tional cognitive processing which does not lead to building a coherence structure of the text. This was observed when partici- pants highlighted sections of the text that were not relevant for the comparison, such as ‘‘Eastern-style steamboat became a finan- cial success in 1807’’. Eye tracking measures show that participants in the interactive highlighting group made the highest number of up–down transitions; however, such transitions did not result in attempting to make connections between these two parts of the text, as shown by the small effect size in the summary test (d = 0.33) (Hypothesis 4). Consequently, providing highlighted text appears to be more efficient in terms of cognitive processing than asking students to highlight text.
On other hand, structured tasks such as filling a graphic orga- nizer appear to promote productive learning activity, such as gen- erative cognitive processing (Mayer, 2009; Sweller, 1999). Our eye tracking measures support these findings, particularly with the interactive graphic organizer. In this case, significantly more up– down and left–right transitions per minute were observed com- pared with its static version, with a very large effect size equal to 3.46 (Hypothesis 4). This shows that students that filled in the gra- phic organizer followed its embedded structure (i.e., compare-and- contrast) to read and comprehend the steamboat passage, as shown also by large effect size in the summary test (d = 1.17).
8.2. Theoretical contributions
Consistent with the depth hypothesis, findings from the learn- ing outcome and eye tracking measures show that each study aid
technique explored in this study—highlighting and graphic orga- nizer—appear to activate different cognitive processes during learning as described in the cognitive theory of multimedia learn- ing (Mayer, 2009, 2011). Eye movement patterns show that both the static and interactive highlighting techniques prime only the cognitive process of selecting during studying a text such as the steamboat passage. Compared with the control group (i.e., text- only) eye movement patterns were not substantially modified by the inclusion of the highlighting study aid, except for attention (i.e., shown by fixation duration and number of fixations) on important words (i.e., those colored in red in the static condition).
On the other hand, eye movement patterns demonstrate that both the static and interactive graphic organizers prime the cogni- tive processes of selecting, organizing and integrating by providing a structure that modifies substantially where the eyes fixate and move (i.e. transitions) within the steamboat passage. This was more evident with the interactive graphic organizer in which eye movements showed that in order to fill in each component (i.e., an empty text-box) of the organizer readers tended to read the text following the structure of the graphic organizer. In case of the filled in graphic organizer (static version), eye movement measures (i.e., transitions) showed less integration between the text and graphic organizer compared with the interactive version. One possible explanation is that the information is already in the graphic orga- nizer within an easy-to-read compare-and-contrast structure, so there is little need to revisit the text too often (Beyer, 1997; Ponce & Mayer, 2014).
8.3. Practical contributions
Two relevant practical contributions can be derived from this study. First, from a teaching point of view, choosing the appropri- ate study technique to instruct students on how to process expos- itory text can have important consequences on learning outcomes. Teaching students to highlight text without appropriate structures (e.g., cause-and-effect or comparison-and-contrast) may help stu- dents select what to highlight, but not help them organize the selections in working memory and to integrate them with previous knowledge from long-term memory. The advantage of graphic organizers is that they provide structures that embed specific pro- cedure on what to select, and how to organize and integrate the learning material (Robinson & Skinner, 1996).
A second important contribution relates to the instructional design of multimedia learning applications (Mayer, 2005). High- lighting important words in texts, which is easier to implement in a multimedia application, can improve retention of information. However, if the goal is to improve comprehension of texts, more elaborated instructional design aids must be included that show not only what is important in the text (e.g. key concepts) but also its underlying structure (e.g., comparison-and-contrast). We have demonstrated that appropriate inclusion of graphic organizers can have a significant impact on improving comprehension of expository texts.

8.4. Limitations and future directions
Appendix B. (continued)
H.R. Ponce, R.E. Mayer / Computers in Human Behavior 41 (2014) 21–32
31
   Two relevant limitations can be recognized in our study. First, the steamboat passage is a short and well-structured expository text, so it would difficult to extrapolate our results to longer texts that may need other reading strategies for their comprehension. Second, other structures found in expository texts (e.g., cause- and-effect, sequence, and hierarchy) may also show distinct read- ing patterns that may require the construction of different eye tracking measures (e.g., transitions between AOIs) compared to the ones used in this study. Therefore, these limitations open the option to expand this study to longer texts and incorporate texts with different structures.
Appendix A. Cloze test
Please fill in the missing words:
Eastern-style steamboats became a financial success in 1807. These one-story boats operated on the _____________ River and other eastern rivers. These rivers were _____________ and suited perfectly the _____________ hulls of the eastern steamboat. The cargo was stored in these _____________ hulls _____________ the main deck. The eastern steamboats used _____________ engines. Western-style steamboats, however, were different. They churned their way up the _____________ waters of the _____________, _____________, and _____________ Rivers. Their hulls were _____________, without room for cargo. The cargo was carried _____________ the main deck or on the superstructure, one or two floors above the main deck. More efficient and dangerous _____________ engines were used and often burned up to 32 cords of wood a day.
Appendix B. Summary scoring rubric
The following rubric was used to score the summaries. Essen- tially, it consists of the possible comparisons made in the steam- boat text. Each comparison receives one point, with a maximum of 9 points.
Attribute
A4 Hull type
A5 Cargo storage
A6 Number of floors
A7 Engine type
A8 Engine efficiency
A9 Engine safety
Final score
References
Value
Deep vs. flat
In deep hull below main deck vs. on main deck
One-story vs. 1 or 2 floors
Low-pressure vs. high pressure
High-pressure engine more efficient than low-pressure
High-pressure engine more dangerous than low-pressure
Example Score
western rivers were shallow The hull on the eastern steamboat was deep meanwhile on the western steamboat the hull was flat
The cargo on the eastern steamboat was stored in deep hulls while on the western steamboat was stored on the main deck Eastern steamboat were one-story while western steamboat were 1or2floors
The engine on the eastern steamboat was low-pressure meanwhile on the western steamboat was high-pressure High-pressure engines were more efficient than low- pressure engines High-pressure engines were more dangerous than low- pressure engines
   Attribute
A1 Steamboat type
A2 Rivers of operation
Value
Eastern-style and western- style were different
Eastern rivers: Hudson river vs. western rivers: Missouri, Ohio, Mississippi
Deep vs. shallow
Example Score
The passage is about the differences between eastern-style and western- style steamboats Eastern steamboat operated on eastern rivers such as Hudson River meanwhile western steamboat operated on western rivers such as Missouri, Ohio, and Mississippi Eastern rivers were deep meanwhile
            A3 River depth
Bell, K. E., & Limber, J. E. (2010). Reading skill, textbook marking, and course performance. Literacy Research and Instruction, 49(1), 56–67.
Beyer, B. K. (1997). Improving student thinking: A comprehensive approach. Boston, MA: Allyn and Bacon.
Cook, L. K., & Mayer, R. E. (1988). Teaching readers about the structure of scientific text. Journal of Educational Psychology, 80, 448–456. http://dx.doi.org/10.1037/ 0022-0663.80.4.448.
Dunlosky, J., Rawson, K. A., Marsh, E. J., Nathan, M. J., & Willingham, D. T. (2013). Improving students’ learning with effective learning techniques: Promising directions from cognitive and educational psychology. Psychological Science in the Public Interest, 14(1), 4–58. http://dx.doi.org/10.1177/1529100612453266.
Fiorella, L., & Mayer, R. E. (2015). Learning as a generative activity: Eight learning strategies that promote understanding. New York: Cambridge University Press.
Fowler, R. L., & Barker, A. S. (1974). Effectiveness of highlighting for retention of text material. Journal of Applied Psychology, 59(3), 358–364. http://dx.doi.org/ 10.1037/h0036750.
Hartley, J., Bartlett, S., & Branthwaite, A. (1980). Underlining can make a difference: Sometimes. The Journal of Educational Research, 73(4), 218–224. http:// dx.doi.org/10.2307/27539753.

32 H.R. Ponce, R.E. Mayer / Computers in Human Behavior 41 (2014) 21–32
Holmqvist, K., Nystrom, M., Anderson, R., Dewhurst, R., Jarodzka, H., & van de Weijer, J. (2011). Eye tracking: A comprehensive guide to methods and measures. Oxford: Oxford University Press.
Hyönä, J., Lorch, R. F., & Kaakinen, J. K. (2002). Individual differences in reading to summarize expository text: Evidence from eye fixation patterns. Journal of Educational Psychology, 94(1), 44–55. http://dx.doi.org/10.1037/0022- 0663.94.1.44.
Johnson, C. I., & Mayer, R. E. (2012). An eye movement analysis of the spatial contiguity effect in multimedia learning. Journal of Experimental Psychology: Applied, 18(2), 178–191. http://dx.doi.org/10.1037/0033-2909.124.3.372 10.1037/a0026923.
Kaakinen, J. K., & Hyönä, J. (2010). Task effects on eye movements during reading. Journal of Experimental Psychology: Learning, Memory, and Cognition, 36(6), 1561–1566. http://dx.doi.org/10.1037/a0020693.
Kiewra, K. A. (1985). Investigating notetaking and review: A depth of processing alternative. Educational Psychologist, 20(1), 23–32.
Kiewra, K. A., Kauffman, D. F., Robinson, D. H., Dubois, N. F., & Staley, R. K. (1999). Supplementing floundering text with adjunct displays. Instructional Science, 27(5), 373–401. http://dx.doi.org/10.1023/a:1003270723360.
Lonka, K., Lindblom-YlÄnne, S., & Maury, S. (1994). The effect of study strategies on learning from text. Learning and Instruction, 4(3), 253–271. http://dx.doi.org/ 10.1016/0959-4752(94)90026-4.
Lorch, R. F. Jr., (1989). Text-signaling devices and their effects on reading and memory processes. Educational Psychology Review, 1(3), 209–234. http:// dx.doi.org/10.1007/bf01320135.
Marzano, R. J., Pickering, D., & Pollock, J. E. (2001). Classroom instruction that works: Research-based strategies for increasing student achievement (p. vi, 178 p). Alexandria, VA: Association for Supervision and Curriculum Development.
Mason, L., Pluchino, P., & Tornatora, M. C. (2013). Effects of picture labeling on science text processing and learning: Evidence from eye movements. Reading Research Quarterly, 48(2), 199–214. http://dx.doi.org/10.1002/rrq.41.
Mayer, R. E. (2005). The Cambridge handbook of multimedia learning. New York: Cambridge University Press.
Mayer, R. E. (2009). Multimedia learning (2nd ed.). New York: Cambridge University Press.
Mayer, R. E. (2011). Applying the science of learning. Boston, MA: Pearson/Allyn & Bacon.
Meyer, B. J. F., & Poon, L. W. (2001). Effects of structure strategy training and signaling on recall of text. Journal of Educational Psychology, 93(1), 141–159. http://dx.doi.org/10.1037/0022-0663.93.1.141.
Moore, D. W., & Readence, J. E. (1984). A quantitative and qualitative review of graphic organizer research. The Journal of Educational Research, 78(1), 11–17. http://dx.doi.org/10.2307/27540086.
National Institute of Child Health and Human Development (2000). Report of the national reading panel. Teaching children to read: An evidence-based assessment of the scientific research literature on reading and its implications for reading instruction: Reports of the subgroups (NIH Publication No. 00-4754). Washington, DC.
Peper, R. J., & Mayer, R. E. (1978). Note taking as a generative activity. Journal of Educational Psychology, 70(4), 514–522.
Ponce, H. R., López, M. J., & Mayer, R. E. (2012). Instructional effectiveness of a computer-supported program for teaching reading comprehension strategies. Computers & Education, 59(4), 1170–1183. http://dx.doi.org/10.1016/ j.compedu.2012.05.013.
Ponce, H. R., & Mayer, R. E. (2014). Qualitatively different cognitive processing during online reading primed by different study activities. Computers in Human Behavior, 30, 121–130. http://dx.doi.org/10.1016/j.chb.2013.07.054.
Porter, A., McMaken, J., Hwang, J., & Yang, R. (2011). Common core standards: The new U.S. intended curriculum. Educational Researcher, 40(3), 103–116. http:// dx.doi.org/10.3102/0013189x11405038.
Rayner, K. (1995). Eye movements in reading: Perceptual and cognitive processes. Canadian Psychology/Psychologie Canadienne, 36(1), 57–58. http://dx.doi.org/ 10.1037/h0084725.
Rayner, K. (1998). Eye movements in reading and information processing: 20 years of research. Psychological Bulletin, 124(3), 372–422. http://dx.doi.org/10.1037/ 0033-2909.124.3.372.
Rayner, K., Chace, K. H., Slattery, T. J., & Ashby, J. (2006). Eye movements as reflections of comprehension processes in reading. Scientific Studies of Reading, 10(3), 241–255. http://dx.doi.org/10.1207/s1532799xssr1003_3.
Robinson, D. H., Katayama, A. D., Beth, A., Odom, S., Hsieh, Y.-P., & Vanderveen, A. (2006). Increasing text comprehension and graphic note taking using a partial graphic organizer. The Journal of Educational Research, 100(2), 103–111.
Robinson, D. H., & Skinner, C. H. (1996). Why graphic organizers facilitate search processes: Fewer words or computationally efficient indexing? Contemporary Educational Psychology, 21(2), 166–180. http://dx.doi.org/10.1006/ ceps.1996.0014.
Stull, A. T., & Mayer, R. E. (2007). Learning by doing versus learning by viewing: Three experimental comparisons of learner-generated versus author-provided graphic organizers. Journal of Educational Psychology, 99(4), 808–820. http:// dx.doi.org/10.1037/0022-0663.80.4.448.
Sweller, J. (1999). Instructional design in technical areas. Camberwell, Australia: ACER Press.
Wade, S. E., Trathen, W., & Schraw, G. (1990). An analysis of spontaneous study strategies. Reading Research Quarterly, 25(2), 147–166. http://dx.doi.org/ 10.2307/747599.
User Model User-Adap Inter (2009) 19:307–339 DOI 10.1007/s11257-009-9066-4
ORIGINAL PAPER
Can eyes reveal interest? Implicit queries from gaze patterns
Antti Ajanki · David R. Hardoon · Samuel Kaski · Kai Puolamäki · John Shawe-Taylor
Received: 7 July 2007 / Accepted: 20 August 2009 / Published online: 10 September 2009 © Springer Science+Business Media B.V. 2009
Abstract We study a new research problem, where an implicit information retrieval query is inferred from eye movements measured when the user is reading, and used to retrieve new documents. In the training phase, the user’s interest is known, and we learn a mapping from how the user looks at a term to the role of the term in the implicit query. Assuming the mapping is universal, that is, the same for all queries in a given domain, we can use it to construct queries even for new topics for which no learning data is available. We constructed a controlled experimental setting to show that when the system has no prior information as to what the user is searching, the eye movements help significantly in the search. This is the case in a proactive search, for instance, where the system monitors the reading behaviour of the user in a new topic. In contrast, during a search or reading session where the set of inspected documents is biased towards being relevant, a stronger strategy is to search for content-wise similar documents than to use the eye movements.
Keywords Eye movements · Implicit relevance feedback · Information retrieval · Machine learning · Support vector machines
The authors appear in alphabetical order.
A. Ajanki (B) · S. Kaski · K. Puolamäki
Department of Information and Computer Science, Helsinki Institute for Information Technology, Helsinki University of Technology (TKK), P.O. Box 5400, 02015 Espoo, Finland
e-mail: antti.ajanki@tkk.fi
D. R. Hardoon · J. Shawe-Taylor
Department of Computer Science, University College London, Gower Street, London, WC1E 6BT, UK
   1 3
308 A. Ajanki et al.
 1 Introduction
Current information retrieval (IR) systems rely mostly on explicit, typed queries, combined with explicit feedback telling the system which of the search results were relevant. The relevance feedback is used to refine the query, and the search converges iteratively towards more relevant documents. The standard web search engines are sim- plified versions of this scheme; they take advantage of the large scale which allows inferring general relevance of documents from link data.
A main problem of this traditional IR paradigm is that formulating good textual queries is a challenging task even for experienced users (Turpin and Scholer 2006). Moreover, query-based searches are only possible if the user knows her information need. The need may also be tacit; there may exist very useful documents that the user does not even try to search, or in a milder form the true interest or information need may be ambiguous to the users. In all these cases it is difficult or impossible to formulate a query explicitly. It would be ideal if the system could infer the interests of the users while they work, and then have some suggestions readily available when the users ask for help. We call this task proactive information retrieval.
A proactive information retrieval system would additionally solve the problem that giving explicit feedback is laborious. Such a system would use implicit feedback to infer relevance. Several forms of implicit feedback have been used (Kelly and Teevan 2003), of which at least click-stream data, time spent during reading, amount of scroll- ing, and exit behaviour have been found to help in predicting explicit feedback ratings (Claypool et al. 2001; Fox et al. 2005; Joachims et al. 2005). While these sources are readily available and useful, they offer limited information about a user’s interests and intentions, and the predictions are far from perfect. Hence it is important to continue searching for new sources of feedback that could be used to complement the existing ones.
Our suggestion is to use implicit feedback from the observed gaze pattern as an alternative or complementary source to infer the users’ intentions. Eye tracking has already been shown to be useful in user modelling: in inferring cognitive states, traits, and performance of the user for personalization purposes and for interaction adapta- tion (Conati and Mertena 2007, and review of earlier works therein). Moreover, eye movements have been shown to be useful in inferring relevance of documents to be used as relevance feedback (Puolamäki et al. 2005). Hence it is imaginable that eye tracking would be useful in inferring other, even more subtle cues about user interests.
We combine the eye movements with the textual content of the documents in a novel way: we use the eye movements to formulate an IR query, which is then used to rank unseen documents with respect to their relevance to the current interests of the user.
In this paper we study the feasibility of this approach. The practical motivation is that eye tracking equipment is becoming cheaper and smaller, and eye tracking data is soon expected to be cheaply available for most applications. If the data turns out to be useful it is then sensible to use eye tracking recordings to complement other sources. The more exciting motivation is that eye tracking data may provide more subtle cues about the users’ interests compared to lower level, time-based features, as has been found in studies of users’ meta-cognition (Conati and Mertena 2007).
1 3
Implicit queries from gaze patterns 309
 Since we expect the eye movement patterns to be very noisy, we study two approaches. The more challenging task is to (1) construct a query from eye move- ments alone. The easier one is to (2) construct a query by combining information from implicit relevance feedback from eye movements and explicit relevance feedback.
Implicit queries have earlier been constructed based solely on the texts the user is working on (Czerwinski et al. 1999; Dumais et al. 2004); in this work we combine this research tradition with more focused eye tracking-based inferences on which parts of the document are interesting. This is a feasibility study investigating whether eye tracking gives valuable information in this task. If it does, the methods can then be optimized and tailored in later studies.
The texts the user is working on could alternatively be interpreted as the context of the user and used to complement explicit queries (Budzik and Hammond 2000). In this work we assume explicit queries are not available, as in earlier works on implicit queries. This setting is commonplace when browsing interesting documents without a clear goal, and in the beginning of a more focused browsing session. If explicit queries are available, the implicit queries inferred from eye tracking could naturally be used to complement them according to existing principles.
To test the new approach, we devise a controlled experimental setting, in which test subjects read through short text snippets searching for documents related to a given topic. During the reading the users’ eye movements are recorded with an eye tracking device. We extract term-specific eye movement features for each document the user reads. The features are then used for predicting importance of the term for the search task. The setup is an extended version of our earlier work (Puolamäki et al. 2005).
To learn a model we need a set of training data where the ground truth is known, but we cannot assume that we will have training data that is representative of all possible (implicit) queries. Hence, the model should be such that it generalizes to new queries. For that purpose we assume that there is a link between relevance or interest and eye movements, and that this link is, to a reasonable extent at least, independent of the actual topic and query. Then a model of the link can be learned from training data about a subset of possible queries, and it will generalize to new ones. In this paper we formulate such a model and test empirically whether the assumption about a universal link between interestingness and eye movements is useful.
We study whether there is information about interests in the eye movements, and whether it can be extracted by models that make the above-mentioned assumptions. Furthermore, we study whether the usefulness of the inferred interests varies as a function of the amount of system’s prior knowledge of the target user topic; while for a new query there is no information available about the potential interesting- ness of new documents, for a topic already studied for some time there is additional information to be leveraged as well. Namely, the proportion of relevant documents among the set of read documents is higher than in the unread ones, due to the search process so far, and this bias can be utilized in making a content-based proactive search.
As another case study, we investigate whether combining eye movements with doc- ument features would help in the standard IR task where explicit relevance feedback is available for a subset of the documents.
1 3
310 A. Ajanki et al.
 2 Earlier work
There has been a lot of interest in implicit relevance feedback techniques in the information retrieval community because they may complement or replace explicit feedback, and thus decrease the need to burden the user. Several implicit feedback measures have been studied before. The literature review by Kelly and Teevan (2003) shows that implicit feedback can indeed improve IR accuracy but there is no consensus about which implicit measures are the most effective.
Claypool et al. (2001) examined how well time spent on a web page, and mouse and keyboard activity can substitute for explicit feedback. They found out that while mouse movements and number of clicks do not correlate strongly with the relevance, read- ing time and amount of scrolling are good indicators for relevance. Fox et al. (2005) carried out a similar comparison of implicit and explicit feedback. They considered not only the display time and scrolling activity but also other observable measures of user behaviour, such as if the document was printed or bookmarked and how the user exited the page. They observed that the two most important features were display time and the way the user left the page. Although several studies have found the display time to be a good indicator, it may be difficult to analyze in practice. In non-controlled settings the distribution of display times is usually skewed towards zero with numerous outliers (Rafter and Smyth 2001).
Click-through data is another well-studied implicit signal. Joachims et al. (2005) evaluated the quality of click-through measurements on a result page of a web search engine using eye movement measurements (but did not use the eye movements as implicit feedback). They concluded that clicks on the search result page can be used to infer relative relevance judgments between the search results.
Eye movements depend on the type of the visual task the user is performing, which suggests that it is possible to use them to infer the task automatically. For example, Howard and Crosby (1993) noted that the fixations tend to be located sequentially when reading relevant bibliographic citations but non-relevant material is examined non-linearly. King (2002) found out that the distributions of fixation locations differ between reading and counting arrows. She trained a neural network that could separate the two tasks quite efficiently using just the eye movements.
Use of eye movements in IR is a relatively new approach. Maglio et al. (2000) and Maglio and Campbell (2003) introduced a prototype attentive agent application which monitors eye movements while the user views web pages, in order to determine whether the user is reading or just browsing. If reading is detected, more information of the topic is sought and displayed. The feasibility of the application was not, however, experimentally verified.
Eye movements have been used in applications that could be broadly classified as eye-movement-based user interfaces, one of the most known examples being a fast pre- dictive eye typing system Dasher (Ward and MacKay 2002). More recently, Fono and Vertegaal (2005) introduced EyeWindows, an attentive windowing technique which uses eye tracking, rather than manual pointing, for selecting focus windows.
Eye movements were first used in an information retrieval task by Salojärvi et al. (2003, 2005a). Discriminative hidden Markov models were applied to estimate the relevance of lines of read text, and the performance of the method was verified in a
1 3
Implicit queries from gaze patterns 311
 controlled experiment. A competition was subsequently set up, where the participants competed in predicting relevance based on the eye movements (Puolamäki and Kaski 2006).
A prototype information retrieval system was introduced by Puolamäki et al. (2005). The system used relevance information combined with collaborative filtering to seed out relevant scientific articles, the task being to infer if the user found text snippets relevant or not. This earlier prototype did not use the textual content of the documents at all, and hence it could not be used to predict the relevance of unseen documents without some other source of information, such as collaborative filtering.
The methods used so far can be characterized as attempting to use eye movements to assess directly the relevance of displayed information. The methods vary from simple evaluation of attention as in EyeWindows to prediction of relevance from the type of eye movements involved, as in the example of discriminative hidden Markov models. In the current paper we use eye movements in a meta-learning task to infer a weighting over terms that can be used to predict the relevance of unseen documents for the user. This can be seen as an extension of earlier work, in the sense that during the application phase the eye movements are processed by the learned function rather than used directly as features for a retrieval algorithm. This approach opens up the possibility of tapping a rich source of potential information about the user through implicit inferences made without the need for direct querying or input.
The experimental setup used here was introduced first in a conference publication containing a brief feasibility study (Hardoon et al. 2007). That was the first study where documents have been ranked based on their textual content and eye movements of the user. Now we extend and complete the previous study, in particular by addressing the issue of bias in the proportion of relevant documents, which is important for a realistic information retrieval scenario where the user is likely to see varying proportions of relevant documents, and by studying the importance of the features for the task. We also provide a detailed analysis and discussion of the methods and related work.
3 Problem and approach
Our objective is to predict from term-specific eye movements a query vector that can be used to evaluate the relevance of yet unseen documents. Our approach is explained in Sect. 3.1. We also utilize the same overall framework in the case where some explicit relevance feedback is available, and we combine explicit feedback with eye movements. This latter task is discussed in Sect. 3.2.6.
3.1 Overall algorithm
We work with the bag-of-words (BOW) representation of the documents. The goal is to construct a query function gw(d), where d is a BOW representation of a new doc- ument, and g is a two-class classifier which predicts whether d is relevant or not. The parameter vector w represents the implicit query; our task is to provide the classifier with such a w that it will classify well according to the user’s interests. The function g is parametrized such that there is a specific parameter wt for each term t.
1 3
312 A. Ajanki et al.
 We assume that there is a link between the eye movements and the importance of a word for the query. More specifically, we assume a parametric functional form wt = fλ(et , st ) for the relationship between eye movement features collected dur- ing reading, denoted collectively by et for term t, query-independent parameters st associated with the term t (e.g. inverse document frequency of term t) and the query parameters wt . The eye movement features describe the way in which the term is viewed (for example, fixation duration on the term and saccade lengths before and after viewing the term). The features are described in more detail in Sect. 3.2.4. The
fλ could in principle be any predictor, with its parameters specified by λ.
The parameters λ of the predictor fλ are learned on a set of training tasks, where the true interest of the user is known. Following our assumptions laid out above, the func- tional form of the predictor fλ is topic-independent. Hence, the training step needs to be done only once. After that the predictor can be applied to previously unseen queries
to produce the query parameters w.
In this work we choose the query function gw(d) to be a Support Vector Machine
(SVM) with the parameter vector w formed of the term-specific parameters wt . As a predictor fλ, which gives the parameters of the SVM, we use standard linear and non-linear regressors (details later). We purposely use standard state-of-the-art machine learning methodologies in this proof-of-concept work to make the approach easily expandable.
3.2 Training
Our main task is to formulate an IR query, using the eye movements as the only feed- back signal. The query need not, however, be understandable by humans; in fact, it suffices to formulate the query in such a way that it can be used by the query function gw(d) to predict relevance for new documents d.
The training data consists of a set of documents and the eye movements of per- sons who read them while they were searching for documents of certain known top- ics. Our aim is to try to infer a query from the eye movement recordings and BOW vectors of the read documents, such that the relevance of new, unseen documents to the topic can be predicted. More detailed description of the data is provided in Sect. 4.1.
The training consists of two phases: first, we learn the predictor parameters λ by optimizing fλ to produce query vectors that are good at separating the known topics in the training set. Next, we apply the learned predictor to a previously unseen topic to infer a query vector w. Finally, we evaluate the performance by ranking unseen test documents according to the query function gw.
The predictor learning phase requires that ground-truth query vectors are known
for each topic. These ground truth vectors w should be such that the query function gw
is able to optimally discriminate between the topics. We approximate the ground-truth
by the weight vectors of SVMs that are trained to discriminate the documents of one
topic from the others. All SVMs mentioned use the default setting of C = 1 and a
linear kernel. We call these vectors ideal weights and denote the ideal vector for topic c
by wc . Section 3.2.2 gives more details about the computation of the ideal weights. ideal
1 3
Implicit queries from gaze patterns 313 The parameter λ is optimized by minimizing the squared error
 àfλ et(j),st(j) −wc(j) à2, ideal,t ( j )
j
where the index j goes over all viewed terms in all training documents, t(j) is the identity of the jth viewed term, and c(j) is the topic of the document where the jth term appeared. After λ has been learned, it will be fixed and used in the subsequent steps. We will have either a standard linear least squares regressor or a non-linear sparse-KPLS as the regressor fλ. They are discussed in more detail in Sect. 3.2.3.
The training set has been constructed to consist of several topics, so that the learned predictor needs to generalize over them and hence become topic-independent. We will test whether this is the case by evaluating the performance on a topic that was not a part of the training set.
The second step in inferring the query for a new topic is constructing the vector w by letting wt = fλ(et , st ) for all terms t that were viewed during the test query. If a term t appears in multiple documents or multiple times in one document, the corresponding feature vector (et , st ) will be set to the average over all the occurrences. Zero is assigned to non-viewed terms. After forming the w we use the query function gw to classify test documents.
3.2.1 Cross validation methodology
We have been specifically careful in designing the experiments such that testing data never affects the learning, and that is why the following procedure may appear slightly complicated. We evaluate the performance using a cross-validation approach where we leave out one topic at a time for testing and use the rest for training. We recorded eye movement data from several test subjects while they were reading text snippets, looking for documents about a given topic. We had 25 such search topics in total. In addition to the documents that were shown during the experiments we have a separate test set of documents without eye movement recordings for evaluation purposes.
We want to learn a mapping, from the eye movements to the weights, that is inde- pendent of the topic. The eye movement features from the testing topic need to be excluded from the training. To this end, the features are partitioned into two sets. The first set, T1, includes the features from the training topics (other topics besides the left-out topic). More specifically, T1 is the collection of (et , st , c) triplets for all words viewed during the training topics, where et and st are the eye movement and query- independent features of term t, and c is the search topic that the user was looking for when this sample was generated. The second set, T2 , includes feature pairs (et , st ) for the viewed words in the left-out topic.
T1 is used for training the parameter vector λ, and T2 for inferring the query vector for the left-out topic. Finally, the learned query is evaluated on an independent test set. Because the set T1 does not contain documents from the left-out topic, the predictor cannot specialize on the left-out topic. Instead, if it is able to learn to perform well on the left-out topic, it must be independent of the query and in that sense universal. The approach is summarized in the pseudocode in Algorithm 1.
 1 3
314 A. Ajanki et al.
  Algorithm 1 Pseudocode of the training and testing procedure
// 1. Construct the ideal weights for each topic c
Train an SVM to discriminate BOW vectors between topic c and other topics Let wicdeal be the parameter vector of the SVM
for each topic c′ // c′ is the left-out topic // 2. Training
 // Learn the regressor parameters λ
T1 ← a set {(et,st,c)} of all viewed words t in the training set, where topic c ̸= c′
λ←argmin   |f (e ,s )−wc(j) |2 λ j∈T1 λ t(j) t(j) ideal,t(j)
// Compute the query vector w for the left-out topic c′
T2 ← a set {(et , st )} of all viewed words t in the left-out topic c′ (average if t occurs several times) wt ← fλ(et,st) for all samples in T2
// 3. Testing
Rank the unseen test documents d from a separate test set according to gw(d) Compute MAP (c′ is the positive topic and all others are negative)
3.2.2 Ideal weights
We use as ground-truth the weight vector of an SVM which has learned to predict, based on full knowledge of the topic and content of learning documents, whether or not a document belongs to the given topic. We call these ground-truth values ideal weights and use them as targets when training a regressor fλ for the same topic. In principle, one could select a different classifier to construct the ideal weights but for ease of integration with the remainder of our system we opt for SVM. The input for this ideal weight SVM is the BOW representation of the document, and the label is +1 if the document is categorized to the current topic, or −1 if it is not. These labels are known for the training documents. Because the ideal weights are computed using one SVM per search topic, we get one weight value for each term in the dictionary for each search topic; the weight represents the term’s “fit” to the given search topic.
We will use the ideal weights to train a regressor described later in Sect. 3.2.3. The regressor is then used to construct a new classifier for predicting the relevance of the unseen documents.
3.2.3 Regression
We use two types of regressors for predicting the terms’ weights wt from the mea- sured eye movement and query-independent textual features, et and st . The simplest mapping we employ is a standard linear regressor
fλ(et,st)= λieeti + λissti, ii
 1 3
Implicit queries from gaze patterns 315
 where the parameter vector λ is divided into two parts, λe and λs . The first one contains the regression coefficients related to the eye movement features e and the latter the coefficients related to the query-independent features s.
In addition to least squares regression, we use a non-linear sparse dual Partial Least Squares (PLS) approach (Dhanjal et al. 2006). This method uses a general framework for feature extraction based on a kernel PLS (Rosipal and Trejo 2001) deflation method. KPLS maps the feature vectors nonlinearly to a feature space, and does ordinary least squares estimation in the new space. The sparse dual PLS selects, in each iteration, the projection for the least squares regression to be a subset of samples that have maximal covariance with the label. In other words, in each iteration of the algorithm a subset of the kernel is computed and the sample (with the feature combination) that gives maximal covariance is selected as the projection.
In the PLS framework we still adhere to our prior assumption of learning a mapping from eye movements to ideal weights, whereas the eye movements are now kernelized. Due to the large number of samples (eye movements) we are unable to compute the full kernel matrix and therefore only compute a small random portion of the kernel at each iteration of the sparse-KPLS algorithm. For a detailed account of sparse-KPLS we refer the reader to (Dhanjal et al. 2006).
We use a Gaussian kernel with the sparse-KPLS where the width parameter σ for the kernels is optimized, per search topic, using tenfold cross validation on the training data.
We normalize the query vector w in the 2-norm.
3.2.4 Features
The gaze direction is an (indirect) indicator of the focus of attention, since accurate viewing is possible only in the central fovea area (1–2 degrees of visual angle). The correspondence is not one-to-one, however, since the attention can be shifted without moving the eyes. The eye movement trajectory is traditionally divided into fixations, during which the eye is fairly motionless, and saccades, rapid eye movements from one fixation to another.
The fixations during reading have previously been observed to last 200–250 ms on average and rarely less than 100 ms (Levy-Schoen and O’Regan 1979). Furthermore, Granaas et al. (1984) have showed that moving text two letters positions at a time at 88 ms intervals hinders reading comprehension substantially, which indicates that sub 100 ms durations are too short for effective reading.
We identified fixation locations from the measured eye movement trajectories by windowing; if the successive points stayed inside 30 pixel square (about 0.6 visual angle for a person sitting at 60cm distance from the screen) for more than 100ms, they were considered to form one fixation. Every fixation was mapped to the closest word, unless the fixation occurred well outside any text (at least 1.5 times the text height), in which case it was discarded. In this paper we only report the results for the fixation time cutoff of 100 ms. However, we ran the same analyzes with the fixation time cutoff of 40 ms, which is the value recommended by the eye tracker manual. The results with the 40 ms cutoff were very similar to the 100 ms cutoff reported in this work.
1 3
316 A. Ajanki et al.
 We extracted 22 eye movement features (denoted by et ) from the recorded eye movement data for each term, and 4 text features (denoted by st ) for the target words of the fixations. The features are listed in Table 1. The eye movement features we are
Table 1 List of features
Eye movement features
1 Integer
2 Integer
3 Binary
4 Binary
5 Continuous
6 Continuous
7 Continuous
8 Continuous
9 Integer
10 Integer
11 Integer
12 Integer
13 Integer
14 Continuous
15 Continuous
16 Integer
17 Continuous
18 Binary
19 Continuous
20 Continuous
21 Continuous
22 Integer
Textual features
23 Integer
24 Continuous
25 Continuous
26 Continuous
Number of fixations to the word
Number of fixations to the word when the word is first encountered
Did a fixation occur when the line that the word was in was encountered for the first time?
Did a fixation occur when the line that the word was in was encountered for the second time?
Duration of the previous fixation when the word was first encountered
The duration of the first fixation when the word was first encountered
Sum of durations of fixations to a word when it is first encountered
Duration of the next fixation when the gaze initially moves on from the word
Distance (in pixels) between the first fixation on the word and previous fixation
Distance (in pixels) between the last fixation on the word and the next fixation
Distance (in pixels) between the fixation preceding the first fixation on a word and the beginning of the word
Distance (in pixels) of the first fixation on the word from the beginning of the word
Distance (in pixels) between the last fixation before leaving the word and the beginning of the word
Sum of all durations of fixations to the word Mean durations of fixations to the word Number of regressions leaving from the word
Sum of durations of fixations during regression initiating from the word
Did a regression initiate from the following word?
Sum of the durations of the fixations on the word during a regression
Mean pupil diameter during fixation
First fixation duration divided by total duration of fixations on the display
Number of words skipped since previous fixation
Length of the word
Position of the word in the document divided by total number of words in the document
Position of the word in the line divided by the line length Logarithm of inverse document frequency of the word
  1 3
Implicit queries from gaze patterns 317
 using have been previously described in a technical report (Salojärvi et al. 2005b). These are typical features in psychological studies (Rayner 1998).
We also used four query-independent features which do not depend on the way the user viewed the document but only on its textual content. The length of the word and the inverse document frequency are related to the level of mental processing required to comprehend the word. The relative position in the document may be related to the relevance of the word, if, for example, the user usually reads only the beginning of the document but sometimes also more if he finds the topic is interesting.
3.2.5 Explicit feedback SVM model
If explicit relevance feedback, that is, the relevance of the training documents, was available and we knew that the implicit query remained the same in the test documents, we would not need to infer the query vector from the eye movements. Instead, it could be computed more directly from the BOW vectors of the training documents. We could simply train an SVM to classify between the training topic and other topics.
Compared to the approach of the previous section this is much simpler. We can skip computing the ideal weights and learning the regressor fλ and, instead, learn the query function gw(d) more directly. We let the query function to be an SVM that discriminates between the relevant and non-relevant documents in the current topic. The parameter vector w is learned during the optimization of the SVM.
The inputs to the SVM are document BOW vectors, similarly to the SVMs that were used in computing the ideal weights (see Sect. 3.2.2). The differences are that the training labels are now the true relevance labels instead of the category labels, and that the training set consists of only the documents shown during the searches where the user’s true interest was the current topic.
The learnt weight vectors are used to classify the test documents. The resulting classifier is referred to as SVMex. It represents the “best imaginable” performance, which implicit feedback cannot realistically be expected to outperform.
3.2.6 Combining eye movements and explicit feedback
Next we consider a situation where we observe both the true relevance labels and eye movements. We want to find out whether using both of these information sources in conjunction to predict the query could improve the classification accu- racy, in comparison with the model of the previous subsection that uses only explicit feedback.
Because we again have the explicit relevancy for the documents we can skip the learning of ideal weights and the regressor fλ, and learn the query vector w directly. The query function gw(d) predicts the relevancy of a document given both the tex- tual content and the eye movements on the document. We choose g to be a two-view classifier called SVM-2K.
We regard the textual content as one representation and the measured eye move- ment feature vectors as a second representation of the document. We project the two
1 3
318 A. Ajanki et al.
 views through distinct feature projection, thus creating two kernels, one for each representation.
The Kernel Canonical Correlation Analysis (KCCA, Hardoon et al. 2004) algorithm looks for directions in the two feature spaces such that when the training data is pro- jected onto those directions the two vectors (one for each view) of values obtained are maximally correlated.
A straightforward way to do classification using two data sets would be to first project the data into the KCCA space and then train an SVM classifier. Though this sequential approach seems effective (Meng et al. 2005), there appears to be no guaran- tee that the directions identified by KCCA will be best suited to the classification task. Farquhar et al. (2006) have shown that the two distinct stages of KCCA and SVM can be combined into a single optimization that was termed as SVM-2K. The training of an SVM-2K model is explained in Appendix A.
3.3 Testing
After the query vector has been learned in the training phase we can predict the rele- vance of unseen test documents. We rank the test documents according to the values of the discriminant function gw(d). The discriminant functions for the implicit, explicit, and combined feedback tasks are described in the following subsections.
If the classifier is accurate the test documents belonging to the current left-out cate- gory should be near the top of the resulting list. The quality of the ranking is measured by average precision (see Sect. 4.3).
3.3.1 Implicit feedback regression models
The compatibility of a test document BOW vector d to the inferred query w is computed by an SVM discriminant function. For linear kernel the function is
gw(d) = wT d + b,
where b is the bias that is learned together with the weights w. The vectors w and d are normalized in 2-norm to keep the length of the document from affecting the similarity measure.
Instead of thresholding the values of the discriminant function, as would be done in a pure classification task, we rank the test documents according to values of the function. As we are only interested in the ranking, the bias term can be left out from g. We measure the goodness of the ranking by average precision (see Sect. 4.3).
3.3.2 Explicit feedback SVM model
The discriminant function is of the same form as in the case of implicit feedback. Of course, the weight vector w is now computed using the explicit feedback.
1 3
Implicit queries from gaze patterns 319
 3.3.3 SVM-2K combining implicit and explicit feedback
The discriminant values of the SVM-2K model for a test document are given by the (1) in Appendix A. Because no eye movement measurements are available for the test documents, the eye movement feature projection φA in (1) is set to zero when computing the decision values for the test documents.
4 Experiments
4.1 Data collection
4.1.1 Document corpus
The document set consists of 750 documents from the Wikipedia. Of them, 500 were used to form the training set and the rest were the test set. The documents have been partitioned into 25 categories by the editors of the Wikipedia. There were 12–23 doc- uments from each topic in the training set and ten documents per topic in the test set. The categories and their sizes are listed in Table 2.
The documents were represented as Term Frequency-Inverse Document Frequency (TFIDF, Salton and McGill 1983) vectors. If the document corpus is denoted by D and the dictionary by T, then the TFIDF weight for a term t ∈ T in a document i ∈ D is the product of term frequency in the document and logarithm of the inverse document frequency of the term in the corpus:
TFIDF(i,t) = nit log à |D| à, à j∈D|njt>0à
where nit denotes the number of occurrences of term t in document i. The dictionary T consisted of all stemmed words in the documents except for numbers and some frequent ‘stop’ words like ‘of’ and ‘the’, which were omitted. Also, words that only appeared in single interest categories were removed from the data in order to create a more realistic scenario, i.e. overlapping words through the different categories. The dictionary included 5306 terms in total.
We truncated the documents so that each had at most 11 lines of text, in order to fit them to the screen. We wanted to keep the test setup as simple as possible, and to avoid the need to scroll the text. To avoid any artifacts that might attract unwanted eye movements, the titles of the documents were removed and no sen- tence was cut in the middle. We made sure that the content of each truncated docu- ment was sufficient for inferring its topic by manually inspecting all documents. We assume that, because the categories are quite diverse and it is fairly easy to categorize the truncated documents into the different categories, the participants would not have needed scrolling in most cases even if it had been possible. This is further supported by the fact that only a minority of the documents had any fixations on the final lines (see Fig. 1).
 1 3
320
A. Ajanki et al.
  Table 2 Summary of the training and test corpora
Topic
Astronomy Ball games Cities
Court systems Dinosaurs Education Elections Family
Film
Government
Internet
Languages
Literature
Music
Natural disasters Olympics
Optical devices
Postal system
Printing
Sculpture
Space exploration Speeches
Television Transportation
Writing systems
Total 500
Number of documents in the training set
Number of documents in the test set
10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10
250
 4.1.2 Participants
There were ten participants in the experiments. They were voluntary post-graduate and senior researchers from Department of Information and Computer Science, Helsinki University of Technology.
4.1.3 Experimental procedure
We measured users’ eye movements when they were reading documents in order to classify the documents into interesting and uninteresting. We artificially con- strained users’ interest by giving them a topic and asked them to identify docu- ments which were related to that topic. Topics were the Wikipedia categories listed in Table 2.
1 3
22 21 12 23 17 22 21 16 20 20 23 21 21 16 19 19 20 22 22 20 19 23 22 22 17

Implicit queries from gaze patterns
321
    Fig. 1 Proportion of documents having fixations on a given line
100
80
60
40
20
0
1 2 3 4 5 6 7 8 9 10 11
Line number
                                       A sample document (on the left) and a screen which the user gets when he has finished reading (on the right)
The user read ten short documents trying to recognize those that were related to the given topic. He was instructed to read the document until he could say whether or not the document was related to the topic and, after making up his mind, to press any key in the keyboard. The key press then replaced the document by a form where the user reported his impression of the relevance by pressing one of two possible keys. The next document was shown immediately after the user had reported his opinion. A sample document and a feedback screen are shown in Fig. 2.
We call a combination of one topic and the 10 documents the user read while searching for that particular topic a session. Each user completed ten sessions, with a different search topic and documents in each. A short break was allowed after the third and the sixth session.
On average half of the training documents in a session were relevant and the rest were randomly drawn from unrelated topics. In total 1,000 documents (= 10 users × 10sessions/user × 10 documents/session) were displayed during the tests. This means that each of the 500 training documents was shown to two users on average.
The answers given by the users agreed in 97% of the cases with the true labels.
Fig. 2
1 3
Percentage of documents having fixations on a given line
322 A. Ajanki et al.
 4.1.4 Eye tracker
The gaze locations on the screen were recorded with a Tobii 1750 eye tracker while the test subjects were reading the documents. The Tobii system consists of an infra-red LED and two cameras mounted on the frame of a computer screen. Tobii measures gaze direction 50 times per second by illuminating both eyes with infrared and measur- ing the light reflected from the cornea. The system is fairly robust to head movements. The user was sitting at a 60 cm distance from a 17 inch computer screen. The text was displayed with quite a large font and spacing so that it would be possible to map the gaze location to the correct word reliably. There was room for at most 11 lines of text on the screen. The eye tracker was calibrated in the beginning of the experiment for each user and after every break.
4.2 Experimental setup
4.2.1 Implicit feedback models
As was discussed in Sect. 3.2.3, we tried both the linear least squares and the non-linear KPLS regressors for predicting the term-specific weights from the eye movement features. The model using linear regressor is referred to as Wlin, and the model using KPLS is called Weye+text (26), where 26 is the dimension of KPLS feature space. Both of these models use the eye movement features et and the query inde- pendent textual features st . To determine how much the extra information from the eye movements boosts the performance we trained a second model that is similar to Weye+text(26), but does not use eye movements at all. To be exact, the differences are that the second model uses only the query independent features, and that the features are computed for each of the words appearing in the document, not just for the viewed words. We refer to this second model as Wtext(4), where 4 is again the number of projection directions in KPLS. The number of projection dimensions are chosen to coincide with the number of features in the corresponding regressor. We have tested with other numbers of directions, too, and the performance does not change much when the number of directions is increased (data not shown here; see Hardoon et al. 2007).
The above regression scheme is performed by pooling the eye movements from all users who completed the same query. The size of training set used to adapt the model to a particular search topic (set T2 in Algorithm 1) is 30–50 documents depending on the topic (3–5 test subjects performed the same query and each of them saw ten doc- uments). About half of them were relevant. If the users employ very different reading strategies it might make more sense to handle them separately. Therefore we also test a model that is otherwise identical to Weye+text(26) but instead of pooling the data each session is handled as a separate, smaller training set consisting of only ten documents. This model is referred to as Wus(26) (for “user specific”) in Table 3.
Figure 3 shows a user’s eye movement trajectory on a document, and the term weights on the same document inferred by the Weye+text (26) regressor from eye move- ments of all test subjects who were searching for documents about Dinosaurs.
1 3
Implicit queries from gaze patterns
323
 Table 3
Random (no feedback)
6.0
The mean average precisions for biased data
 TFIDF Wlin (impl. (no feedback) feedback)
73.4 37.8
Wtext (4) (impl. feedback)
19.9
Weye+text (26) (impl. feedback)
46.3
Wus (26) (impl. feedback)
31.3
SVMex SVM-2K (impl. (impl. feedback) feedback)
75.1 77.0
    Fig. 3 On the left, eye movement trajectory. The document is about Dinosaurs, which is also the topic the reader was interested in. The circles mark the fixations, and the radius of a circle is proportional to the duration of the fixation. On the right, weights inferred from eye movements. The thickness of the underline denotes the magnitude of the weight. Gray words do not appear in the dictionary
4.2.2 Explicit feedback models
For reference we also tested how much the performance can be improved if explicit relevance feedback is available. The explicit feedback can be used alone or combined with the information from the eye movement measurements. If the eye movements are excluded, only the TFIDF vectors of the documents are available. In this case we used a standard SVM classifier that was trained to discriminate between relevant and non-relevant samples (Sect. 3.2.5). The resulting model is called SVMex. To combine explicit feedback and eye movements we used the SVM-2K model of Sect. 3.2.6. This model is referred to as SVM-2K below.
4.2.3 Baseline models
We compare the results to two baseline models that do not utilize any relevance feed- back at all. The simpler one just ranks the test documents randomly.
The second baseline model ranks the test documents by the average similarity of their textual content with the training documents. The training and testing documents are presented as 2-norm normalized TFIDF vectors. We compute mean cosine distance (which equals to dot product for normalized vectors) of the test documents to the train- ing documents. More formally, for each training set s with documents y j , j ∈ T r (s ), and for each test document xi we compute
g ( x i ) = 1   x iT y j , |Tr(s)| j∈Tr(s) ||xi||||yj||
  1 3
324 A. Ajanki et al.
 where Tr(s) is the set of all (both positive and negative) training documents in the training set s and the document lengths are taken into account by normalizing the vectors. The test documents are then sorted according to the cosine distance, in the order of similarity with the training documents.
4.2.4 About bias in the learning set
In a typical information retrieval setup, the proportion of relevant documents the user receives during a search session will be larger than on average in the corpus. This happens because an information retrieval system naturally aims for a good precision, or proportion of relevant documents of all results. This creates a bias: the proportion of relevant documents is larger in the learning set (seen documents) than in the test set (unseen documents). If the precision of the search results is good enough, then as a result of the bias even the learning document set as such is a reasonably good query—without any explicit or implicit relevance feedback! This has a direct effect on the performance of any information retrieval algorithm. Hence we will inspect the performance of our methods as a function of the bias.
In our experiments, half of the documents in a session were positive on average and others were randomly sampled from the negative topics. Because the positive docu- ments form a large cluster while negative documents are likely to be more scattered, even a simple TFIDF baseline model can perform well in this setup as the mean vector is drawn near to the large positive cluster. On the other hand, in a more realistic setup we can’t assume such a large fraction of the seen documents would be relevant, and therefore models using just the textual content of the documents are likely to perform much worse. Implicit or explicit feedback can help the search engine to identify the relevant documents even when the bias is low.
To test our algorithm under smaller bias we created new training sets by re-balancing the data in the original sessions. We constructed new artificial sessions by dividing the documents (and their corresponding eye movement measurements) anew. The new sessions have only one positive document, and therefore the positive topic will not stand out from the rest. We also limited the proportions of the topics in the test set to be the same as those in the training set. This way there is no extra information available in the setup that could bias the results, and the random ranker is a fair baseline.
Each session was divided into n new sets by taking one of the n positive and all negative documents in the original session. If two or more negative documents hap- pened to belong to the same topic, all possible one-document-per-topic combinations were used as the new training sets constructed from that session. Unlike in the original setup, where we always used all test documents for testing, we now construct a sep- arate test set for each training set by taking only the test documents from the same topics that appear in the training set. Thus, the proportion of the topics in the training and testing is the same but the size of the test set depends slightly on the training set. We call these resampled sets unbiased training sets.
There were some documents where the eye tracker had failed to capture any eye movements at all. A likely reason is that the user was probably sitting too far or too close to the monitor while reading these documents. When constructing the unbiased
1 3
Implicit queries from gaze patterns 325
 sessions we left out all sessions which would have included at least one of these doc- uments without any eye movements. In total, we get 694 unbiased sessions which had 88 distinct pairs of user and topic.
Training of the classifiers for the unbiased case is done identically to the training in the biased case in Sect. 3.2. The learning algorithm again iterates over all sessions and learns the regression parameters from data collected from all other topics except the left-out topic. The training set (denoted by T1 in Sect. 3.2.1) again consists of features collected from all the sessions except for the left-out topic. Because T1 is the same for all new sessions which were constructed from a single original session, also the regression parameters will be the same for those sessions. The training set T2 for learning the term weights wt contains (et , st ) feature pairs from the documents in the left-out session. The sessions with the same topic are not pooled, as was done in the biased case, because doing so would alter the amount of bias. The size of T2 is 4–8 documents depending on the session. The model is learned and the predictions are made the same way as before.
In the testing phase the performance of the learned models is tested on the cor- responding test sets. To get a single performance index for each user/topic pair, we computed averages of the performance figures over sessions which had the same com- bination of users and topics.
We compare the performance of the algorithms on the biased and the unbiased data sets.
4.3 Results
4.3.1 Performance measure
To compare the goodness of the rankings of the unseen test documents produced by the different methods we computed mean average precisions (MAP).
Average precision measures how well the positive documents are positioned in a given ranking. It is calculated as a mean of average value of precision at the positive ranks:
AVGPREC=1 R i, R i=1 ri
where R is the number of positive examples in the test set and the ri are the rankings of the positive examples, ordered such that ri < ri+1. The mean average precision is the mean of average precisions of different information needs or, our case, sessions.
We inferred a query w with each method for each session and ranked the test doc- uments according to their predicted similarity to the inferred query by computing the discriminant function values gw(d) for each test document d. The discriminant functions were described in Sect. 3.3. The quality of the ranking was established by computing the average precision. The final MAP value is the mean of the average precisions of all sessions. We computed MAP for all methods both in the biased and unbiased setting. The full results for both cases are in Appendix B.
  1 3
326
A. Ajanki et al.
 Table 4
Random (no feed- back)
25.7
The mean average precisions for unbiased data
 TFIDF (no feedback)
25.9
Wlin (impl. feedback)
28.1
Wtext(4)
(impl.
feedback) feedback) feedback)
24.8 27.4 80.0
4.3.2 About statistical tests
The performances of the models reported in Tables 3 and 4 are significantly different when tested with non-parametric ANOVA ( p ≪ 0.0001 (biased case) and p = 0.018 (unbiased case) for differences between all baseline and implicit feedback models, Friedman’s Test). This indicates that there is always at least one method for which the performance differs significantly from the others. However, since we are not inter- ested in comparing all methods, but only certain pre-defined pairs, it is sufficient to use pairwise tests.
In the biased case it is clear that TFIDF performs well and random performs poorly. Instead of comparing to those, we are interested to see if the eye movement features bring in additional information compared to using just the text features. If yes, we expect that a model which takes eye movement information into account starts to perform better than text-only models when the bias is reduced. Therefore, we test the significance of Weye+text (26) versus Wtext (4). We additionally investigate the need for user-specific models versus pooling all data together, by comparing Wus(26) versus Weye+text (26).
In the unbiased case, in addition to testing the difference between eye movement and text-only models, we compare the performance of the eye movement models to the random ranking, which is a fair baseline after the bias is removed.
We use Wilcoxon Signed Rank Test to compare the classification performances of the pairs of algorithms (Weye+text(26) versus Wtext(4); and Weye+text(26) versus Wus(26)). We use the non-parametric Wilcoxon Signed Rank Test instead of paired t -test because the average precision values are not normally distributed. Wilcoxon test is known to be less stringent than the t-test. In comparisons with the random model, however, the Wilcoxon Signed Rank Test is not applicable, because the random model does not have a single ranking of the test set documents, but a distribution of rankings. In comparison with the random model (Wlin or Weye+text(26) vs. random) in the unbi- ased case in Sect. 4.3.5 we used a standard permutation test for which we sampled 100,000 random rankings.
4.3.3 Baseline performance
The performance of the random baseline model in each session was tested by sampling 100,000 random rankings of the test documents and computing average precisions for each. To summarize the performance of the random model in Table 3, we first com- puted session-wise mean average precisions over the random rankings and finally took average of them. The MAP is 6.0% in the biased case and 25.7% in the unbiased case.
1 3
Weye+text (26) (impl.
SVMex
SVM-2K (impl. feedback)
80.0
(impl.

Implicit queries from gaze patterns
327
    90
80
70
60
50
40
30
20
10
0
Wtext(4) Weye+text(26)
                                                                                                          Average precisions of Wtext(4) and Weye+text(26) in different topics in the biased case. The topics are sorted according to the performance of Weye+text model
The baseline TFIDF model can often identify the correct topic in the biased setting because the positive documents form the largest cluster among the training documents. This results in achieving a MAP of 73.4%, which is the highest of all tested models in the biased case. In the unbiased case in Table 4, the positive document does not stand out from the rest of the training set, and therefore the performance of the TFIDF model (25.9%) is close to random.
4.3.4 Regression models on biased data
In the biased case (Table 3) the difference in average precision between the non-linear regression model Weye+text(26), which combines eye movement and textual features, and the textual feature model Wtext(4) is statistically significant (46.3 vs. 19.9%, p = 0.00005, Wilcoxon Signed Rank Test). This implies that the eye movement fea- tures contain information that helps in the complex task of inferring the hidden query. Figure 4 shows the difference in performance of the two models. The availability of eye movement information improves the performance clearly on several topics, but in two cases the text-only model outperforms the eye movement model.
Weye+text(26), which was trained pooling together the data from all users who completed the same query, is significantly better than Wus(26), which used the user specific average precisions (46.3 vs. 31.3%, p = 0.0001, Wilcoxon Signed Rank Test).
Fig. 4
1 3
Average precision
Languages Ball games
Dinosaurs Postal system
Olympics Printing
Elections Space exploration
Natural disasters Cities
Education Internet
Government Speeches
Optical devices Court systems
Film Television
Sculpture Transportation
Writing systems Family
Astronomy Literature
Music
328 A. Ajanki et al.
 This result suggests that the method benefits from the larger training sets created by pooling the data form different users.
We next tested how much the results depend on the specific topics, that is, how universal the results are over the choice of topics. For this, we resampled with replace- ment topics among the original 25 topics. To study the effect of the number of topics, we repeated the analysis for different set sizes between 2 and 25 topics. For each topic set size, we draw 10,000 random topic sets, and computed the difference in average precision between Wtext(4) and Weye+text(26) in each replicate. We computed BCa corrected estimates of the 95% confidence intervals (DiCiccio and Efron 1996) for the difference between the two methods based on the obtained bootstrap distribution. To keep the computational workload down we employed the regression parameters we had already learned on the full corpus instead of retraining them on the sampled, smaller topic sets. Note that all training and testing has still been done on completely sepa- rate sets. The model using eye movement features was better (the confidence interval included only positive values, i.e. values for which the average precision of the eye movement model was better than that of the text-only model) on all sets larger than three topics. In other words, the model performs reliably regardless of which topics are chosen.
4.3.5 Regression models on unbiased data
The results for the unbiased data are shown in Table 4. The linear regression model Wlin attains the highest MAP of all implicit feedback models with the non-linear Weye+text(26) following closely. Note that the results in Table 4 are not directly com- parable to the results on biased data in Table 3, because in the unbiased case the training sets are much smaller (4–8 documents in the unbiased case vs. 30–50 documents in the biased case) and the measurements from different users are not pooled as is discussed in Section 4.2.4.
Combination of eye movement features and textual features gives better perfor- mance than the textual features alone (Weye+text(26) vs. Wtext(4), p = 0.028). To test our assumption that the implicit feedback can improve over a random ranking when the artificial bias of our training set is removed, we compared the results of Weye+text(26) to random. As discussed earlier in Sect. 4.3.2 the observed Weye+text(26) MAP val- ues are compared to the distribution of the MAP value of the random model using a permutation test. We randomly permuted the relevance labels of the test documents 100,000 times in each session, computed the mean average precision, and counted the proportion of the permutations that had at least as high MAP as the eye movement models. Only 2.7% of the permuted samples had at least as high MAP as Weye+text(26) (27.4% vs. 25.7%, p = 0.027, Permutation Test), and 0.26% were at least as good as Wlin (28.1 vs. 25.7%, p = 0.0026, Permutation Test).
We studied the effect of the choice of topics on the performance using a similar bootstrap approach as in Sect. 4.3.4. We sampled the topics with replacement and con- structed sets consisting of 2 to 25 topics. We generated 10,000 samples for each set size and estimated the 95% confidence intervals for the difference in average precision between Wtext(4) and Weye+text(26). The model that uses both text and eye move- ment features performed significantly better than the text-only model (assessed by the
1 3
Implicit queries from gaze patterns
329
    70
60
50
40
30
20
10
0
random Weye+text(26)
                                                                   ...
                           Fig. 5 Average precisions of ten best performing (left-hand side) and ten worst performing sessions (right-hand side) in the unbiased case. The session are sorted according to the p-value of the Permuta- tion Test between random ranking and Weye+text(26)
confidence interval including only positive values, i.e. values for which the eye movement model was better) with 21 or more topics. The variance of the bootstrap estimate gets larger as the number of topics and thus also the number of average precision samples decreases. At least 21 topics are required for getting statistically significant results with the current amount of training data; it is likely that with more test subjects, or more read documents per topic, fewer topics would be needed for a statistically significant difference.
To get further insight into the performance of the model we took a look at the individual sessions. The average precisions of the best and worst performing sessions (sorted according to the p-value of the Permutation Test between random ranking and Weye+text(26)) are shown in Fig. 5. In 10 out of 88 sessions the average pre- cision of Weye+text(26) is significantly better than the random model (p ≤ 0.05, Permutation Test). The average improvement in MAP over random in these que- ries was 24.5, which is considerably higher than the overall change in MAP for all sessions 1.7 (= 27.4−25.7). Some topics appear very frequently among the top-10 sessions; Olympics occur three times, and Court systems, Languages, and Printing occur two times each. All of these topics were queried in three sessions in total, except for Languages which was the positive topic in four sessions. The fact that they appear so frequently among the highest performing sessions indicates that there
1 3
Average precision
Printing (user 10) Olympics (user 4)
Court systems (user 1) Olympics (user 9)
Languages (user 10) Languages (user 5)
Ball games (user 9) Printing (user 6)
Olympics (user 3) Court systems (user 4)
Literature (user 10) Internet (user 9)
Optical devices (user 5) Astronomy (user 5)
Writing systems (user 2) Astronomy (user 8)
Internet (user 2) Family (user 2)
Internet (user 3) Family (user 6)
330 A. Ajanki et al.
 exists queries for which the implicit feedback works well. On the other hand, because the overall MAP of the implicit feedback model is so close to the MAP of random ranking there clearly are other queries which do not benefit from the eye movement feedback.
4.3.6 Feature selection
The eye movement features (Sect. 3.2.4) used in the experiments have been previously proposed in the context of various psychological studies. There is no evi- dence that all of them are needed for the current task of predicting the interest of a user. It is an important open question what kind of features are best suited for this task.
To get some insight on the relative importance of the features, we analyzed the linear regression model Wlin for which such analysis is straightforward. We used the t-test to find out which regression coefficients differed significantly from zero. There were three eye movement features and one text feature that had Bonferroni corrected
p-values less than 0.05 in all sessions, both in the biased and unbiased case. These features were “saccade length before first fixation to the word,” “did a regression ini- tiate from the following word,” “duration of the first fixation to the word divided by the total fixation durations on the document,” and “relative position of the word on the document.” In addition to these, two features were significant in a subset of sessions either in the biased or the unbiased setting. The feature “duration of the next fixation after leaving the word” was significant in 9 out of 25 sessions in the biased case and 25 out of 694 sessions in the unbiased case, and “duration of a regression starting from this word” was significant in 2 out of 25 biased sessions.
The performance of the linear regression model in the biased setting increased from 37.8 to 40.9% when only the five features with significant contribution in at least nine sessions were selected. On the other hand, the same subset is not optimal for the non- linear regression model. The MAP of Weye+text (26) decreases from 46.3 to 39.9% with the set of five selected features. In the unbiased case, switching from the full feature set to the set of five important features actually impairs the linear regression model. The MAP decreases from 28.1 to 26.8%. The MAP of Weye+text(26) increases a little bit from 27.4 to 27.7%. In summary, the extracted features are clearly significant but probably not completely sufficient.
4.3.7 Performance of eye movements combined with explicit feedback
The average precision of the explicit feedback classifier SVMex, which was discussed in Sect. 3.2.5, is significantly higher than any of the eye movement models. This is, of course, to be expected because explicit feedback gives much more accurate information than eye movements. As a side note, the fact that the TFIDF baseline model attains almost as high performance without any feedback as the SVMex with known relevance labels shows how substantial the effect of the bias really is in our setup.
Our initial assumption was that combining eye movements with the explicit rel- evance feedback improves overall performance over using explicit feedback only.
1 3
Implicit queries from gaze patterns 331
 Comparing SVMex and SVM-2K results for the biased setting in Table 5 shows that this is not true for all search topics. Nevertheless, Table 3 shows that the overall precision for the biased setting is improved slightly by combining the two sources of information. The situation is even more clear in the unbiased setting (Table 4), where the SVM-2K and SVMex models have equal MAP. The equality of performance is consistent across all categories in Table 6 leading us to believe that the selection of eye movement features used in this study does not improve the overall performance when combined with textual information using SVM-2K. It is apparent that the explicit feedback is sufficient to learn the discrimination in the unbiased case.
5 Discussion
We addressed the extremely hard task of constructing a query in an information retrieval task, given neither an explicit query nor explicit relevance feedback. Only eye movement measurements for a small set of viewed snippets, and the text content of the snippets were available. This is a prototype of a task where the intent or interests of the user are inferred from implicit feedback signals, and used to anticipate the users’ actions.
We presented a proof-of-concept solution for this new problem based on the assump- tion that there is a link between eye movements and the relevance, and that the link is independent of the query. We were able to learn a “universal predictor of rele- vance predictors” from a collected database of queries, their relevant and irrelevant documents, and the corresponding eye movements. “Universal” here means inde- pendent of the actual query in the domain of Wikipedia. We validated the proposed solution on a real dataset, and showed that the predictions were better than those of a simple model which utilized only the textual content of the documents for new queries.
There may be a dependence between the gaze pattern and the topic of interest, even within our set of Wikipedia topics. A topic-specific predictor could therefore perform better for that particular topic, but it would be unable to predict the relevance of yet unseen topics. Our goal was to make universal, or topic-independent, predictor of relevance. We accomplished this by explicitly constructing our predictor such that it is invariant with respect to the permutation of the words—that is, the predictor does not take the semantic meaning of the words into account. The fact that we were still able to reach a statistically significant average prediction results on topics unseen in the training data shows that there is a link between the gaze patterns and interests. The statistically significant features, discussed in Sect. 4.3.6, give hints of the nature of this link. The exact nature or interpretation of this connection needs to be left as a topic of further study.
Notice that any method used to discriminate between categories, based on the BOW representation of the documents, such as ours, requires the categories to have different term frequency distributions. If the term frequency distributions of the cate- gories are too similar to each other the BOW methods are expected to perform worse. In our experiments we used Wikipedia categories which have a relatively good sep- aration in BOW representations. However, it remains an open question whether the
1 3
332 A. Ajanki et al.
 eye movements could effectively be used to find finer distinctions, that is, to find most discriminative words even if the relevant and irrelevant categories were very similar to each other.
We also addressed the issue of ‘bias’, which is likely to occur in a real world information retrieval situation, where the user is likely to view proportionally more relevant than irrelevant documents. More specifically, when browsing to look for inter- esting documents, there is no bias, and the bias increases as a function of the length of a focused search or browsing session. Both extremes are important for a practical proactive information retrieval system.
The bias complicates the evaluation of eye tracking results. We showed that taking the eye movement measurements into account improves the precision of the infor- mation retrieval, both in the presence and absence of the bias. Eye tracking is more important when there is no bias, since then content-based searches do not help at all. With heavy bias the relative improvement is much smaller. In percentage units the improvements are rather modest, as eye movements are a quite noisy indirect indicator of relevance. Nevertheless, assuming eye tracking signal is cheaply available, it would be a useful source to complement alternative implicit feedback sources. This holds in particular for applications with little bias towards read documents being relevant, such as in browsing applications.
We further experimented with a model where the textual content of the documents and explicit relevance feedback given by the user (whether or not the user thinks the document is relevant to the search topic) were assumed available. As expected, the pure explicit feedback improved the precision significantly to 75.1%. Our results show that also in the biased case, taking the eye movements into account we can further improve the precision by about two of percentage units.
In this paper, we studied whether eye movements are at all suitable as a source of implicit feedback for learning an universal predictor of relevance. The method is still a long way from being usable in practical applications, and there are several important extensions to be studied in the future. One interesting research topic is replacing the general-purpose machine learning components we used here with ones that are better tailored to the task. There are also problems on the practical level: in order to reliably connect gaze location to the correct word using current eye tracker technology, the size of the font has to be fairly large, which restricts the amount of text on the screen. Another factor that would be worth studying is what kind of eye movement features are optimal for the task.
We conclude that in constructing a query, eye movements provide a useful implicit feedback channel. As expected, the feedback obtained from eye movements is less informative than relevance feedback typed in by the user, but nonetheless this implicit feedback can be exploited. The eye movements may be the only available source of relevance information in the unbiased case where nothing can be deduced from the textual content. In practical applications all available feedback channels, in addition to the eye movements, should of course be utilized; the practical implication of this study is the finding that it is a good idea to include eye movement data if they are cheaply available. The future challenge is to improve the gain in precision obtained from the eye movements, in addition to making this new feedback channel more practically applicable.
1 3
Implicit queries from gaze patterns 333
 Acknowledgments AA and SK belong to Adaptive Informatics Research Centre, a centre of excellence of the Academy of Finland. The authors thank Wray Buntine from the Complex Systems Computation Group at University of Helsinki for providing the Wikipedia data, and Craig Saunders and Steve Gunn from the ISIS Research Group at University of Southampton for many fruitful discussions. This work was supported in part by the IST Programme of the European Community, under the PASCAL 2 Network of Excellence, and in part by TKK MIDE programme, project UI-ART. This publication only reflects the authors’ views. All rights are reserved because of other commitments.
Appendix
A Optimization of SVM-2K
SVM-2K combines KCCA-projection and a regular SVM by introducing the con- straint of similarity between two 1-dimensional projections. The extra constraint is chosen slightly differently from the 2-norm that characterizes KCCA, which typically finds a (varying in number) sequence of projection directions that then can be used as the feature space for training an SVM. Denote the two projections (views) of a training sample xi by φA(xi) and φB(xi). In order to obtain sparse set of projection directions we take an ε-insensitive 1-norm using slack variables to measure the amount by which points fail to meet ε similarity:
|⟨wA,φA(xi)⟩+bA −⟨wB,φB(xi)⟩−bB|≤ηi +ε,
wherewA,bA (wB,bB)aretheweightandthresholdofthefirst(second)SVM,andηi are slack variables. Combining this constraint with the usual 1-norm SVM constraints and allowing different regularization constants gives the following optimization prob- lem:
min
suchthat
1 2 1 2 A  l A B  l B  l
L = 2∥wA∥ + 2∥wB∥ +C ξi +C ξi + D i=1 i=1
ηi i=1
  |⟨wA,φA(xi)⟩+bA −⟨wB,φB(xi)⟩−bB|≤ηi +ε yi (⟨wA,φA(xi)⟩+bA)≥1−ξA
yi (⟨wB,φB(xi)⟩+bB)≥1−ξB i
ξA ≥0, ξB ≥0, ηi ≥0 allfor 1≤i ≤l. ii
The final SVM-2K decision function is then h(x) = sign( f (x)), where
f (x)=0.5 wˆA,φA(x) +bˆA + wˆB,φB(x) +bˆB =0.5(fA(x)+ fB(x)). (1)
B Supplementary results
See Tables 5 and 6.
i
1 3
334
Table 5 Results in the biased setting
TFIDF Wlin Astronomy 53.8 13.5
Wtext (4) 33.3
48.7 15.1 19.0 24.0 17.7 29.2 12.0 11.0
8.9
9.1 23.8 7.2 10.4 29.1 10.0 26.4 11.5 42.2 9.8 6.4 19.4 33.4 26.6 14.3 19.9
Weye+text (26) 15.8
81.2 46.2 36.0 77.9 46.1 68.8 16.8 33.4 41.5 44.6 86.7 12.1 11.7 66.3 70.2 38.7 76.9 69.0 31.6 67.6 40.7 32.3 27.5 18.0 46.3
Wus (26) 20.0
37.9 37.8 44.3 42.6 38.6 34.5
5.7 17.7 25.4 11.7 52.6 11.5 11.7 34.6 45.3 23.2 66.3 63.7 25.7 42.8 19.2 28.0 18.0 23.6 31.3
A. Ajanki et al.
SVMex SVM-2K
58.0 57.4 100.0 100.0 100.0 100.0
74.8 71.3 86.0 87.4 73.3 82.4 76.0 83.6 65.0 66.7 68.6 59.2 55.5 53.4 49.7 55.0 95.0 95.0 26.8 34.2 72.4 74.2 99.1 98.3 76.4 80.0 85.2 91.6 94.3 95.0 86.2 85.5 80.1 79.5 88.5 89.7 84.8 84.8 71.2 73.8 66.1 66.5 45.4 60.3 75.1 77.0
   Ball games Cities
Court systems Dinosaurs Education Elections Family
Film
Government Internet Languages Literature
Music
Natural disasters Olympics
Optical devices Postal system Printing Sculpture
Space exploration Speeches Television Transportation Writing systems Average
100.0 83.2 93.5 32.0 71.6 44.0 99.1 59.7 82.6 30.6 81.1 61.3 44.0 6.7 56.1 16.7 54.8 30.8 47.0 44.8 90.6 60.2 40.3 15.5 68.4 8.0 81.9 62.9 88.6 53.4 93.3 26.6 90.6 50.5 84.2 63.4 69.5 34.9 88.4 49.5 79.0 26.5 63.5 26.8 61.8 25.6 51.6 17.2 73.4 37.8
 These have been computed on a test set of 250 documents, with ten documents being positive
Table 6 Results in the unbiased setting
 User Topic
1 Ball games
1 Dinosaurs
1 Space exploration 1 Literature
1 Government
1 Court systems
1 Cities
1 Film
Pos/test Random TFIDF Wlin set size
Wtext (4) 27.0
30.4 22.0 15.6 18.5 30.0 20.9 18.8
Weye+text SVMex (26)
30.2 99.0 29.3 88.6 29.3 76.0 18.3 59.0 26.4 67.5 51.3 72.1 40.3 82.6 16.5 77.3
SVM-2K
99.0 88.7 76.1 59.3 67.8 71.9 82.6 77.1
 10/50 25.7 10/50 25.7 10/50 25.7 10/60 21.9 10/50 25.7 10/60 21.9 10/50 25.7 10/60 21.9
30.7 24.3 35.9 34.5 34.1 31.9 12.3 16.3 22.4 29.9 50.1 56.4 34.8 42.1 23.3 16.3
 1 3
Implicit queries from gaze patterns Table 6 continued
335
SVMex SVM-2K
77.7 78.1
88.4 88.5
86.1 86.2
87.1 86.9
85.7 85.5
89.0 89.1
57.2 57.2
76.8 76.8
77.7 77.7
86.0 85.9
94.7 94.7
99.0 99.0
55.8 55.4
93.5 93.7
77.0 77.0 89.0 89.1
57.2 57.2 76.8 76.8 77.7 77.7 86.0 85.9 94.7 94.7 99.0 99.0 83.5 83.6 89.0 89.1 72.7 72.7 81.8 81.5 74.1 74.1 88.1 88.4 84.0 84.1 91.3 91.2 81.1 81.2 92.6 92.7 86.0 86.1 94.5 94.4 67.8 67.7 99.9 99.9 84.9 84.8 89.3 89.3 81.3 81.1
  User Topic
1 Sculpture
1 Natural disasters
2 Education
2 Printing
2 Writing systems
2 Optical devices
2 Internet
2 Family
2 Television
2 Speeches
2 Postal system
2 Languages
3 Music
3 Olympics
3 Astronomy
3 Optical devices
3 Internet
3 Family
3 Television
3 Speeches
3 Postal system
3 Languages
4 Music
4 Olympics
4 Literature
4 Dinosaurs
4 Internet
4 Natural disasters 4 Court systems
4 Education
4 Speeches
4 Space exploration
5 Writing systems
5 Postal system 5 Transportation 5 Languages
5 Cities
5 Elections
5 Astronomy
Pos/test Random TFIDF Wlin set size
Wtext (4) Weye+text (26)
15.8 16.4
13.5 16.8
23.5 30.8
33.0 34.1
15.6 15.2
32.7 23.0
12.0 12.4
24.5 17.8
23.7 17.6
38.1 33.8
22.0 26.7
51.3 28.2
14.3 26.2
30.9 43.0
24.3 18.4 31.7 23.8
12.0 11.3 24.9 27.4 23.8 23.9 37.8 47.2 22.2 24.4 53.1 30.7 20.0 25.6 19.6 53.6 22.5 25.7 23.0 19.3 26.2 31.6 18.2 24.3 23.7 49.1 18.9 29.1 15.9 15.2 23.9 35.6 32.7 33.2 17.4 16.0 21.8 27.7 74.1 64.7 20.1 18.5 51.7 43.5 18.9 19.2
 10/60 21.9
10/60 21.9
10/40 31.3
10/40 31.3
10/50 25.7
10/40 31.3
10/60 21.9
10/40 31.3
10/50 25.7
10/40 31.3
10/60 21.9
10/70 19.1
10/60 21.9
10/50 25.7
10/50 25.7 10/40 31.3
10/60 21.9 10/40 31.3 10/50 25.7 10/40 31.3 10/60 21.9 10/70 19.1 10/40 31.3 10/60 21.9 10/40 31.3 10/70 19.1 10/40 31.3 10/50 25.7 10/40 31.3 10/70 19.1 10/60 21.9 10/40 31.3 10/40 31.3 10/70 19.1 10/50 25.7 10/30 40.2 10/50 25.7 10/30 40.2 10/40 31.3
14.8 23.0
20.3 17.5
28.0 21.6
46.6 34.0
17.0 15.5
23.2 25.1
12.8 11.8
26.7 17.5
17.0 18.1
26.1 35.9
24.1 21.7
35.4 31.4
26.9 20.8
39.1 54.4
19.2 20.5 23.2 21.9
12.8 11.5 26.7 28.9 17.0 21.7 26.1 45.0 24.1 24.2 35.4 44.1 28.8 24.1 30.0 56.8 25.4 25.2 14.6 23.9 22.2 30.0 19.8 23.0 38.9 43.5 19.3 24.3 14.6 17.9 29.0 30.0 23.2 27.0 22.8 13.7 24.4 28.9 69.7 66.0 19.3 21.4 54.0 41.2 17.3 18.1
 1 3
336
Table 6 continued
User Topic
5 Government
5 Optical devices
5 Sculpture
6 Speeches
6 Family
6 Education
6 Printing
6 Space exploration
6 Writing systems
6 Music
7 Transportation
7 Ball games
8 Optical devices
8 Astronomy
8 Television
8 Film
8 Literature
8 Dinosaurs
8 Space exploration 8 Transportation
8 Sculpture
8 Government
9 Television
9 Optical devices 9 Olympics
9 Internet
9 Speeches
9 Ball games
9 Film
9 Cities
9 Education
9 Natural disasters
10 Court systems
10 Elections
10 Printing
10 Languages
10 Writing systems 10 Literature
10 Music
Pos/test Random TFIDF Wlin set size
Wtext (4) Weye+text (26)
35.9 26.5
19.4 16.5
31.4 26.2
23.3 38.6
14.8 12.9
27.0 39.9
17.8 38.3
18.0 24.9
17.3 21.4
11.6 17.5
20.5 20.4
40.1 22.7 13.6 14.7
14.3 10.8 28.5 32.5 20.2 27.1 26.9 37.1 21.3 24.8 14.6 29.4 36.7 26.5 13.6 18.5 18.0 24.7 21.1 21.3 12.0 18.7 25.6 51.3 17.1 17.0 13.9 14.2 40.1 40.1 21.0 37.1 17.0 16.0 37.8 28.2 20.9 21.6 19.7 18.4 42.7 18.7 36.6 66.9 22.1 44.2 17.4 14.9 13.9 14.1 47.6 34.8
A. Ajanki et al.
SVMex SVM-2K
60.3 60.1
79.2 79.4
85.8 85.8
87.5 87.5
78.3 78.3
88.9 89.0
91.4 91.0
67.6 67.6
52.3 52.1
71.6 72.0
73.1 73.1
98.7 98.7 85.4 85.3
67.4 67.4 79.9 79.9 69.3 69.4 72.1 72.1 95.9 96.0 87.1 87.2 74.6 74.5 70.5 70.5 76.3 75.9 65.8 66.0 56.4 56.4 90.5 90.6 64.2 64.8 89.0 89.2 93.5 93.2 79.5 79.5 71.5 71.5 77.9 78.4 70.5 70.4 80.6 80.5 88.9 88.9 92.4 92.3 94.9 94.9 70.9 70.7 41.0 40.8 81.3 81.3
   10/60 21.9
10/50 25.7
10/40 31.3
10/40 31.3
10/50 25.7
10/50 25.7
10/60 21.9
10/70 19.1
10/60 21.9
10/80 16.9
10/50 25.7
10/50 25.7 10/60 21.9
10/70 19.1 10/40 31.3 10/50 25.7 10/30 40.2 10/50 25.7 10/70 19.1 10/40 31.3 10/70 19.1 10/50 25.7 10/50 25.7 10/70 19.1 10/50 25.7 10/50 25.7 10/60 21.9 10/60 21.9 10/40 31.3 10/60 21.9 10/50 25.7 10/50 25.7 10/60 21.9 10/60 21.9 10/50 25.7 10/60 21.9 10/70 19.1 10/60 21.9 10/30 40.2
24.0 33.2
26.7 16.5
26.2 24.9
29.7 54.3
13.7 13.0
44.7 36.7
18.1 46.9
33.8 30.6
13.4 23.8
11.8 16.9
18.6 20.5
43.8 22.3 12.5 14.5
14.0 15.2 27.0 31.9 21.0 25.1 27.5 37.8 28.1 22.3 24.3 38.9 21.6 27.6 12.2 19.0 14.0 21.8 16.2 19.8 11.9 15.8 49.3 44.6 26.0 19.5 14.1 16.7 30.2 46.9 31.1 43.0 14.9 15.0 24.5 35.9 25.1 21.8 41.9 16.4 32.9 16.6 40.3 67.4 37.7 58.7 11.9 17.2 12.6 14.8 48.2 32.3
 1 3
Implicit queries from gaze patterns 337 Table 6 continued
  User Topic
10 Astronomy 10 Postal system
Average
Pos/test Random TFIDF Wlin Wtext(4) Weye+text SVMex SVM-2K
set size
10/60 21.9 10/40 31.3
16.8 16.2 24.5 26.3 30.1 30.9
(26)
15.5 44.5 44.4
39.6 93.5 93.5
27.4 80.0 80.0
 25.7 25.9
28.1 24.8
 The size of the test set and the proportion of positive documents in it vary from session to session (third column)
References
Budzik, J. Hammond, K.J.: User interactions with everyday applications as context for just-in-time information access. In: 5th International Conference on Intelligent User Interfaces, pp. 44–51. ACM, New Orleans (2000)
Claypool, M., Le, P., Wased, M., Brown, D.: Implicit interest indicators. In: 6th International Conference on Intelligent User Interfaces, pp. 33–40. Santa Fe (2001)
Conati, C., Mertena, C.: Eye-tracking for user modeling in exploratory learning environments: an empirical evaluation. Knowl. Based Syst. 20, 557–574 (2007)
Czerwinski, M., Dumais, S., Robertson, G., Dziadosz, S., Tiernan, S., van Dantzich, M.: Visualizing implicit queries for information management and retrieval. In: SIGCHI Conference on Human Factors in Computing Systems, pp. 560–567. Pittsburgh (1999)
Dhanjal, C., Gunn, S.R., Shawe-Taylor, J.: Sparse feature extraction using generalised partial least squares. In: IEEE International Workshop on Machine Learning for Signal Processing, pp. 27–32. Maynooth (2006)
DiCiccio, T.J., Efron, B.: Bootstrap confidence intervals. Stat. Sci. 11, 189–228 (1996)
Dumais, S., Cutrell, E., Sarin, R., Horvitz, E.: Implicit queries (IQ) for contextualized search. In: 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, p. 594.
ACM, Sheffield (2004)
Farquhar, J.D.R., Hardoon, D.R., Meng, H., Shawe-Taylor, J., Szedmak, S. : Two view learning: SVM-
2K, theory and practice. In: Weiss, Y., Schölkopf, B., Platt, J. (eds.) Advances in Neural Information
Processing Systems, pp. 355–362. MIT Press, Cambridge (2006)
Fono, D., Vertegaal, R.: EyeWindows: evaluation of eye-controlled zooming windows for focus
selection. In: SIGCHI Conference on Human Factors in Computing Systems, pp. 151–160. ACM
Press, Portland (2005)
Fox, S., Karnawat, K., Mydland, M., Dumais, S., White, T.: Evaluating implicit measures to improve web
search. ACM Trans. Inf. Syst. 23, 147–168 (2005)
Granaas, M.M., McKay, T.D., Laham, R.D., Hurt, L.D., Juola, J.F.: Reading moving text on a CRT screen.
Hum. Factors 26, 97–104 (1984)
Hardoon, D.R., Szedmak, S.R., Shawe-Taylor, J.R.: Canonical correlation analysis: an overview with
application to learning methods. Neural Comput. 16, 2639–2664 (2004)
Hardoon, D.R., Ajanki, A., Puolamaki, K., Shawe-Taylor, J., Kaski, S.: Information retrieval by inferring
implicit queries from eye retrieval by inferring implicit queries from eye movements. In: 11th Intelligence and Statistics. San Juan. Electronic proceedings at http://www.stat.umn.edu/~aistat/ proceedings/start.htm (2007)
Howard, D.L., Crosby, M.E.: Snapshots from the eye: towards strategies for viewing biblographic cita- tions. In: Savendy, G., Smith, M.J. (eds.) Human-Computer Interaction: Software and Hardware Interfaces, pp. 488–493. Elsevier, Amsterdam (1993)
Joachims, T., Granka, L., Pan, B., Hembrooke, H., Gay, G.: Accurately interpreting clickthrough data as implicit feedback. In: 28th Annual International ACM SIGIR Conference on Research and Develop- ment in Information Retrieval, pp. 154–161. ACM, Salvador (2005)
Kelly, D., Teevan, J.: Implicit feedback for inferring user preference: a bibliography. ACM SIGIR Forum 37, 18–28 (2003)
1 3
338 A. Ajanki et al.
 King, L.: The relationship between scene and eye movements. In: 35th Annual Hawaii International Conference on System Sciences, p. 136b. Big Island (2002)
Levy-Schoen, A., O’Regan, K. : The control of eye movements in reading. In: Kolers, P.A., Wrolstad, M.E., Bouma, H. (eds.) Processing of visible language, pp. 7–36. Plenum Press, New York (1979)
Maglio, P.P., Campbell, C.S.: Attentive agents. Commun. ACM 46, 47–51 (2003)
Maglio, P.P., Barrett, R., Campbell, C.S., Selker, T.: SUITOR: an attentive information system. In: Interna-
tional Conference on Intelligent User Interfaces, pp. 169–176. ACM, New Orleans (2000)
Meng, H., Hardoon, D.R., Shawe-Taylor, J., Szedmak, S.: Generic object recognition by distinct features combination in machine learning. In: 17th Annual Symposium on Electronic Imaging, pp. 90–98.
San Jose (2005)
Puolamäki, K., Kaski, S. (eds.): Proceedings of the NIPS 2005 Workshop on Machine Learning for Implicit
Feedback and User Modeling. Helsinki University of Technology, Otaniemi. http://www.cis.hut.fi/
inips2005/ (2006)
Puolamäki, K., Salojärvi, J., Savia, E., Simola, J. Kaski, S.: Combining eye movements and collaborative
filtering for proactive information retrieval. In: 28th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval, pp. 146–153. Salvador, (2005)
Rafter, R., Smyth, B.: Passive profiling from server logs in an online recruitment environment. In: Workshop
on Intelligent Techniques for Web Personalization, pp. 35–41. Seattle (2001)
Rayner, K.: Eye movements in reading and information processing: 20years of research. Psychol. Bull.
124, 372–422 (1998)
Rosipal, R., Trejo, L.J.: Kernel partial least squares regression in reproducing kernel Hilbert space. J. Mach.
Learn. Res. 2, 97–123 (2001)
Salojärvi, J., Kojo, I., Simola, J., Kaski, S.: Can relevance be inferred from eye movements in informa-
tion retrieval?. In: Workshop on Self-Organizing Maps. Kyushu Institute of Technology, Hibikino,
pp. 261–266. Kitakyushu (2003)
Salojärvi, J., Puolamäki, K., Kaski, S.: Implicit relevance feedback from eye movements. In: Duch, W.,
Kacprzyk, J., Oja, E., Zadrozny, S. (eds.) Artificial Neural Networks: Biological Inspirations—ICANN
2005, Lecture Notes in Computer Science 3696, pp. 513–518. Springer, Berlin (2005a)
Salojärvi, J., Puolamäki, K., Simola, J., Kovanen, L., Kojo, I. Kaski, S.: Inferring relevance from eye move- ments: feature extraction. Technical Report A82, Helsinki University of Technology, Publications
in Computer and Information Science. http://www.cis.hut.fi/eyechallenge2005/ (2005b)
Salton, G., McGill, M.J.: Introduction to Modern Information Retrieval. Vol. 1, McGraw-Hill,
New York (1983)
Turpin, A., Scholer, F.: User performance versus precision measures for simple search tasks. In: 29th
Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,
pp. 11–18. Seattle (2006)
Ward, D.J., MacKay, D.J.C.: Fast hands-free writing by gaze direction. Nature 418, 838 (2002)
Author Biographies
Antti Ajanki is a Ph.D. candidate in Computer Science at Helsinki University of Technology. He received his M.Sc. (Tech.) degree in Computer Science from Helsinki University of Technology in 2006. His research interests include machine learning and information retrieval. Previous research has included work in the field of bioinformatics.
Dr. David R. Hardoon is a research fellow at the University College, London. He is currently working on projects that are focused on learning the structure of music, medical analysis, multilingual and multi-modal integration. He has a keen interest in multi-view learning, kernel methods, regression, and sparsity. He has previously worked on various research projects in the fields of Taxonomy, Image analysis, classification and content based retrieval systems. David received his first class B.Sc. Hons. in Computer Science with Artificial Intelligence from the Royal Holloway, University of London and his PhD in Computer Science in the field of Machine Learning from the University of Southampton. He has also received the PhD PASCAL label award from his active participation in the PASCAL network (More information about him can be found at http://www.DavidRoiHardoon.com/).
1 3
Implicit queries from gaze patterns 339
 Prof. Samuel Kaski is Professor of Computer Science at Helsinki University of Technology, vice director of the Adaptive Informatics Research Centre, a center of excellence of the Academy of Finland, and group leader in Helsinki Institute for Information Technology HIIT. Dr. Kaski received his M.Sc. and D.Sc. (PhD) degrees in Computer Science from Helsinki University of Technology. His main research field is statisti- cal machine learning, with applications in bioinformatics and information retrieval. He has authored 130 refereed papers in these fields.
Dr. Kai Puolamäki is a lecturing researcher (assistant professor) in the Department of Computer Science at the Helsinki University of Technology. He completed his Ph.D. in 2001 in theoretical physics from the University of Helsinki. His primary interests lie in the areas of data mining, machine learning and related algorithms, and especially their application in ecology and user modelling.
Prof. John Shawe-Taylor has been the Director of the Centre for Computational Statistics and Machine Learning at University College, London since July 2006. He obtained a PhD in Mathematics at Royal Holloway, University of London in 1986. He subsequently completed an MSc in the Foundations of Ad- vanced Information Technology at Imperial College. He was promoted to Professor of Computing Science in 1996. He has published over 150 research papers. He led the ISIS research group at the University of Southampton from 2003 to 2006. He is the scientific coordinator of the EC funded Network of Excellence PASCAL.
1 3
    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond
GEORG BUSCHER, ANDREAS DENGEL, RALF BIEDERT, and LUDGER VAN ELST, German Research Center for Artificial Intelligence (DFKI)
  Reading is one of the most frequent activities of knowledge workers. Eye tracking can provide information on what document parts users read, and how they were read. This article aims at generating implicit relevance feedback from eye movements that can be used for information retrieval personalization and further applications.
We report the findings from two studies which examine the relation between several eye movement measures and user-perceived relevance of read text passages. The results show that the measures are generally noisy, but after personalizing them we find clear relations between the measures and relevance. In addition, the second study demonstrates the effect of using reading behavior as implicit relevance feedback for personalizing search. The results indicate that gaze-based feedback is very useful and can greatly improve the quality of Web search. The article concludes with an outlook introducing attentive documents keeping track of how users consume them. Based on eye movement feedback, we describe a number of possible applications to make working with documents more effective.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval—Relevance feedback; H.1.2 [Models and Principles]: User/Machine Systems—Human informa- tion processing
General Terms: Experimentation, Human Factors, Measurement
Additional Key Words and Phrases: Relevance feedback, eye movement measures, personalization, attentive documents
ACM Reference Format:
Buscher, G., Dengel, A., Biedert, R., and Van Elst, L. 2012. Attentive documents: Eye tracking as implicit feedback for information retrieval and beyond. ACM Trans. Interact. Intell. Syst. 1, 2, Article 9 (January 2012), 30 pages.
DOI = 10.1145/2070719.2070722 http://doi.acm.org/10.1145/2070719.2070722
1. INTRODUCTION
Forty years ago, Herbert Simon pointed out that attention is a scarce resource in an information-rich world [Simon 1969, 1971]. Therefore, the way we do utilize our “attention budget” reflects to a certain degree our own preferences: We try to focus on these pieces of information that we think are most relevant, interesting, or useful to us in our current situation. Obviously, in an attention economy, where attention might be understood as a currency [Davenport and Beck 2001], there may exist many factors
This work was supported by the German Federal Ministry of Education, Science, Research, and Technology (bmb+f): Project Mymory, grant 011WF01; Project Perspecting, grant 011W08002.
Authors’ addresses: G. Buscher (contact author), Microsoft Corporation, One Microsoft Way, Redmond, WA 98052; email: georg@gbuscher.com; A. Dengel, R. Biedert, L. Van Elst, Knowledge Management Department, German Research Center for Artificial Intelligence (DFKI), Trippstadter Strasse 122, 67663, Kaiserslautern, Germany.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212) 869-0481, or permissions@acm.org.
⃝c 2012 ACM 2160-6455/2012/01-ART9 $10.00
DOI 10.1145/2070719.2070722 http://doi.acm.org/10.1145/2070719.2070722
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.
9

    9:2 G. Buscher et al.
that distract us from a self-directed utilization of our “attention budget.” The tight coupling of Web search and advertising is just one example. Either way, the extent to which a piece of information is interesting or relevant to us influences how we pay attention to it and how we perceive it.
This is especially true for textual information, which, in its various manifestations, has become an important source of knowledge in professional as well as in private settings. For example, we read emails, reports, papers, blogs, presentation slides, etc., in order to acquire information, establish our opinions, and base our decisions. What we read is influencing what we think and what we know; and how we read reflects, amongst other cognitive dispositions, the handling of our attention resources. For example, on the one hand, we may skim parts of a document being of little interest or of which the content is already known to us. On the other hand, we may intensively read text if it seems relevant or interesting. In some cases, we may get stuck or ignore text that is too difficult to understand or too detailed for us to follow.
While we do not have direct access to attention and cognitive activity during reading, there is evidence that eye movements are tightly coupled with these processes in our brains [Liversedge and Findlay 2000] and may serve as measurable indicators for such processes. Therefore, by analyzing eye movements in detail, we might be able to infer how the reader perceived a text he or she has read, that is, whether it was relevant, irrelevant, too difficult to understand, etc.
Technology for measuring eye movements has evolved rapidly during the last couple of years. Nowadays, eye tracking devices are relatively unobtrusive to use and have sufficiently high precision to be useful for analyzing eye movements during reading. Whereas good commercial eye trackers are still too expensive to be widely employed at office workplaces, a boost in the development of low-fidelity, open-source eye-tracking systems and software could be observed.1 It is conceivable that they will further develop so that eye tracking systems will get more wide-spread in the future.
All in all, affordable eye tracking technology together with methods to interpret that data in terms of a reader’s attention not only gives us the opportunity to analyze how humans deal with that scarce resource. It also has the potential to facilitate the development of information systems that are optimized in this regard and to design new kinds of applications. For example, documents may become attentive in the sense that they record the way they are consumed by the reader. Such usage information could be stored in association with the documents so that the documents do not forget when, how, and what parts of them the reader paid attention to. These attentive documents could, for example, form the basis for improving and personalizing search, making the reading process more effective, and supporting collaborative work.
The purpose of this article is to explore how useful eye tracking is as an interactive method to generate implicit feedback while reading. Within the scope of this article we focus on interpreting and using eye movements as implicit relevance feedback in Web search scenarios. We analyze eye gaze data in order to determine what parts of a doc- ument were read, and which of these parts were also relevant to the reader. Therefore, we implemented an algorithm that analyzes raw gaze data from the eye tracker and detects reading behavior. Based on eye movements during reading, we compute several gaze-based measures and explore their relation to individually perceived relevance of read text in two eye tracking studies. We show that the amount of reading and skim- ming changes with relevance and topicality of the viewed text and demonstrate that one of the most expressive measures for relevance is coherently read text length, that is, the length of text the user has read line by line without skipping any part.
1 http://www.gazegroup.org/.
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:3
We then apply eye tracking together with the studied measures of relevance in order to determine which parts of a text the reader has read, how intensely these parts have been read, and finally to estimate how relevant they have been to him or her. Finally, we demonstrate the usefulness of the reading detection algorithm and the studied eye gaze measures in the application domain of Web search. Here we show that implicit relevance feedback from eye tracking can greatly improve the quality search engine result rankings by over 25% compared to the original ranking, and by over 8% compared to baseline reranking methods based on nongaze implicit feedback.
The remainder of the article is structured as follows: First, we give an overview of related research (Section 2) and explain the foundations for our basic reading detection method (Section 3). We then explore the relation of several eye movement measures to user-perceived relevance of read text. To this end, we report on the results of two eye tracking studies employing two different reading scenarios (Sections 4 and 5). We demonstrate the value of gaze-based implicit relevance feedback for personalization in information retrieval (Section 5), and finally give an outlook of further applications for attentive documents that seem possible in the future.
2. BACKGROUND AND RELATED WORK
In the human-computer interaction domain, eye tracking has been applied primar- ily in diagnostic applications to learn how interfaces are used [Cutrell and Guan 2007; Buscher et al. 2009a] and directly in interactive applications such as eye typing [Majaranta and Ra ̈iha ̈ 2002].
In our work, we aim at interactive gaze-based applications, however, not based on direct control from the eyes. Since the eyes naturally did not evolve as tools but rather as perceptual organs and since many eye movements are unconscious, it is often difficult to use gaze for direct control. However, since eye movements are relatively tightly coupled with cognitive processes [Liversedge and Findlay 2000], there is a great deal of implicit information that may be inferred by analyzing eye movements in our daily work. This implicit information, for instance, about how and where we paid attention is very valuable feedback that can be used to support the individual user’s work. Therefore, we aim at using gaze information in a passive, indiscernible way so that the computer system can assist the user.
In the following, we review related work first with respect to reading-related eye gaze feedback. Then we particularly focus on the application of implicit feedback in the information retrieval domain, how implicit feedback is commonly assessed, and how eye tracking could be of use here.
2.1. Towards Attentive Documents
There is only very limited research focusing on the broader idea of what we call atten- tive documents, that is, documents actively keeping track of how they are consumed by users and applications based on this data.
One of the first occurrences of a similar idea can be found in a paper by Hill et al. [1992]. They introduce the interesting concept of computational wear for digital doc- uments in the style of wear for physical documents. Documents actively keep track of user activities with respect to reading (i.e., read wear) and editing (i.e., edit wear). This is done by measuring display time of different document parts and by recording editing interactions. In their implemented version, wear (i.e., the amount of usage of document parts) is visualized on the scrollbars for a document. The authors argue that such information can be very helpful for professional document work. For example, read wear could be used to show the user which parts of a document have been most interesting to him/her before. In that way, it can provide support for improved refinding and recognition. Edit wear could be useful to get informed about document parts that
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    9:4 G. Buscher et al.
have been edited by coauthors. The latter concept can now be found in most modern word processing software applications.
Ohno [2004] develops the concept of read wear further and applies an eye tracker to keep track of eye gaze traces on documents. The central idea is to improve document browsing by making it easier to find and recognize information read before in long documents. The main conceptual advancement of our method over the system imple- mented by Hill et al. is the employment of measures derived from gaze traces in order to determine the intensity of reading. Therefore, the author uses fixation duration on document parts as a direct and simple measure for the approximation of reading intensity.
Xu et al. [2009] also apply eye tracking to record how much a user pays attention to different document parts. Similar to Ohno, they use fixation duration on document terms as a direct measure of reading intensity (or “interestingness”). However, in contrast to Ohno, they use this kind of information as implicit feedback in order to generate personalized document summaries. Such personalized summaries emphasize these parts and terms of a document that have been read intensively before. The authors show that summaries generated in this way are more reflective of a user’s reading interest and summary preference.
In a more interactive way, Hyrskykari et al. [2003] analyze eye movements on the fly while a user is reading and tries to automatically detect comprehension difficulties on foreign language texts. The basic idea is to use the total gaze duration on a word in order to decide whether help for this word or the entire sentence should be provided. Overall, the work shows that it is difficult to detect understanding difficulties on the fly. However, the general idea is intriguing since the text being read plays an active role in the reading process because it actively observes and helps the reader understand it.
2.2. Implicit Feedback for Information Retrieval
Implicit relevance feedback for information retrieval is an important area of research and has been tackled by many different researchers (see Kelly and Teevan [2003] for an early overview). It has been shown that incorporating explicit relevance feedback, that is, asking the user directly for relevance judgments of documents for a given query, can drastically improve the quality of search result rankings [Rocchio 1971; Salton and Buckley 1990]. However, explicitly asking users for relevance judgments imposes an additional burden on the user and causes additional cognitive load. Therefore, work about implicit relevance feedback generally aims at analyzing user context or inter- preting user interactions in order to generate relevance feedback for search implicitly, without causing any additional cognitive load for the user. Most of the research in this area is based on implicit feedback from user context or user interactions not related to eye tracking. However, there is also some recent work based on gaze data.
2.2.1. Non-Gaze-Based Feedback. One of the most frequently researched implicit feed- back data sources is display time, that is, the duration a document is visible on the screen. As one of the earliest, Morita and Shinoda [1994] analyzed reading time for doc- uments and its correlation to explicit relevance judgments for these documents. They found a strong tendency of users to spend more time on interesting documents than on uninteresting ones; a finding that has also been reported by Claypool et al. [2001] and Fox et al. [2005]. However, in more complex and naturalistic settings, Kelly and Belkin could not find any general relationship between display time and the users’ explicit relevance judgments for documents [Kelly and Belkin 2001, 2004. But when taking different task types into account White and Kelly [2006] found clear signals in display time and showed that retrieval performance could be improved. Buscher et al. [2009b] analyze display time in greater detail and measure the duration different document
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:5
parts have been visible on the screen. They report considerable improvements when employing such fine-grained implicit feedback for the improvement of search result ranking. In addition, Gyllstrom [2009] shows that re-finding information in documents on the desktop can be sped up when incorporating information about which document parts had been visible on the screen before.
In addition to display time for documents, it has been found that the amount of scrolling on a Web page [Claypool et al. 2001], click-through for documents in a browser [Fox et al. 2005; Joachims et al. 2007], and exit type for a Web page [Fox et al. 2005] are good indicators of interest. Agichtein et al. [2006] have shown that search result ranking can be considerably improved when combining implicit feedback from several implicit relevance indicators. In important finding by Melucci and White [2007] is that individually personalizing such measures leads to additional improvements.
Most of the previously cited work was based on implicit feedback acquired for entire documents. However, there is only little work focusing on implicit feedback on the segment level, that is, implicit feedback for document parts. [Golovchinsky et al. 1999] recorded highlighting and underlining behavior of users working with documents and also analyzed notes in margin of a document in order to infer individually perceived relevance of different document parts. Using this kind of implicit feedback, the authors showed that the quality of search can be significantly improved. A somewhat similar approach has been examined in Ahn et al. [2008] where users could copy and paste interesting document parts in a personal notebook. They also reported improvements of search engine ranking.
2.2.2. Gaze-Based Feedback. There is some research focusing on gaining implicit rele- vance feedback from eye movements. Balatsoukas and Ruthven [2010] compute basic eye movement measures from eye movements on search engine results pages and inves- tigate their expressiveness with regard to a variety of relevance criteria. They report that result entries that differ in topicality have strongest effects on eye movement measures, but interestingly criteria like familiarity with the information content also play a role. Moe et al. [2007] use a qualitative and exploratory approach and identify eye movement measures that are correlated with explicit relevance ratings for the read text. While most of their tested measures were inconclusive, they found that the amount of reading behavior is informative with respect to relevance of the read text. Loboda et al. [2011] inspected the signal in eye movement measures for inferring relevance of single words and found that sentence-terminal words in particular attract more and longer fixations. Brooks et al. [2006] also focused on determining what gaze-based measures are most helpful in estimating relevance of single paragraphs in documents. In a preliminary study, they found that relevant text parts caused a higher number of fixations and regressions. Furthermore, Puolama ̈ ki et al. [2005] combined a number of eye tracking measures and trained HMM-based classifiers to predict relevance for pre- viously read text. They found that predicting relevance by analyzing eye movements is possible to some extent. However, they did not examine which gaze-based measures were most correlated with relevance. Ajanki et al. [2009] further built on this work and aimed at automatically inferring queries from eye movements while reading. They trained support vector machines based on gaze data to determine text parts that have been relevant to the user.
Buscher et al. [2008b] used eye movement measures to detect parts of longer doc- uments that have been read intensively. They used these read parts of documents as implicit feedback for query expansion and re-ranking in an information retrieval scenario and reported considerable improvements of search engine result quality as measured by explicit user ratings. Cole et al. [2010] further investigated the effect
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    9:6 G. Buscher et al.
of different tasks on reading behavior and found that switches between reading and skimming behavior are implicit indicators of the current task.
In summary, gaze is an excellent data source for providing information about how much attention the user paid to what locations and contents on the screen. Therefore, it seems to be well suited to provide information about which documents and document parts have been interesting or relevant to the user. As previous literature already indicates to some extent, this knowledge can be very valuable as implicit relevance feedback for information retrieval applications.
In this article, we examine the suitability of different eye movement measures for in- ferring relevance feedback on the passage level in detail. First, we review findings from research in reading psychology. Motivated by these findings, we introduce a reading detection method from Buscher et al. [2008a] and describe how we compute a variety of different eye movement measures. Next, we report results from two eye tracking user studies aiming at finding most useful features for detecting relevance. Finally, to show the applicability of the results from the first two studies, we report on extended analy- sis of a study previously partly published in Buscher et al. [2009b] demonstrating the value of gaze-based implicit relevance feedback in an information retrieval scenario.
3. EYE TRACKING
Interpreting eye tracking data can be tricky and requires careful consideration. Gener- ally, eye movements are relatively tightly coupled with cognitive processes [Liversedge and Findlay 2000]. Therefore, they contain valuable information making it possible to infer information about processes in the brain. However, it has to be born in mind that eye movements are often unconscious in nature and there are likely to be a large number of unknown factors influencing them. Hence, gaze data is generally very noisy. In addition, eye trackers introduce further inaccuracy when estimating the eyes’ focal point.
Most previous research employs gaze-based measures in a rather simple and direct way. For example, a fixation on a word is usually interpreted as user interest in or relevance of that word. However, gaze data is noisy and fixations can have different causes depending on the cognitive state of the user. Therefore, a fixation on a word may not always be meaningful. To reduce noise in gaze data, our approach is to detect reading behavior first based on specific spatial and temporal patterns in eye gaze traces. Then, we only focus on gaze data during reading behavior and ignore everything else. While in general, the focal points of our eyes are not always related to the point of visual attention, there is strong evidence that they match during reading behavior [Rayner 1998]. Hence, the gaze data we are relying on for our analysis is recorded only while users are reading, that is, while they are actively consuming and thinking about textual contents.
3.1. Reading Psychology
Automatically detecting reading behavior by analyzing eye gaze patterns is one of the key steps we take to reduce noise from eye trackers. Implementing a robust reading detection method is possible when taking into account existing knowledge from the area of reading psychology. Here, a great deal of research has been done during last one hundred years concerning eye movements while reading. When reading silently, as summed up in Rayner [1998], the eye shows a very characteristic behavior composed of fixations and saccades. A fixation is a time of about 250ms on average during which the eye is steadily gazing at one point. A saccade is a rapid, ballistic eye movement from one fixation to the next. A typical left-to-right saccade is 7–9 letter spaces long. Approximately 10–15% of the eye movements during reading are regressions, that is, movements to the left along the current line or to a previously read line. Visual
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:7
and saccades. A fixation is a time of about 250ms on average when the eye is steadily gazing at one point. A saccade is a rapid, ballistic eye movement from one fixation to the next. The mean left-to-right saccade size is 7-9 letter spaces. It
Fig. 1. Typical eye movement pattern while reading. Circles mark fixations; connecting lines depict saccades.
information can only be perceived during fixations and not during saccades. Words can be identified only up to approximately 7–8 letter spaces to the right of the fixation point. However, the total perceptual span, where at least some useful information about the text can be extracted, extends about 14–15 letter spaces to the right of the fixation point.
There is high variability of the aforementioned average values both with respect to individual differences between readers as well as with respect to document-induced differences for the same reader. For example, the fixation durations for the same reader can vary between 100–500ms, while saccade sizes can range from 1 to 15 characters. Among many other factors, this variability is influenced by the difficulty of the read text, word predictability, background knowledge, and reading strategy of the reader [Rayner 1998].
3.2. Reading Detection Method
We used these insights about typical eye movements while reading to implement a reading detection method. The method works in several steps. First, fixations are detected by grouping together nearby gaze coordinates for a duration of at least 100ms. Second, the transitions from one fixation to the next, that is, the saccades, are classified according to their distances and directions. Classification of saccades is based on a set of heuristics derived from previous research in reading psychology [Rayner 1998]. In that way, typical saccades belonging to reading behavior can be detected and differentiated from unrelated eye movements. Third, using some simple heuristics, it is determined whether a sequence of saccades over a line of text is more characteristic for reading or for skimming behavior based on the lengths and the composition of the saccades. Figure 1 shows an example for the typical placement of fixations and saccades while reading. More details about the reading detection method can be found in Buscher et al. [2008a].
3.3. Eye Movement Measures
Eventually, one of our goals for this work was to detect what parts of a document have been interesting or relevant to a user and to use this knowledge for the personalization of information retrieval methods. Most previous research applying eye tracking for information retrieval used gaze data directly to infer interestingness or relevance of single terms in a text. For example, Ohno [2004] and Xu et al. [2009] computed very simple gaze-based measures like fixation duration on every term in a document and then determined the “best,” for instance, most looked at, terms for further use.
However, the contents and the relations a text conveys are mostly based on specific combinations of many words in sentences and paragraphs rather than single isolated terms. For example, regressions on a word in a text do not necessarily mean interest or relevance of that word but can rather be caused by difficulties in understanding the word or the entire sentence. However, what the regression can tell us is that the reader pays close attention to the text and tries to understand it. Therefore, in order to detect relevance of document parts, we do not apply gaze-based measures on single terms directly, but aggregate them on the level of sentences and paragraphs. Overall, we aim to determine relevance at the level of paragraphs. The basic assumption is that
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    9:8 G. Buscher et al.
if, for example, a paragraph is interesting or relevant to the reader, then this will be reflected in his or her eye movement pattern over that text part.
Hence, all of the gaze-based measures described in the following are computed as average values of fixations and saccades on entire paragraphs of text. Note, that we only include fixations and saccades belonging to reading or skimming behavior for the computation of the measures. For the following studies, we are focusing on 5 different gaze-based measures.
—Average fixation duration is computed as the sum of the durations of all fixations on a paragraph divided by the number of fixations on that paragraph. There is abundant evidence that fixation duration is influenced by the text is currently being fixated [Rayner 1998]. Some previous work uses this measure directly as an indicator of relevance [Ohno 2004; Xu et al. 2009]. However, it is not clear yet to what extent it relates to relevance.
—Average forward saccade length is the average length of left-to-right saccades. Sac- cade size is also known to be influenced by characteristics of the text [Rayner 1998]. —Regression ratio is computed as the number of regressions divided by the total num- ber of saccades on a paragraph. There is some indication in previous research stat- ing that higher regression ratios signify relevance of the read text [Moe et al. 2007;
Brooks et al. 2006].
—Thorough reading ratio is computed as the length of text that has been detected
as read by our reading detection method divided by the length of read or skimmed text. Thus, it is a measure for the reading intensity of a user: the more parts of a paragraph read instead of skimmed, the higher the value of this measure. A similar measure has been found to be related to relevance of read text [Moe et al. 2007].
—Coherently read text length measures the length of text in characters that has been read coherently without skipping any text in between. The assumption underlying this measure is that users may start and quickly stop reading a paragraph if the contained information is irrelevant. Then, they may skip the irrelevant paragraph and jump to the next one to continue. In contrast, if the information seems relevant, users may continue reading line by line without skipping any text in between.
3.4. Personalization of the Measures
As stated in Rayner [1998], there is high variability of most eye movement measures both within as well as between readers. Therefore, it is difficult to build methods estimating relevance of read text based on absolute values of gaze-based measures (e.g., compare Moe et al. [2007]). However, when individually personalizing such measures, they become more expressive so that precise implicit relevance feedback is easier to achieve as previous research suggests for non-gaze-based feedback data [Melucci and White 2007].
We personalize each of the recorded gaze-based measures as follows. First, we deter- mine the distribution of a measure for an individual user by analyzing all of his or her recorded eye movement data during reading. Then, we compute the upper and lower whiskers (limits) concerning the measure’s value distribution as it is typically done for generating box plots (i.e., lower whisker = max(minimum value, lower quartile − 1.5 * interquartile range); upper whisker = min(maximum value, upper quartile + 1.5 * interquartile range); see Figure 2) [Wilcox 2005]. The upper and lower whiskers define a user-specific interval containing most of the measured values. Outliers do not fall within such intervals and, hence, do not distort them.
Next, the computed absolute values of the eye movement measures are normalized with respect to the individual whisker-intervals. This results in a percentage for each absolute value stating its relative position in the appropriate interval. Outliers that
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:9
 maximum
minimum
Value distribu on (1)
outlier
       1.5* interquar le range
interquar le range contains 50% of the data
1.5* interquar le
                        Fig. 2.
Personalization of a measure by mapping absolute values into a [0,1]-interval.
range
Whisker determina on (2)
1 (upper whisker) 0.9
0.5
0.2
0 (lower whisker)
do not lie within a whisker-interval are mapped to 0 or 1 depending on whether they are smaller or greater than the lower or upper whiskers, respectively. Figure 2 shows an example for a measure’s value distribution, the respective determination of the whiskers, and the normalization of the measured absolute values.
4. STUDY 1: RELEVANCE ESTIMATION BASED ON EYE MOVEMENTS
The first study is of exploratory nature and analyzes eye movement measures while reading texts with varying degrees of difficulty, novelty, and topicality. Specifically, we 1. examine the relation between reading behavior and explicit user judgments, and 2. analyze the effect of measure personalization.
4.1. Methods
4.1.1. Experimental Design and Procedure. We designed an eye tracking experiment where 19 participants had to read through a number of short documents and judge their relevance with respect to a research task we provided.
The task. The same task was given to all participants. They were told to prepare a presentation about technical aspects of renewable energies. Therefore, we provided them with documents their presentation should be based on. Their task was to read through all of the documents and judge them according to their individual assessments of usefulness for the presentation.
In order to make the task more realistic, we told the participants that the exper- iment was composed of two different parts. The first part described above, that is, looking through and judging the usefulness of each document, was just to categorize the documents for later use. They were told that during the second part, that is, pre- sentation writing, the documents would be presented to them again in an order based on their own categorization from the first part. However, this imaginary second part was made up only to let the task seem more realistic. An experiment has always been stopped after the judgment phase was finished.
The documents. We provided all participants with the same set of 16 documents from Wikipedia or newspapers in randomized order. 14 of the documents were on topic and dealt with different aspects of renewable energies, for instance, characteristics of renewable resources, technical explanations of machines, political opinions, and
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.
Normalized interval (3)

    9:10 G. Buscher et al.
examples of power plants. The remaining two documents were completely off topic (one article was about theory of history; one was about a famous castle).
The 14 on-topic documents were carefully selected to cover a broad range of difficulty and novelty. Some were rather easy to understand, containing only facts that could be assumed to be known by most participants, and some contained rather difficult and complicated contents, for instance, one document explained chemical and physical processes in photovoltaic solar panels on the atomic level. Also, some documents were closer and some more distantly related to the main task about technical aspects of renewable energies. For example, some documents dealt with technical explanations whereas others covered political aspects.
All of the documents were approximately one screen page long and contained about 350 words on average. A font size of 12pt was chosen. A line of text was about 720 pixels wide, which corresponded to approximately 120 characters. The documents were written in German and consisted only of plain text.
Explicit judgments. We provided the participants with the following categorization scheme for their explicit judgments.
—“Useful for the presentation” → label: “useful”
—“Useful, but contents completely known” → label: “known” —“Probably useful, but too difficult to understand” → label: “difficult” —“Rather useless” → label: “useless”
This categorization scheme for explicit judgments needs some discussion. Re- searchers in the domain of information retrieval usually employ one-dimensional rel- evance judgment scales with the two poles “relevant” and “irrelevant”. However, there is a multitude of factors and dimensions influencing how users perceive the quality (or relevance) of a document for solving their current task. For example, as summed up by Chen and Xu [2005], relevance is influenced by topicality, novelty, reliability, understandability, and scope of the contents.
The contents of documents can be characterized in all of these dimensions indepen- dently. All of these independent characteristics can cause different behavior in the reader, and different eye movement patterns in particular. For example, if a document is perfectly on topic but already completely known to the reader, he or she may read it differently than a document that is only partly on topic but that contains many new and interesting facts. Probably the reader will come to the same personal explicit judgment for both documents in the end with respect to a one-dimensional relevance judgment scale. However, we expect to see differences in reading behavior on both documents due to the differences in novelty and understandability.
Out of the five dimensions of relevance named above, novelty and understandability are presumably most dependent on individual users. Individual characteristics like knowledge, background, vocabulary, and intelligence influence both individually per- ceived novelty as well as understandability. Since we expected to see differences with respect to both dimensions in the recorded eye movements, we decided against the typical one-dimensional judgment scale but decided for the four point categorization scheme including the two entries “known” and “difficult.” Now, the differences between the four categories have to be clarified: The category “useful” is used for documents that are relevant on all (or most) of the five dimensions of relevance. “Known” documents are relevant on all of the relevance dimensions except for novelty. Likewise “difficult” documents are relevant on all dimensions except for understandability. “Useless” doc- uments do not fit into the three other categories, e.g., because they are off topic.
Procedure. An experimental run proceeded as follows. First, the eye tracker needed to be calibrated using a 9-point calibration method. Next, the participant read through
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:11
  80% 70% 60% 50% 40% 30% 20% 10%
0%
skimmed read
useless & off topic (N=38)
 23%
52%
useful
(N=134) (N=46)
difficult (N=33)
useless & on topic (N=53)
31%
43%
known
23%
43%
Explicit judgment
26%
34%
18%
16%
Fig. 3. Percentage of read text for documents broken down by explicit judgments.
the task description informing about the first and the imaginary second phase of the experiment, and also about the explicit categorization scheme. After this introduction, the participant had to look through each of the 16 documents sequentially and provide his or her explicit judgment. A typical experimental run took around 30 minutes.
4.1.2. Apparatus. The experiment was performed on a Tobii2 1750 eye tracker at a screen resolution of 1280x1024 pixels. The eye tracker has a tracking frequency of 50Hz and an accuracy of 0.5◦ of visual angle. For calibrating the device we used the software ClearView from Tobii. To analyze the eye movements, detect reading behavior, and compute the gaze-based measures, we applied our own implemented software using the Tobii SDK. All documents were displayed by the Firefox3 browser for which we implemented a plug-in asking for explicit judgments every time a document was exited.
4.1.3. Participants. Nineteen participants (10 female) took part in the experiment and produced valid eye tracking data. All of them were undergraduate or graduate stu- dents attracted by advertisements for the study, and they had German as their native language. They ranged in age from 22 to 32 years (mean = 24.2, σ = 3.2) and had a variety of different majors.
4.1.4. Measures. Using our reading and skimming detection method, we analyzed the participants’ eye movement patterns while reading the assigned documents. Based on the eye movements belonging to reading behavior (as detected by our method), we computed the previously described gaze-based measures. Therefore, the data for the analysis had the following structure: for every document and participant we had an explicit judgment and a set of gaze traces that were used to compute gaze-based measures.
4.2. Results
4.2.1. Distribution of Reading Behavior. Figure 3 shows the amount of reading and skim- ming behavior on documents broken down by explicit judgment. The number N of documents in each category is given in the category descriptors.
2 http://www.tobii.com.
3 http://www.mozilla.com/firefox/.
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.
     % of document read or skimmed
    9:12
G. Buscher et al.
Table I.
Means and standard deviations for the measures “average forward saccade length”, “thorough
reading ratio”, and “coherently read text length” split by explicit judgment
   relevance judgment
           mean forw. sacc. length mean σ
   thorough reading ra o mean σ
     coher. read text length mean σ
   relevant
known
difficult
useless & on topic
9.36 le ers* 9.99 le ers 9.80 le ers* 10.41 le ers
      2.84 3.20 3.03 3.57
0.67* 0.56 0.62* 0.54*
     0.36 0.30 0.39 0.43
465 le ers* 409 le ers 426 le ers* 317 le ers*
430
381
438
347
 useless & off topic
10.95 le ers
3.23
0.41
0.42
233 le ers
277
               It should be noted that the category “useless” is further split into “useless & on topic” and “useless & off topic” where on topic refers to documents that have at least something to do with the main topic of the task (14 of the 16 documents), and where off topic refers to the 2 completely off-topic documents. Topicality was determined by the authors, not by the participants. All participants judged the two off-topic documents and some additional on-topic documents as useless.
The differences in the amount of read plus skimmed text of each document are clearly statistically significant between the category “useless & off topic” and all remaining categories at the 0.01 level (e.g., t-test for “useful” vs. “useless & off topic”: t(170) = 9.6, α < 0.01). However, the differences in the amount of reading plus skimming between the categories “useful” and “known” and between “difficult” and “useless & on topic” are not statistically significant.
The ratio between reading and skimming behavior is different for the 5 categories in Figure 3. Documents being on topic were mostly read instead of skimmed. On the contrary, useless off topic documents lead to slightly more skimming than reading behavior (however, not statistically significant).
Discussion. The combined amount of reading and skimming behavior on the 14 on- topic documents is surprisingly similar. Participants read or skimmed only 15% less of on-topic documents judged useless than of documents judged useful. Also, there does not seem to be a difference in the amount of reading or skimming on useful and known documents. Only the difference to useless off-topic documents is clear and statistically significant.
The findings show that documents being on topic in some way do induce reading behavior of a similar amount. It seems that users keep reading or skimming if docu- ments are on topic and if they can be expected to contain useful information. A reason for this is to a great extend the characteristic of the task. Since users were asked to assess entire documents, they tended not to skip much text. In contrast to topicality, the factors novelty and understandability seem to have only slight influence on the amount of reading behavior.
The ratio between reading and skimming behavior while viewing documents seems to be related to the participants’ final explicit judgments. One of our measures, the thorough reading measure, is computed as this ratio and will be analyzed in more detail in the following.
4.2.2. Gaze Measures to Estimate Relevance.
Absolute measures. Table I presents means and standard deviations for the measures “average forward saccade length,” “thorough reading ratio,” and “coherently read text length” across participants and documents. An asterisk signifies statistical significance of the differences to the category “useless & off topic” (student’s unpaired, two-sided t-test using a significance level of α < 0.01).
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:13
        20 15 10
5
0
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45
par cipant 1 Regression ra o par cipant 2
16 14 12 10
8 6 4 2 0
    Count
Count
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
par cipant 1 Personalized regression ra o par cipant 2
        Fig. 4. Absolute and personalized value distribution of “regression ratio” for two participants.
Table II.
Absolute and personalized means and standard deviations for the measures “regression
ratio” and “average fixation duration” split by explicit judgment
       regression ra o average fixa on dura on
  relevance judgment
relevant
known
difficult
useless & on topic useless & off topic
absolute personalized absolute mean σ mean σ mean σ
personalized mean σ
9.55* 0.22 0.52 0.23 0.52 0.22 0.50 0.25 0.48 0.27
                                  0.16 0.13 0.16 0.11 0.16 0.13 0.17 0.16 0.14 0.18
0.42* 0.27 0.44* 0.34 0.42* 0.26 0.39 0.30 0.34 0.33
219ms 67 228ms 75 202ms 71 215ms 68 208ms 81
        Concerning the two measures “average fixation duration” (overall mean = 216ms, σ = 71) and “regression ratio” (overall mean = 0.16, σ = 0.14), we could not determine any significant differences with respect to the five relevance categories.
Personalization. We were surprised not to see any significant differences for fixation duration and regression ratio since previous literature used these measures to estimate relevance [Ohno 2004; Xu et al. 2009; Moe et al. 2007; Brooks et al. 2006]. Therefore, we analyzed the individual distributions of the measured values for each participant.
It turned out that there are big individual differences. Figure 4 (left) shows the absolute value distribution for the measure ”regression ratio” for two participants. Participant 1 typically performs more regressions during reading (mean = 29%) than participant 2 (mean = 17%). As we know from the literature [Rayner 1998], these differences do not necessarily mean that one of the participants found the documents generally more relevant than the other. Rather, these types of individual differences are highly dependent on the typical reading strategy and expertise of the reader.
In order to achieve comparability of the measured values across participants, we per- sonalized them by applying the personalization method from Section 3.4. The resulting distributions for “regression ratio” for participants 1 and 2 are presented in Figure 4 (right).
Personalized measures. Table II presents absolute as well as personalized means and standard deviations for the measures “regression ratio” and “average fixation duration,” calculated across participants and documents. Again, an asterisk signifies statistical significance of the difference to the category “useless & off topic” (determined by a student’s unpaired, two-sided t-test using a significance level of α < 0.01).
While none of the differences concerning the category “useless & off topic” are sta- tistically significant with respect to the absolute values for both measures, some of
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    9:14 G. Buscher et al.
the differences are significant after personalizing them. Particularly for the measure “regression ratio” a clear trend can be observed. For “average fixation duration,” only a slight tendency can be found.
Discussion. The most expressive measure with respect to perceived relevance is based on the length of coherently read text. Participants read about 4 coherent lines of text (corresponding to 465 characters in out setting) in paragraphs that belonged to documents judged useful. In contrast, they only read 2 coherent lines of text (233 characters) in paragraphs belonging to useless off-topic documents. This difference shows how much users tend to skip text if it does not appear to be relevant.
There is evidence that mean forward saccade length and thorough reading ratio are related to relevance of viewed text. On the one hand, the length of left-to-right saccades decreases with perceived relevance of text. On the other hand, the percentage of read (vs. skimmed) text increases with perceived relevance.
With respect to “regression ratio” and “average fixation duration” we find that the measures are not very discriminative when considering values on their absolute scale. There exist great differences across participants with respect to the individual dis- tributions for these measures. But after individually personalizing the measures, we see clear signals for “regression ratio”: the percentage of regressions during reading generally increases with perceived relevance. However, concerning “average fixation duration”, we only see a very slight trend that duration increases with perceived rele- vance. But there is little evidence for it.
The results show that mean forward saccade length, thorough reading ratio, and regression ratio are all influenced by perceived relevance of read text. However, the finding that fixation duration is the least expressive measure is somewhat surprising since it is most frequently used in previous research as a simple indicator for relevance [Ohno 2004; Xu et al. 2009].
4.3. Conclusion
The study shows that reading and skimming behavior is strongly influenced by rel- evance and especially by topicality of text. Whether a document is on or off topic is clearly indicated by the amount of reading behavior. Therefore, the amount of reading behavior alone gives a good first approximation of whether some text has been relevant to the user.
Relevance, and topicality in particular, are also reflected in most of our analyzed eye movement measures. “Coherently read text length,” “thorough reading ratio,” and “regression ratio” increase with perceived relevance of the read text. “Mean forward saccade length” decreases with perceived relevance. Interestingly, we did not find strong evidence that fixation duration is influenced by relevance.
Even in our rather homogeneous set of participants (i.e., young students at univer- sity) we found large individual differences across participants. Some eye movement measures (particularly “regression ratio”) had different, individually specific distri- butions and mean values. Therefore, any signals in these measures with respect to relevance were overwhelmed by the noise that was introduced by individual differ- ences. However, these individual differences seem to be systematic, so that personaliz- ing and normalizing the measures greatly increased their expressivity with respect to relevance.
The found relationships of eye movement measures with perceived relevance of the text may be very useful for generating relevance feedback on the fly for improving and personalizing information retrieval. The next study will further validate the relation- ships and measure their impact on information retrieval personalization.
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:15
5. STUDY 2: READING BEHAVIOR AS IMPLICIT FEEDBACK FOR SEARCH
The first study demonstrates that detecting reading behavior is very valuable and useful, and that there clearly are signals in eye movements with respect to individually perceived relevance. However, the experimental scenario was very specific in that the assigned documents were all either completely on or off topic and that they were all very short and homogeneous. In addition, the given task, that is, assessing the relevance and usefulness of documents, could have had a high impact on the observed reading behavior. In more realistic settings, tasks can be much more directed to finding specific information, and documents can be much longer and can contain some parts that are relevant and other parts that are irrelevant to the task. Also, the first study comprised a document assessment task which presumably leads to just one specific type of reading behavior. It can be expected that reading behavior on such structurally different documents and for a different task is more diverse.
For this second study we had two fundamental goals. First, we wanted to validate the findings from the previous study for different task and document characteristics, and second, we wanted to prove that gaze-based feedback is not only full of potential for estimating relevant document parts, but that it is actually very useful as implicit relevance feedback on the segment level for information retrieval.
In specific, concerning validating study 1 results and estimating relevant document parts, the aims for this study are (1) to study reading behavior in general on long documents that contain relevant as well as irrelevant parts, (2) to find out whether the relations between eye movement measures and relevance found in the previous study are also valid for different document structures and a more goal-directed task, and (3) to determine the influence of an additional factor on reading behavior, that is, familiarity with the document.
For the second fundamental goals, in order to assess the effect of such gaze-based relevance feedback on information retrieval, we compare it to baseline implicit feed- back methods in a search personalization scenario comprising query expansion and re-ranking of search results. (Results for this part of the study have partly been pub- lished in Buscher et al. [2009b] before.) The baseline methods do not comprise any information from eye tracking at all. They rather create implicit feedback incorporat- ing information from the user query and the contents of the documents the user has viewed immediately before querying the search engine. To quantify the effect of our implicit feedback methods on search result re-ranking, we also compare them with the original ranking from the commercial Web search engine Live Search.4
The information retrieval scenario we are using to evaluate such implicit relevance feedback is as follows. We assume that a user has an information need in mind and looks through some documents in order to find relevant information. Next, he or she queries a search engine to find more related information. At this point, we aim at personalizing the result ranking that is produced by the search engine since we know which documents the user has viewed before and how he or she has viewed them. Thus, we use the user’s viewing behavior as implicit feedback for the next query and can either expand the query with additional relevant terms or re-rank the result list generated by the search engine. The search engine should then return more relevant results to the user.
5.1. Methods
5.1.1. Experimental Design and Procedure. We designed an eye tracking experiment where participants had to look through four long documents in order to read up on
4 http://www.live.com/.
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    9:16 G. Buscher et al.
a given topic (the reading phase), and then search for more related information using a search engine and provide us with explicit relevance ratings for the returned search results (the searching phase).
The Tasks. The two topics we gave to each participant were
(1) perception (about the variety of perceptual organs of animals and how they work) and
(2) thermoregulation (about thermoregulation mechanisms of animals used to control their body temperature).
In order to read up on each topic, the participants had to look through the four provided long documents. They were not asked to provide any explicit relevance judgments of any kind while viewing the documents.
After viewing the four documents with one of the topics in mind they had to pose three different queries to the search engine (searching phase). The first query should always find more related information about the general topic of the task. The two remaining queries should return information about two specific subtopics. For example concerning the topic perception, the participants should find more information about visual and magnetic perception in particular. The general topics for the queries were told orally to the participants. They were free in formulating them.
For each of their queries, the participants were asked to provide explicit relevance judgments for the top 20 of the results on a 6 point relevance scale ranging from “– – –” to “+++”. To judge a result, they were told not rely on the search result abstract visible on the search engine results page but to open the document and look through it. In contrast to study 1, this more common relevance rating scale was employed to be able to measure the performance of different implicit feedback methods using standard metrics.
In order to keep them focused on the respective topic of the task, they had moderate time pressure, that is, about 15 minutes for the reading phase and an additional 15 minutes for the searching phase for each task.
The Documents for the Reading Phase. For reading up on the two topics concerning both tasks during the reading phase, the participants had to use the same four assigned documents. Each document was taken from the German version of Wikipedia5 and was about a different animal species, i.e., about snakes, bees, dogs, and seals. The original Wikipedia articles were modified slightly so that all documents had a comparable length of around 4200 words and so that approximately 6% of the contained text was relevant for each of the two task topics. Each article dealt with a great variety of aspects concerning the respective animal species. However, each article contained some relevant subsections with respect to both topics. We placed these subsections in mostly random positions of the document, with the exception that they never occured in the very beginning or the very end of a document. Since we knew about the positions of the relevant sections, we did not need explicit relevance judgments for text passages from the participants.
The documents were mostly composed of plain text at a font size of 12pt. All of the documents contained some pictures. We removed the table of contents from the original Wikipedia articles and any other means of navigating within the document. Section headings were kept, but not in bold font.
Reranking Procedures. The Web search interface they had to use was modified by us so that we could analyze personalization techniques based on implicit feedback collected from the reading phase.
5 http://de.wikipedia.org.
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:17
To assess the effect of the implicit feedback methods on the final ranking of the search results, we applied two different procedures: query expansion and reranking. The two procedures were not applied simultaneously but we rather split the participants of the study in two separate groups. The first group of 17 participants used a query interface that comprised reranking. For the second group of 15 participants we applied query expansion. However, the search interfaces looked exactly alike and it was not possible for the participants to identify which group they belonged to.
Task Procedure. To make the experiment look as realistic as possible to the partici- pants, the story and the task protocol were designed as follows. The participants had to imagine being journalists having to write articles for a newspaper. We provided them with a simulated email from their imaginary advisor stating the topics (i.e., perception and thermoregulation) they had to write their next newspaper article on. The emails contained our four preselected documents as attachments which should help them to get started with the topic. After having looked through the four documents within about 15 minutes of time (reading phase), the participants had to employ our search interface to find more related information for another 15 minutes (searching phase).
After having finished with the reading phase and the searching phase for one topic, the participants had to repeat the exact same procedure for the other topic. Using the same four documents for both topics guaranteed that the participants were familiar with the documents’ structure for the second reading session. However, the Web search session in between the two reading sessions ensured that the participants could not exactly remember the structure of the documents anymore. Half of the participants started the experiment using the topic perception and the other half started with thermoregulation.
Completing the task for both topics together usually took one hour per participant. The eye tracker was used during the reading phase only.
In summary, the procedure looked as follows from the participants’ viewpoint.
(1) They were informed about the task and the current topic in a simulated email.
(2) They had to look through four preselected long documents that contained some
parts relevant to their topic.
(3) They had to query a Web search interface three times to find more related infor-
mation.
(4) Foreachquerytheyhadtoproviderelevanceratingsforthetop20returnedresults.
This procedure was repeated twice, that is, for both topics.
5.1.2. Implicit Feedback Methods. For personalization purposes of search engine result rankings, we analyze and compare five different implicit feedback methods. Overall, we evaluated implicit feedback methods based on all investigated eye movement features. However, since they turned out to have very similar performance, we report only the results from the three most interesting such implicit feedback methods. In all cases, implicit feedback is generated based on the documents the participant viewed during the reading phase, that is, immediately before querying the search engine. All of the implicit feedback methods extract a list of terms most characteristic to describe the user context based on their feedback source. These terms are then either used to expand the user query or to re-rank results from the Web search engine.
Method Reading. This method is based on eye tracking and reading detection. It first concatenates all documents viewed by the user resulting in one large context document d. Second, it creates a filtered context document dP by removing all text parts from
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    9:18 G. Buscher et al.
d that have neither been read nor skimmed. Third, most characteristic terms are extracted from document dP simply based on their t f × idf scores.6
Method ReadLength(l). This method is an extension of the previous method. It is not solely based on the reading and skimming detection method but also incorporates information from the measure “coherently read text length” to decide whether a certain part of read text should be counted as positive or negative relevance feedback. As demonstrated in the previous two studies, length of coherently read text is strongly related to relevance. Therefore, terms occurring in long coherently read text parts should be better descriptors of the user’s topic of interest than terms in short read text parts.
In more detail, the method uses the same large context document d as in the previous method. It creates two filtered context documents dP and dN . Document dP contains all parts of d that have been read or skimmed and that have a “coherently read text length” of at least l characters. On the contrary, dN contains all parts of d that have been read or skimmed but whose “coherently read text length” has been smaller than l characters. Next, scores for all terms w in dP are computed using the following formula.6
tf(w,dP) ×idf(w). (1) tf(w,dP)+tf(w,dN)
t f (w, dX) denotes the term frequency of term w in document dX.
Based on this computation, terms that are very specific for long read text parts will
get higher scores than terms that also appear in short read text parts.
Method ReadExtremes(l1, l2). This method is an immediate extension of the previous method. It also uses the same large context document d and creates two filtered context documents dP and dN. However, document dP contains all parts of d that have been read or skimmed and that have a “coherently read text length” of at least l2 characters, and dN contains those that are smaller than l1 characters. Scores for all single terms included in dP and dN are computed as in the previous method.
The difference to the method ReadLength(l) is that only these read text passages are used as positive or negative feedback that are close to both extremes of the distribution in terms of coherently read text length. Read text passages with an intermediate length are ignored.
Method FullDocument. As a simple baseline, this method just uses information about what documents have been viewed before issuing a query. Therefore, it just extracts the most characteristic terms of the four documents we provided to the participants. The documents are not filtered any further. Most characteristic terms are determined according to their t f × idf values.6
Method QueryFocus. This method can be seen as a pseudorelevance feedback method on the segment level of documents. It takes into account which documents the partici- pant has viewed before issuing the query, and it also considers the issued query itself. The query is used to find these parts of the previously viewed documents that are relevant to the query at hand.
In more detail, the context document d (as before) is split into a set of paragraphs. For each paragraph, a ranking score is determined with respect to the user query using Lucene7 and a BM25 ranking function. Then, the best paragraphs are selected, that is, the paragraphs achieving a ranking score not worse than half of the ranking
6For the computation of idf values Wikipedia was used as a document corpus. 7 http://lucene.apache.org/.
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:19
score achieved by the best matching paragraph. A virtual document dP is created concatenating all selected paragraphs. As before, the most characteristic terms are then extracted from dP based on their t f × idf scores.
5.1.3. Personalization Procedures. There were mainly two reasons why we decided to examine the effect of the implicit feedback methods using two procedures, i.e., re- ranking and query expansion.
First, reranking has the technical advantage that implicit feedback methods (i.e., the method ReadLength(l) with different parameter settings l can be examined even after an experimental run is over. All the data needed to compute the quality of feedback methods can be stored offline: eye tracking data for generating feedback, and relevance judgments for the top k results of the original user query. This is not possible using the query expansion procedure since every new feedback method results in a new unique query. Every unique query can result in a completely different set of result documents for which we needed to ask the participant again to provide relevance judgments.
Second, query expansion has the potential advantage of higher personalization im- pact. Since every expanded query is unique, it can retrieve new documents from the Web and can exclude others compared to the nonexpanded user query. Therefore, we assumed that expanding the original user query with additional terms would have much larger effects on the quality of the final ranking than just reranking the top k results for the original user query.
Reranking Procedure. For this procedure, each user query was first submitted to the Live Search engine using their SDK in order to retrieve the top 20 result Web pages. The pages were automatically downloaded and stored locally. The results were presented to the participants in randomized order (whom we informed about the randomization). After collecting relevance judgments for all 20 results from the participant, we had all necessary data for offline posteriori reranking:
—the original user query,
—the top 20 resulting Web documents,
—relevance judgments for these documents,
—eye tracking data specifying how the user has viewed the four assigned documents
during the reading phase of an experimental session.
To re-rank the top 20 results we performed several steps: First, Lucene8 was used to generate an index over the top 20 result documents. Second, each implicit feedback method to evaluate was executed on the four provided, previously read documents resulting in a list of characteristic terms paired with their achieved scores. Third, the original user query was expanded by the extracted terms weighted by their scores. Weights were also added to the original user query terms so that the weight of all expansion terms together accounted for 60% of all term weights. Overall, an expanded query contained at most 19 terms (due to technical reasons). Fourth, Lucene was applied to rerank the top 20 original result documents using the expanded query.
Query Expansion Procedure. With this procedure we aimed at comparing some of the feedback methods from above. Based on the results we got from the reranking procedure (which was analyzed first; see the result section for further details), we decided to consider the methods Reading as a representative gaze-based feedback method and QueryFocus as the best non-gaze-based method. We further wanted to compare the results to the original ranking from the Web search engine Live Search for the original user query.
8 http://lucene.apache.org/.
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    9:20 G. Buscher et al.
To expand and evaluate the queries, we proceeded as follows. First, each implicit feedback method to evaluate extracted a list of most characteristic terms from the four assigned documents. Second, each user query was expanded with the top m extracted terms so that the resulting expanded query had the following syntax:
termU1...termUn (termE1 OR...ORtermEm)
Hence, the original user search terms termUi appeared in a space-separated list whereas the extracted terms termEi were connected with OR operators. Due to technical reasons of the Web search engine Live Search, a query contained at most 19 terms again, that is, n + m ≤ 19. Third, each expanded query was submitted to Live Search. Fourth, the top returned results from the original user query and all expanded query variants were merged together and presented to the participant in randomized order. All presented documents were then judged by the participant so that we could compute and compare the quality of the returned result lists from the original and the expanded queries.
5.1.4. Apparatus. The experiment was performed using the same Tobii 1750 eye tracker that was applied before in the first study (Section 4.1.2). The screen resolution was kept at 1280x1024 pixels. Before starting an experimental run, the device was calibrated using ClearView and a 9-point calibration method. To detect reading, we used the same implemented methods and techniques as before. The documents and the search interface were presented in a Firefox browser.
5.1.5. Participants. The study was performed by 32 participants (6 female). Most of them were undergraduate students, some were graduates taking a variety of different majors. All of them had German as their native language. Their age ranged between 20 and 28 years (mean = 22.7, σ = 1.7).
A first group of 17 participants was assigned to the re-ranking procedure. The re- maining 15 participants were assigned to the query expansion procedure.
5.1.6. Measures. Reading-related measures. As in the first study, we applied our read- ing and skimming detection method to classify gaze traces as either belonging to read- ing, to skimming, or to non-reading-related behavior. Based on the eye movements belonging to reading or skimming, we computed the same gaze-based measures as be- fore on the level of paragraphs in the documents. Hence, the structure of the data used for the analysis was as follows: for every participant and every paragraph in each doc- ument we had a set of gaze traces and corresponding eye movement measures and we knew whether the respective paragraph was relevant to the topic of the task at hand.
Information retrieval measures. To measure the quality of the different search result lists generated based on the different implicit feedback methods, we computed three well accepted information retrieval metrics, each focusing on different aspects:
—Precision at K. P(K) computes the fraction of relevant documents within the top K results. However, the order of the top K documents does not matter. Since this measure needs binary relevance judgments, we split the 6-point relevance scale that was used in the experiment into two groups – positive and negative.
—DCG at K. The Discounted Cumulative Gain [Ja ̈rvelin and Keka ̈la ̈inen 2000] DCG(K) is different from P(K) in that it considers the order of the top K results and in that it is not bound to binary relevance judgments. It is computed as
DCG(K) =  K  2r( j) − 1 /log(1 + j) (2) j=1
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:21
 100% 80% 60% 40% 20% 0%
84%
read text as a frac on of relevant text
3%
read text as a frac on of irrelevant text
  100% 80% 60% 40% 20% 0%
72%
relevant text as a frac on of read text
28%
irrelevant text as a frac on of read text
  Fig. 5. Distribution of reading behavior. Left: read text as a fraction of relevant/irrelevant text. Right: relevant/irrelevant text as a fraction of read text.
Table III. Means and Standard Deviations for All Eye Movement Measures from the Second Study
r( j) corresponds to the relevance score of the jth result entry. In our case, the judg- ments + + +, ++, + led to relevance scores of 3, 2, and 1, respectively, whereas all negative judgments got a relevance score of 0.
—MAP. Mean Average Precision is a single value metric to measure the quality of a set of result lists. It is calculated as the mean of average precision values for all result lists in the set. Concerning one result list, average precision is computed as the mean of the P(K) values after each relevant document was retrieved.
5.2. Results: Reading Behavior
5.2.1. Validation: Distribution of Reading Behavior. Figure 5 shows the distribution of read- ing behavior on the four assigned documents with respect to relevance of the read text parts. In general, 84% of the text that was relevant has been read (Figure 5 left) and 72% of all text that has been read was relevant (Figure 5 right).
Discussion. The results shows that reading behavior is very focused on the relevant parts of the text. Irrelevant parts are largely ignored. Compared to the previous study, the tendency to skip irrelevant parts is much greater (i.e., 34% of irrelevant off-topic documents have been read or skimmed in the previous study, while only 3% or irrelevant parts have been read or skimmed in this study). One of the reasons for this tendency is the different document structure in this study: most of the document parts were irrelevant to the topic of the task. In contrast, most of the documents in the previous study were completely on topic and most of them were judged relevant.
Almost 3/4 of all text that has been read or skimmed was relevant. This shows that reading behavior alone is already a good indicator for relevance.
5.2.2. Gaze-Based Measures. Table III gives an overview of mean values and stan- dard deviations for the measures that were also discussed in the first study. “Regres- sion ratio” and “average fixation duration” are both computed in their individually
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.
 measure
    relevan  mean σ
            rrelevant mean σ
   mean forward saccade length * thorough reading ra o * coherently read text length * regression ra o (personalized) * average fixa on dura on (personalized)
  7.8 le ers 0.53 351 le ers 0.41 0.51
         3.62 0.35 278 0.24 0.22
11.8 le ers 0.40 121 le ers 0.30 0.49
 7.68 0.45 163 0.33 0.25
       % (± SEM)
% (± SEM)
    9:22 G. Buscher et al.
  100% 80% 60% 40% 20% 0%
86% 83%
read text as a frac on of relevant text
first view second view
5.4% 0.9%
read text as a frac on of irrelevant text
   100% 80% 60% 40% 20% 0%
85%
relevant text as a frac on of read text
first view
second view 41%
15%
irrelevant text as a frac on of read text
 59%
 Fig. 6. Distribution of reading behavior split by first and second view of each document. Left: read text as a fraction of relevant/irrelevant text. Right: relevant/irrelevant text as a fraction of read text.
personalized forms, as before. An asterisk signifies that the differences of the means on relevant and on irrelevant text parts are statistically significant (student’s unpaired, two-sided t-test using a significance level of α < 0.01).
Compared to the results from the first study, we see the exact same trends in this study: The length of saccades is typically smaller on relevant text sections. “Thorough reading ratio,” “coherently read text length,” and “regression ratio” are increased on relevant parts. Like in the previous study, we could not determine any statistically significant difference in “average fixation duration” with respect to relevance.
However, when comparing the specific mean values of the measures for relevant and irrelevant parts between the two studies, then some differences are evident. “Mean forward saccade length” has a much wider value range in this study. The values for “thorough reading ratio” are more spread in the previous study. “Coherently read text length” is generally shorter in this study. In contrast, the value distribution of personalized “regression ratio” has very similar characteristics in both studies.
Discussion. In general, the results verify and confirm the findings from the first study. They show that reliable signals can be found in the four measures “mean forward sac- cade length,” “thorough reading ratio,” “coherently read text length,” and “regression ratio” with respect to relevance. Again, “coherently read text length” is the most ex- pressive measure.
However, the mean values for most of the measures seem to depend on further factors related to task and document structure and are different in both studies. Therefore, it might be difficult to find generic classifiers for predicting relevance that work in a variety of different settings since most of the measures are not robust enough and are not independent from task and document structure. In this respect, personalized “regression ratio” seems to be the most robust measure considering both study settings.
5.2.3. Differences Between First-Time and Second-Time Reading. Every document has been presented twice to every participant, once for each of the two topics. Figure 6 shows the distribution of reading behavior again, but now split by first and second view of each document. While a similar amount of the relevant document parts were read during both views, 6 times less irrelevant text was read during the second view (Figure 6, left). This is also reflected in the amount of relevant or irrelevant text as a fraction of all read text (Figure 6, right).
We also analyzed whether there were differences concerning the value distribution of the measures with respect to first and second view of a document. We found a significant difference concerning coherently read text length being longer for first views than for second views, both for relevant and irrelevant text parts (to determine significance we
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.
    % (± SEM)
% (± SEM)
    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:23 Table IV. MAP and DCG at K = 10 for Different Parameter Settings l for the Methods
ReadLength(l) and ReadExtremes(l1, l2)
used Bonferroni correction of α = 0.001). Beyond that, we could not find any further sig- nificant differences between first and second view concerning the remaining measures.
Discussion. The results demonstrate that reading behavior becomes much more fo- cused on relevant document parts if users have already seen a document and are familiar with its structure. This means that reading behavior alone is already very precise in pointing out relevant parts of known documents (with a precision of 85% and a recall of 83%).
Interestingly, most of the eye movement measures are very robust and do not show significant differences with respect to document familiarity. This is important to know when trying to build generic classifiers to determine relevance based on eye movements.
5.3. Results: Implicit Feedback
All 32 participants together issued 192 queries and gave relevance judgments for 3497 result entries. We describe our findings in this section as follows: First, we evalu- ate the best parameter settings for the implicit feedback methods ReadLength(l) and ReadExtremes(l1,l2). Second, we compare the quality of the result lists produced by the different implicit feedback methods, both using the re-ranking as well as the query expansion procedure.
5.3.1. Evaluating Parameter Settings. Based on the data from the re-ranking procedure, we could test several values for the parameter l of the method ReadLength(l) and the parameters l1 and l2 of the method ReadExtremes(l1, l2). Table IV shows MAP and DCG scores at K = 10 after re-ranking the top 20 search results from the Web search engine based on the two implicit feedback method. The best rankings could be achieved with a parameter l = 50 characters, and l1 = 100, l2 = 400 characters.
Discussion. Given the results from the previous analyses, it is not surprising that we can see slight improvements of the ranking quality when filtering read text by coherently read text length. Regarding the method ReadExtremes(l1,l2) we find that the best parameters are close to the mean values for coherently text lengths on relevant and irrelevant texts, and overall this method seems to be superior to ReadLength(l). This shows that read text passages of lengths between about 100 and 400 characters introduce a considerable amount of noise and should not be used as positive or negative relevace feedback.
5.3.2. Comparison of Method Performance. The diagrams in Figure 7 show the per- formances of the different implicit feedback methods with respect to the reranking procedure as well as the quality of the original ranking from the Web search engine. In addition, Table V presents MAP and DCG scores at K = 10 both for the reranking and the query expansion procedure. Asterisks denote statistical significance of the
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.
  Variant
  MAP
DCG
ReadLength(0 chars)
0.747
9.14
ReadLength(50 chars)
0.749
9.18
ReadLength(100 chars)
0.733
9.17
ReadLength(150 chars)
0.736
9.15
ReadLength(200 chars)
0.734
9.06
ReadLength(250 chars)
0.740
9.26
ReadLength(300 chars)
0.743
9.18
ReadLength(350 chars)
0.737
9.14
 Variant
 MAP
 DCG
ReadExtremes(100,300)
0.752
9.23
ReadExtremes(100,350)
0.748
9.26
ReadExtremes(100,400)
0.756
9.28
ReadExtremes(100,450)
0.753
9.31
ReadExtremes(150,400)
0.755
9.27
ReadExtremes(50,400)
0.753
9.27

 9:24
0.80
0.75
0.70
0.65
P(K)
0.60 0.55 0.50 0.45
0.60 0.50 0.40 0.30
10
Reading
ReadLength(50) 9 ReadExtremes(100,400) FullDocument 8 QueryFocus
Original
G. Buscher et al.
Reading ReadLength(50) ReadExtremes(100,400) FullDocument QueryFocus
Original
1 2 3 4 5 6 7 8 9 10
P(K)
0.70 0.60 0.50
1 2 3 4 5 6 7 8 9 10
Reading ReadExtremes(100,400) QueryFocus
ReadLength(50) FullDocument Original
1.00
0.90
0.80 QueryFocus
KK
Fig. 7. Precision and DCG at recall levels K for the re-ranking procedure.
1 2 3 4K5 6 7 8 9 10
Fig. 8. Precision at different recall levels K for poorly performing queries (left) and highly performing
queries (right).
Table V. MAP and DCG at K = 10 for the Reranking and Query Expansion Procedure
Variant
(1) Reading
(2) ReadLength(50)
(3) ReadExtremes(100,400) (4) FullDocument
(5) QueryFocus
(6) Original
MAP
0.748 *4,5 0.749 *4,5 0.756 *4,5 0.697 0.709 0.729
DCG
9.17 *4,5 9.18 *4,5 9.28 *4,5 8.83 9.06 8.66
7
DCG(K)
6 5 4 3
P(K)
Original
Re-Ranking
Query Expansion MAP DCG
0.762 8.58 *6
0.751 8.21 0.751 8.05
differences to the respective numbered methods (determined by a student’s paired, two-sided t-test using a significance level of α < 0.05).
While the gaze-based feedback methods produced significantly better rankings than the non-gaze-based feedback methods, none of the four implicit feedback methods led to significantly better or worse rankings compared to the original ranking from the Web search engine. Hence, inspired by Agichtein et al. [2006] we further explored the effect of implicit feedback by dividing all user queries into two groups: poorly performing queries where the Web search engine produced rankings with MAP scores ≤ 0.7, and highly performing queries (MAP > 0.7). The former group contained 38, the latter group 64 queries.
Figure 8 shows precision-recall diagrams for poorly and highly performing queries with respect to the reranking procedure. Table VI provides respective MAP and DCG values at K = 10. Asterisks denote statistical significance of the differences as before.
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.
1 2 3 4K5 6 7 8 9 10
Reading ReadLength(50) ReadExtremes(100,400) FullDocument
    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:25
Table VI. MAP and DCG at K = 10 for Queries with Poor and High Original Performance
   Variant
    MAP
MAP of original ranking
<= 0.7 > 0.7
DCG MAP DCG
      (1) Reading
      0.63 *6
6.64 *4,6
      0.82 *4,5
10.67
 (2) ReadLength(50)
0.63 *4,6
6.63 *4,6
0.82 *4,5
10.69
 (3) ReadExtremes(100,400)
0.63 *4,6
6.49 *6
0.83 *4,5
10.95 *5
 (4) FullDocument
0.58 *6
6.00
0.76
10.52
 (5) QueryFocus
0.62 *6
6.80 *1,2,4,6
0.76
10.40
 (6) Original
0.50
5.18
0.86 *4,5
10.73
                   Discussion. Largest gains were achieved by method ReadExtremes(100,400), that is, a 8.5% gain in MAP compared to the simple non-gaze-based feedback method FullDocument, and a 7.2% gain in DCG compared to the original ranking from Live Search. All three gaze-based feedback methods lead to overall improvements compared to the original ranking from the Web search engine. In contrast, the two non-gaze-based methods could also lead to impairments, especially in the re-ranking scenario.
This difference in quality between the three gaze-based and the two non-gaze-based methods becomes particularly evident with regard to their performance on highly per- forming queries. The methods FullDocument and QueryFocus significantly worsened retrieval performance. In contrast, we could not find significant impairments caused by the gaze-based methods. However, with respect to poorly performing queries, all implicit feedback methods greatly improved the quality of the ranking, that is, up to 27% in MAP.
5.4. Conclusion
Overall, the results of this study verify and confirm the findings from the first study and demonstrate the value and usefulness of gaze-based feedback as implicit relevance feedback in information retrieval.
Concerning reading-related measures we found the same relationships as in the previous study, even in a different setting with long documents containing relevant and irrelevant parts, and for a more goal-directed task: First, the amount of reading behavior is strongly influenced by and very focused on relevant text. Second, the ana- lyzed gaze-based measures show the same trends concerning increases or decreases on relevant or irrelevant text. However, the distributions of their absolute values are dif- ferent from the previous study (i.e., their means). They seem to depend on the general experimental setting including task and document structure. Third, “fixation duration” seems to be indifferent to relevance again; there were again no significant differences to detect in this respect.
The factor of document familiarity has considerable effects on how users view documents. If users have previous knowledge about the document structure, then they are much more focused while reading. But interestingly, document familiarity had no noticeable effects on eye movement measures with respect to relevance. It did not influence the general trends concerning increases or decreases of the measures on relevant text.
An important insight that can be drawn from both studies is that classifiers to detect relevance based on eye movement measures will not be trivial to build and need to consider relative differences within their value distributions. Classifiers that are just based on absolute values of the measures are unlikely to work for two reasons. First, there are great individual differences. They can be accounted for when personalizing
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    9:26 G. Buscher et al.
the measures. Second, there are differences with respect to the general setting, that is, task and document structures. How these setting-induced differences can be accounted for is not yet clear; further research is needed here.
Applying gaze-based feedback as relevance feedback in information retrieval turns out to be very useful. Compared to the non-gaze-based feedback methods, it leads to considerable improvements of the search result list quality both using a reranking and a query expansion procedure. Interestingly, the gaze-based methods are even signif- icantly better than QueryFocus, which is the only method incorporating information about the current user query and therefore may use document parts directly related to the query as feedback. However, document parts that have been read or skimmed before are evidently more useful as implicit feedback.
Compared to the original result list quality from the Web search engine, the effects of the feedback methods are less distinct. With respect to queries that lead to poor result list qualities from the Web search engine, all implicit feedback methods lead to great improvements. Particularly QueryFocus performs at a comparable level as the gaze- based methods. However, when focusing on queries already yielding good quality result lists then both non-gaze-based methods entail considerable impairments. In contrast, the gaze-based methods do not worsen the result list quality much.
Surprisingly, we could not determine great differences between the three gaze-based methods. We expected to see much larger improvements from the method ReadLength(l) and ReadExtremes(l1,l2) since they additionally apply the advanced measure “coher- ently read text length” which has been proven to be most expressive with respect to relevance. This leads to the conclusion that a bit more irrelevant text as context (used by the method Reading) does not hurt retrieval performance much. Hence, reading and skimming behavior alone without considering any more advanced gaze-based mea- sures is already a very effective source for implicit relevance feedback for information retrieval.
However, it has to be kept in mind that this is an exploratory study with specific assumptions on task and document characteristics. First, the documents the partic- ipants could read had a very similar structure in that their subsections dealt with largely distinct subtopics (as it is typically the case with Wikipedia articles). Relevant and irrelevant text sections may be less differentiable in other types of documents so that the identification of relevant read text sections may be more difficult, and thus, the relevance measures would be more noisy. Second, whereas the goal-directed task of reading up on a topic for information gathering and then searching for more in- formation on the Web is fairly frequent and generalizeable, time pressure can differ vastly. This may have a significant effect on reading behavior that remains to be es- timated in future work. Third, as we demonstrated in this study, reading behavior changes considerably with document familiarity. While our study showed that expres- sive relevance-related eye movement measures are not much affected by changes in short-term document familiarity, there may be considerable effects coming from long- term document familiarity, that is, if the user is familiar with the document for a long time through multiple re-readings.
Yet overall, this study demonstrates the usefulness of gaze-based feedback for infor- mation retrieval personalization. There are many areas of application for such rather simple implicit feedback, for instance, as a search filter in order to refind previously read document parts, as a document interaction measure that can be used to highlight document parts the user has paid most attention to, as a source for creating topical con- texts for a user that can be applied as implicit feedback for information retrieval, etc. In the following, we give an outlook of a potential future set of applications: attentive documents.
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:27
[Rayner 1998], the eye shows a very characteristic behavior composed of fixations and saccades. A fixation is a time of about 250ms on average when the eye is steadily gazing at one point. A saccade is a rapid, ballistic eye movement from one fixation to the next. The mean left-to-right saccade size is 7-9 letter spaces. It depends on the font size and is relatively invariant concerning the distance between the eyes and the text.
An enormous amount of research has been done during last one hundred years concerning eye movements while reading. When reading silently, as summed up in [Rayner 1998], the eye shows a very characteristic behavior composed of fixations and saccades. A fixation is a time of about 250ms on average when the eye is steadily gazing at one point. A saccade is a rapid, ballistic eye movement from one fixation to the next. The mean left-to-right saccade size is 7-9 letter spaces. It depends on the font size and is relatively invariant concerning the distance between the eyes and the text.
                                                                       Annotation (Read) Delete author: Georg
start date: 07.12.2009 10:46:08 End date: 07.12.2009 10:46:12 length: 226 chars
mean fixation duration: 217ms mean saccade length: 9.4 chars regression ratio: 13.9%
 Fig. 9.
tations containing several eye movement measures (bottom).
An attentive Wiki document records what the user is reading (top) and automatically creates anno-
6. OUTLOOK: ATTENTIVE DOCUMENTS
Attentive documents are documents that keep track of how they are used. This is not only restricted to editing processes which can already be tracked by modern commercial word processing systems. Since users generally read more than they write, information about how documents and their parts are consumed (and read in particular) can be even more important. Therefore, information about what document parts have been read and how they have been read should be stored in association with the documents. As we demonstrated, eye tracking is overall very well suited to detect reading behavior and to determine differences in reading behavior, e.g., with respect to user-perceived relevance.
We built a prototype system described in detail in van Elst et al. [2008] and Kiesel et al. [2008] illustrating how attentive documents could work. The prototype system itself is a Wiki that allows for semantic annotations of text parts in documents. It can be used to manage the personal information space on the user’s desktop. When a user is reading some part of a Wiki document, our reading detection method detects reading or skimming behavior on the fly, computes appropriate gaze-based measures, and then sends this information to the Wiki. Subsequently, the Wiki creates an annotation for the read text passage containing the values from the gaze-based measures. Figure 9 shows an example of such an annotation. Of course, these annotations can be displayed to the user, but they are not visible per se.
Such attentive documents make it possible to design new and innovative applications. For example in the search domain, it is conceivable to use feedback from reading directly as implicit relevance feedback. From this, Web search can effectively be personalized (as demonstrated in study 2). Furthermore, when trying to refind information on the desktop, search filters could be employed that focus and search only in these document parts that have been viewed before by the user. Also, since a system managing attentive documents (such as the local Wiki) knows what the user has read or skipped, it could point out information that is relevant and new to the user, that is, information that has not been read before.
Apart from search, new kinds of applications can be imagined when focusing on the reading process of one particular document. For instance, when a user opens a document that he or she has read a while ago, the system could highlight the parts of
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    9:28 G. Buscher et al.
the document that have been most relevant before. This could help the user to refresh his or her memory. Because contextual clues help to find and reconstruct the thoughts one had before, such highlighting of formerly read text parts can speed up the process of recontextualizing. Furthermore, to make reading itself more efficient, the system could notice when the user is just scanning a document very quickly in order to get a rough idea of its contents. In this case, words in the text that do not convey much content (e.g., stop words) can be grayed out so that the scanning process gets more focused and effective [Biedert et al. 2010b]. Additionally, it is also conceivable that the system can detect difficulties of understanding during reading, e.g., on words in a foreign language. Then, it could automatically provide translations or further explanations [Hyrskykari et al. 2003]. Applications aiming at reading entertainment are also imaginable [Biedert et al. 2010a].
The sketched use-cases are just some compelling applications for gaze-based feedback from reading documents. An almost arbitrary number of further applications can be imagined.
7. SUMMARY AND CONCLUSION
In conclusion, it can be stated that gaze-based feedback about what has been read and how it has been read is very valuable to determine whether viewed document parts have been relevant to an individual user. Furthermore, when using this information as implicit relevance feedback it can greatly improve and personalize information retrieval methods.
Reading behavior is very focused on relevant parts of documents, especially when users are working with long documents, and there is strong evidence that individually perceived relevance of read text influences eye movement measures like the number of regressions during reading, the typical length of saccades, etc. We determined the re- lations between relevance and gaze-based measures and validated them in two studies using different task types and document structures. Interestingly, the popular measure “fixation duration” does not seem to be related to perceived relevance.
We found good relations between gaze-based measures and user-perceived relevance, as well as a great variation in the measures caused by individual differences and dif- ferences in task and document structure. Individually personalizing the measures can greatly improve their expressivity and can account for individual differences. However, how variation caused by task and document structure can be accounted for is not clear yet and stays for further research.
Since users typically read in a very focused way, information about what document parts have been read can be used as an indicator of relevance. We demonstrated its usefulness as implicit relevance feedback for personalizing Web search in a further study. The results of the study show that gaze-based feedback is much more effective than non-gaze-based relevance feedback baselines.
Finally, in an outlook, we sketched potential innovative applications based on at- tentive documents, i.e., documents that keep track of how they have been read by the user.
ACKNOWLEDGMENTS
We thank Tristan King for his help improving the style of the paper.
REFERENCES
AGICHTEIN, E., BRILL, E., AND DUMAIS, S. 2006. Improving web search ranking by incorporating user behavior information. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’06). ACM, New York, NY, 19–26.
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:29
AHN, J. W., BRUSILOVSKY, P., HE, D., GRADY, J., AND LI, Q. 2008. Personalized web exploration with task models. In Proceedings of the 17th International Conference on World Wide Web (WWW’08). ACM, New York, NY, 1–10.
AJANKI, A., HARDOON, D., KASKI, S., PUOLAMA ̈ KI, K., AND SHAWE-TAYLOR, J. 2009. Can eyes reveal interest? Implicit queries from gaze patterns. User Model. User-Adapt. Interact. 19, 307–339.
BALATSOUKAS, P. AND RUTHVEN, I. 2010. The use of relevance criteria during predictive judgment: An eye tracking approach. Proc. Amer. Soc. Info. Sci. Techn. 47, 1, 1–10.
BIEDERT, R., BUSCHER, G., AND DENGEL, A. 2010a. The eyebook – using eye tracking to enhance the reading experience. Informatik-Spektrum 33, 3, 272–281.
BIEDERT, R., BUSCHER, G., SCHWARZ, S., HEES, J., AND DENGEL, A. 2010b. Text 2.0. In CHI’10: Extended Abstracts on Human Factors in Computing Systems. ACM Press, New York, NY, 4003–4008.
BROOKS, P., PHANG, K. Y., BRADLEY, R., OARD, D., WHITE, R., AND GUIMBRETIERE, F. 2006. Measuring the utility of gaze detection for task modeling: A preliminary study. In Proceedings of the International Conference on Intelligent User Interfaces (IUI’06). (Workshop on Intelligent User Interfaces for Intelligence Analysis).
BUSCHER, G., CUTRELL, E., AND MORRIS, M. R. 2009a. What do you see when you’re surfing?: using eye tracking to predict salient regions of web pages. In Proceedings of the 27th International Conference on Human Factors in Computing Systems (CHI’09). ACM, New York, NY, 21–30.
BUSCHER, G., DENGEL, A., AND VAN ELST, L. 2008a. Eye movements as implicit relevance feedback. In CHI’08: Extended Abstracts on Human Factors in Computing Systems. ACM, New York, NY, 2991–2996.
BUSCHER, G., DENGEL, A., AND VAN ELST, L. 2008b. Query expansion using gaze-based feedback on the subdoc- ument level. In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’08). ACM, New York, NY, 387–394.
BUSCHER, G., VAN ELST, L., AND DENGEL, A. 2009b. Segment-level display time as implicit feedback: a comparison to eye tracking. In Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’09). ACM, New York, NY, 67–74.
CHEN, Z. AND XU, Y. 2005. User-oriented relevance judgment: A conceptual model. In Proceedings of the 38th Annual Hawaii International Conference on System Sciences (HICSS’05). IEEE Computer Society, Los Alamitos, CA, 101.2.
CLAYPOOL, M., LE, P., WASED, M., AND BROWN, D. 2001. Implicit interest indicators. In Proceedings of the 6th International Conference on Intelligent User Interfaces (IUI’01). ACM Press, New York, NY, 33–40. COLE, M. J., GWIZDKA, J., BIERIG, R., BELKIN, N. J., LIU, J., LIU, C., AND ZHANG, X. 2010. Linking search tasks with
low-level eye movement patterns. In Proceedings of the 28th Annual European Conference on Cognitive
Ergonomics (ECCE’10). ACM, New York, NY, 109–116.
CUTRELL, E. AND GUAN, Z. 2007. What are you looking for?: an eye-tracking study of information usage in web
search. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI’07).
ACM Press, New York, NY, 407–416.
DAVENPORT, T. H. AND BECK, J. C. 2001. The Attention Economy: Understanding the New Currency of Business.
Harvard Business School Press.
VAN ELST, L., KIESEL, M., SCHWARZ, S., BUSCHER, G., AND LAUER, A. 2008. Contextualized Knowledge Acquisition
in a Personal Semantic Wiki. In Proceedings of the 16th International Conference on Knowledge Engi- neering and Knowledge Management (EKAW’08). Springer, Lecture Notes in Computer Science vol. 5268, 172–187.
FOX, S., KARNAWAT, K., MYDLAND, M., DUMAIS, S., AND WHITE, T. 2005. Evaluating implicit measures to improve web search. ACM Trans. Inform. Syst. 23, 2, 147–168.
GOLOVCHINSKY, G., PRICE, M. N., AND SCHILIT, B. N. 1999. From reading to retrieval: freeform ink annotations as queries. In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’99). ACM Press, New York, NY, 19–25.
GYLLSTROM, K. 2009. Passages through time: chronicling users’ information interaction history by recording when and what they read. In Proceedings of the 13th International Conference on Intelligent User Interfaces (IUI’09). ACM, New York, NY, 147–156.
HILL, W. C., HOLLAN, J. D., WROBLEWSKI, D., AND MCCANDLESS, T. 1992. Edit wear and read wear. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI’92). ACM Press, New York, NY, 3–9.
HYRSKYKARI, A., MAJARANTA, P., AND RA ̈IHA ̈, K.-J. 2003. Proactive response to eye movements. In Proceedings of the International Conferece on Human-Computer Interaction (INTERACT’03). 129–136.
JA ̈RVELIN, K. AND KEKA ̈LA ̈INEN, J. 2000. Ir evaluation methods for retrieving highly relevant documents. In
Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’00). ACM, New York, NY, 41–48.
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    9:30 G. Buscher et al.
JOACHIMS, T., GRANKA, L., PAN, B., HEMBROOKE, H., RADLINSKI, F., AND GAY, G. 2007. Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search. ACM Trans. Inform. Syst. 25, 2.
KELLY, D. AND BELKIN, N. J. 2001. Reading time, scrolling and interaction: exploring implicit sources of user preferences for relevance feedback. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’01). ACM, New York, NY, 408–409.
KELLY, D. AND BELKIN, N. J. 2004. Display time as implicit feedback: understanding task effects. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’04). ACM Press, New York, NY, 377–384.
KELLY, D. AND TEEVAN, J. 2003. Implicit feedback for inferring user preference: a bibliography. SIGIR Fo- rum 37, 2, 18–28.
KIESEL, M., SCHWARZ, S., VAN ELST, L., AND BUSCHER, G. 2008. Using attention and context information for annotations in a semantic wiki. In Proceedings of the 3rd Semantic Wiki Workshop (SemWiki’08).
LIVERSEDGE, S. P. AND FINDLAY, J. M. 2000. Saccadic eye movements and cognition. Trends Cogn. Sci. 4, 1, 6–14.
LOBODA, T. D., BRUSILOVSKY, P., AND BRUNSTEIN, J. 2011. Inferring word relevance from eye-movements of readers. In Proceedings of the 16th International Conference on Intelligent User Interfaces (IUI’11). ACM, New York, NY, 175–184.
MAJARANTA, P. AND RA ̈IHA ̈, K.-J. 2002. Twenty years of eye typing: systems and design issues. In Proceedings of the Symposium on Eye Tracking Research & Applications (ETRA’02). ACM, New York, NY, 15–22.
MELUCCI, M. AND WHITE, R. W. 2007. Discovering hidden contextual factors for implicit feedback. In Proceed- ings of the CIR’07 Workshop on Context-Based Information Retrieval (in conjunction with CONTEXT’07). MOE, K. K., JENSEN, J. M., AND LARSEN, B. 2007. A qualitative look at eye-tracking for implicit relevance feedback. In Proceedings of the 2nd International Workshop on Context-Based Information Retrieval.
B.-L. Doan, J. Jose, and M. Melucci, Eds., 36–47.
MORITA, M. AND SHINODA, Y. 1994. Information filtering based on user behavior analysis and best match text
retrieval. In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR’94). Springer, 272–281.
OHNO, T. 2004. Eyeprint: support of document browsing with eye gaze trace. In Proceedings of the 6th
International Conference on Multimodal Interfaces (ICMI’04). ACM, New York, NY, 16–23.
PUOLAMA ̈KI, K., SALOJA ̈RVI, J., SAVIA, E., SIMOLA, J., AND KASKI, S. 2005. Combining eye movements and collabo- rative filtering for proactive information retrieval. In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’05). ACM Press, New
York, NY, 146–153.
RAYNER, K. 1998. Eye movements in reading and information processing: 20 years of research. Psych.
Bull. 124, 3, 372–422.
ROCCHIO, J. 1971. The SMART Retrieval System: Experiments in Automatic Document Processing. Prentice
Hall, 313–323.
SALTON, G. AND BUCKLEY, C. 1990. Improving retrieval performance by relevance feedback. J. Amer. Soc. Inf.
Sci. 41, 4, 288–297.
SIMON, H. A. 1969. The Sciences of the Artificial. MIT Press.
SIMON, H. A. 1971. Designing organizations for an information rich world. In Computers, Communications
and the Public Interest. Johns Hopkins Press, 38–51.
WHITE, R. W. AND KELLY, D. 2006. A study on the effects of personalization and task information on implicit
feedback performance. In Proceedings of the 15th ACM International Conference on Information and
Knowledge Management (CIKM’06). ACM, New York, NY, 297–306.
WILCOX, R. 2005. Introduction to Robust Estimation and Hypothesis Testing, 2nd Ed. Elsevier Academic
Press.
XU, S., JIANG, H., AND LAU, F. C. 2009. User-oriented document summarization through vision-based eye-
tracking. In Proceedings of the 13th International Conference on Intelligent User Interfaces (IUI’09). ACM, New York, NY, 7–16.
Received December 2010; revised June 2011; accepted August 2011
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.
View publication stats
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities from Eye Gaze Data
BEN STEICHEN, CRISTINA CONATI, and GIUSEPPE CARENINI, University of British Columbia
Information visualization systems have traditionally followed a one-size-fits-all model, typically ignoring an individual user’s needs, abilities, and preferences. However, recent research has indicated that visualization performance could be improved by adapting aspects of the visualization to the individual user. To this end,
this article presents research aimed at supporting the design of novel user-adaptive visualization systems.
In particular, we discuss results on using information on user eye gaze patterns while interacting with a
given visualization to predict properties of the user’s visualization task; the user’s performance (in terms
of predicted task completion time); and the user’s individual cognitive abilities, such as perceptual speed,
visual working memory, and verbal working memory. We provide a detailed analysis of different eye gaze
feature sets, as well as over-time accuracies. We show that these predictions are significantly better than
a baseline classifier even during the early stages of visualization usage. These findings are then discussed 11 with a view to designing visualization systems that can adapt to the individual user in real time.
Categories and Subject Descriptors: H.5.m [Miscellaneous] General Terms: Human Factors, Experimentation
Additional Key Words and Phrases: Adaptive information visualization, eye tracking, adaptation, machine learning
ACM Reference Format:
Ben Steichen, Cristina Conati, and Giuseppe Carenini. 2014. Inferring visualization task properties, user performance, and user cognitive abilities from eye gaze data. ACM Trans. Interact. Intell. Syst. 4, 2, Article 11 (July 2014), 29 pages.
DOI: http://dx.doi.org/10.1145/2633043
1. INTRODUCTION
Information visualization is a thriving area of human-computer interaction that aims to help users in managing and understanding increasing amounts of information. Although visualization systems have gained in terms of general usage and usabil- ity, they have traditionally been designed using a one-size-fits-all approach, typically ignoring an individual user’s needs, abilities, and preferences. To better assist each user during visualization tasks, recent research has started to investigate novel user- adaptive visualizations that can dynamically infer relevant user characteristics and provide appropriate interventions tailored to these characteristics. Initial research on user-adaptive visualizations has already provided evidence for improved user perfor- mance (e.g., time on task, task accuracy), such as by using click behavior to infer and
The reviewing of this article was managed by associate editor Judy Kay.
Authors’ addresses: B. Steichen, C. Conati, and G. Carenini, Department of Computer Science, Univer- sity of British Columbia, Department of Computer Science, Vancouver, Canada; emails: steichen@cs.ubc.ca, conati@cs.ubc.ca, carenini@cs.ubc.ca.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212) 869-0481, or permissions@acm.org.
⃝c 2014 ACM 2160-6455/2014/07-ART11 $15.00
DOI: http://dx.doi.org/10.1145/2633043
  ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
11:2 B. Steichen et al.
adapt to suboptimal usage patterns [Gotz and Wen 2009], or by considering a user’s visualization selections to infer and adapt to a user’s visualization expertise and prefer- ences [Grawemeyer 2006]. In terms of intervention mechanisms, these initial systems have typically investigated recommending visualizations that are most suitable for the current task and/or appropriate for a particular user’s preference and expertise.
Our long-term goal is to extend this research on user-adaptive visualization in a number of aspects. First, we aim to expand the set of features that the system can adapt to by including visualization task properties (e.g., task type, task difficulty), as well as a user’s properties beyond expertise and performance, such as cognitive abilities that have been shown to influence visualization performance. Second, although existing research has looked at improving visualization performance solely using information on a user’s direct interaction (e.g., mouse clicks), we aim to provide assistance exploiting additional, potentially complementary data sources (e.g., eye tracking). Third, whereas existing work has focused on interventions that recommend alternative visualizations, we envision to also deliver interventions that can dynamically help the user with the current visualization (e.g., through highlighting relevant visualization elements).
In this article, we address the first two aspects by investigating to what extent a va- riety of visualization task properties (task type, complexity, and difficulty), the user’s performance (in terms of task completion time), the user’s visualization expertise, and three different cognitive abilities (perceptual speed, visual working memory, and verbal working memory) can be inferred from a user’s eye gaze behavior. For all of these dimen- sions, we found statistically significant results, except for user visualization expertise.
We focus on gaze behavior because visual scanning and processing are fundamental components of working with any visualization (and the only components for noninter- active visualizations). Specifically, we ask the following two research questions:
Q1. To what extent can a user’s current task, performance, and/or long-term cognitive abilities and visualization expertise be inferred from eye gaze data?
Q2. Which gaze features are the most informative?
The motivation of this work is twofold. First, to provide appropriate support, an adaptive system needs to know about the user’s current task characteristics, as well as her expected performance. For example, if the system knows that the user is cur- rently performing a “filter” task (i.e., trying to find data cases that satisfy a particular condition [Amar et al. 2005]) and appears to be slow (i.e., the user is predicted to have a high completion time), the system could adaptively de-emphasize nonrelevant data to reduce the user’s cognitive load. Correspondingly, if a system knows individual user characteristics, it will be able to provide user-specific support. For example, since cer- tain cognitive ability levels have already been shown to lead to lower performance (e.g., low perceptual speed leads to decreases in speed and accuracy [Conati and Maclaren 2008; Velez et al. 2005; Toker et al. 2012], low-ability users might benefit most from adaptive support. Furthermore, Carenini et al. [2014] have shown that the effect of visualization interventions can depend on such characteristics—for example, showing that a user’s subjective rating of different highlighting mechanisms is affected by visual working memory. Therefore, adaptive support not only requires identifying users who are currently “struggling” with the task but also consists of predicting and tailoring to each user’s individual characteristics.
Second, we are interested in determining which eye gaze features are most informa- tive for classification. As will be shown in this article, different task and user classifica- tions rely on different features, suggesting that adaptive applications need to monitor specific features depending on the intended adaptation purpose. Moreover, the discov- ery of the most discriminatory features might also provide new suggestions with regard
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities 11:3
to how the system can adapt to support the different tasks and/or user characteristics. For example, if the number of gaze transitions to a certain area of interest (AOI) (e.g., a graph’s legend) is found to be very high for users with low cognitive abilities, and knowing that these users are typically less efficient and/or effective on their tasks, we may want to provide help that focuses particularly on reducing the need for visits to this area. Similarly, by finding gaze behaviors that are often exhibited by high-ability users, we might devise adaptive interventions that can encourage low-ability users to also change to such behavior.
This research was first presented in Steichen et al. [2013]. Here, we expand on that work with additional experiments on predicting the task difficulty, as well as the users’ expected performance (in terms of completion time). Second, to better evaluate the feasibility of real-time classification, we calculated results at absolute time intervals for each of our experiments, such as the level of accuracy after seeing 5s of gaze data (see Section 5.1). Third, for each of our experiments, we also provide results for a dataset that only includes information on AOIs. Last, in addition to classification accuracy, we also calculated area under receiver operating characteristic (ROC) results—a measure commonly used in machine learning to measure the discriminatory power of models [Egan 1975]—to further strengthen the validity of our findings.
The remainder of this article is structured as follows. First, we provide an overview of related research in adaptive visualization and eye tracking, as well as the most recent findings on the impact of individual user differences in visualization. Next, we present the user study that provided the gaze data for our research. This is followed by a series of classification experiments that we ran on this gaze data to answer the research questions outlined previously. Finally, we conclude with a discussion of the overall findings and outline several directions for future work.
2. RELATED WORK
Adaptation and personalization have long been established as effective techniques to support individual users in a variety of tasks and applications, including personal- ized search and adaptive hypermedia [Steichen et al. 2012], and desktop assistance and e-learning [Jameson 2008]. By contrast, information visualization research has traditionally maintained a static, one-size-fits-all approach by ignoring an individual user’s needs, abilities, and preferences. In particular, early automatic visualization systems have focused only on adapting the visualization to task or data properties that are known a priori [Casner 1991; Mackinlay 1986] rather than dynamically infer- ring individual properties during visualization usage. An exception to this nonadaptive paradigm is presented in Grawemeyer [2006], where users’ visualization expertise and preferences are dynamically inferred through monitoring visualization selections (e.g., how long it takes a user to decide which visualization to choose). Using this inferred level of user expertise and preferences, the system then attempts to recommend the most suitable visualizations for subsequent tasks. Results from the user studies in Grawemeyer [2006] show that the recommendations indeed lead to better user per- formance in terms of task effectiveness (i.e., accuracy), as well as user efficiency (i.e., time on task). However, this work does not actively monitor a user during a task and thus cannot adapt in real time to help the user with the current task. In contrast, the system developed by Gotz and Wen [2009] actively monitors real-time user behavior during visualization usage to infer needs for intervention. In their work, interaction data (i.e., mouse clicks) are constantly tracked to detect suboptimal usage patterns— that is, activities of users that are of a repetitive (and hence inefficient) nature. Each of these suboptimal patterns indicates that an alternative visualization may be more suitable to the current user activity. The patterns used in their paper include scan- ning (a user is iteratively inspecting over similar visual objects), flipping (iteratively
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
11:4 B. Steichen et al.
changing filter constraints), swapping (repeatedly rearranging the order of data dimen- sions), and drilling (repeatedly filtering down along orthogonal dimensions). Once these patterns are detected, the system triggers adaptation interventions similar to those in Grawemeyer [2006], namely they recommend alternative visualizations that may be more suitable for the current activity (e.g., the location of a set of hotels may be best viewed using a map visualization rather than a user having to repeatedly drill down to this information for each result). However, there are a number of shortcomings of this work. First of all, the usage patterns, as well as the respective visualization recommen- dations, are determined by experts a priori rather than being based on experimental findings. Second, their system is only able to provide adaptations for visualizations that allow users to interact directly with the visualizations either through mouse clicks or other forms of direct user input. This approach therefore does not work if a user is simply “looking” at a visualization without manipulating its controls/data. Third, their patterns do not try to infer general (low-level) visualization tasks (e.g., filter, compute derived value). Last, their approach does not attempt to adapt to any individual user characteristics.
As mentioned previously, since visual scanning and processing are fundamental com- ponents of working with any visualization (they are in fact the only components for noninteractive visualizations), it is important to consider eye tracking as a source of real-time information on user behavior. Although this technology is currently confined to research environments (mostly due to the high cost of eye-tracking devices), the rapid development in affordable, mainstream eye-tracking solutions (e.g., using stan- dard Web cams) will enable the widespread application of these techniques in the near future [Sesma et al. 2012]. In the field of cognitive and perceptual psychology, the use of eye tracking has long been established as a suitable means for analyzing user at- tention patterns in information processing tasks [Rayner 1998]. Similarly, research in this field has investigated the impact of individual user differences on basic reading and search tasks [Rayner 1995]. More recently, the fields of human-computer inter- action and information visualization have also started to use eye-tracking technology to investigate trends and differences in user attention patterns and cognitive/decision processing. This research has typically focused on either identifying pattern differences for different visualizations [Goldberg and Helfman 2011] or task types (e.g., reading vs. mathematical reasoning) [Iqbal and Bailey 2004], or on explaining differences in user accuracy between alternative interfaces [Plumlee and Ware 2006]. However, these studies have generally only attempted to gain insights into differences in gaze behav- iors for different tasks and/or interfaces rather than providing a means for directly driving adaptive systems. In particular, these analyses have typically consisted of of- fline processes that require further human analysis (e.g., manually analyzing eye gaze coordinate plots [Iqbal and Bailey 2004]). In terms of actually using raw eye-tracking data for real-time prediction, most research has so far focused on identifying the user’s cognitive processes while she is performing nonvisualization activities, such as during exploratory e-learning [Kardan and Conati 2012; Conati and Merten 2007], quizzes [Courtemanche et al. 2011], simple puzzle games [Eivazi and Bednarik 2011], or infor- mation search tasks (e.g., word search) [Simola et al. 2008]. By contrast, our gaze-based work focuses on information visualization, where a user’s main activity is to perform simple visualization lookup and comparison tasks.
It is also important to note that none of the preceding approaches has attempted to adapt to user differences other than expertise. However, recent research has shown that other user traits can in fact significantly influence task performance, espe- cially in the field of information visualization. For example, a user’s spatial abilities have been shown to influence a user’s performance in visual navigation [Chen and Czerwinski 1997] and information search tasks [Westerman and Cribbin 2000].
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities 11:5
Similarly, Ziemkiewicz et al. [2011] and Green and Fisher [2010] have looked at the influence of a user’s personality traits, showing that locus of control (internal vs. ex- ternal) can impact visualization performance. Cognitive measures such as perceptual speed and visual working memory have particularly been shown to influence a user’s ability to complete basic visualization tasks effectively [Conati and Maclaren 2008; Velez et al. 2005]. For example, it has been shown that users with high perceptual speed have significantly faster completion times and accuracy on certain tasks. These results have been confirmed and extended in a recent study by Toker et al. [2012], where perceptual speed, visual and verbal working memory, and user expertise were shown to influence not only a user’s task performance but also satisfaction regarding different visualization types. Most recently, it was found that these individual user differences have an impact on different user eye gaze measures Toker et al. [2013], which directly serves as the motivation for the work in this article on using gaze data to dynamically identify and adapt to user cognitive abilities.
3. USER STUDY
As mentioned in the Introduction, this article is part of our ongoing work on designing user-adaptive information visualizations. In particular, our research studies both the effect that different user characteristics have on visualization performance and the real-time detection of task and user characteristics to be able provide appropriate interventions (the focus of this article). For these purposes, we conducted a user study during which users had to perform a battery of visualization tasks using two alternative basic visualization techniques, namely bar graphs (Figure 1, top) and radar graphs (Figure 1, bottom). By choosing two different types of visualizations, we aimed to investigate the generality of our results.
Bar graphs were chosen because they are one of the most popular and effective visualization techniques. We chose radar graphs because although they are often con- sidered inferior to bar graphs on common information seeking tasks [Few 2005], they are still widely used for multivariate data. Furthermore, there are indications that radar graphs may be just as effective as bar graphs for more complex tasks [Toker et al. 2012].
3.1. Study Tasks
The task domain used in the study required users to evaluate the performance of one or two students in eight different academic courses (using an artificial dataset). We chose this domain to avoid an effect of participant’s domain expertise on our results. In the context of this domain, we developed a set of tasks that varied both in task type and task complexity. In terms of different task types, we based our questions on a set of general visualization tasks that had been identified by Amar et al. [2005] to be “representative of the kinds of specific questions that a person may ask when working with a data set.” To keep the study conditions manageable, we chose a selection of five task types: retrieve value (RV), filter (FI), compute derived value (CDV), find extremum (FE), and sort (SO). The types were chosen so that each of our two target visualizations would be suitable to support them. Example questions for each of these task types are shown in Table I.
To vary the task complexity, we differentiated between single and double tasks. Single tasks required participants to compare one student’s performance with the class aver- age for the eight academic courses (e.g., “In how many courses is Alice below the class average?”), whereas double tasks required participants to compare the performance of two students with the class average (e.g., “Find the courses in which Andrea is below the class average and Diana is above it.”). In total, our study comprised five single tasks, one for each task type (i.e., RV1, FI1, CDV1, FE1, SO1) and four double tasks (RV2,
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
11:6 B. Steichen et al.
 Fig. 1. Sample bar (top) and radar graph (bottom). Table I. Example Task Questions
CDV2, FI2a, FI2b), meaning that the most fine-grained task type/complexity classifi- cation could consist of nine classes (see classification experiments in Section 5.2).
3.2. Cognitive Abilities
The long-term user traits that we investigated in this study consisted of the following three cognitive abilities: perceptual speed (a measure of speed when performing simple perceptual tasks), verbal working memory (a measure of storage and manipulation
  RV
 Did Alice receive a higher mark in Marine Biology or Painting?
  FI
 In which course(s) is Mary above the class average? (Select all that apply).
  CDV
 In how many courses is Alice below the class average?
  FE
 In which course does Alice deviate most from the class average?
  SO
 What are Mary’s two strongest courses?
      ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities 11:7
capacity of verbal information), and visual working memory (a measure of storage and manipulation capacity of visual and spatial information). Perceptual speed and visual working memory were selected because they were among the perceptual abilities explored by Velez et al. [2005], as well as among the set that Conati and Maclaren [2008] found to impact user performance with radar graphs and a multiscale dimension visualizer (MDV). We also chose verbal working memory because we hypothesized that it may affect a user’s performance with a visualization in terms of how the user processes its textual components (e.g., legends).
3.3. Study Procedure
Thirty-five subjects (18 female) participated in the experiment, ranging in age from 19 to 35 years. Participants were recruited via advertising at our university, with the aim of collecting a heterogeneous pool with suitable variability in their cognitive abilities. Ten participants were computer science students, whereas the rest came from a variety of backgrounds, including microbiology, economics, classical archaeology, and film production. The experiment was designed and pilot tested to fit in a single session lasting at most 1 hour. Participants began by completing tests for three cognitive measures: a computer-based OSPAN test for verbal working memory [Turner and Engle 1989] (lasting between 7 and 12 minutes), a computer-based test for visual working memory [Fukuda and Vogel 2009] (10 minutes long), and a paper-based P-3 test for perceptual speed [Ekstrom and U.S. Office of Naval Research 1996] (3 minutes long). The experiment was conducted on a Pentium 4, 3.2GHz, with 2GB of RAM and a Tobii T120 eye tracker as the main display. Tobii T120 is a remote eye tracker embedded in a 17” display, providing unobtrusive eye tracking. After a short calibration of the eye tracker, participants underwent a training phase to familiarize themselves with the two visualizations and study tasks. Participants then performed 14 tasks per visualization—that is, 2 × 5 single and 1 × 4 double (note that each user saw the exact same 28 graphs). The presentation order with respect to visualization type was fully counterbalanced across subjects. Although there was still a remaining training effect (as previously reported in Toker et al. [2012] and recently discussed in Toker et al. [2014]), the counterbalancing ensured that the data was balanced for our classification experiments (albeit containing some potential noise that may have slightly decreased some classification accuracies).
For each task, users were presented with a radar/bar graph displaying the relevant data, along with a textual question (Figures 2 and 3). Participants would then select their answer from a drop-down list, along with their confidence in their answer (be- tween 1 and 5), and click OK to advance to the next task. The experimental software was fully automated and coded in Python.
4. EYE TRACKING MEASURES AND FEATURES
An eye tracker captures gaze information through fixations (i.e., maintaining gaze at one point on the screen) and saccades (i.e., a quick movement of gaze from one fixation point to another), which can be analyzed to derive a viewer’s attention patterns. For our experiments, we generated a large set of eye-tracking features by calculating statistics on basic eye-tracking measures (Table II).
Of these basic measures, fixation rate, number of fixations, and fixation Duration are widely used in eye tracking studies. In addition, we included saccade length (e.g., distance d in Figure 4), relative saccade angle (e.g., angle y in Figure 4) and absolute saccade angle (e.g., angle x in Figure 4), as suggested in Goldberg and Helfman [2010], because these measures are potentially useful for summarizing trends in user attention patterns within a specific interaction window, such as if the user’s gaze follows a planned sequence (as opposed to being scattered).
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
11:8
B. Steichen et al.
 Fig. 2.
Sample experimental screen of a single-complexity radar graph.
 Fig. 3.
Sample experimental screen of a double-complexity bar graph.
Table II. Description of Basic Eye-Tracking Measures
   Basic Gaze Measures
 Description
  Number of fixations
 Number of eye fixations detected during an interval of interest
  Fixation rate
 Number of fixations divided by time interval, e.g., fixations per millisecond
  Fixation duration
 Time duration of an individual fixation
  Saccade length
 Distance between the two fixations delimiting the saccade (d in Fig. 4)
  Relative saccade angles
 The angle between the two consecutive saccades (e.g., angle y in Fig. 4)
  Absolute saccade angles
 The angle between a saccade and the horizontal (e.g., angle x in Fig. 4)
       ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities 11:9
 Fig. 4. Saccade-based eye measures.
Fig. 5. The five AOI regions defined over a bar graph.
The raw gaze data from the Tobii eye tracker was processed using our open-source data analysis toolkit EMDAT, which is freely available for download and extension by the research community.1 The toolkit computes statistics such as sum, average, and standard deviation over the eye-tracking measures with respect to (1) the overall screen, to get a sense of the complete interaction with the task (high-level measures from now on) and (2) specific AOIs, identifying parts of the interface relevant for understanding a user’s attention processes during each task (AOI-level measures from now on). A total of five AOIs were defined for each of the two visualizations.
These regions were selected to capture the distinctive and typical components of the two visualizations used in the study. Figures 5 and 6 show how these AOIs map onto bar and radar graph components, respectively.
—High area: Covers the upper half of the data elements of each visualization. This area is the graphical portion of an Infovis that contains the relevant data values. On the bar graph, it corresponds to a rectangle over the top half of the vertical bars (see Figure 5); for the radar graph, it corresponds to the combined area of the eight trapezoidal regions covering the data points (see Figure 6).
1 https://www.cs.ubc.ca/group/iui/EMDAT/index.html.
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.

11:10 B. Steichen et al.
 Fig. 6. The five AOI regions defined over a radar graph.
—Low area: Covers the lower half of the data elements for each visualization. —Labels: Covers all of the data labels in each graph.
—Question text: Covers the text describing the task to be performed.
—Legend: Covers the legend showing the mapping between each student and the color
of the visualization elements that represent her performance.
The selection of these five AOIs is the result of a trade-off between having detailed information on user attention over areas that are salient for task execution and keeping the number of AOIs manageable for real-time computation. Note that the data values used in the experiment generally ranged between 40 and 100, thereby providing an opportunity to have both “High Area” and a “Low Area” AOIs. If the displayed data values were to range between 0 and 100, there would only be room for a single “Data Value” AOI. However, as will be shown in this article, the Low Area AOI did not play a significant role in our classification experiments; therefore, the results are likely to hold in case there is only one such Data Value AOI.
Overall, a total of 74 features were calculated from the gaze data (Table III). For experimental purposes, we differentiated between a feature set that contained all features, including high-level and AOI features (called the Full set from now on), one that did not contain features relating to AOIs—that is, only containing the task-level features (called the No AOI set), and one that only contained features relating to AOIs (called the Only AOI set). This differentiation was chosen to evaluate the relative information gain attained from AOI and non-AOI features—for instance, how much can be inferred from a user’s gaze with and without information on the specific visualization at which the user is looking (similar to what was done in Bondareva et al. [2013] for assessing student learning with an intelligent tutoring system).
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities 11:11 Table III. Eye-Tracking Features
   HIGH-LEVEL FEATURES
   Fixations (2): Number of fixations, fixation rate
   Fixation durations (3): Sum, mean, std. deviation
   Saccade length (3): Sum, mean, std. deviation
   Relative saccade angles (3): Sum, mean, std. deviation
   Absolute saccade angles (3): Sum, mean, std. deviation
   AOI-LEVEL FEATURES (for each AOI)
   Number of fixations in AOI (5)
   Sum and mean of fixation durations in AOI (10)
   Time to first fixation in AOI (5)
   Longest fixation in AOI (5)
   Proportion of total number of fixations in AOI (5)
   Proportion of total fixation durations in AOI (5)
   Proportion of total number of transitions from this AOI to every other AOI (including self-transitions) (25)
 5. CLASSIFICATION EXPERIMENTS
The classification experiments described in this section use the previously mentioned features to infer a number of task properties, user performance, and user cognitive traits. In particular, we investigate the extent to which these factors can be inferred from gaze data (research question Q1 in the Introduction), as well as what gaze features are most important for classification (research question Q2).
First, we provide a quick overview of the experimental process used for classification. This is followed by a detailed analysis of each of the classification results, which in- cludes classification accuracy for task type (at different granularities); task complexity; task difficulty; user performance; and accuracy on classifying the three user cognitive abilities of perceptual speed, visual working memory, and verbal working memory. In addition, we ran a classification experiment for predicting the currently active visu- alization type (i.e., bar graph vs. radar graph) to evaluate the extent to which this information can be inferred when it is not available to the system (i.e., if the visual- ization system and the eye-tracking component are independent). We conclude with a summary of the overall results, as well as a discussion regarding the extent to which these results could be used for providing adaptive visualizations.
5.1. Experimental Process
Using the gaze features described in the previous section, we generated a number of datasets to simulate partial observation of gaze data during each task. We used two different processes for generating this “over-time” data, each serving a distinct analysis purpose. First, we generated partial observation datasets based on relative length, such as the first 10%, 20%, 30%, and so forth, of each trial. The goal of this analysis was to determine if there are observable eye gaze patterns regardless of the actual time it took users to finish the task—for example, if the first 20% of a user’s interaction is particularly good at classifying user characteristics. Although this approach can give valuable insights into generalizable trends and patterns, it requires a task to be fully completed to determine what constitutes 100% of the interaction. For this reason, we also generated a second batch of partial observation datasets based on absolute length— that is, the first 1,000ms, 2,000s, 3,000ms, and so forth, of each trial. These datasets are more accurate in simulating classification accuracies while a user is interacting with (i.e., looking at) the visualization. The goal of this analysis is hence to investigate the feasibility of real-time interventions when integrating the classification component into a live user-adaptive visualization system.
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
11:12 B. Steichen et al.
Each of the datasets has a total number of 725 instances, which is a result of pruning the complete set of 980 trials (i.e., 35 subjects × 14 tasks × 2 visitations) to only contain trials with 90% of valid gaze samples.2
We used the WEKA data mining toolkit [Hall et al. 2009] for model learning and eval- uation. For model learning, we tried a number of different classifier types (Decision Trees, Support Vector Machines (SVMs), Neural Networks, and Logistic Regression) with feature selection and 10-fold cross-validation for model evaluation. We used the standard evaluation metrics of Accuracy and Area under ROC. In all of our experiments, Logistic Regression (LR from now on) was the classifier with the highest accuracy and ROC. In the following sections, the performance of this classifier is evaluated on the Full, No AOI, and Only AOI datasets. As a baseline for comparison, we use a clas- sifier that always selects the most likely class—for example, for task complexity, the baseline classifier would always predict a task to be single, since there are more single tasks overall (thus failing in all cases of double tasks). Results are generated using the WEKA experiment API with the default 10(repetition) ∗ 10(cross-validation) set- ting, and statistical significance is tested using t-tests with Bonferroni adjustment on pairwise comparisons between the different classifiers. All reported results are sta- tistically significant (at p < 0.05), unless mentioned otherwise. In cases of two-class classifications, we also present the strongest features generated by feature selection. For simplicity, in the case of multiclass classifications, we do not present feature se- lection in detail, given that this involves presenting n-1 feature selection results (with n = number of classes). Instead, we discuss the impact of features only with respect to the performance of the Full versus No AOI versus Only AOI datasets. In addition, note that in some cases, the y-axis has been readjusted to better show the relative classification performances over time (particularly for the cognitive abilities).
5.2. Classification Results for Task Types and Task Complexity
As explained in Section 3.3, users performed tasks of varying type and complexity. In this section, we first show that task type can be predicted with reasonable accuracy even when tasks are defined at a very fine granularity (in our case comprising nine different task types). Our analysis also reveals that some tasks are frequently con- fused with one other, suggesting that some fine-grained task types may actually entail similar user strategies. We then present classification results for more coarse-grained task type classifications (five task types, three task types), as well as a two-class clas- sification for task complexity (single vs. double scenario tasks). We show that we can achieve high accuracies for each of these predictions; that a classifier using all eye gaze information generally performs best; and that classification accuracy generally increases after seeing more data.
Task type—nine tasks. The most fine-grained analysis splits tasks into nine different classes, one for each separate question type-complexity combination used in the study, such as Single Retrieve Value (RV1) and Double Retrieve Value (RV2). Because of the high number of classes, this case represents a difficult multiclass classification challenge, with a baseline classification accuracy of only 15.45% (i.e., always predicting RV1, since this task is, after data pruning, the most common task with 112 out of 725 instances). Nonetheless, when looking at the over-time performance of the LR classifier using the full feature dataset (LR-Full from now on), a classification accuracy of 56.60% can be achieved after seeing all available data (i.e., after 100%) (Figure 7).
2Note that gaze data during a trial can be lost due to the subject looking off the screen; due to loss of calibration from rapid movement, blinking, or other such events; or due to blocking of the infrared beam to the user’s eyes (e.g., by the user’s hands).
 ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities 11:13
 Fig. 7. Task type—classification accuracy (nine tasks).
Table IV. Overall Average Task Classification (Nine Tasks) Accuracy and ROC for Percentage
and Absolute Timings
As shown in Figure 7, classification accuracy grows continuously as more gaze data becomes available, going higher than 50% after seeing 60% of the data. As shown in Table IV, the average classification accuracy over time for LR-Full is 44.81% and for LR-OnlyAOI is 43.57% (with the difference not being statistically significant). Results are not as good for the LR classifier using the No AOI dataset (LR-NoAOI from now on). The average accuracy over time for this classifier is 29.69%, and its maximum accuracy after seeing all of the data is 30.61%, both statistically significantly lower than the corresponding accuracies for LR-Full. Moreover, the accuracy of the LR-NoAOI classifier is not statistically significantly better than the baseline until after seeing 60% of the data. These differences in performance for the Full versus No AOI versus Only AOI datasets indicate that AOI-related features have a strong impact on classification accuracy for task type at this granularity.
As shown in Figure 8, we found a similar trend when comparing classifiers using the Area under ROC measure, with LR-Full and LR-Only AOI again performing best from 20% of the interaction onward (albeit with slightly different margins compared to the accuracy results). In fact, for almost all of the other experiments described in the following sections, we found similar results for both accuracy and Area under ROC. We will therefore only focus on accuracy results from here onward, except for instances where there was indeed a noticeable difference between the two measures.
In addition to these experiments regarding the percentage of observed interactions, we also test classification performance when using absolute time cut-offs. Both the LR- Full and the LR-OnlyAOI classifiers significantly outperform the baseline classifier from the outset (Figure 9), with accuracies close to 40% after only 5,000ms. However, the LR-NoAOI classifier is only statistically significantly better than baseline after 8,000ms.
   Baseline
  Full
No AOI
  Only AOI
 Percentage (Accuracy)
 15.45
  44.81
29.69
  43.57
 Percentage (ROC)
 0.50
  0.74
0.68
  0.72
 Absolute (Accuracy)
 15.45
  41.65
25.03
  40.71
 Absolute (ROC)
 0.50
  0.75
0.67
  0.71
     ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
11:14 B. Steichen et al.
 Fig. 8. Task type—Area under ROC results (nine tasks).
Fig. 9. Task type—classification accuracy at absolute time cut-offs (nine tasks).
When analyzing sources of errors in the confusion matrix, the most commonly con- fused task pairs were single filter (FI1) and double filter (FI2) (which is intuitive given the common task type), as well as single find extremum (FE1) and single sort (SO1) (further discussed in the next section).
Last, as mentioned in Section 5.1, when comparing different types of classifier models (e.g., SVMs, Decision Trees, Neural Networks), we always found LR to yield the highest results (in fact, statistically significantly higher than other models). As an example of this comparison, Figure 10 shows how the various classifiers performed for the nine- task-type classification. As can be seen in this figure, all of the classifiers outperform the baseline from the outset. However, LR performs best across all time intervals. This trend was found across all of our experiments, and we will therefore only discuss the LR results from here onward.
Task type—five tasks. In addition to the fine-grained task analysis involving nine different tasks, we also investigated classifying task type from gaze data when type is defined at a coarser level of granularity that ignores the complexity difference be- tween single and double tasks—for example, ignoring the difference between retrieve value when one student is mentioned in the question text (RV1) as opposed to when two students are involved (RV2). Ignoring this difference leaves us with five different
 ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities 11:15
 Fig. 10. Comparison of classifier models, each using the Full dataset.
Table V. Overall Average Task Classification (Five Tasks) Accuracy and ROC for Percentage
and Absolute Timings
classes, corresponding to five different task types from Amar’s taxonomy (i.e., retrieve value (RV), filter (FI), compute derived value (CDV), find extremum (FE), and sort (SO)). From the point of view of inferring task type with the goal of providing adaptive interventions specific to tasks types, this five-class classification task is very meaning- ful, because the classes represent general task types recognized as being common for information visualization.
Similar to the nine-class classification, the trends of the relative and absolute time datasets (i.e., percentage of total observation vs. time in milliseconds) are very com- parable. Since the absolute time dataset is more interesting for integration into a live adaptive system (as discussed in Section 5.1.), we only discuss the results of this analysis from here onward. LR-Full reaches an average accuracy of 53.36% over time (Table V), an accuracy of 50% after 8,000ms, and a maximum accuracy of 63.32% after seeing all data. LR-Full statistically significantly outperforms the baseline’s accuracy (27.86%, i.e., always choosing filter, the most common task with 202 instances) from the start. As was the case with nine tasks, LR-OnlyAOI is not statistically significantly different from the LR-Full classifier. Similarly, removing AOI-related features statis- tically significantly reduces accuracy, as shown by the performance of LR-NoAOI in Figure 11. Moreover, this classifier only starts to be statistically significantly better than baseline after 3,000ms.
Although the accuracies are clearly improved over the nine-task classification (as to be expected due to the reduction of the classification complexity), it is still arguable if they are acceptable for real-time fully adaptive interventions. However, one could certainly consider using such a classifier in a mixed-initiative system, where a range of task-appropriate interventions could be recommended to a user rather than applied automatically. This would still potentially reduce a user’s workload while not inter- rupting a task with inappropriate interventions. In addition, it is worth noting that
    Baseline
 Full
 No AOI
 Only AOI
 Percentage (Accuracy)
  27.86
 53.36
 41.98
 51.77
 Percentage (ROC)
  0.5
 0.74
 0.67
 0.72
 Absolute (Accuracy)
  27.86
 50.00
 35.35
 48.29
 Absolute (ROC)
  0.5
 0.75
 0.66
 0.72
     ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
11:16 B. Steichen et al.
 Fig. 11. Task type—classification accuracy (five tasks).
the predictions are solely based on eye gaze data, and that the integration of other data sources (e.g., interaction data) could probably complement and improve results.
When analyzing sources of errors in the confusion matrix, we found two pairs of tasks that are most often confused with one other. The first pair involves the tasks compute derived value (CDV) and filter (FI). For example, in 57% of the cases where CDV was misclassified, the predicted class was FI. This result is not surprising, since both of these tasks essentially involve applying a filter to all data values (e.g., finding values above a given threshold), with the difference being that CDV requires an additional computation (e.g., “In how many courses is student X above the class average?”). Thus, FI can be regarded as a subtask of CDV for the questions used in our study. In fact, as noted by Amar et al. [2005], the filter task “is used as a subtask in many other questions.” Adaptations that particularly support this FI task may therefore also be of use to CDV tasks if they contain such a subcomponent. The second pair of tasks often confused with each other involves find extremum (FE) and sort (SO). For example, in 38% of the cases where FE was misclassified, the predicted class was SO. This result is again not surprising given the nature of these two tasks. FE involves going through all values to find the highest value(s) from a set of values, whereas SO involves sorting all values from highest to lowest. Thus, FE essentially involves a subpart of the steps necessary to perform an SO task. This finding confirms the observation in the taxonomy of Amar et al. [2005] that “sorting is generally a substrate for extreme value finding.”
The aforementioned relations between the two pairs of frequently confused tasks suggest that combining each pair into one new task type and building a classifier that can recognize this combined type is still valuable for adaptation, since adaptations could be provided to support the common subtask. Thus, in the next section, we evaluate the accuracy of a classifier for task type that involves three classes: FI-CDV (combined), SO-FE (combined), and RV.
Task type—three tasks. When considering only three different task types, LR-Full reaches an average accuracy of 68.42% over time and an accuracy of 70% after only 8,000ms. LR-Full statistically significantly outperforms the baseline’s accuracy (48.14%) from the start.
As was the case with nine and five tasks, the performance of LR-OnlyAOI is very similar to the performance of LR-Full. Similarly, removing AOI-related features once again statistically significantly reduces accuracy, as shown by the performance of
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities 11:17
 Fig. 12. Task type—classification accuracy (three tasks).
Table VI. Overall Average Task Classification (Three Tasks) Accuracy and ROC for Percentage
and Absolute Timings
LR-NoAOI in Figure 12 and Table VI. This classifier only reaches an average accu- racy of 56.75% over time and only statistically significantly outperforms the baseline after 10,000ms.
With accuracies reaching 70% after only 8s, one can certainly envision the use of such a classifier to assist a user in an adaptive visualization system. Interventions could consist of recommendations, as well as direct automatic visualization adaptations. However, to validate the practicality of this approach, we will need to run further user studies with a fully implemented adaptive system.
Task types—summary of results. In summary, we found that across all task type granularities, LR with the Full dataset outperformed both the baseline and LR with the No AOI dataset, but not the LR-OnlyAOI classifier, showing the importance of having AOI-related features for task-type classification. Figure 13 summarizes the results in terms of average accuracy over time. As expected, accuracy for all of the classifiers increases as task granularity gets coarser. Although only the classification of three tasks with the LR-Full classifier reaches accuracies that may be suitable for providing reliable task-based interventions, we see these results as being very important for two reasons. First, as we argued earlier, suitable interventions can be provided even if task type is recognized at this coarser level. Second, our results have been obtained by using relatively simple eye gaze features that do not capture gaze patterns beyond simple transitions between two AOIs. Using more complex gaze patterns or additional sources of information to guide classification (see discussion in Section 5.6), it is likely that we can increase accuracy on all of our classification tasks.
Task complexity. The classifier in this experiment predicts if the user is attending to a task of the single or double scenario. As discussed in the Section 3, this dis- tinction provides a measure for task complexity. LR-Full and LR-OnlyAOI are still the most accurate classifiers, with statistically significantly higher average accuracy (80.39%/80.22%) over time than both LR-NoAOI (74.76%) and the baseline classifier
  Percentage (Accuracy)
  48.14
 68.42
56.75
  67.00
 Percentage (ROC)
  0.5
 0.81
0.65
  0.80
 Absolute (Accuracy)
  48.13
 67.10
52.56
  65.91
 Absolute (ROC)
  0.5
 0.80
0.58
  0.80
    ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
11:18 B. Steichen et al.
 Fig. 13. Task type—average classification accuracy over time for different task granularities.
Fig. 14. Task complexity—classification accuracy.
Table VII. Overall Average Complexity Classification Accuracy and ROC for Percentage
and Absolute Timings
(72.69%, i.e., always choosing single, since this is the dominant class with 527 out of the 725 instances). The top classifiers, LR-Full and LR-OnlyAOI, are once again not statistically significantly different from each other. It should be noted that at 72.69%, the baseline accuracy is relatively high in this experiment, since users performed more than twice as many single tasks than double ones. Nevertheless, all three LR classifiers performed higher, with accuracies reaching up to 84.45% for the Full dataset (Figure 14 and Table VII). Accuracy again improved with more data being observed, and each of the feature sets outperformed the baseline after relatively low amounts of observed data.
   Baseline
  Full
No AOI
  Only AOI
 Percentage (Accuracy)
 72.69
  80.39
74.76
  80.22
 Percentage (ROC)
 0.50
  0.81
0.73
  0.80
 Absolute (Accuracy)
 72.69
  79.96
75.02
  79.74
 Absolute (ROC)
 0.50
  0.81
0.69
  0.80
      ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities 11:19
Since task complexity consists of a simple two-class classification, we also investi- gated which specific features from the feature selection process are contributing the most to the classification (note that for multiclass logistic regression involving n classes, this analysis would have been too cumbersome because it would need to consider n-1 feature selection results). The most predictive features were “proportion of total fixation durations in legend AOI,” “sum of fixation durations in legend AOI,” and “proportion- ate number of transitions from/to legend AOI to/from high AOI.” With increased task complexity, we found that the use of the graph legend increased considerably, both in terms of proportion of total fixation durations (compared to all other AOIs), as well as in terms of transitions (i.e., there were more transitions to and from the legend). This result shows that an increase in data series has an effect on how much users may need to refer back to the legend during a visualization task, as to be expected. Nevertheless, it is an interesting finding that such an increase in complexity can be captured in real time using simple eye gaze measures, which may in turn allow a user-adaptive system to provide adaptations for more complex tasks (e.g., provide support for better legend access and processing).
5.3. Classification Results for Task Difficulty
In addition to “task complexity,” which we defined based on the number of data series, we also tried to predict the overall “difficulty” of a task, which we defined based on a combination of subjective and objective measures. For this measure, we again found similar relative performances of the different classifiers, and that accuracy generally improves with more data. We will now first describe in detail how we generated a difficulty value for each task, followed by the detailed results of the classification experiments.
Definition of task difficulty. Defining tasks as being easy or difficult a priori is chal- lenging, since difficulty depends on user expertise and perceptual abilities, which were varied on purpose in our study. We therefore defined task difficulty a posteriori, based on four different measures (two objective and two subjective) aggregated using a prin- cipal component analysis (PCA). Because there was a ceiling effect on task correctness, our first objective measure of task difficulty is task completion time (assuming that, in general, more time is needed for more difficult tasks). However, longer completion times may also simply be an indication of a task being longer while not necessarily be- ing more difficult. Therefore, our second objective measure of difficulty is the standard deviation of completion time for each task across all users. A high value of this metric indicates a high variability among users’ completion times, an indicator that the task may be difficult or confusing for some users.
Our two chosen subjective measures of task difficulty are based on the users’ reported confidence of their performance, which was elicited after each task. The first subjective measure is the average confidence reported by users on each task. Intuitively, less difficult tasks would have higher values for this average. However, we also want to take into account that some users may tend to be more confident overall than other users. Therefore, our second subjective measure is the average deviation of confidence for each task across all users and is computed as follows. For each user, we look at their average confidence across their tasks. Then, for each task, we compute the deviation of confidence as the difference between the user’s reported confidence for that task and the user’s average confidence across tasks. Finally, for each task, we average the deviation of confidence across all users. This average indicates for which tasks users were giving confidence ratings that were above or below their typical rating.
To combine the preceding four variables, we performed a PCA, which is a form of dimensionality reduction that allows one to identify and combine groups of interrelated
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
11:20 B. Steichen et al.
 Fig. 15. Task difficulty—classification accuracy.
Fig. 16. Task difficulty—classification results using the Area under ROC measure.
variables into components more suitable for data analysis. A PCA on our four measures of task difficulty resulted in one output component. Bartlett’s test of sphericity (x2 = 73.35, df = 6, p < .001) indicated that the PCA was appropriate. Kaiser’s sampling adequacy was 0.55, and all variables showed a communality >0.52, which was above the acceptable limit of 0.5. The component that we generated had an eigenvalue over Kaiser’s criterion of 1 and explained 62.22% of the variance. In summary, we used the output component generated by this PCA (i.e., dimensional reduction) as the measure of task difficulty, and for classification purposes, we labeled tasks with a negative component as easy and tasks with a positive component as difficult (resulting in 497 easy and 228 difficult trials). Each of the easy/difficult classes included both bar and radar graph trials, as well as both single and double task trials, thereby showing that difficulty was not solely confined to radar graphs and/or double complexity tasks.
Classification results. Our classification experiments on this task difficulty property showed slightly different patterns (Figures 15 and 16) compared to task complexity— that is, a nonsmooth curve between 5,000ms and 10,000ms. However, as noted in Provost et al. [1998], for datasets that are imbalanced (as was the case for task dif- ficulty), accuracy is less reliable. When analyzing the Area under ROC results, we
 ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities 11:21
Table VIII. Overall Average Difficulty Classification Accuracy and ROC for Percentage and Absolute Timings
found that the curve for this measure was indeed relatively smooth (see Figure 16) and that the LR-Full and LR-OnlyAOI datasets once again statistically significantly outperformed LR-NoAOI, as well as the baseline (Table VIII). Relatively high accu- racies/Area under ROC are reached after only 5,000ms (72%/0.68) and reach up to 78%/0.86 for LR-Full and LR-OnlyAOI. This classifier could hence be used in an adap- tive system to support users during “hard” tasks. However, there is still significant room for improvement in terms of the accuracies, especially compared to the baseline (which could potentially come from different data sources, such as input devices).
In terms of “how” to provide this support, it is again worth investigating the feature selection results to see which particular eye gaze behaviors are most indicative of a user facing a difficult task. The three most predictive features for task difficulty were “proportionate number of transitions from labels to high AOI,” “sum of fixation duration in label AOI,” and “proportion of total fixation durations in high AOI.” These findings indicate that difficult tasks might require users to do more repeated visits of the actual graph values (and their associated labels), perhaps due to a task being ambiguous or simply requiring more cognitive effort. Therefore, if in addition to determining the task type our classifier flags the current task as difficult (e.g., due to the user performing repeated visits to the high AOI area, and/or spending a high proportionate amount of time in this area), the system should try to provide assistive support to the user through an adaptive intervention that reduces this effort.
5.4. Classification Results for User Performance
In addition to predicting what particular task a user is performing (as well as the task’s complexity/difficulty), we also aimed at predicting “how well” a user is/will be performing on this current task. The reasoning for this experiment is that while there might be situations where we could provide adaptations purely based on a predicted task type and/or task characteristics, an adaptive visualization system is arguably most appropriate when a user is currently performing “suboptimally.” In particular, since adaptive interventions could introduce slight disruptions in a user’s workflow, it might be best to only provide interventions when we detect a “slow” user (while ignoring users who are already performing well). To estimate a user’s current performance, we hence are trying to predict if a user’s completion time will be above or below the median time (based on all users) for this particular task. Overall, our results showed that performance can be predicted well within the first stages of the prediction and that more general features (i.e., not specific to any AOIs) are particularly discriminative.
Figure 17 shows the overall classification results based on the 725 valid trials, of which 357 were labeled as “slow” and 368 were labeled as “fast” (note that we did not run separate classification experiments per task). As can be seen in this figure, LR-Full achieves the highest overall accuracies, reaching 65% after only 5,000ms. Overall, all three classifiers are closely matched, with an average accuracy of 65% for LR-Full and 62% for both LR-NoAOI and LR-OnlyAOI (baseline 51%) (Table IX). Nonetheless, the differences observed in the early stages (at 5,000ms) are statistically significant, as are all performances compared to the baseline (from 2,000ms onward).
   Baseline
  Full
No AOI
  Only AOI
 Percentage (Accuracy)
 68.41
  76.02
74.46
  75.78
 Percentage (ROC)
 0.50
  0.79
0.75
  0.78
 Absolute (Accuracy)
 68.41
  72.12
70.36
  71.56
 Absolute (ROC)
 0.50
  0.73
0.65
  0.72
     ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
11:22 B. Steichen et al.
 Fig. 17. User performance—classification accuracy.
Table IX. Overall Average Performance Classification Accuracy and ROC for Percentage
and Absolute Timings
Interestingly, LR-NoAOI outperforms LR-OnlyAOI in the early stages of a user’s task, showing that a user’s overall performance may be predicted using features that are independent of any AOIs. When analyzing feature selection results for LR-Full at this early stage, we found that “mean saccade length,” “standard deviation of absolute saccade angles,” and “standard deviation of saccade length” were most predictive (each being non-AOI features). When implementing an adaptive intervention system that targets the early phases of a user’s interaction, it may therefore be sufficient to base the classification on non-AOI related features.
However, after 10,000ms, it is once again the LR-OnlyAOI classifier that closely matches the LR-Full classifier, showing that AOI-related information positively con- tributes to classification accuracy in the later stages. The most predictive features in these stages were “proportion of total number of fixations in text AOI,” “proportion of total fixation durations in text AOI,” and “proportionate number of transitions from text to low AOI,” indicating that users with lower performances (in terms of time) refer more often to the textual information associated with the graph (which could consist of the graph’s caption).
5.5. Classification Results for Cognitive Abilities
In this section, we discuss classification results relating to inferring a user’s level of visual working memory, verbal working memory, and perceptual speed. The specific task of each of the three classifiers is to infer if a user belongs to either the High or Low category for that measure (based on a median split).
In general, we found similar results across each of these three classification exper- iments. First, we found that AOI features are again very useful for predictions and that most classifiers outperform the baseline from the outset. Although average accu- racies for even the best classifier were rather low (between 56% and 60%; Tables X, XI, and XII), it has to be noted again that these experiments are solely based on simple
  Baseline
  Full
No AOI
  Only AOI
 Percentage (Accuracy)
 50.75
  65.20
62.45
  62.23
 Percentage (ROC)
 0.50
  0.73
0.69
  0.69
 Absolute (Accuracy)
 50.75
  63.20
61.27
  61.09
 Absolute (ROC)
 0.50
  0.68
0.66
  0.66
      ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities
11:23
Table X. Overall Average Visual Working Memory Classification Accuracy and ROC for Percentage and Absolute Timings
Table XI. Overall Average Verbal Working Memory Classification Accuracy and ROC for Percentage and Absolute Timings
Table XII. Overall Average Perceptual Speed Classification Accuracy and ROC for Percentage and Absolute Timings
eye-tracking measures, which may be improved using additional sources of informa- tion (see overall result discussion in Section 5.6). Second, we made several interesting observations when analyzing the accuracies at different data cut-off points. In partic- ular, for each of the experiments, the peak accuracies were actually found during the early stages of each trial, as opposed to after all data had been observed (as found in most of the other classification experiments described earlier). This pattern suggests that a user’s cognitive abilities most strongly affect a user’s gaze patterns during the initial phase of a visualization task (as shown in Figures 18, 19, and 20) and that these patterns are increasingly “diluted” by other factors (e.g., task type) as the task goes on. Although this goes against the intuition that more data generally helps classification, the analysis of feature selection actually provided some sensible explanations for this finding (discussed next).
For visual working memory, the peak accuracy of 60% occurred after 6,000ms (see Figure 18). When analyzing the features that received the highest coefficient during feature selection, we found that the time to first fixation for text, label, and high AOIs played an important role in classifying users. We found that high visual working memory users had lower times to first fixation (indicated by a negative coefficient), meaning that they were very quick at scanning the various AOIs of the visualization.
Similarly, for verbal working memory, the highest classification accuracy for both LR-Full (64%) and LR-OnlyAOI (61%) was found after observing only 3,000ms (see Figure 19).
When analyzing the feature selection results for LR-Full, we found that features related to the text and label AOI most strongly contributed to the classification accuracy. In particular, high verbal working memory users spent less time in the text AOI, both overall and in proportion to other AOIs. Since users are most likely to read the question text at the beginning of each task, it therefore seems intuitive that the highest accuracies were found after only a few seconds of the data had been observed.
   Baseline
  Full
No AOI
  Only AOI
 Percentage (Accuracy)
 54.90
  57.47
56.78
  56.95
 Percentage (ROC)
 0.50
  0.60
0.57
  0.59
 Absolute (Accuracy)
 54.73
  56.93
54.89
  57.21
 Absolute (ROC)
 0.50
  0.59
0.52
  0.59
        Baseline
  Full
No AOI
  Only AOI
 Percentage (Accuracy)
 52.14
  60.75
55.40
  59.83
 Percentage (ROC)
 0.50
  0.65
0.58
  0.64
 Absolute (Accuracy)
 52.14
  60.33
55.84
  59.82
 Absolute (ROC)
 0.50
  0.65
0.57
  0.64
        Baseline
  Full
No AOI
  Only AOI
 Percentage (Accuracy)
 50.07
  57.07
53.19
  57.12
 Percentage (ROC)
 0.50
  0.59
0.54
  0.59
 Absolute (Accuracy)
 50.07
  56.29
52.45
  56.35
 Absolute (ROC)
 0.50
  0.58
0.53
  0.58
     ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
11:24 B. Steichen et al.
 Fig. 18. Visual working memory—classification accuracy for relative time slices.
 Fig. 19. Verbal working memory—classification accuracy.
 Fig. 20. Perceptual speed—classification accuracy for absolute time slices.
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities 11:25
Table XIII. Overall Average Visualization Type Classification Accuracy and ROC for Percentage and Absolute Timings
A similar pattern was observed for the perceptual speed classification experiments, where the highest accuracy for LR-Full (59%) was found after only 2,000ms of data had been observed (see Figure 20). Interestingly, the LR-NoAOI classifier outperformed LR-Full and LR-OnlyAOI after 20,000ms.
When analyzing the feature selection results during the early stages, we found that features related to the label and legend AOIs had the strongest coefficients. In par- ticular, we found that high perceptual speed users had a “lower number of fixations in the legend AOI” and “lower proportion of total number of fixations in the legend AOI.” This finding may indicate that low perceptual speed users would benefit from adaptations relating to this particular AOI (through highlighting, facilitating easier access, etc.). In addition, low perceptual speed users had a longer “longest fixation in the label AOI” and a lower “fixation rate” compared to high perceptual speed users. Again, this may indicate that we can provide adaptations particularly tailored toward the label AOI—for example, by temporarily increasing the size of relevant labels to support low perceptual speed users.
However, during the later stages, we found that in addition to fixation rate, the No AOI features of “mean fixation duration” and “sum of absolute saccade angles” were the most predictive (hence explaining the better performance of the LR-NoAOI classifier).
Visualization type. As shown in almost all of the preceding classification results, the inclusion of AOI-related features is critical toward generating predictions for task properties, user performance, and user cognitive measures. However, having these AOI- related features requires knowing which visualization is currently active. Although there are many scenarios in which this information is indeed available to an adap- tive component (i.e., if the adaptation component is part of the visualization system), this is not always the case. For example, if an adaptive component were to run as a standalone system in parallel to a separate visualization system (in the context of an information retrieval task when the user gets back a visualization, or in case the adaptive component acts as a complement to a third-party analysis tools, etc.), it would first be necessary to infer the currently active visualization type to utilize the right AOIs for accurate task/user classifications. Thus, in this section, we present results on whether visualization type can be inferred from gaze data. Since AOI information would not be available for this task, LR-Full and LR-OnlyAOI are not applicable in a realistic scenario. For the LR-NoAOI classifier, the average accuracy is 66.58%, which is statistically significantly higher than the baseline (52.69%) (Table XIII). As shown in Figure 21 (note that LR-Full and LR-OnlyAOI are included for completeness), the accuracy of LR-NoAOI continuously grows as more gaze data is observed, reaching 66% after 5,000ms and leveling off at around 70% after 2,0000ms. All accuracies are statistically significantly higher than baseline after only 3,000ms. Although these re- sults are encouraging, further research needs to be conducted in terms of improving accuracies to employ these techniques in a live system (see discussion in Section 5.6).
Regarding the feature selection for this NoAOI classifier, we found that users have different viewing patterns in terms of path angles. Specifically (and not surprisingly), users have more horizontal viewing patterns in the bar graph (lower mean absolute
   Baseline
  Full
No AOI
  Only AOI
 Percentage (Accuracy)
 52.69
  83.45
66.58
  80.39
 Percentage (ROC)
 0.50
  0.91
0.73
  0.88
 Absolute (Accuracy)
 52.69
  84.31
64.63
  81.67
 Absolute (ROC)
 0.50
  0.91
0.69
  0.88
     ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
11:26 B. Steichen et al.
 Fig. 21. Visualization type—classification accuracy.
path angles) and more “erratic” saccades (higher standard deviation of absolute path angles), whereas in the radar graph users follow a circular trajectory to view the various data points (indicated by a higher mean of absolute path angles) and have more uniform saccades due to the proximity of the labels to the respective data points (indicated by a lower standard deviation of absolute path angles).
5.6. Summary of Results and Discussion
As outlined in the Introduction, the specific goals of our experiments were to investigate the extent to which a user’s current visualization task properties, a user’s performance, and a user’s long-term cognitive abilities could be inferred solely based on eye gaze data (Q1), as well as which gaze features would be the most informative (Q2). By running a number of classification experiments and analyzing in detail the effects of different feature sets, we have found several interesting results regarding these research questions.
We found that a user’s eye gaze behavior provides evidence about each of the visual- ization task types and characteristics, user performance, and user cognitive abilities. In particular, we showed that for each classification task, gaze-behavior–based pre- dictions outperform a baseline classifier (except for user expertise, which we hence did not discuss further) (Q1). Moreover, we show that for most of the predictions, the classification accuracy is statistically significantly higher even after only partial data observations. We have shown that for some experiments, accuracy is actually highest at the beginning of each task, indicating that a user’s eye gaze at this time may con- tain the most relevant information regarding the target characteristics. These results provide very encouraging evidence that user eye gaze behavior could indeed be used for driving adaptive systems, particularly given that the experiments used a relatively simple set of features. Interestingly, LR consistently achieved the highest accuracies compared to other machine learning models, such as SVM or Decision Trees. Although we do not have an intuitive explanation for this finding, several other works have simi- larly found LR to perform well with eye gaze data [Kardan and Conati 2012; Bondareva et al. 2013].
It may be argued that from a practical point of view, some of the accuracies are not sufficiently high to be exploited in a live system. In particular, the accuracies relating to the cognitive abilities yielded results that were only in the 55% to 60% range. However, depending on the nature of the intervention/guidance that is being provided, it can be envisioned that if the system is unsure about the user’s classification, some
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities 11:27
minimal adjustments can be done, followed by continued tracking to see if performance improves. Nevertheless, further research should be conducted to improve the presented accuracies. On the one hand, we envision that the addition of sequence features (e.g., scan path patterns) could provide even more information about the various tasks and user characteristics. On the other hand, eye-tracking data could be integrated with other sources, such as interaction data, if this information is available. Similarly, there are further sources of information that could potentially be added to such a system. For instance, it may be possible to infer the user’s task through automatic graph analysis (e.g., based on computer vision techniques [Elzer et al. 2011] or natural language processing (e.g., by processing a visualization’s caption).
We also obtained very interesting results regarding the more fine-grained details of each classification experiment. In particular, we found that depending on the goal of the classification, different features are most informative for different task/user characteristics (Q2). For example, we found that the legend usage increases for more complex tasks (i.e., tasks that have more data series) and label usage for generally more difficult tasks, suggesting that users could benefit from interventions relating to these particular AOIs. Similarly, we found that low perceptual speed users spend more time in the legend, suggesting that these users may benefit from interventions that particularly relate to this AOI (e.g., giving these elements more emphasis or providing easier access). These detailed analyses thereby not only provide evidence to what extent different characteristics can be inferred but also how a system may adapt to individual differences.
In terms of general trends regarding the most informative features, we found that for each of the classification runs, AOI-related features were crucial toward more accurate predictions. Thus, it may be argued that to build effective adaptive visualizations, a system needs to be aware of the currently active visualization. We therefore also showed that even in the case of the system not knowing this information a priori (e.g., if the adaptive component is not directly attached to the visualization system), it is possible to infer the visualization type solely based on a user’s eye gaze with 70% accuracy. Again, this accuracy may potentially be improved with additional, more sophisticated features such as sequential scan paths.
Our experiments have only investigated two simple visualization techniques; how- ever, there are many results that may be generalized to a wider array of visualization designs. In particular, we have shown that many of the important features are ac- tually based on generic AOIs that are common to most types of visualizations, such as a graph’s labels or legend. Similarly, while the study only focused on an artificial dataset involving student grades, the actual tasks were derived from an established set of general, low-level analysis tasks for information visualization [Amar et al. 2005] and may therefore be generalized to other application domains.
Our analysis also included a novel way of investigating task type/subtype similarity, since our study of confusion matrices (see Section 5.2) revealed common eye gaze patterns for certain types of tasks. This type of analysis may also be used for further research purposes, such as to determine which type of adaptation to provide to best support different task types/subtypes or common user strategies.
Last, although our experiments have shown results regarding the classification of different task and user characteristics—for instance, what to adapt to and to a certain extent how to adapt—more work needs to be carried out in terms of predicting when adaptive assistance is required. In particular, further research is necessary relating to the identification of potential user confusion or cognitive overload, which is related to the detection of “suboptimal usage patterns” that was discussed in related work by Gotz and Wen [2009].
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
11:28 B. Steichen et al.
6. CONCLUSIONS AND FUTURE WORK
In conclusion, we have presented research results showing that a user’s eye gaze is a valuable source to infer a number of task and user characteristics. In particular, we have shown encouraging results using simple machine learning techniques on simple eye-tracking metrics, even after only partial data has been observed. The study has therefore provided a first step toward our long-term goal of designing user-adaptive information visualizations.
The next step of this research is to design user studies that focus on the effect of differ- ent adaptive interventions (e.g., highlighting, drawing reference lines, recommending alternative visualizations) on a user’s performance, both in general and in relation to different tasks and individual user differences. These studies will also need to focus on different degrees of intervention intrusiveness, such as comparing fully adaptive versus mixed-initiative approaches. Following this investigation, we hope to develop a fully integrated adaptive information visualization system that is able to dynami- cally provide adaptive interventions that are informed by real-time user behavior data. Last, we will investigate the detection of user confusion/cognitive overload, as well as the usage of more complex features such as sequential scan paths, to improve on the results presented in this article.
REFERENCES
R. Amar, J. Eagan, and J. Stasko. 2005. Low-level components of analytic activity in information visualization. In Proceedings of the 2005 IEEE Symposium on Information Visualization. 15–21.
D. Bondareva, C. Conati, R. Feyzi-Behnagh, J. M. Harley, R. Azevedo, and F. Bouchet. 2013. Inferring learning from gaze data during interaction with an environment to support self-regulated learning. In Artificial Intelligence in Education. Lecture Notes in Computer Science, Vol. 7926, 229–238.
G. Carenini, C. Conati, E. Hoque, B. Steichen, D. Toker, and J. T. Enns. 2014. Highlighting interventions and user differences: Informing adaptive information visualization support. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI’14). 1835–1844.
S. M. Casner. 1991. Task-analytic approach to the automated design of graphic presentations. ACM Trans- actions on Graphics 10, 111–151.
C. Chen and M. Czerwinski. 1997. Spatial ability and visual navigation: An empirical study. New Review of Hypermedia and Multimedia 3, 67–89.
C. Conati and H. Maclaren. 2008. Exploring the role of individual differences in information visualization. In Proceedings of the Working Conference on Advanced Visual Interfaces (AVI’08). 199–206.
C. Conati and C. Merten. 2007. Eye-tracking for user modeling in exploratory learning environments: An empirical evaluation. Knowledge-Based Systems 20, 557–574.
F. Courtemanche, E. A ̈ımeur, A. Dufresne, M. Najjar, and F. Mpondo. 2011. Activity recognition using eye-gaze movements and traditional interactions. Interacting with Computers 23, 202–213.
J. P. Egan. 1975. Signal Detection Theory and ROC-Analysis. Academic Press.
S. Eivazi and R. Bednarik. 2011. Predicting problem-solving behavior and performance levels from visual at-
tention data. In Proceedings of the 2nd Workshop on Eye Gaze in Intelligent Human Machine Interaction
(IUI’11). 9–16.
R. B. Ekstrom and U.S. Office of Naval Research. 1996. Manual for Kit of Factor-Referenced Cognitive Tests.
Educational Testing Service.
S. Elzer, S. Carberry, and I. Zukerman. 2011. The automated understanding of simple bar charts. Artificial
Intelligence 175, 526–555.
S. Few. 2005. Keep Radar Graphs Below the Radar—Far Below. Perceptual Edge.
K. Fukuda and E. K. Vogel. 2009. Human variation in overriding attentional capture. Journal of Neuroscience
29, 8726–8733.
J. Goldberg and J. Helfman. 2011. Eye tracking for visualization evaluation: Reading values on linear versus
radial graphs. Information Visualization 10, 182–195.
J. H. Goldberg and J. I. Helfman. 2010. Comparing information graphics: A critical look at eye tracking. In
Proceedings of the 3rd BELIV’10 Workshop: BEyond Time and Errors: Novel evaLuation Methods for
Information Visualization (BELIV’10). 71–78.
D. Gotz and Z. Wen. 2009. Behavior-driven visualization recommendation. In Proceedings of the 14th Inter-
national Conference on Intelligent User Interfaces (IUI’09). 315–324.
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities 11:29
B. Grawemeyer. 2006. Evaluation of ERST: An external representation selection tutor. In Proceedings of the 4th International Conference on Diagrammatic Representation and Inference (Diagrams’06). 154–167.
T. M. Green and B. Fisher. 2010. Towards the personal equation of interaction: The impact of personality factors on visual analytics interface interaction. In Proceedings of the 2010 IEEE Symposium on Visual Analytics Science and Technology (VAST’10). 203–210.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. 2009. The WEKA data mining software: An update. SIGKDD Explorations Newsletter 11, 10–18.
S. T. Iqbal and B. P. Bailey. 2004. Using eye gaze patterns to identify user tasks. Presented at the the Grace Hopper Celebration of Women in Computing.
A. Jameson. 2008. Adaptive interfaces and agents. In A. Sears and J. A. Jacko (Eds.), The Human-Computer Interaction Handbook: Fundamentals, Evolving Technologies and Emerging Applications (2nd ed.). CRC Press, Boca Raton, FL, 433–458.
S. Kardan and C. Conati. 2012. Exploring gaze data for determining user learning with an interactive simulation. In Proceedings of the 20th International Conference on User Modeling, Adaptation, and Personalization (UMAP’12). 126–138.
J. Mackinlay. 1986. Automating the design of graphical presentations of relational information. ACM Trans- actions on Graphics 5, 110–141.
M. D. Plumlee and C. Ware. 2006. Zooming versus multiple window interfaces: Cognitive costs of visual comparisons. ACM Transactions on Computer-Human Interaction 13, 179–209.
F. J. Provost, T. Fawcett, and R. Kohavi. 1998. The case against accuracy estimation for comparing induction algorithms. In Proceedings of the 15th International Conference on Machine Learning (ICML’98). 445– 453.
K. Rayner. 1995. Eye movements and cognitive processes in reading, visual search, and scene perception. Studies in Visual Information Processing, 3–22.
K. Rayner. 1998. Eye movements in reading and information processing: 20 years of research. Psychological Bulletin 124, 372–422.
L. Sesma, A. Villanueva, and R. Cabeza. 2012. Evaluation of pupil center-eye corner vector for gaze esti- mation using a Web cam. In Proceedings of the Symposium on Eye Tracking Research and Applications (ETRA’12). 217–220.
J. Simola, J. Saloja ̈rvi, and I. Kojo. 2008. Using hidden Markov model to uncover processing states from eye movements in information search tasks. Cognitive Systems Research 9, 237–251.
B. Steichen, H. Ashman, and V. Wade. 2012. A comparative survey of personalised information retrieval and adaptive hypermedia techniques. Information Processing and Management 48, 4, 698–724.
B. Steichen, G. Carenini, and C. Conati. 2013. User-adaptive information visualization: Using eye gaze data to infer visualization tasks and user cognitive abilities. In Proceedings of the 2013 International Conference on Intelligent User Interfaces (IUI’13). 317–328.
D. Toker, C. Conati, G. Carenini, and M. Haraty. 2012. Towards adaptive information visualization: On the influence of user characteristics. In Proceedings of the 20th International Conference on User Modeling, Adaptation, and Personalization (UMAP’12). 274–285.
D. Toker, C. Conati, B. Steichen, and G. Carenini. 2013. Individual user characteristics and information visualization: Connecting the dots through eye tracking. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI’13). 295–304.
D. Toker, B. Steichen, M. Gingerich, C. Conati, and G. Carenini. 2014. Towards facilitating user skill ac- quisition: Identifying untrained visualization users through eye tracking. In Proceedings of the 19th International Conference on Intelligent User Interfaces (IUI’14). 105–114.
M. L. Turner and R. W. Engle. 1989. Is working memory capacity task dependent? Journal of Memory and Language 28, 127–154.
M. C. Velez, D. Silver, and M. Tremaine. 2005. Understanding visualization through spatial ability differ- ences. in: IEEE Visualization, 2005. VIS 05. In Proceedings of IEEE Visualization (VIS’05). 511–518.
S. Westerman and T. Cribbin. 2000. Mapping semantic information in virtual space: Dimensions, variance and individual differences. Journal of Human-Computer Studies 53, 765–787.
C. Ziemkiewicz, R. J. Crouser, A. R. Yauilla, S. L. Su, W. Ribarsky, and R. Chang. 2011. How locus of control influences compatibility with visualization style. In Proceedings of the 2011 IEEE Conference on Visual Analytics Science and Technology (VAST’11). 81–90.
Received August 2013; revised February 2014; accepted March 2014
ACM Transactions on Interactive Intelligent Systems, Vol. 4, No. 2, Article 11, Publication date: July 2014.
 Towards Using Gaze Properties to Detect Language Proficiency
Abstract
Humans are inherently skilled at using subtle physiological cues from other persons, for example gaze direction in a conversation. Personal computers have yet to explore this implicit input modality. In a study with 14 participants, we investigate how a user’s gaze can be leveraged in adaptive computer systems. In particular, we examine the impact of different languages on eye movements by presenting simple questions in multiple languages to our participants. We found that fixation duration is sufficient to ascertain if a user is highly proficient in a given language. We propose how these findings could be used to implement adaptive visualizations that react implicitly on the user’s gaze.
Author Keywords
Eye tracking; pattern recognition; adaptive visualization.
ACM Classification Keywords
H.5.m [Information interfaces and presentation (e.g., HCI)]: Miscellaneous.; H.1.2 [Model and Principles]: User/Machine Systems—Human information processing
Introduction
During conversations, we usually pay attention to the view direction of our conversation partner [4, 5]. This enables us to detect if the other person is bored or not engaged in the
Jakob Karolus
University of Stuttgart Pfaffenwaldring 5a
70569 Stuttgart, Germany jakob.karolus@vis.uni- stuttgart.de
Paweł W. Woźniak
University of Stuttgart Pfaffenwaldring 5a
70569 Stuttgart, Germany pawel.wozniak@vis.uni- stuttgart.de
Lewis L. Chuang
Max Planck Institute for Biological Cybernetics Spemannstr. 38
72076 Tübingen, Germany lewis.chuang@tuebingen.mpg.de
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. NordiCHI ’16, October 23 - 27, 2016, Gothenburg, Sweden
Copyright is held by the owner/author(s).
Publication rights licensed to ACM.
ACM 978-1-4503-4763-1/16/10...$15.00
DOI: http://dx.doi.org/10.1145/2971485.2996753

 Language English German Danish
Dutch
Finnish
French
Greek Romanian   ro
conversation. Changing the topic or requesting attention from the partner is a common countermeasure.
When we interact with personal computer systems communicating on this level is not yet possible. In virtual environments, gaze directional cues have been researched as a predictor of conversational attention [13, 14]. It has been shown that a system that reacts to the user’s gaze, e.g. a virtual agent [8], increases emotional response and attentional allocation by the user. In our work, we examine how gaze characteristics can explain the user’s behavior and their context. We then suggest how these characteristics can be utilized to build adaptive user interfaces.
We propose an approach that leverages the distinctiveness of gaze characteristics as a means to infer whether the user needs assistance or not. In particular, we looked at how the eye movements of users changed when confronted with different languages. In our study, we presented the participants with simple questions in varying languages while recording their gaze with an eye tracking device. While studies regarding people’s language proficiencies are plentiful, they often rely on an extensive analysis given eye movement data from multiple read documents [9, 17]. For a real-time adaptive system, the prospect of having the users read a whole document first to recognize that they did not understand the content is not feasible. Thus, we specifically aim to recognize a user’s understanding as accurately as possible given only a short sentence in the respective language.
Our preliminary results show that a model relying solely on a person’s mean fixation duration is sufficient to ascertain whether the user is highly proficient in the respective language. We contribute results from an eye tracking study with 14 participants that show the feasibility of such an approach.
Study Design
Nineteen participants volunteered for this study, of which the data of 14 participants (7 female, age: 19-36 years) were used for further analysis, based on data quality1. Two participants already had prior experience with eye tracking studies. All of them were native German speakers.
We collected simple questions in 13 different languages (15 questions each). The respective translations were provided by either native or highly proficient users of that language. Table 2 shows a few example questions. Most are part of the Indo-European language family [2], yet we included some outliers such as Finnish or Hungarian to analyze their effect. See Table 1 for a complete overview.
Our setup consisted of a 22 inch LCD display and a remote eye tracker (SMI RED 250) that was attached at the bottom of the screen. Our participants were seated 0.5 to 1 meters away from the display in a small cubicle. Figure 2 shows a picture of the apparatus.
After introducing the participants to our study, they were asked to sign the provided consent form and supply demographic information as well as rate their reading level for specific languages based on the Common European Framework of Reference for Languages [1]. Additionally the participants were asked to provide their proficiency level for languages not listed.
During the experiment, we displayed a total of 150 questions (randomized) one by one to the participants. Each question was visible for exactly ten seconds and could be answered by a single key press, such as one letter or one number (see Table 2). During this time the user was able to provide an answer by pressing the respective key. Eye
1For the other participants the data was not reliable due to glasses or make-up interfering with the recording.
ISO 639-1 [6] en
de
da
     nl fi fr el
    Spanish Turkish Slovenian Arabic Hungarian   hu
Table 1: Languages in this study.
es tr sl ar

Language
English French Danish Finnish
Example questions
 How many days are within a week? Combien de jours y a-t-il dans une semaine? Hvor mange dage er der i en uge?
Kuinka monta päivää on viikossa?
What is the first letter of your first name? Quelle est la première lettre de votre prénom? Hvad er det første bogstav i dit fornavn? Mikä on etunimesi ensimmäinen kirjain?
Table 2: Two example questions in four different languages.
  Figure 2: Apparatus showing LCD monitor with attached eye tracking device.
movements were recorded at a rate of 250Hz. The experiment was conducted in two sessions with an intermediate break in between to allow for relaxation. Figure 1 illustrates an example scanpath of one participant.
Figure 1: Fixations (circles) and saccades (lines) of one participant plotted on top of the respective question.
Results and Discussion
We applied several post processing steps to the obtained eye tracking data including event detection using a velocity based fixation algorithm [11] and blink removal. We discarded any eye movement data after the point in time when the participants provided an answer for the respective question. For the other questions, we decided to limit the examination time to five seconds after the stimulus was presented. Additionally we discarded outliers that were not sensible (lower than 50ms and higher than 600ms) given reported values for fixation duration during reading tasks in literature [10, 12].
In a preliminary analysis, we are interested in how different languages affect the user’s fixations. A simple descriptive statistic is the average fixation duration. Thus, we discarded all saccades and computed the mean fixation duration for each participant for a given language. To compare these results within our participants we normalized the mean fixation duration over all languages per participant. As a normalization factor we used the mean fixation duration of each participant given all fixation data we had collected from the respective participant. Figure 3 shows a violin plot of the distribution of the mean fixation duration (normalized) of each participant per language.
To evaluate whether there is a significant effect of the language proficiency on the average fixation duration, we ran a one-way repeated measures ANOVA. The grand mean of the average fixation duration (normalized) was 1.0. The lowest was 0.80 for proficiency level C22, while the non-proficient level, i.e. no knowledge at all3, showed the highest with 1.04. The main effect of proficiency level on the average fixation duration was statistically significant (F6,175 = 6.583, p < .001). A post-hoc Tukey HSD test revealed that the mean fixation durations of non-proficient readers were significantly longer than C1 (p < 0.05) and C2 (p < 0.001) readers.
2the highest proficiency level according to the CEFR [1] 3less than A1, with regard to the CEFR [1]

     Figure 3: Violin plot showing the distribution of the average fixation duration (normalized) of each participants per language. Language code is given by ISO 639-1 [6] (see Table 1). Color coding refers to the most prominent proficiency level within the language group, such as C-level readers for German.
Conclusion and Future Work
Our results show that even short text snippets are sufficient to infer if a user is highly proficient given the respective language. The questions are not part of a single, long body of text and are randomized in the order that they were displayed. Thus, the independence of each question still holds.
We have shown that by comparing the average fixation duration over an interval of a maximum of five seconds, we can make an adequate statement on whether the user understands the shown content or not. Furthermore, this method does not require a disruptive calibration of the eye tracking device as relative gaze position is sufficient to calculate the fixation duration. Especially low-cost eye tracking devices struggle with accurate calibration and drifts over time, making them a prime target for our approach. In the future, an incorporation of eye tracking
devices into already existing wearable computing interfaces, such as the Google Glasses might very well be possible.
Our vision is to utilize such distinct gaze characteristics in real-world scenarios where people interact with public displays. Gaze-based language switching enables foreigners to switch to a suitable language where explicit interaction might be troublesome or even impossible, e.g. at an airport or other commuting places (see Figure 4). Research already reports that off-the-shelf cameras are sufficient for simple gaze-based interaction in a tablet scenario [15] and in larger setups on wall-sized displays [16].
On a technical level, we will look into further gaze characteristics that might be key indicators of a user’s understanding. Furthermore, we will examine the usefulness of low-level scanpath comparison [3] that utilizes geometrical representations to calculate similarities between different scanpaths [7].
In our future work, we will focus on building adaptive visualizations systems that leverage gaze properties demonstrated here and recognize when the users struggle with their task.
Figure 4: Public display in Japan. There is no way for non Japanese-speaking people to change the language. An adaptive gaze-based system could bridge this gap. © CEphoto, Uwe Aranas.
Acknowledgements
We thank all colleagues and friends that provided us with the necessary language translations. This research is financially supported by the German Research Foundation (DFG) within the project C02 of SFB/Transregio 161 and supported by the SimTech Cluster of Excellence. Ethical approval for this study was obtained from the Ethics Committee at the University of Konstanz.
References
[1] Council of Europe. Common European Framework of Reference for Languages: Learning, Teaching, Assessment. Applied Linguistics Non Series. Cambridge University Press, 2001.
[2] Deutscher, G. The Unfolding of Language: An
Evolutionary Tour of Mankind’s Greatest Invention. Henry Holt and Company, 2006.
[3] Feusner, M., and Lukoff, B. Testing for statistically significant differences between groups of scan patterns. In Proceedings of the 2008 Symposium on Eye Tracking Research & Applications, ETRA ’08, ACM (2008), 43–46.
[4] Foulsham, T., Cheng, J. T., Tracy, J. L., Henrich, J., and Kingstone, A. Gaze allocation in a dynamic situation: Effects of social status and speaking. Cognition 117, 3 (Dec. 2010), 319–331.
[5] Hirvenkari, L., Ruusuvuori, J., Saarinen, V.-M., Kivioja, M., Peräkylä, A., and Hari, R. Influence of Turn-Taking in a Two-Person Conversation on the Gaze of a Viewer. PLoS ONE 8, 8 (Aug. 2013).
[6] International Organization for Standardization. Codes for the representation of names of languages – Part 1: Alpha-2 code. Standard, Geneva, CH, July 2002.
[7] Jarodzka, H., Holmqvist, K., and Nyström, M. A Vector-based, Multidimensional Scanpath Similarity Measure. In Proceedings of the 2010 Symposium on Eye-Tracking Research & Applications, ETRA ’10, ACM (New York, NY, USA, 2010), 211–218.
[8] Marschner, L., Pannasch, S., Schulz, J., and Graupner, S.-T. Social communication with virtual agents: The effects of body and gaze direction on attention and emotional responding in human observers. International Journal of Psychophysiology 97, 2 (Aug. 2015), 85–92.
[9] Martínez-Gómez, P., and Aizawa, A. Recognition of Understanding Level and Language Skill Using Measurements of Reading Behavior. In Proceedings of the 19th International Conference on Intelligent User Interfaces, IUI ’14, ACM (New York, NY, USA, 2014), 95–104.
[10] Rayner, K., Slattery, T. J., and Bélanger, N. N. Eye movements, the perceptual span, and reading speed. Psychonomic bulletin & review 17, 6 (2010), 834–839.

[11] Salvucci, D. D., and Goldberg, J. H. Identifying fixations and saccades in eye-tracking protocols. In Proceedings of the 2000 Symposium on Eye Tracking Research & Applications, ACM (2000), 71–78.
[12] Sereno, S. C., and Rayner, K. Measuring word recognition in reading: Eye movements and event-related potentials. Trends in cognitive sciences 7, 11 (2003), 489–493.
[13] Steptoe, W., Wolff, R., Murgia, A., Guimaraes, E., Rae, J., Sharkey, P., Roberts, D., and Steed, A. Eye-tracking for Avatar Eye-gaze and Interactional Analysis in Immersive Collaborative Virtual Environments. In Proceedings of the 2008 ACM Conference on Computer Supported Cooperative Work, CSCW ’08, ACM (New York, NY, USA, 2008), 197–200.
[14] Vertegaal, R., Slagter, R., van der Veer, G., and Nijholt, A. Eye Gaze Patterns in Conversations: There is More to Conversational Agents Than Meets the Eyes. In Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems, CHI ’01, ACM
(New York, NY, USA, 2001), 301–308.
[15] Wood, E., and Bulling, A. EyeTab: Model-based gaze
estimation on unmodified tablet computers. In
Proceedings of the Symposium on Eye Tracking Research and Applications, ETRA ’14, ACM (2014), 207–210.
[16] Yamazoe, H., Utsumi, A., Yonezawa, T., and Abe, S. Remote gaze estimation with a single camera based on facial-feature tracking without special calibration actions. In Proceedings of the 2008 Symposium on Eye Tracking Research & Applications, ETRA ’08, ACM (2008), 245–250.
[17] Yoshimura, K., Kise, K., and Kunze, K. The eye as the window of the language ability: Estimation of English skills by analyzing eye movement while reading documents. In Document Analysis and Recognition (ICDAR), 2015 13th International Conference on, IEEE (2015), 251–255.
   See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/266149144
An eye movement analysis of highlighting and graphic organizer study aids for learning from expository text
Article in Computers in Human Behavior · December 2014 DOI: 10.1016/j.chb.2014.09.010
CITATIONS READS
6 103
2 authors:
Hector Ponce
University of Santiago, Chile
20 PUBLICATIONS 71 CITATIONS SEE PROFILE
Richard Mayer
University of California, Santa Barbara
369 PUBLICATIONS 33,933 CITATIONS SEE PROFILE
      Some of the authors of this publication are also working on these related projects:
Using Cognitive Science to Guide Video Media Creation in Rural West Africa View project Learning Glass: A New Platform for Promoting STEM Engagement and Learning View project
   All content following this page was uploaded by Hector Ponce on 18 April 2016.
The user has requested enhancement of the downloaded file. All in-text references underlined in blue are added to the original document and are linked to publications on ResearchGate, letting you access and read them immediately.
Computers in Human Behavior 41 (2014) 21–32
     Contents lists available at ScienceDirect Computers in Human Behavior journal homepage: www.elsevier.com/locate/comphumbeh
 An eye movement analysis of highlighting and graphic organizer study aids for learning from expository text
Hector R. Ponce a,⇑, Richard E. Mayer b,1
a Faculty of Management and Economics, University of Santiago of Chile, Av. L. B. O’Higgins 3363, Santiago, Chile
b Department of Psychological and Brain Sciences, University of California, Santa Barbara, CA 93106-9660, USA
   article info
Article history:
Keywords:
Highlighting
Graphic organizers
Study strategies
Eye tracking Computer-based learning
1. Objective
The goal of the present study is to examine how study aids such as highlighting and graphic organizers affect learning from expos- itory text, such as exemplified in Fig. 1, including both the process and outcome of learning. This goal is in line with growing theoret- ical demands for applying the science of learning to study strate- gies (Dunlosky, Rawson, Marsh, Nathan, & Willingham, 2013; Mayer, 2011) as well as growing practical demands of reform efforts such as the Common Core Standards in the U.S. to focus more on expository text (Porter, McMaken, Hwang, & Yang, 2011). In this study, we use eye tracking measures (such as number of fixations and eye fixation transitions, as summarized in the top of Table 1) to provide a picture of the process of learning and we use a multi-leveled posttest (consisting of tests of memory and comprehension, as summarized in the bottom of Table 1) to pro- vide a picture of the outcome of learning.
⇑ Corresponding author. Tel.: +56 2 27180716.
E-mail addresses: hector.ponce@usach.cl (H.R. Ponce), rich.mayer@psych.ucsb.
edu (R.E. Mayer).
1 Tel.: +1 805 8932472.
http://dx.doi.org/10.1016/j.chb.2014.09.010
0747-5632/Ó 2014 Elsevier Ltd. All rights reserved.
abstract
This study uses eye tracking technology to examine how study aids such as highlighting and graphic organizers affect cognitive processing during learning. Participants were 130 college students randomly assigned to one of five experimental conditions. In the control group, students read a plain text; in two behaviorally passive conditions, students read a text with key words colored in red or read the same text along with a filled-in graphic organizer; and in two behaviorally active conditions, students either high- lighted key words in a text or filled in an empty graphic organizer. Students took tests of rote memory (cloze test) and comprehension (summary test). Asking students to fill in a graphic organizer or providing a filled-in graphic organizer resulted in improvements in performance on both tests, whereas asking stu- dents to highlight the text or providing highlighted text improved performance only in the rote memory test compared to students who did not receive any study aids. Eye tracking measures showed that high- lighting (in both conditions) primed the cognitive process of selecting: students spent more time fixating on those words colored in red compared with the control condition. In contrast, eye tracking measures showed that graphic organizers (in both conditions) primed the cognitive processes of selecting, organiz- ing and integrating since the inclusion of an organizer substantially affected both where their eyes fixated and moved (i.e. transitions) within the text.
   Ó 2014 Elsevier Ltd. All rights reserved.
2. Research on highlighting and graphic organizers
Highlighting text is part of a more general technique known as typographical cuing that also includes underlining, boldface, capi- talization, and colored types (Fowler & Barker, 1974). In this article, we refer to this variety of text marking techniques as highlighting. Among the advantages of highlighting text is that it is simple, easy to use, and training is not necessary (Dunlosky et al., 2013). Impor- tantly, several studies have found that highlighting text is among the most common study techniques. For example, in a survey of more than 500 medical students on spontaneous study strategies conducted by Lonka, Lindblom-YlÄnne, and Maury (1994), stu- dents reported the following study techniques as the most com- monly used ones: underlining (88% of participants), note-taking (68% of participants), and defining concepts (49% of participants). Similarly, Wade, Trathen, and Schraw (1990) found that when col- lege students were asked to study a text by using whatever tech- nique they knew, most students employed some form of highlighting followed by verbatim note-taking.
Research on highlighting and related signaling techniques can be divided into (a) studies in which participants are asked to mark words or sentences in a text (reader-generated highlighting), and

22 H.R. Ponce, R.E. Mayer / Computers in Human Behavior 41 (2014) 21–32
(b) instructional design studies in which participants receive learn- ing material with text already highlighted (experimenter-provided highlighting). Research on the effectiveness on both types of research designs has generated mixed results. Some studies have found positive effects while others have found neutral or even det- rimental effects (Bell & Limber, 2010; Dunlosky et al., 2013; Hartley, Bartlett, & Branthwaite, 1980). Notably, one of the most consistent findings is that highlighting can improve memory for the text that has been highlighted (Lorch, 1989). For instance, in a study conducted by Hartley et al. (1980), where sixth grade stu- dents were asked to read either a passage that contained under- lined text (experimenter-provided highlighting) or the same passage but with plain text, they found that the group that read the highlighted passage outperformed the group that read the plain-text passage as measured by a cloze test administered imme- diately and after a one-week delay.
Another study technique recommended to improve reading comprehension is the use of graphic organizers (Fiorella & Mayer, 2015; National Institute of Child Health and Human Development, 2000), although students do not tend to use them as much as the highlighting technique. Graphic organizers are spa- tial layouts that embed different types of text structures, usually associated with expository texts, such as compare-and-contrast (e.g. a matrix), sequence (e.g., flow-chart), and cause-and-effect (e.g., fishbone) (Cook & Mayer, 1988; Ponce, López, & Mayer, 2012). The use of graphic organizers is intended to prime deeper processing of the text in order to identify, first, the text structure (e.g., compare-and-contrast, cause-and-effect, etc.), and, second, the respective components and relationships within the text (e.g., similarities and differences, lists of causes and effects, etc.) (Beyer, 1997).
Similar to the highlighting study technique, research on graphic organizers can be divided into (a) studies in which students are asked to complete an empty organizer along with a text (reader- generated graphic organizers) and (b) studies in which students are provided with filled in graphic organizers along with a text (experimenter-provided graphic organizers). Meta-analyses have concluded that graphic organizers are an effective study technique for improving learning (Marzano, Pickering, & Pollock, 2001; Moore & Readence, 1984), although there are circumstances in which the use of graphic organizers does not lead to better learn- ing. For example, in a study conducted by Stull and Mayer (2007), in which college students read a biology text along with graphic organizers, the graphic organizers were effective in improving learning in cases when a small number of organizers were used (e.g., one or two per paragraph) but not effective when
they were overused (e.g., more than two organizers per paragraph). In a large scale study, Ponce et al. (2012) found that elementary school students significantly improved their reading comprehen- sion skills, measured through a standardized test, after they received scaffolded practice in the use of graphic organizers and when such practice was integrated within the language arts curriculum.
3. Eye movement during reading
In the literature we did not find studies that examine eye move- ment behavior that results from using specific reading comprehen- sion strategies such as highlighting text or filling in a graphic organizer. Most studies on eye movement during reading have focused on cognitive processes directly associated with reading texts (e.g., attention), the influence of textual variables (e.g., con- ceptual density and semantic relationships between words), and discourse factors (e.g., topic structure) (Hyönä, Lorch, & Kaakinen, 2002; Rayner, 1998).
Research on eye movement uses two main measures to study reading patterns: saccade and fixation. A saccade corresponds to a brief and a rapid movement of the eyes from one location to another in the text (principally words). In contrast, a fixation is the action of the eyes coming to rest on part of the text (a word or part of a word) for a very short period of time (Rayner, 1998). Fixation data reveal attention processes and it is only during fixa- tions that information is encoded (Rayner, Chace, Slattery, & Ashby, 2006).
During reading, patterns of fixations and saccades are influ- enced by text difficulty, task demands and use of graphics. For example, eye movement studies demonstrate that reading difficult texts requires more cognitive processing than reading simpler texts. Difficult parts of the text (e.g., uncommon words or syntac- tically complex sentences) demand more attention; which is reflected in readers’ tendency to fixate more and for longer periods of time; with an increase in regressions (i.e., backward saccades) and decrease in saccade length (Rayner, 1995; Rayner et al., 2006).
Task demands also appear to influence attentional processes during reading, as shown by Kaakinen and Hyönä (2010). In their study, a group of students was instructed to read a text for proof- reading purposes and another group to read a text for comprehen- sion purposes. Students who were asked to proofread a text showed an increase in the number of fixations, decrease in saccade length and an increase in fixation duration compared with stu- dents that were instructed to read for comprehension.
The spatial disposition of text and graphics affects cognitive processing during learning as well. Johnson and Mayer (2012) report three experiments that study the effects of the spatial con- tiguity principle (i.e., placing text near the corresponding part of a graphic tends to have a positive effect on learning) by examining eye movement behavior. In one of the experiments, a group was instructed to read and study a separated presentation (i.e., a dia- gram of a braking system separated from a text explaining such system) and another group was instructed to read and study an integrated presentation (i.e., the same diagram but the text was segmented and located next to each mechanism that constituted the braking system). Eye movement data showed that participants in the integrated condition made significantly more saccades between the text and the diagram (i.e., indicating attempts to inte- grate words and picture) and more saccades from the text to the corresponding part of the diagram (i.e., indicating successful inte- gration of words and picture) than participants in the separated condition. No significant differences were observed in total fixation time on diagrams and on text (i.e., indicating attentional focus on words and pictures).
                         Fig. 1. The steamboat passage in the text-only condition.
Table 1
Measures of eye-tracking and learning outcomes.
Name
Measures of eye-tracking
Total fixation time (s) Number of fixations Up–down transitions
Left–right transitions
Measures of learning outcomes
Summary test score Cloze test score
Description
Total duration eyes spend looking at each AOI
Total number of fixations eyes spend on each AOI Number of times eyes move from the top portion of the passage to the bottom portion of the passage or vice versa Number of times eyes move from the passage (top or bottom) to the graphic organizer area or vice versa
Number of key comparisons recalled Number of key verbatim terms filled in
H.R. Ponce, R.E. Mayer / Computers in Human Behavior 41 (2014) 21–32 23
     Our goal in this study is to use eye movement metrics such as number of fixations and number of transitions (as summarized in the top of Table 1) to examine the effect of highlighting and graphic organizers study aids on reading patterns and to use comprehen- sion and memory tests (as summarized in the bottom of Table 1) to examine the effects of highlighting and graphic organizers on learning outcomes.
4. Theoretical framework
The present study extends the growing body of research on the effectiveness of various types of text processing strategies, such as note-taking (Kiewra, 1985; Peper & Mayer, 1978), text-signaling techniques (Lorch, 1989), graphic organizers (Ponce et al., 2012; Robinson et al., 2006) and outlining based on rhetorical structures (Cook & Mayer, 1988; Meyer & Poon, 2001). The main hypothesis (which we call the depth hypothesis) is that highlighting and gra- phic organizer techniques foster qualitatively different cognitive processes during learning and therefore produce qualitatively dif- ferent learning outcomes. In particular, the depth hypothesis pro- poses that some study techniques (e.g., highlighting) promote a superficial processing of the text that improve rote memory of the highlighted material whereas others techniques (e.g., graphic organizer) promote a deeper processing of the text that foster dee- per comprehension of the text.
The depth hypothesis draws on the idea that the quality of a learning outcome depends on the kinds of the cognitive processes that learners engage in during learning, as described in the cogni- tive theory of multimedia learning (Mayer, 2009, 2011). The lear- ner can engage in three different cognitive processes during learning from an expository text such as shown in Fig. 1: the pro- cess of selecting, that is, paying attention to relevant information (e.g., highlighted text); the process of organizing, that is, building a coherent structure of the incoming information (e.g., compare- and-contrast structure); and the process of integrating, that is, relating new information with existing relevant knowledge acti- vated from long-term memory (e.g., text structure).
In reading the steamboat passage shown in Fig. 1, students might adopt a default strategy of memorizing a list of isolated facts or a constructive strategy of building an organized knowledge struc- ture such as a compare-and-contrast matrix in which two elements (eastern-style steamboats and western style steam boats) are com- pared along several dimensions (e.g., rivers, type of hull, type of engine, number of decks, etc.). Alternatively, students might not put much effort into reading the passage regardless of their reading strategy, and thereby not learn much of anything at all.
How can we prime students to engage in deep cognitive pro- cessing during learning, such as structure building? A study aid technique intended to foster structure building is to provide gra- phic organizers to accompany the text (which we call static graphic
Cognitive process
Selecting: attentional focus on each AOI
Selecting: Attentional focus on each AOI
Organizing and integrating: Attempts to relate sentences in the top section with sentences in the bottom section to establish a comparison Organizing and integrating: Attempts to integrate the graphic organize with the text and build a compare-and-contrast structure of the text
Comprehension based on a comparison-and-contrast composition. Relational sentences
Memorization of pieces of information
organizers as shown in Fig. 2) or to ask learners to fill in graphic organizers (which we call interactive graphic organizers as shown in Fig. 3). These kinds of study aids are intended to prime three kinds of cognitive processes during learning: selecting relevant information to put into a structure, organizing the relevant mate- rial into a coherent structure (i.e., a matrix in this case), and inte- grating the new knowledge with relevant prior knowledge (e.g., previous experience with compare-and-contrast structures). These cognitive activities should be reflected in improvements both on memory tests and comprehension tests.
In contrast, a study aid technique aimed mainly at memorizing isolated facts is to highlight key material in red print (which we call static highlighting as shown in Fig. 4) or to ask students to highlight key material in red print (which we call interactive high- lighting as shown in Fig. 5). These kinds of study activities are intended mainly to prime the cognitive process of selecting, which would be reflected in improvements on memory tests but not com- prehension tests.
5. Hypotheses
In this study we primarily examine the depth hypothesis, which holds that not only people learn more deeply from structure-build- ing study aids (such as graphic organizers) than memorization study aids (such as highlighting) but also that the learning process is affected differently when these study aids are adopted, as shown by the eye movement analysis. We define the predictions in terms of eye movements and learning outcomes, as follows.
  Fig. 2. The steamboat passage in the highlighted condition.

24 H.R. Ponce, R.E. Mayer / Computers in Human Behavior 41 (2014) 21–32
  Fig. 3. The steamboat passage and the filled in graphic organizer.
5.1. Eye movements hypotheses
Regarding eye tracking metrics involving number of eye fixa- tions (as an indication of the process of selecting), the depth hypothesis predicts more fixations on the important words colored in red in the static highlighting condition (i.e., guide learner’s selec- tive attention) compared with the text-only condition (Hypothesis 1). Additionally, we expect that participants in the static graphic organizer condition will fixate less on the text compared with the text-only group but will spend more time fixating on the gra- phic organizer area, showing the activation of the cognitive pro- cesses of selecting as well by guiding learner’s selective attention to information found on the organizer (Hypothesis 2).
Regarding eye tracking metrics involving transitions, we con- sider transitions between the top and bottom sections (i.e., up– down transitions) of the steamboat passage as an indication that attempts at making specific comparisons within the text are being made and therefore the activation of the cognitive process of
organizing and integrating (i.e., given the structure of the steam- boat passage it is necessary to relate sentences in the top section with sentences in bottom section). Similarly, left–right transitions also reflect attempts to integrate information. Consequently, the depth hypothesis predicts more transitions per minute (as an indi- cation of integrating) for the graphic organizer groups than the highlighting groups (Hypothesis 3).
As a secondary hypothesis, the interactivity hypothesis is that people learn more when they are primed to engage in hands-on activity during learning through interactive study aids (such as interactive highlighting and interactive graphic organizers) rather than static study aids (such as static highlighting or static graphic organizers). Based on the interactivity hypothesis, we expect to observe more activity in terms of transitions between the top and bottom sections (i.e., up–down transitions) and vice versa in the interactive conditions compared with the static conditions. Additionally, we expect to observe significantly more transitions between the text and the graphic organizer area (i.e., right–left
  Fig. 4. The steamboat passage and the interactive graphic organizer.
transitions) in the IGO group compared with the SGO since partic- ipants in the IGO group need to read the text more often to fill in the graphic organizer (Hypothesis 4).
5.2. Learning outcomes hypotheses
Regarding the memory test (which we consider a reflection of the cognitive process of selecting), the depth hypothesis predicts that the graphic organizer groups (i.e., IGO and SGO groups) and the highlighting groups (i.e., static and interactive highlighting groups) will all outperform the text-only group (Hypothesis 5). In contrast, on the comprehension test (which is intended to reflect the cognitive process of integrating as well as selecting and orga- nizing), the depth hypothesis predicts that the graphic organizer groups will score significantly higher than the text-only group but the highlighting groups will not (Hypothesis 6).
Finally, the interactivity hypothesis predicts that the interactive highlighting group will outperform the static highlighting group (Hypothesis 7) and the IGO group will outperform the SGO group (Hypothesis 8) on the comprehension test and the memory test. It should be noted that the depth hypothesis does not make this prediction because behavioral activity does not guarantee appro- priate cognitive activity.
6. Method
6.1. Participants and design
The participants were 130 college students recruited from the Psychology Subject Pool at the University of California, Santa Bar- bara. The average age of the participants was 19.45years (SD = 1.26) and the proportion of females was 0.83. Forty-five per- cent of the participants were freshmen (first year), 29% were soph- omores (second year), 16% were juniors (third year) and 10% were seniors (fourth year). The mean rating on self-rated prior knowl- edge about steamboats was 1.68 (SD = 0.75) in a scale of 1–5, which is in the very low range.
There were five groups based on a between-subject design, with 26 participants in each group: text-only group, which read a plain text; static highlighting group, which read a text with key words colored in red; interactive highlighting group, which read a text with an editor so learners could highlight portions in red; static graphic organizer (SGO) group, which read a text along with a filled in graphic organizer; and interactive graphic organizer (IGO)
group, which read a text along with an empty graphic organizer for learners to fill in.
The groups did not differ significantly from one another (at p < .05) in mean age, self-rated prior knowledge of steamboats, or proportion of men and women.
6.2. Materials
The instructional materials consisted of five versions of a 123- word expository passage on steamboats adapted from Meyer and Poon (2001), and implemented in PowerPoint as shown in Fig. 1. This text compares two types of steamboats (i.e., eastern-style and western-style) along several dimensions (e.g., river depth, cargo storage engine type, etc.), so the rhetorical structure of this text is compare-and-contrast (Cook & Mayer, 1988). The first 6 sen- tences of the passage describe eastern-style steamboats and the last 7 sentences describe western-style steamboats. The Flesch reading ease index was 0.48 and the Flesch-Kincaid grade level was 10; indicating that the steamboat passage can easily be under- stood by students in 10th grade (i.e., 15–16 years old students).
As shown in Fig. 1, in the text-only version (control group), the steamboat passage was presented as the only window on the screen. As shown in Fig. 2, the static highlighting version was iden- tical to the text-only condition except that 28 words were high- lighted in red. The highlighted words were the elements being compared between eastern-style and western-style steamboats (e.g., Hudson River vs. Missouri, Ohio and Mississippi, deep vs. flat, low-pressure vs. high-pressure, etc.). The objective was to direct readers’ attention to these words in order to make readers more easily aware of the comparisons.
As shown in Fig. 3, the static graphic organizer (SGO) version contained the steamboat passage on the left side and a filled in gra- phic organizer on the right side of the screen. On the top section of the graphic organizer, ‘‘eastern-style steamboat’’ was printed in the left corner and ‘‘western-style steamboat’’ was printed on the right corner. In the middle section, each attribute or dimension was explicitly indicated with the respective value for each type of steamboat. The same values that appear in red in the highlighted condition were presented in the graphic organizer, although the comparison is made more explicit in the graphic organizer by pro- viding additional structural information.
As shown in Fig. 4, the interactive highlighting version presents the steamboat passage within an editor application with function- alities to highlight text. The interactive highlighting editor is an
H.R. Ponce, R.E. Mayer / Computers in Human Behavior 41 (2014) 21–32 25
  Fig. 5. The steamboat passage in the editor for highlighting.

26 H.R. Ponce, R.E. Mayer / Computers in Human Behavior 41 (2014) 21–32
application developed in Adobe Flash. The editor implements func- tions to highlight text in various colors but students were asked to highlight only with the color red. This version also has functional- ity to undo any piece of highlighted text once the participant has already colored part of the text, if required.
As shown in Fig. 5, the interactive graphic organizer (IGO) ver- sion presents the steamboat passage on the left side and an embed- ded graphic organizer on the right side. The graphic organizer was implemented in Adobe Flash so it was simple to integrate into PowerPoint as an embedded object. The interactive graphic orga- nizer was presented with empty boxes so the learner could type in the two elements to be compared, could add information about one dimension and its respective values. Learners could add as many additional dimensions as they wished, and type in the name of each dimension and its values for the eastern-style and western- style steamboats.
The test materials consisted of a cloze test and a summary test. The cloze test was devised to measure verbatim memory of the les- son. The cloze test consisted of a sheet of paper containing the steamboat passage but with 13 words eliminated and replaced with blank spaces for participants to fill in. The following words were eliminated: Hudson, deep (three times), below, low-pressure, shallow, Missouri, Ohio, Mississippi, flat, on, high-pressure (as shown in Appendix A). These are the words that were highlighted in red in the highlighted version of the lesson or used in the cells of the graphic organizer in the graphic organizer version of the lesson. One point was given for each exact word that was filled in cor- rectly, yielding a maximum score of 13 points.
The summary test was devised to measure comprehension of the text and was presented on a computer screen. The summary test consisted of a computer-based blank MS-Word document that participants used to type a summary of the steamboat passage without access to the original version. The exact instruction to par- ticipants was: ‘‘Please write a summary of the steamboat lesson. You have a maximum of 5 min.’’ To score the summary, we created a 9-item rubric consisting of comparisons between the eastern steamboat and the western steamboat on each of nine dimensions (as shown in Appendix B). One point was given for each complete comparison in the summary, either written in one complete sen- tence (e.g., eastern steamboats have deep hulls and western steam- boats have flat hulls) or written in two separate sections (e.g., as in the original text). The maximum score was 9 points.
The informed consent form described the experiment, summa- rized the participant’s rights, and contained a space for the partic- ipant’s signature. The participant pre-questionnaire solicited basic demographic information concerning age, sex, and year in college. The participant post-questionnaire consisted of five statements that asked for ratings on a 5-point scale about previous knowledge of steamboats, difficulty and effort in learning the steamboat les- son, and satisfaction and motivation by asking participants their level of agreement with the following statements ‘‘I like using this computer program’’ and ‘‘I would like more lessons like this one’’.
The apparatus consisted of a Tobii T-60 eye tracking system used to present the passage and record eye movements and a Dell computer where the Tobii Studio software was installed.
6.3. Measures
6.3.1. Measures of eye movements
First, we computed total fixation time and number of fixations in the regions analyzed for each area of the screen (as summarized as the first two measures in Table 1). Second, we established areas of interest (AOIs), which are defined as regions in the stimulus where a researcher focuses his/her attention to examine eye move- ment patterns (e.g. words, sentences, and images), and we counted
the number of transitions (or saccades) between AOIs (Holmqvist et al., 2011).
In our study, the steamboat passage was divided into two AOIs: the top section of the text (i.e., the first 6 sentences) and bottom section of the text (i.e., the last 7 sentences) (see Fig. 1). The reason for this two initial AOIs is that in order to make a comparison between different attributes (e.g., hull types) associated with the steamboats it is necessary to locate the information in the top sec- tion and later to compare it with the respective information in the bottom section. This processing takes place in working memory, which has limited capacity, so numerous fixations and transitions on key parts of the texts may be required. In addition, for the static and interactive graphic organizer groups, the graphic organizer (on the right side of the screen) was defined as a third area of interest. Finally, an orthogonal set of AOIs was constructed to compare highlighted words (i.e., the red words in the static highlighting group) and the non-highlighted words.
We counted two types of transitions: (1) transitions between top and bottom sections and vice versa of the steamboat passage (which we refer to as up–down transitions) and (2) transitions between the steamboat passage and the graphic organizer area and vice versa (which we refer to as left–right transitions). We view up–down transitions as an indication of the cognitive process of organizing and integrating since the learner is attempting to relate specific sentences from the top and bottom sections of the steamboat passage to establish comparisons (as summarized in the third measure in Table 1). Similarly, we consider left–right tran- sitions as attempts to integrate the graphic organizer with the text and construct a compare-and-contrast structure of the steamboat passage (as summarized in the fourth measure in Table 1).
In previous studies, transition measures have been used to reflect attempts at integrating information from two or more AOIs. For instance, Johnson and Mayer (2012) counted the number of transitions between an AOI containing a text and an AOI containing a picture (about a braking system) as attempts at engaging in the cognitive process of integrating during learning. Similarly, Mason, Pluchino, and Tornatora (2013) used second-pass fixation time as a measure of transitions between a text and a picture which repre- sents cognitive efforts at integrating written and pictorial representations.
To facilitate and compare eye tracking measures, the text was formatted exactly in the same way for the text-only, static high- lighting, and static graphic organizer groups in terms of font type and size, space between words and sentences, and location of the text on the screen. In case of the interactive graphic organizer group, the font of the text was slightly smaller than the previous conditions to create more space and better accommodate the func- tionalities of the graphic organizer. Finally, due to the restricted functionalities of the editor application used in the interactive highlighting group, the size and location of the text on the screen were slightly different; so the AOIs were defined separately for this version of text. Data from one participant in the highlighting con- dition was excluded in the eye movement analysis due to the low- quality of eye tracking data.
6.3.2. Measures of learning outcomes
Two measures of learning outcomes were used: rote memory, measured via a cloze test, and reading comprehension, measured by asking the learner to write a summary (as summarized as the fifth and sixth measures, respectively, in Table 1). The cloze test is intended as a test of verbatim memory, and hence is referred to as a memory test; the summary test is intended as a test of how the material is organized, and hence is referred to as a com- prehension test. Cloze tests and summary tests have been used before as a measure of memory and reading comprehension respectively in studies involving the highlighting (Hartley et al.,

1980) and graphic organizer techniques (Kiewra, Kauffman, Robinson, Dubois, & Staley, 1999; Ponce & Mayer, 2014).
6.4. Procedure
Participants were randomly assigned to one of the five groups and were tested individually. As the participant arrived, he or she was seated in the testing room in front of the Tobii’s computer screen. The experimenter explained the aims of the study—to examine eye movement patterns while students read a text. Participants were asked to read and sign the consent form (which explained human subject protections) and to complete the partic- ipant pre-questionnaire (which solicited basic demographic infor- mation concerning age, sex, and year in school).
Next, the experimenter explained the eye tracking technology and proceeded with eye tracking calibration for the participant. Upon completion of calibration, the participant received verbal and written instructions on how to proceed based on his or her treatment group. In the text-only, static highlighting, and static graphic organizer groups, all participants were asked to read and study the text for a maximum of 2 min. In the interactive highlight- ing group and the interactive graphic organizer group, the experi- menter gave each participant a brief explanation of the functionalities of the highlighting editor or graphic organizer appli- cation, respectively. For training purposes, participants were asked to read a brief text that compared Arizona and Rhode Island in terms of population, climate and size. Participants in the highlight- ing conditions were asked to highlight key ideas on the practice text by first selecting a word or group of words and color them in red (using the respective function in the application). Similar to the interactive highlighting group, participants in the interactive graphic organizer group were asked to fill in the graphic organizer by using the same practice text. In both cases, the training sessions lasted about 5 min. After the training session, each participant received verbal and written instructions to read and study the steamboat text for a maximum of 4 min, using the highlighting editor or graphic organizer application corresponding to his or her treatment group. Participants in all five groups were also told that after studying the text they would be tested on the material.
For each condition, immediately after each participant studied the steamboat passage and the eye tracking recording was finished, the participant was asked to write a short summary of the text by typing directly into a MS-Word document on a computer. The experimenter informed the participant that the time limit to write the summary was 4 min. The participants were not allowed to review the original text, their own highlighted text, or their graphic organizer. After writing the summary, the paper-based cloze test was administered, with instructions that participants had 2 min to fill in the blank spaces in the test.
The final stage was to complete the post-questionnaire, in which participants rated their previous knowledge on steamboats, perceived difficulty and effort in performing the task, and satisfac- tion with the application. Finally, participants were debriefed, thanked, and dismissed. We adhered to standards for treatment of human subjects.
7. Results
7.1. Eye tracking results
The primary concern in this study is whether highlighting and graphic organizer study techniques prime different cognitive pro- cessing as shown by eye movement analysis. Our main hypothesis is that highlighting primes the cognitive process of selecting (i.e., focus learner’s attention on memorizing isolated facts) whereas
graphic organizers prime the cognitive processes of selecting, orga- nizing and integrating (i.e., encourage learners to build knowledge structures). We view eye fixations (time and number) as an indica- tion of the cognitive process of selecting and transitions between AOIs as an indication of the cognitive processes of organizing and integrating.
7.1.1. Eye fixations as an indication of the cognitive process of selecting
First, we compared eye fixation time and number between sta- tic highlighting and text-only groups. Highlighting is intended mainly to guide the learner’s selective attention to the highlighted material. Table 2 shows that the mean number of fixations and the mean total fixation time on the highlighted words in the text were greater for students in the static highlighting group than those in the text-only group. A t-test conducted on the fixation data sum- marized in Table 2 shows that the groups differed significantly on number of fixations, t(49) = 2.53, p < .05, d = 0.70, indicating that the static highlighting group outscored the text-only group. Additionally, a t-test conducted on the total fixation time data summarized in Table 2 shows that the groups differed significantly on total fixation time, t(49) = 2.18, p < .05, d = 0.61, indicating that static highlighting group outscored the text-only group. Consistent with the depth hypothesis (Hypothesis 1), these patterns of results provide some validation of the idea that the highlighting technique serves to guide the cognitive process of selecting.
Eye fixation data show that the graphic organizer also primes the cognitive process of selecting, as previously predicted (Hypoth- esis 2). Table 3 shows the mean total fixation time on each AOI for of the text-only and SGO groups. It can be observed that partici- pants in the SGO group fixated in average significantly less on the text compared with the text-only group, with t(50) = 5.64, p < .001. The SGO spent a substantial amount of time fixating on the filled-in graphic organizer, with 58.2% on the text and 41.8% on the graphic organizer. Consequently, these results confirm that the graphic organizer serve to guide learners’ selective attention to the information provided on the organizer.
7.1.2. Transitions as an indication of the cognitive processes of organizing and integrating
We view up–down transitions (and left–right transitions, where applicable) as measures of the deeper cognitive processes of orga- nizing and integrating. The depth hypothesis predicts that graphic organizers will prime more transitions than highlighting (Hypoth- esis 3). Table 4 summarizes the number of transitions and the number of transitions per minute for each group. We computed the mean number of transitions per minute in order make compar- isons between groups that received 2 min of study time (text-only, static highlighting, SGO) and those that received 4 min (interactive highlighting, and IGO).
First, we tested whether reading a highlighted text induces more transitions than reading a plain text. If a larger number of up–down transitions were observed for the highlighted group in comparison with the text-only group it would be indicative of the cognitive process of organizing and integrating, since it would show that the reader is attempting to integrate and organize differ- ent sections of the text by making specific comparisons (e.g., top section: ‘‘cargo was stored...below the main deck’’ vs. bottom
Table 2
Means and standard deviations on time and number of fixations on highlighted text.
H.R. Ponce, R.E. Mayer / Computers in Human Behavior 41 (2014) 21–32 27
    Group
Text-only
Static highlighting
N Time (s) Number of fixations M SD M SD
26 24.27 4.82 90.88 19.87 25 28.06 7.38 108.64 29.59

28 H.R. Ponce, R.E. Mayer / Computers in Human Behavior 41 (2014) 21–32
Table 3
Means and standard deviations on total fixation time in each area of interest (in seconds).
  Group
Text-only
Static graphic organizer
Table 4
N Top Bottom GO Total time
M SD M SD M SD M SD
      26 51.76 13.00 26 33.80 12.28
48.64 11.29 32.09 13.95
N Up–down transitions
M SD M
26 13.08 6.37 –
26 5.46 3.80 12.46 25 16.20 8.42 –
26 24.92 7.10 68.31 26 31.81 14.07 –
– – 47.29 23.46
Total transitions
M SD M SD
100.39 20.61 113.18 16.65
 Means and standard deviations on the transitions between AOIs.
  Group
Text-only
Static graphic organizer Static highlighting Interactive graphic organizer Interactive highlighting
Left–right transitions SD
Transitions per minute
      – 8.68
– 19.48 –
13.08 6.37 17.92 8.89 16.20 8.42 93.23 21.99 31.81 14.07
7.63 3.30 9.35 4.13 9.31 4.44
28.29 6.54 10.23 3.81
 section: ‘‘cargo was carried on the main deck’’). Yet, the depth hypothesis predicts that participants in the static highlighting con- dition would make similar number of up–down transitions com- pared with the text-only group. This hypothesis was confirmed, with t(49) = 1.50, p > .05, indicating that participants in the static highlighting conditions made few attempts to organize and inte- grate different sections of the text.
Second, we tested whether the SGO group induced more transi- tions than the text-only and static highlighting groups. It was observed that the total number of transitions per minutes was not significantly different, with F(2,74)=1.56, p>.05; however, the nature of such transitions was considerably different. Analyz- ing the transitions between the text and graphic organizer (i.e., left–right transitions in Table 4) for the SGO condition; it seems that participants actively used the graphic organizer for organizing and constructing a comparison-and-contrast mental representa- tion of the steamboat passage. The average number of left–right transitions was significantly higher than the average number of up–down transitions in the SGO condition, with t(25) = 3.56, p < .01, which is consistent with the depth hypothesis.
Third, according to the interactivity hypothesis, we expect more cognitive processing when hands-on activities are used to study a lesson (Hypothesis 4). Therefore, we tested whether highlighting text induces more transitions than highlighted text. As shown in the up–down transition column of Table 4, the interactive high- lighting group made considerable more transitions than the static highlighting group. However, if we take time on task into account, it can be observed that the difference between these two groups in terms of transitions per minutes was not statistically significant, t(49) = .80, p > .05.
Fourth, we tested whether asking students to filling in a graphic organizer to analyze a text induce more transitions than given them a filled in graphic organizer. In this case, the proportion of transitions was examined. In relation to the total number of tran- sitions between AOIs, the proportions of up–down transitions were 30% and 27% in the static and interactive graphic organizer condi- tions, respectively; meanwhile, the proportions of left–right transi- tions were 70% and 73%, respectively. These proportions were not statistically different between groups (p > .05). Therefore, the eye movement analysis appears to show that the graphic organizer affected reading patterns in similar fashion in both conditions. However, the number of transitions per minutes shows that the IGO group performed significantly more transitions than the SGO group, with t(50) = 12.48, p < .001 and d = 3.46. In the SGO condi- tion there is no need to make as many transitions between the text and graphic organizer area as in the IGO condition. The tendency in
the SGO condition is to examine both areas (i.e. text and graphic organizer) as independent areas. In contrast, the need to fill in the graphic organizer in the IGO condition makes it necessary to fully integrate the text and graphic organizer area, as shown by the large number of transitions.
7.2. Learning outcomes results
The eye movement analysis is consistent with the idea that the highlighting study aid primes the cognitive process of selecting whereas the graphic organizer study aid primes the cognitive pro- cess of selecting, organizing and integrating. We examine in this section differences on learning outcomes generated by these study aids. Thus, the cloze test was intended to measure rote memory for pieces of information in the lesson (indicating the degree to which the learner engaged in the cognitive process of selecting during learning). The summary test was intended to measure comprehen- sion of the lesson (indicating the degree to which the learner engaged in the cognitive processes of selecting, organizing and integrating during learning).
7.2.1. Depth hypothesis
According to the depth hypothesis, we predicted that all four of the treatments would promote the cognitive process of selecting as compared to the control group and therefore improve performance on the cloze test (Hypothesis 5). The left side of Table 5 shows the mean score (and standard deviation) of each group on the cloze test. An ANOVA showed a significant difference among groups, F(4, 125) = 6.08, p < .001. As predicted, follow-up post hoc analysis using a Dunnett test revealed that all four instructional treatment groups outperformed the text-only group with p < .05. Consistent with predictions, the effect sizes were large for the static graphic
Table 5
Means and standard deviations on cloze test and summary test for each group.
 Group
Text-only
Static graphic organizer Static highlighting Interactive graphic
organizer Interactive highlighting
N Cloze test
M SD d M
Summary test SD d
    26 4.19 2.79 26 6.62* 3.01 26 6.88* 3.39 26 8.27* 2.82
26 6.54* 3.14
– 2.69 .84 4.42* .87 3.92
1.45 5.35*
.79 3.50
2.15 2.58 2.43 1.94
– .89
.49 1.17
2.20
.33
 Note: Effect size (d) computed in relation to the text-only group. * Group scored significantly higher than the text-only group.
organizer (d = 0.84), IGO group (d = 1.45), static highlighting group (d = 0.87), and interactive highlighting group (d = 0.79).
The depth hypothesis predicts that the two graphic organizer groups would outperform the text-only group on the summary test but the two highlighting groups would not outperform the text- only group on the summary test (Hypothesis 6). The right side of Table 5 shows the mean score (and standard deviation) of each group on the summary test. As an initial step, an ANOVA showed a significant difference among groups, F(4, 125) = 5.00, p < .001. As predicted by the depth hypothesis, follow-up post hoc analysis using a Dunnett test showed that the SGO and IGO groups signifi- cantly outperformed the text-only group with p < .05, whereas the other groups did not. Also consistent with predictions of the depth hypothesis large effect sizes were obtained for the SGO group (d = 0.89) and the IGO group (d = 1.17) but smaller effect sizes were obtained for the static highlighting group (d = 0.49) and the inter- active highlighting group (d = 0.33).
Overall, the pattern of results shown in Table 5 is most consis- tent with the depth hypothesis that highlighting primes the cogni- tive process of selecting (reflected in improvements on the cloze test) whereas graphic organizers prime the cognitive processes of selecting, organizing and integrating (reflected in improvements on both the cloze test and the summary test).
7.2.2. Interactivity hypothesis
According to interactivity hypothesis, asking students to high- light a text (i.e., interactive highlighting) should result in better learning than designing a lesson with experimenter-provided high- lighted text (i.e., static highlighting) (Hypothesis 7). We did not find evidence to confirm this hypothesis. A t-test revealed that stu- dents in the static highlighting group did not obtain significantly different scores than students in the interactive highlighting group on the cloze test, t(50) = .38, p > .05, nor on the summary test, t(50) = .66, p > .05. Also in contrast to the interactivity hypothesis, the effect size favoring the static highlighting group was d = 0.10 on the cloze test and d = 0.18 on the summary test, which are regarded as inconsequential.
Furthermore, according to the interactivity hypothesis, asking students to actively fill in a graphic organizer (i.e., IGO) should result in better learning when giving them a lesson with author- provided filled-in graphic organizer (i.e., SGO) (Hypothesis 8). Con- sistent with predictions, on the cloze test, students in the IGO group significantly outperformed students in the SGO group, t(50) = 2.05, p < .05, yielding an effect size of d = 0.57, favoring the IGO group. On the summary test, the difference was not signif- icant, t(50) = 1.46, p > .05, and the effect size favoring the IGO group was d = 0.41.
Overall, based on the results reported in this section, we con- clude that the interactive activity hypothesis was not supported in its most stringent form. In short, learner activity per se does not necessarily cause learning. It appears that interactivity may not be effective for unstructured tasks such as highlighting in which the learner may engage in unproductive activities but may be effective for structured tasks such as filling in a graphic orga- nizer as long as the learner engages in productive activities.
In order to better pinpoint when interactivity might contribute to learning, we examined how well participants used the interac- tive highlighting and graphic organizer learning strategies and its effects on learning outcomes. To assess the highlighting strategy, first, we counted the total number of words highlighted by the par- ticipants using the color red, and second, we counted how many of these words corresponded to those colored in red in the static highlighting condition (i.e. common words). Third, we computed a ratio between total of highlighted words divided by common words (i.e. proportion of corresponding words). Fourth, we com- puted a correlation between this ratio and the cloze and summary
scores. The Pearson correlation between the proportion of corre- sponding words and cloze score was r(26) = .51, p < .05, and with the summary score, it was r(26) = .28, p > .05.
In relation to the IGO condition, we counted the number of cor- rect dimensions and respective values filled in the graphic orga- nizer by each participant. The mean number of dimensions and respective values identified by the participants was 4.15 (SD = 1.57), out of a maximum score of 7.0 (i.e., corresponding to the seven dimensions contained in the static graphic organizer). The Pearson correlation between correctly identified dimensions and values on the graphic organizer and cloze score was r(26) = .71, p < .001, and with the summary score, it was r(26) = .76, p < .001.
Overall, this correlational analysis suggests that the highlight- ing study strategy mostly prime memorization of isolated facts (i.e., as measured by a cloze test) whereas the study strategy of cre- ating a graphic organizer tends to support both building a coherent cognitive structure (i.e., as measured by the summary test) and memorization of the facts that go into it (i.e., as measured by a cloze test).
7.3. Effort, motivation, difficulty and satisfaction
Finally, this section presents the results on the post-question- naire that asked students to rate (on a 5-point scale) their level of effort and difficulty in learning the steamboat lesson and moti- vation and satisfaction with the lesson.
Based on the study aid hypothesis, adding a study aid to a les- son such as a graphic organizer or highlighted text is expected to increase cognitive processing compared with reading a plain text and therefore require more mental effort. On the other hand, study aids are expected to decrease extraneous processing compared with reading a text without aids and consequently reduce the per- ceived difficulty of the lesson. The first two columns of Table 6 show the average results (and standard deviation) for perceived effort and difficulty. Contrary to expectation, we did not find a sig- nificant difference for perceived effort. Regarding difficulty, stu- dents in the text-only condition perceived the highest level of difficult with the lesson. In a pair-wise comparison, using a Dun- nett test, the IGO group was the only one that showed a significant difference in terms of difficulty compared with the text-only group (p < .05) and with an effect size of  0.71.
Another aspect measured was a dimension of motivation (third column in Table 6) with the question ‘‘I would like more lesson like this one,’’ which was devised to measure persistence on a learning task (an aspect of motivation) (third column in Table 6). We did not find significant differences between groups. The last item mea- sured was overall satisfaction with the lesson (see last column in Table 6) with the question ‘‘I like using this computer program.’’ In average, the group that showed the least satisfaction with the lesson was the text-only group. In a pair-wise comparison, using a Dunnett test, the SGO group was the only one that showed a sig- nificant difference in terms of satisfaction compared with the text- only group (p < .05) and with an effect size of d = 0.98. The IGO group resulted with an effect size of 0.58, followed by the static highlighting (d = 0.45) and interactive highlighting (d = 0.34).
8. Discussion
8.1. Empirical contributions
First, we found evidence to support the depth hypothesis, which holds that shallow study aids promote low-level processing (Hypothesis 1) such as selecting relevant information whereas dee- per study aids such as graphic organizers promote both low-level
H.R. Ponce, R.E. Mayer / Computers in Human Behavior 41 (2014) 21–32 29
30 H.R. Ponce, R.E. Mayer / Computers in Human Behavior 41 (2014) 21–32 Table 6
Mean ratings and standard deviations on effort, difficulty, motivation, and satisfaction. Group Effort Difficulty
Motivation Satisfaction
      M SD M
Text-only 3.15 0.88 2.77 Static graphic organizer 3.35 0.80 2.50 Static highlighting 3.38 0.80 2.38 Interactive graphic organizer 3.19 0.85 2.15* Interactive highlighting 3.27 0.67 2.42
Group scored significantly higher than the text-only group.
SD M SD
0.91 3.77 0.71 0.71 3.96 0.66 0.75 3.77 0.65 0.83 3.77 0.59 0.86 3.69 0.79
M SD
3.31 0.62 3.92* 0.63 3.58 0.58 3.65 0.56 3.54 0.71
   *
processing (Hypothesis 2) and high-level processing such as orga- nizing relevant information (Hypothesis 3). Thus, adding study aids such as highlighting and graphic organizer to a lesson improves learning outcomes significantly more compared to a lesson with- out such aids (i.e., text-only) when the test taps rote memory for the presented material (Hypothesis 5), but only graphic organizers (both static and interactive) produce improvements when the test taps comprehension (Hypothesis 6). This pattern is consistent with previous empirical studies that have found that highlighting text tends to improve memory of the text that has been marked (Hartley et al., 1980; Lorch, 1989) but it does not support focusing on connections between ideas in the text (Dunlosky et al., 2013). Our analysis of eye tracking measures further supports this find- ing: students in the highlighting conditions spent significantly more time looking at that part of the text that was highlighted in comparison with the text-only group, with medium effect sizes of 0.70 for number of fixations and 0.61 for fixation time (Hypoth- esis 1).
Second, interactivity per se, that is, adding hands-on activities such as asking students to highlight text or fill in a graphic orga- nizer does not automatically lead to better learning compared with the static (experimenter-provided) versions of the same study aids (Hypotheses 7 and 8). Unstructured tasks such highlighting appear to engage students in unproductive activity, such as extraneous cognitive processing (Mayer, 2009; Sweller, 1999) that is, addi- tional cognitive processing which does not lead to building a coherence structure of the text. This was observed when partici- pants highlighted sections of the text that were not relevant for the comparison, such as ‘‘Eastern-style steamboat became a finan- cial success in 1807’’. Eye tracking measures show that participants in the interactive highlighting group made the highest number of up–down transitions; however, such transitions did not result in attempting to make connections between these two parts of the text, as shown by the small effect size in the summary test (d = 0.33) (Hypothesis 4). Consequently, providing highlighted text appears to be more efficient in terms of cognitive processing than asking students to highlight text.
On other hand, structured tasks such as filling a graphic orga- nizer appear to promote productive learning activity, such as gen- erative cognitive processing (Mayer, 2009; Sweller, 1999). Our eye tracking measures support these findings, particularly with the interactive graphic organizer. In this case, significantly more up– down and left–right transitions per minute were observed com- pared with its static version, with a very large effect size equal to 3.46 (Hypothesis 4). This shows that students that filled in the gra- phic organizer followed its embedded structure (i.e., compare-and- contrast) to read and comprehend the steamboat passage, as shown also by large effect size in the summary test (d = 1.17).
8.2. Theoretical contributions
Consistent with the depth hypothesis, findings from the learn- ing outcome and eye tracking measures show that each study aid
technique explored in this study—highlighting and graphic orga- nizer—appear to activate different cognitive processes during learning as described in the cognitive theory of multimedia learn- ing (Mayer, 2009, 2011). Eye movement patterns show that both the static and interactive highlighting techniques prime only the cognitive process of selecting during studying a text such as the steamboat passage. Compared with the control group (i.e., text- only) eye movement patterns were not substantially modified by the inclusion of the highlighting study aid, except for attention (i.e., shown by fixation duration and number of fixations) on important words (i.e., those colored in red in the static condition).
On the other hand, eye movement patterns demonstrate that both the static and interactive graphic organizers prime the cogni- tive processes of selecting, organizing and integrating by providing a structure that modifies substantially where the eyes fixate and move (i.e. transitions) within the steamboat passage. This was more evident with the interactive graphic organizer in which eye movements showed that in order to fill in each component (i.e., an empty text-box) of the organizer readers tended to read the text following the structure of the graphic organizer. In case of the filled in graphic organizer (static version), eye movement measures (i.e., transitions) showed less integration between the text and graphic organizer compared with the interactive version. One possible explanation is that the information is already in the graphic orga- nizer within an easy-to-read compare-and-contrast structure, so there is little need to revisit the text too often (Beyer, 1997; Ponce & Mayer, 2014).
8.3. Practical contributions
Two relevant practical contributions can be derived from this study. First, from a teaching point of view, choosing the appropri- ate study technique to instruct students on how to process expos- itory text can have important consequences on learning outcomes. Teaching students to highlight text without appropriate structures (e.g., cause-and-effect or comparison-and-contrast) may help stu- dents select what to highlight, but not help them organize the selections in working memory and to integrate them with previous knowledge from long-term memory. The advantage of graphic organizers is that they provide structures that embed specific pro- cedure on what to select, and how to organize and integrate the learning material (Robinson & Skinner, 1996).
A second important contribution relates to the instructional design of multimedia learning applications (Mayer, 2005). High- lighting important words in texts, which is easier to implement in a multimedia application, can improve retention of information. However, if the goal is to improve comprehension of texts, more elaborated instructional design aids must be included that show not only what is important in the text (e.g. key concepts) but also its underlying structure (e.g., comparison-and-contrast). We have demonstrated that appropriate inclusion of graphic organizers can have a significant impact on improving comprehension of expository texts.

8.4. Limitations and future directions
Appendix B. (continued)
H.R. Ponce, R.E. Mayer / Computers in Human Behavior 41 (2014) 21–32
31
   Two relevant limitations can be recognized in our study. First, the steamboat passage is a short and well-structured expository text, so it would difficult to extrapolate our results to longer texts that may need other reading strategies for their comprehension. Second, other structures found in expository texts (e.g., cause- and-effect, sequence, and hierarchy) may also show distinct read- ing patterns that may require the construction of different eye tracking measures (e.g., transitions between AOIs) compared to the ones used in this study. Therefore, these limitations open the option to expand this study to longer texts and incorporate texts with different structures.
Appendix A. Cloze test
Please fill in the missing words:
Eastern-style steamboats became a financial success in 1807. These one-story boats operated on the _____________ River and other eastern rivers. These rivers were _____________ and suited perfectly the _____________ hulls of the eastern steamboat. The cargo was stored in these _____________ hulls _____________ the main deck. The eastern steamboats used _____________ engines. Western-style steamboats, however, were different. They churned their way up the _____________ waters of the _____________, _____________, and _____________ Rivers. Their hulls were _____________, without room for cargo. The cargo was carried _____________ the main deck or on the superstructure, one or two floors above the main deck. More efficient and dangerous _____________ engines were used and often burned up to 32 cords of wood a day.
Appendix B. Summary scoring rubric
The following rubric was used to score the summaries. Essen- tially, it consists of the possible comparisons made in the steam- boat text. Each comparison receives one point, with a maximum of 9 points.
Attribute
A4 Hull type
A5 Cargo storage
A6 Number of floors
A7 Engine type
A8 Engine efficiency
A9 Engine safety
Final score
References
Value
Deep vs. flat
In deep hull below main deck vs. on main deck
One-story vs. 1 or 2 floors
Low-pressure vs. high pressure
High-pressure engine more efficient than low-pressure
High-pressure engine more dangerous than low-pressure
Example Score
western rivers were shallow The hull on the eastern steamboat was deep meanwhile on the western steamboat the hull was flat
The cargo on the eastern steamboat was stored in deep hulls while on the western steamboat was stored on the main deck Eastern steamboat were one-story while western steamboat were 1or2floors
The engine on the eastern steamboat was low-pressure meanwhile on the western steamboat was high-pressure High-pressure engines were more efficient than low- pressure engines High-pressure engines were more dangerous than low- pressure engines
   Attribute
A1 Steamboat type
A2 Rivers of operation
Value
Eastern-style and western- style were different
Eastern rivers: Hudson river vs. western rivers: Missouri, Ohio, Mississippi
Deep vs. shallow
Example Score
The passage is about the differences between eastern-style and western- style steamboats Eastern steamboat operated on eastern rivers such as Hudson River meanwhile western steamboat operated on western rivers such as Missouri, Ohio, and Mississippi Eastern rivers were deep meanwhile
            A3 River depth
Bell, K. E., & Limber, J. E. (2010). Reading skill, textbook marking, and course performance. Literacy Research and Instruction, 49(1), 56–67.
Beyer, B. K. (1997). Improving student thinking: A comprehensive approach. Boston, MA: Allyn and Bacon.
Cook, L. K., & Mayer, R. E. (1988). Teaching readers about the structure of scientific text. Journal of Educational Psychology, 80, 448–456. http://dx.doi.org/10.1037/ 0022-0663.80.4.448.
Dunlosky, J., Rawson, K. A., Marsh, E. J., Nathan, M. J., & Willingham, D. T. (2013). Improving students’ learning with effective learning techniques: Promising directions from cognitive and educational psychology. Psychological Science in the Public Interest, 14(1), 4–58. http://dx.doi.org/10.1177/1529100612453266.
Fiorella, L., & Mayer, R. E. (2015). Learning as a generative activity: Eight learning strategies that promote understanding. New York: Cambridge University Press.
Fowler, R. L., & Barker, A. S. (1974). Effectiveness of highlighting for retention of text material. Journal of Applied Psychology, 59(3), 358–364. http://dx.doi.org/ 10.1037/h0036750.
Hartley, J., Bartlett, S., & Branthwaite, A. (1980). Underlining can make a difference: Sometimes. The Journal of Educational Research, 73(4), 218–224. http:// dx.doi.org/10.2307/27539753.

32 H.R. Ponce, R.E. Mayer / Computers in Human Behavior 41 (2014) 21–32
Holmqvist, K., Nystrom, M., Anderson, R., Dewhurst, R., Jarodzka, H., & van de Weijer, J. (2011). Eye tracking: A comprehensive guide to methods and measures. Oxford: Oxford University Press.
Hyönä, J., Lorch, R. F., & Kaakinen, J. K. (2002). Individual differences in reading to summarize expository text: Evidence from eye fixation patterns. Journal of Educational Psychology, 94(1), 44–55. http://dx.doi.org/10.1037/0022- 0663.94.1.44.
Johnson, C. I., & Mayer, R. E. (2012). An eye movement analysis of the spatial contiguity effect in multimedia learning. Journal of Experimental Psychology: Applied, 18(2), 178–191. http://dx.doi.org/10.1037/0033-2909.124.3.372 10.1037/a0026923.
Kaakinen, J. K., & Hyönä, J. (2010). Task effects on eye movements during reading. Journal of Experimental Psychology: Learning, Memory, and Cognition, 36(6), 1561–1566. http://dx.doi.org/10.1037/a0020693.
Kiewra, K. A. (1985). Investigating notetaking and review: A depth of processing alternative. Educational Psychologist, 20(1), 23–32.
Kiewra, K. A., Kauffman, D. F., Robinson, D. H., Dubois, N. F., & Staley, R. K. (1999). Supplementing floundering text with adjunct displays. Instructional Science, 27(5), 373–401. http://dx.doi.org/10.1023/a:1003270723360.
Lonka, K., Lindblom-YlÄnne, S., & Maury, S. (1994). The effect of study strategies on learning from text. Learning and Instruction, 4(3), 253–271. http://dx.doi.org/ 10.1016/0959-4752(94)90026-4.
Lorch, R. F. Jr., (1989). Text-signaling devices and their effects on reading and memory processes. Educational Psychology Review, 1(3), 209–234. http:// dx.doi.org/10.1007/bf01320135.
Marzano, R. J., Pickering, D., & Pollock, J. E. (2001). Classroom instruction that works: Research-based strategies for increasing student achievement (p. vi, 178 p). Alexandria, VA: Association for Supervision and Curriculum Development.
Mason, L., Pluchino, P., & Tornatora, M. C. (2013). Effects of picture labeling on science text processing and learning: Evidence from eye movements. Reading Research Quarterly, 48(2), 199–214. http://dx.doi.org/10.1002/rrq.41.
Mayer, R. E. (2005). The Cambridge handbook of multimedia learning. New York: Cambridge University Press.
Mayer, R. E. (2009). Multimedia learning (2nd ed.). New York: Cambridge University Press.
Mayer, R. E. (2011). Applying the science of learning. Boston, MA: Pearson/Allyn & Bacon.
Meyer, B. J. F., & Poon, L. W. (2001). Effects of structure strategy training and signaling on recall of text. Journal of Educational Psychology, 93(1), 141–159. http://dx.doi.org/10.1037/0022-0663.93.1.141.
Moore, D. W., & Readence, J. E. (1984). A quantitative and qualitative review of graphic organizer research. The Journal of Educational Research, 78(1), 11–17. http://dx.doi.org/10.2307/27540086.
National Institute of Child Health and Human Development (2000). Report of the national reading panel. Teaching children to read: An evidence-based assessment of the scientific research literature on reading and its implications for reading instruction: Reports of the subgroups (NIH Publication No. 00-4754). Washington, DC.
Peper, R. J., & Mayer, R. E. (1978). Note taking as a generative activity. Journal of Educational Psychology, 70(4), 514–522.
Ponce, H. R., López, M. J., & Mayer, R. E. (2012). Instructional effectiveness of a computer-supported program for teaching reading comprehension strategies. Computers & Education, 59(4), 1170–1183. http://dx.doi.org/10.1016/ j.compedu.2012.05.013.
Ponce, H. R., & Mayer, R. E. (2014). Qualitatively different cognitive processing during online reading primed by different study activities. Computers in Human Behavior, 30, 121–130. http://dx.doi.org/10.1016/j.chb.2013.07.054.
Porter, A., McMaken, J., Hwang, J., & Yang, R. (2011). Common core standards: The new U.S. intended curriculum. Educational Researcher, 40(3), 103–116. http:// dx.doi.org/10.3102/0013189x11405038.
Rayner, K. (1995). Eye movements in reading: Perceptual and cognitive processes. Canadian Psychology/Psychologie Canadienne, 36(1), 57–58. http://dx.doi.org/ 10.1037/h0084725.
Rayner, K. (1998). Eye movements in reading and information processing: 20 years of research. Psychological Bulletin, 124(3), 372–422. http://dx.doi.org/10.1037/ 0033-2909.124.3.372.
Rayner, K., Chace, K. H., Slattery, T. J., & Ashby, J. (2006). Eye movements as reflections of comprehension processes in reading. Scientific Studies of Reading, 10(3), 241–255. http://dx.doi.org/10.1207/s1532799xssr1003_3.
Robinson, D. H., Katayama, A. D., Beth, A., Odom, S., Hsieh, Y.-P., & Vanderveen, A. (2006). Increasing text comprehension and graphic note taking using a partial graphic organizer. The Journal of Educational Research, 100(2), 103–111.
Robinson, D. H., & Skinner, C. H. (1996). Why graphic organizers facilitate search processes: Fewer words or computationally efficient indexing? Contemporary Educational Psychology, 21(2), 166–180. http://dx.doi.org/10.1006/ ceps.1996.0014.
Stull, A. T., & Mayer, R. E. (2007). Learning by doing versus learning by viewing: Three experimental comparisons of learner-generated versus author-provided graphic organizers. Journal of Educational Psychology, 99(4), 808–820. http:// dx.doi.org/10.1037/0022-0663.80.4.448.
Sweller, J. (1999). Instructional design in technical areas. Camberwell, Australia: ACER Press.
Wade, S. E., Trathen, W., & Schraw, G. (1990). An analysis of spontaneous study strategies. Reading Research Quarterly, 25(2), 147–166. http://dx.doi.org/ 10.2307/747599.
User Model User-Adap Inter (2009) 19:307–339 DOI 10.1007/s11257-009-9066-4
ORIGINAL PAPER
Can eyes reveal interest? Implicit queries from gaze patterns
Antti Ajanki · David R. Hardoon · Samuel Kaski · Kai Puolamäki · John Shawe-Taylor
Received: 7 July 2007 / Accepted: 20 August 2009 / Published online: 10 September 2009 © Springer Science+Business Media B.V. 2009
Abstract We study a new research problem, where an implicit information retrieval query is inferred from eye movements measured when the user is reading, and used to retrieve new documents. In the training phase, the user’s interest is known, and we learn a mapping from how the user looks at a term to the role of the term in the implicit query. Assuming the mapping is universal, that is, the same for all queries in a given domain, we can use it to construct queries even for new topics for which no learning data is available. We constructed a controlled experimental setting to show that when the system has no prior information as to what the user is searching, the eye movements help significantly in the search. This is the case in a proactive search, for instance, where the system monitors the reading behaviour of the user in a new topic. In contrast, during a search or reading session where the set of inspected documents is biased towards being relevant, a stronger strategy is to search for content-wise similar documents than to use the eye movements.
Keywords Eye movements · Implicit relevance feedback · Information retrieval · Machine learning · Support vector machines
The authors appear in alphabetical order.
A. Ajanki (B) · S. Kaski · K. Puolamäki
Department of Information and Computer Science, Helsinki Institute for Information Technology, Helsinki University of Technology (TKK), P.O. Box 5400, 02015 Espoo, Finland
e-mail: antti.ajanki@tkk.fi
D. R. Hardoon · J. Shawe-Taylor
Department of Computer Science, University College London, Gower Street, London, WC1E 6BT, UK
   1 3
308 A. Ajanki et al.
 1 Introduction
Current information retrieval (IR) systems rely mostly on explicit, typed queries, combined with explicit feedback telling the system which of the search results were relevant. The relevance feedback is used to refine the query, and the search converges iteratively towards more relevant documents. The standard web search engines are sim- plified versions of this scheme; they take advantage of the large scale which allows inferring general relevance of documents from link data.
A main problem of this traditional IR paradigm is that formulating good textual queries is a challenging task even for experienced users (Turpin and Scholer 2006). Moreover, query-based searches are only possible if the user knows her information need. The need may also be tacit; there may exist very useful documents that the user does not even try to search, or in a milder form the true interest or information need may be ambiguous to the users. In all these cases it is difficult or impossible to formulate a query explicitly. It would be ideal if the system could infer the interests of the users while they work, and then have some suggestions readily available when the users ask for help. We call this task proactive information retrieval.
A proactive information retrieval system would additionally solve the problem that giving explicit feedback is laborious. Such a system would use implicit feedback to infer relevance. Several forms of implicit feedback have been used (Kelly and Teevan 2003), of which at least click-stream data, time spent during reading, amount of scroll- ing, and exit behaviour have been found to help in predicting explicit feedback ratings (Claypool et al. 2001; Fox et al. 2005; Joachims et al. 2005). While these sources are readily available and useful, they offer limited information about a user’s interests and intentions, and the predictions are far from perfect. Hence it is important to continue searching for new sources of feedback that could be used to complement the existing ones.
Our suggestion is to use implicit feedback from the observed gaze pattern as an alternative or complementary source to infer the users’ intentions. Eye tracking has already been shown to be useful in user modelling: in inferring cognitive states, traits, and performance of the user for personalization purposes and for interaction adapta- tion (Conati and Mertena 2007, and review of earlier works therein). Moreover, eye movements have been shown to be useful in inferring relevance of documents to be used as relevance feedback (Puolamäki et al. 2005). Hence it is imaginable that eye tracking would be useful in inferring other, even more subtle cues about user interests.
We combine the eye movements with the textual content of the documents in a novel way: we use the eye movements to formulate an IR query, which is then used to rank unseen documents with respect to their relevance to the current interests of the user.
In this paper we study the feasibility of this approach. The practical motivation is that eye tracking equipment is becoming cheaper and smaller, and eye tracking data is soon expected to be cheaply available for most applications. If the data turns out to be useful it is then sensible to use eye tracking recordings to complement other sources. The more exciting motivation is that eye tracking data may provide more subtle cues about the users’ interests compared to lower level, time-based features, as has been found in studies of users’ meta-cognition (Conati and Mertena 2007).
1 3
Implicit queries from gaze patterns 309
 Since we expect the eye movement patterns to be very noisy, we study two approaches. The more challenging task is to (1) construct a query from eye move- ments alone. The easier one is to (2) construct a query by combining information from implicit relevance feedback from eye movements and explicit relevance feedback.
Implicit queries have earlier been constructed based solely on the texts the user is working on (Czerwinski et al. 1999; Dumais et al. 2004); in this work we combine this research tradition with more focused eye tracking-based inferences on which parts of the document are interesting. This is a feasibility study investigating whether eye tracking gives valuable information in this task. If it does, the methods can then be optimized and tailored in later studies.
The texts the user is working on could alternatively be interpreted as the context of the user and used to complement explicit queries (Budzik and Hammond 2000). In this work we assume explicit queries are not available, as in earlier works on implicit queries. This setting is commonplace when browsing interesting documents without a clear goal, and in the beginning of a more focused browsing session. If explicit queries are available, the implicit queries inferred from eye tracking could naturally be used to complement them according to existing principles.
To test the new approach, we devise a controlled experimental setting, in which test subjects read through short text snippets searching for documents related to a given topic. During the reading the users’ eye movements are recorded with an eye tracking device. We extract term-specific eye movement features for each document the user reads. The features are then used for predicting importance of the term for the search task. The setup is an extended version of our earlier work (Puolamäki et al. 2005).
To learn a model we need a set of training data where the ground truth is known, but we cannot assume that we will have training data that is representative of all possible (implicit) queries. Hence, the model should be such that it generalizes to new queries. For that purpose we assume that there is a link between relevance or interest and eye movements, and that this link is, to a reasonable extent at least, independent of the actual topic and query. Then a model of the link can be learned from training data about a subset of possible queries, and it will generalize to new ones. In this paper we formulate such a model and test empirically whether the assumption about a universal link between interestingness and eye movements is useful.
We study whether there is information about interests in the eye movements, and whether it can be extracted by models that make the above-mentioned assumptions. Furthermore, we study whether the usefulness of the inferred interests varies as a function of the amount of system’s prior knowledge of the target user topic; while for a new query there is no information available about the potential interesting- ness of new documents, for a topic already studied for some time there is additional information to be leveraged as well. Namely, the proportion of relevant documents among the set of read documents is higher than in the unread ones, due to the search process so far, and this bias can be utilized in making a content-based proactive search.
As another case study, we investigate whether combining eye movements with doc- ument features would help in the standard IR task where explicit relevance feedback is available for a subset of the documents.
1 3
310 A. Ajanki et al.
 2 Earlier work
There has been a lot of interest in implicit relevance feedback techniques in the information retrieval community because they may complement or replace explicit feedback, and thus decrease the need to burden the user. Several implicit feedback measures have been studied before. The literature review by Kelly and Teevan (2003) shows that implicit feedback can indeed improve IR accuracy but there is no consensus about which implicit measures are the most effective.
Claypool et al. (2001) examined how well time spent on a web page, and mouse and keyboard activity can substitute for explicit feedback. They found out that while mouse movements and number of clicks do not correlate strongly with the relevance, read- ing time and amount of scrolling are good indicators for relevance. Fox et al. (2005) carried out a similar comparison of implicit and explicit feedback. They considered not only the display time and scrolling activity but also other observable measures of user behaviour, such as if the document was printed or bookmarked and how the user exited the page. They observed that the two most important features were display time and the way the user left the page. Although several studies have found the display time to be a good indicator, it may be difficult to analyze in practice. In non-controlled settings the distribution of display times is usually skewed towards zero with numerous outliers (Rafter and Smyth 2001).
Click-through data is another well-studied implicit signal. Joachims et al. (2005) evaluated the quality of click-through measurements on a result page of a web search engine using eye movement measurements (but did not use the eye movements as implicit feedback). They concluded that clicks on the search result page can be used to infer relative relevance judgments between the search results.
Eye movements depend on the type of the visual task the user is performing, which suggests that it is possible to use them to infer the task automatically. For example, Howard and Crosby (1993) noted that the fixations tend to be located sequentially when reading relevant bibliographic citations but non-relevant material is examined non-linearly. King (2002) found out that the distributions of fixation locations differ between reading and counting arrows. She trained a neural network that could separate the two tasks quite efficiently using just the eye movements.
Use of eye movements in IR is a relatively new approach. Maglio et al. (2000) and Maglio and Campbell (2003) introduced a prototype attentive agent application which monitors eye movements while the user views web pages, in order to determine whether the user is reading or just browsing. If reading is detected, more information of the topic is sought and displayed. The feasibility of the application was not, however, experimentally verified.
Eye movements have been used in applications that could be broadly classified as eye-movement-based user interfaces, one of the most known examples being a fast pre- dictive eye typing system Dasher (Ward and MacKay 2002). More recently, Fono and Vertegaal (2005) introduced EyeWindows, an attentive windowing technique which uses eye tracking, rather than manual pointing, for selecting focus windows.
Eye movements were first used in an information retrieval task by Salojärvi et al. (2003, 2005a). Discriminative hidden Markov models were applied to estimate the relevance of lines of read text, and the performance of the method was verified in a
1 3
Implicit queries from gaze patterns 311
 controlled experiment. A competition was subsequently set up, where the participants competed in predicting relevance based on the eye movements (Puolamäki and Kaski 2006).
A prototype information retrieval system was introduced by Puolamäki et al. (2005). The system used relevance information combined with collaborative filtering to seed out relevant scientific articles, the task being to infer if the user found text snippets relevant or not. This earlier prototype did not use the textual content of the documents at all, and hence it could not be used to predict the relevance of unseen documents without some other source of information, such as collaborative filtering.
The methods used so far can be characterized as attempting to use eye movements to assess directly the relevance of displayed information. The methods vary from simple evaluation of attention as in EyeWindows to prediction of relevance from the type of eye movements involved, as in the example of discriminative hidden Markov models. In the current paper we use eye movements in a meta-learning task to infer a weighting over terms that can be used to predict the relevance of unseen documents for the user. This can be seen as an extension of earlier work, in the sense that during the application phase the eye movements are processed by the learned function rather than used directly as features for a retrieval algorithm. This approach opens up the possibility of tapping a rich source of potential information about the user through implicit inferences made without the need for direct querying or input.
The experimental setup used here was introduced first in a conference publication containing a brief feasibility study (Hardoon et al. 2007). That was the first study where documents have been ranked based on their textual content and eye movements of the user. Now we extend and complete the previous study, in particular by addressing the issue of bias in the proportion of relevant documents, which is important for a realistic information retrieval scenario where the user is likely to see varying proportions of relevant documents, and by studying the importance of the features for the task. We also provide a detailed analysis and discussion of the methods and related work.
3 Problem and approach
Our objective is to predict from term-specific eye movements a query vector that can be used to evaluate the relevance of yet unseen documents. Our approach is explained in Sect. 3.1. We also utilize the same overall framework in the case where some explicit relevance feedback is available, and we combine explicit feedback with eye movements. This latter task is discussed in Sect. 3.2.6.
3.1 Overall algorithm
We work with the bag-of-words (BOW) representation of the documents. The goal is to construct a query function gw(d), where d is a BOW representation of a new doc- ument, and g is a two-class classifier which predicts whether d is relevant or not. The parameter vector w represents the implicit query; our task is to provide the classifier with such a w that it will classify well according to the user’s interests. The function g is parametrized such that there is a specific parameter wt for each term t.
1 3
312 A. Ajanki et al.
 We assume that there is a link between the eye movements and the importance of a word for the query. More specifically, we assume a parametric functional form wt = fλ(et , st ) for the relationship between eye movement features collected dur- ing reading, denoted collectively by et for term t, query-independent parameters st associated with the term t (e.g. inverse document frequency of term t) and the query parameters wt . The eye movement features describe the way in which the term is viewed (for example, fixation duration on the term and saccade lengths before and after viewing the term). The features are described in more detail in Sect. 3.2.4. The
fλ could in principle be any predictor, with its parameters specified by λ.
The parameters λ of the predictor fλ are learned on a set of training tasks, where the true interest of the user is known. Following our assumptions laid out above, the func- tional form of the predictor fλ is topic-independent. Hence, the training step needs to be done only once. After that the predictor can be applied to previously unseen queries
to produce the query parameters w.
In this work we choose the query function gw(d) to be a Support Vector Machine
(SVM) with the parameter vector w formed of the term-specific parameters wt . As a predictor fλ, which gives the parameters of the SVM, we use standard linear and non-linear regressors (details later). We purposely use standard state-of-the-art machine learning methodologies in this proof-of-concept work to make the approach easily expandable.
3.2 Training
Our main task is to formulate an IR query, using the eye movements as the only feed- back signal. The query need not, however, be understandable by humans; in fact, it suffices to formulate the query in such a way that it can be used by the query function gw(d) to predict relevance for new documents d.
The training data consists of a set of documents and the eye movements of per- sons who read them while they were searching for documents of certain known top- ics. Our aim is to try to infer a query from the eye movement recordings and BOW vectors of the read documents, such that the relevance of new, unseen documents to the topic can be predicted. More detailed description of the data is provided in Sect. 4.1.
The training consists of two phases: first, we learn the predictor parameters λ by optimizing fλ to produce query vectors that are good at separating the known topics in the training set. Next, we apply the learned predictor to a previously unseen topic to infer a query vector w. Finally, we evaluate the performance by ranking unseen test documents according to the query function gw.
The predictor learning phase requires that ground-truth query vectors are known
for each topic. These ground truth vectors w should be such that the query function gw
is able to optimally discriminate between the topics. We approximate the ground-truth
by the weight vectors of SVMs that are trained to discriminate the documents of one
topic from the others. All SVMs mentioned use the default setting of C = 1 and a
linear kernel. We call these vectors ideal weights and denote the ideal vector for topic c
by wc . Section 3.2.2 gives more details about the computation of the ideal weights. ideal
1 3
Implicit queries from gaze patterns 313 The parameter λ is optimized by minimizing the squared error
 àfλ et(j),st(j) −wc(j) à2, ideal,t ( j )
j
where the index j goes over all viewed terms in all training documents, t(j) is the identity of the jth viewed term, and c(j) is the topic of the document where the jth term appeared. After λ has been learned, it will be fixed and used in the subsequent steps. We will have either a standard linear least squares regressor or a non-linear sparse-KPLS as the regressor fλ. They are discussed in more detail in Sect. 3.2.3.
The training set has been constructed to consist of several topics, so that the learned predictor needs to generalize over them and hence become topic-independent. We will test whether this is the case by evaluating the performance on a topic that was not a part of the training set.
The second step in inferring the query for a new topic is constructing the vector w by letting wt = fλ(et , st ) for all terms t that were viewed during the test query. If a term t appears in multiple documents or multiple times in one document, the corresponding feature vector (et , st ) will be set to the average over all the occurrences. Zero is assigned to non-viewed terms. After forming the w we use the query function gw to classify test documents.
3.2.1 Cross validation methodology
We have been specifically careful in designing the experiments such that testing data never affects the learning, and that is why the following procedure may appear slightly complicated. We evaluate the performance using a cross-validation approach where we leave out one topic at a time for testing and use the rest for training. We recorded eye movement data from several test subjects while they were reading text snippets, looking for documents about a given topic. We had 25 such search topics in total. In addition to the documents that were shown during the experiments we have a separate test set of documents without eye movement recordings for evaluation purposes.
We want to learn a mapping, from the eye movements to the weights, that is inde- pendent of the topic. The eye movement features from the testing topic need to be excluded from the training. To this end, the features are partitioned into two sets. The first set, T1, includes the features from the training topics (other topics besides the left-out topic). More specifically, T1 is the collection of (et , st , c) triplets for all words viewed during the training topics, where et and st are the eye movement and query- independent features of term t, and c is the search topic that the user was looking for when this sample was generated. The second set, T2 , includes feature pairs (et , st ) for the viewed words in the left-out topic.
T1 is used for training the parameter vector λ, and T2 for inferring the query vector for the left-out topic. Finally, the learned query is evaluated on an independent test set. Because the set T1 does not contain documents from the left-out topic, the predictor cannot specialize on the left-out topic. Instead, if it is able to learn to perform well on the left-out topic, it must be independent of the query and in that sense universal. The approach is summarized in the pseudocode in Algorithm 1.
 1 3
314 A. Ajanki et al.
  Algorithm 1 Pseudocode of the training and testing procedure
// 1. Construct the ideal weights for each topic c
Train an SVM to discriminate BOW vectors between topic c and other topics Let wicdeal be the parameter vector of the SVM
for each topic c′ // c′ is the left-out topic // 2. Training
 // Learn the regressor parameters λ
T1 ← a set {(et,st,c)} of all viewed words t in the training set, where topic c ̸= c′
λ←argmin   |f (e ,s )−wc(j) |2 λ j∈T1 λ t(j) t(j) ideal,t(j)
// Compute the query vector w for the left-out topic c′
T2 ← a set {(et , st )} of all viewed words t in the left-out topic c′ (average if t occurs several times) wt ← fλ(et,st) for all samples in T2
// 3. Testing
Rank the unseen test documents d from a separate test set according to gw(d) Compute MAP (c′ is the positive topic and all others are negative)
3.2.2 Ideal weights
We use as ground-truth the weight vector of an SVM which has learned to predict, based on full knowledge of the topic and content of learning documents, whether or not a document belongs to the given topic. We call these ground-truth values ideal weights and use them as targets when training a regressor fλ for the same topic. In principle, one could select a different classifier to construct the ideal weights but for ease of integration with the remainder of our system we opt for SVM. The input for this ideal weight SVM is the BOW representation of the document, and the label is +1 if the document is categorized to the current topic, or −1 if it is not. These labels are known for the training documents. Because the ideal weights are computed using one SVM per search topic, we get one weight value for each term in the dictionary for each search topic; the weight represents the term’s “fit” to the given search topic.
We will use the ideal weights to train a regressor described later in Sect. 3.2.3. The regressor is then used to construct a new classifier for predicting the relevance of the unseen documents.
3.2.3 Regression
We use two types of regressors for predicting the terms’ weights wt from the mea- sured eye movement and query-independent textual features, et and st . The simplest mapping we employ is a standard linear regressor
fλ(et,st)= λieeti + λissti, ii
 1 3
Implicit queries from gaze patterns 315
 where the parameter vector λ is divided into two parts, λe and λs . The first one contains the regression coefficients related to the eye movement features e and the latter the coefficients related to the query-independent features s.
In addition to least squares regression, we use a non-linear sparse dual Partial Least Squares (PLS) approach (Dhanjal et al. 2006). This method uses a general framework for feature extraction based on a kernel PLS (Rosipal and Trejo 2001) deflation method. KPLS maps the feature vectors nonlinearly to a feature space, and does ordinary least squares estimation in the new space. The sparse dual PLS selects, in each iteration, the projection for the least squares regression to be a subset of samples that have maximal covariance with the label. In other words, in each iteration of the algorithm a subset of the kernel is computed and the sample (with the feature combination) that gives maximal covariance is selected as the projection.
In the PLS framework we still adhere to our prior assumption of learning a mapping from eye movements to ideal weights, whereas the eye movements are now kernelized. Due to the large number of samples (eye movements) we are unable to compute the full kernel matrix and therefore only compute a small random portion of the kernel at each iteration of the sparse-KPLS algorithm. For a detailed account of sparse-KPLS we refer the reader to (Dhanjal et al. 2006).
We use a Gaussian kernel with the sparse-KPLS where the width parameter σ for the kernels is optimized, per search topic, using tenfold cross validation on the training data.
We normalize the query vector w in the 2-norm.
3.2.4 Features
The gaze direction is an (indirect) indicator of the focus of attention, since accurate viewing is possible only in the central fovea area (1–2 degrees of visual angle). The correspondence is not one-to-one, however, since the attention can be shifted without moving the eyes. The eye movement trajectory is traditionally divided into fixations, during which the eye is fairly motionless, and saccades, rapid eye movements from one fixation to another.
The fixations during reading have previously been observed to last 200–250 ms on average and rarely less than 100 ms (Levy-Schoen and O’Regan 1979). Furthermore, Granaas et al. (1984) have showed that moving text two letters positions at a time at 88 ms intervals hinders reading comprehension substantially, which indicates that sub 100 ms durations are too short for effective reading.
We identified fixation locations from the measured eye movement trajectories by windowing; if the successive points stayed inside 30 pixel square (about 0.6 visual angle for a person sitting at 60cm distance from the screen) for more than 100ms, they were considered to form one fixation. Every fixation was mapped to the closest word, unless the fixation occurred well outside any text (at least 1.5 times the text height), in which case it was discarded. In this paper we only report the results for the fixation time cutoff of 100 ms. However, we ran the same analyzes with the fixation time cutoff of 40 ms, which is the value recommended by the eye tracker manual. The results with the 40 ms cutoff were very similar to the 100 ms cutoff reported in this work.
1 3
316 A. Ajanki et al.
 We extracted 22 eye movement features (denoted by et ) from the recorded eye movement data for each term, and 4 text features (denoted by st ) for the target words of the fixations. The features are listed in Table 1. The eye movement features we are
Table 1 List of features
Eye movement features
1 Integer
2 Integer
3 Binary
4 Binary
5 Continuous
6 Continuous
7 Continuous
8 Continuous
9 Integer
10 Integer
11 Integer
12 Integer
13 Integer
14 Continuous
15 Continuous
16 Integer
17 Continuous
18 Binary
19 Continuous
20 Continuous
21 Continuous
22 Integer
Textual features
23 Integer
24 Continuous
25 Continuous
26 Continuous
Number of fixations to the word
Number of fixations to the word when the word is first encountered
Did a fixation occur when the line that the word was in was encountered for the first time?
Did a fixation occur when the line that the word was in was encountered for the second time?
Duration of the previous fixation when the word was first encountered
The duration of the first fixation when the word was first encountered
Sum of durations of fixations to a word when it is first encountered
Duration of the next fixation when the gaze initially moves on from the word
Distance (in pixels) between the first fixation on the word and previous fixation
Distance (in pixels) between the last fixation on the word and the next fixation
Distance (in pixels) between the fixation preceding the first fixation on a word and the beginning of the word
Distance (in pixels) of the first fixation on the word from the beginning of the word
Distance (in pixels) between the last fixation before leaving the word and the beginning of the word
Sum of all durations of fixations to the word Mean durations of fixations to the word Number of regressions leaving from the word
Sum of durations of fixations during regression initiating from the word
Did a regression initiate from the following word?
Sum of the durations of the fixations on the word during a regression
Mean pupil diameter during fixation
First fixation duration divided by total duration of fixations on the display
Number of words skipped since previous fixation
Length of the word
Position of the word in the document divided by total number of words in the document
Position of the word in the line divided by the line length Logarithm of inverse document frequency of the word
  1 3
Implicit queries from gaze patterns 317
 using have been previously described in a technical report (Salojärvi et al. 2005b). These are typical features in psychological studies (Rayner 1998).
We also used four query-independent features which do not depend on the way the user viewed the document but only on its textual content. The length of the word and the inverse document frequency are related to the level of mental processing required to comprehend the word. The relative position in the document may be related to the relevance of the word, if, for example, the user usually reads only the beginning of the document but sometimes also more if he finds the topic is interesting.
3.2.5 Explicit feedback SVM model
If explicit relevance feedback, that is, the relevance of the training documents, was available and we knew that the implicit query remained the same in the test documents, we would not need to infer the query vector from the eye movements. Instead, it could be computed more directly from the BOW vectors of the training documents. We could simply train an SVM to classify between the training topic and other topics.
Compared to the approach of the previous section this is much simpler. We can skip computing the ideal weights and learning the regressor fλ and, instead, learn the query function gw(d) more directly. We let the query function to be an SVM that discriminates between the relevant and non-relevant documents in the current topic. The parameter vector w is learned during the optimization of the SVM.
The inputs to the SVM are document BOW vectors, similarly to the SVMs that were used in computing the ideal weights (see Sect. 3.2.2). The differences are that the training labels are now the true relevance labels instead of the category labels, and that the training set consists of only the documents shown during the searches where the user’s true interest was the current topic.
The learnt weight vectors are used to classify the test documents. The resulting classifier is referred to as SVMex. It represents the “best imaginable” performance, which implicit feedback cannot realistically be expected to outperform.
3.2.6 Combining eye movements and explicit feedback
Next we consider a situation where we observe both the true relevance labels and eye movements. We want to find out whether using both of these information sources in conjunction to predict the query could improve the classification accu- racy, in comparison with the model of the previous subsection that uses only explicit feedback.
Because we again have the explicit relevancy for the documents we can skip the learning of ideal weights and the regressor fλ, and learn the query vector w directly. The query function gw(d) predicts the relevancy of a document given both the tex- tual content and the eye movements on the document. We choose g to be a two-view classifier called SVM-2K.
We regard the textual content as one representation and the measured eye move- ment feature vectors as a second representation of the document. We project the two
1 3
318 A. Ajanki et al.
 views through distinct feature projection, thus creating two kernels, one for each representation.
The Kernel Canonical Correlation Analysis (KCCA, Hardoon et al. 2004) algorithm looks for directions in the two feature spaces such that when the training data is pro- jected onto those directions the two vectors (one for each view) of values obtained are maximally correlated.
A straightforward way to do classification using two data sets would be to first project the data into the KCCA space and then train an SVM classifier. Though this sequential approach seems effective (Meng et al. 2005), there appears to be no guaran- tee that the directions identified by KCCA will be best suited to the classification task. Farquhar et al. (2006) have shown that the two distinct stages of KCCA and SVM can be combined into a single optimization that was termed as SVM-2K. The training of an SVM-2K model is explained in Appendix A.
3.3 Testing
After the query vector has been learned in the training phase we can predict the rele- vance of unseen test documents. We rank the test documents according to the values of the discriminant function gw(d). The discriminant functions for the implicit, explicit, and combined feedback tasks are described in the following subsections.
If the classifier is accurate the test documents belonging to the current left-out cate- gory should be near the top of the resulting list. The quality of the ranking is measured by average precision (see Sect. 4.3).
3.3.1 Implicit feedback regression models
The compatibility of a test document BOW vector d to the inferred query w is computed by an SVM discriminant function. For linear kernel the function is
gw(d) = wT d + b,
where b is the bias that is learned together with the weights w. The vectors w and d are normalized in 2-norm to keep the length of the document from affecting the similarity measure.
Instead of thresholding the values of the discriminant function, as would be done in a pure classification task, we rank the test documents according to values of the function. As we are only interested in the ranking, the bias term can be left out from g. We measure the goodness of the ranking by average precision (see Sect. 4.3).
3.3.2 Explicit feedback SVM model
The discriminant function is of the same form as in the case of implicit feedback. Of course, the weight vector w is now computed using the explicit feedback.
1 3
Implicit queries from gaze patterns 319
 3.3.3 SVM-2K combining implicit and explicit feedback
The discriminant values of the SVM-2K model for a test document are given by the (1) in Appendix A. Because no eye movement measurements are available for the test documents, the eye movement feature projection φA in (1) is set to zero when computing the decision values for the test documents.
4 Experiments
4.1 Data collection
4.1.1 Document corpus
The document set consists of 750 documents from the Wikipedia. Of them, 500 were used to form the training set and the rest were the test set. The documents have been partitioned into 25 categories by the editors of the Wikipedia. There were 12–23 doc- uments from each topic in the training set and ten documents per topic in the test set. The categories and their sizes are listed in Table 2.
The documents were represented as Term Frequency-Inverse Document Frequency (TFIDF, Salton and McGill 1983) vectors. If the document corpus is denoted by D and the dictionary by T, then the TFIDF weight for a term t ∈ T in a document i ∈ D is the product of term frequency in the document and logarithm of the inverse document frequency of the term in the corpus:
TFIDF(i,t) = nit log à |D| à, à j∈D|njt>0à
where nit denotes the number of occurrences of term t in document i. The dictionary T consisted of all stemmed words in the documents except for numbers and some frequent ‘stop’ words like ‘of’ and ‘the’, which were omitted. Also, words that only appeared in single interest categories were removed from the data in order to create a more realistic scenario, i.e. overlapping words through the different categories. The dictionary included 5306 terms in total.
We truncated the documents so that each had at most 11 lines of text, in order to fit them to the screen. We wanted to keep the test setup as simple as possible, and to avoid the need to scroll the text. To avoid any artifacts that might attract unwanted eye movements, the titles of the documents were removed and no sen- tence was cut in the middle. We made sure that the content of each truncated docu- ment was sufficient for inferring its topic by manually inspecting all documents. We assume that, because the categories are quite diverse and it is fairly easy to categorize the truncated documents into the different categories, the participants would not have needed scrolling in most cases even if it had been possible. This is further supported by the fact that only a minority of the documents had any fixations on the final lines (see Fig. 1).
 1 3
320
A. Ajanki et al.
  Table 2 Summary of the training and test corpora
Topic
Astronomy Ball games Cities
Court systems Dinosaurs Education Elections Family
Film
Government
Internet
Languages
Literature
Music
Natural disasters Olympics
Optical devices
Postal system
Printing
Sculpture
Space exploration Speeches
Television Transportation
Writing systems
Total 500
Number of documents in the training set
Number of documents in the test set
10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10
250
 4.1.2 Participants
There were ten participants in the experiments. They were voluntary post-graduate and senior researchers from Department of Information and Computer Science, Helsinki University of Technology.
4.1.3 Experimental procedure
We measured users’ eye movements when they were reading documents in order to classify the documents into interesting and uninteresting. We artificially con- strained users’ interest by giving them a topic and asked them to identify docu- ments which were related to that topic. Topics were the Wikipedia categories listed in Table 2.
1 3
22 21 12 23 17 22 21 16 20 20 23 21 21 16 19 19 20 22 22 20 19 23 22 22 17

Implicit queries from gaze patterns
321
    Fig. 1 Proportion of documents having fixations on a given line
100
80
60
40
20
0
1 2 3 4 5 6 7 8 9 10 11
Line number
                                       A sample document (on the left) and a screen which the user gets when he has finished reading (on the right)
The user read ten short documents trying to recognize those that were related to the given topic. He was instructed to read the document until he could say whether or not the document was related to the topic and, after making up his mind, to press any key in the keyboard. The key press then replaced the document by a form where the user reported his impression of the relevance by pressing one of two possible keys. The next document was shown immediately after the user had reported his opinion. A sample document and a feedback screen are shown in Fig. 2.
We call a combination of one topic and the 10 documents the user read while searching for that particular topic a session. Each user completed ten sessions, with a different search topic and documents in each. A short break was allowed after the third and the sixth session.
On average half of the training documents in a session were relevant and the rest were randomly drawn from unrelated topics. In total 1,000 documents (= 10 users × 10sessions/user × 10 documents/session) were displayed during the tests. This means that each of the 500 training documents was shown to two users on average.
The answers given by the users agreed in 97% of the cases with the true labels.
Fig. 2
1 3
Percentage of documents having fixations on a given line
322 A. Ajanki et al.
 4.1.4 Eye tracker
The gaze locations on the screen were recorded with a Tobii 1750 eye tracker while the test subjects were reading the documents. The Tobii system consists of an infra-red LED and two cameras mounted on the frame of a computer screen. Tobii measures gaze direction 50 times per second by illuminating both eyes with infrared and measur- ing the light reflected from the cornea. The system is fairly robust to head movements. The user was sitting at a 60 cm distance from a 17 inch computer screen. The text was displayed with quite a large font and spacing so that it would be possible to map the gaze location to the correct word reliably. There was room for at most 11 lines of text on the screen. The eye tracker was calibrated in the beginning of the experiment for each user and after every break.
4.2 Experimental setup
4.2.1 Implicit feedback models
As was discussed in Sect. 3.2.3, we tried both the linear least squares and the non-linear KPLS regressors for predicting the term-specific weights from the eye movement features. The model using linear regressor is referred to as Wlin, and the model using KPLS is called Weye+text (26), where 26 is the dimension of KPLS feature space. Both of these models use the eye movement features et and the query inde- pendent textual features st . To determine how much the extra information from the eye movements boosts the performance we trained a second model that is similar to Weye+text(26), but does not use eye movements at all. To be exact, the differences are that the second model uses only the query independent features, and that the features are computed for each of the words appearing in the document, not just for the viewed words. We refer to this second model as Wtext(4), where 4 is again the number of projection directions in KPLS. The number of projection dimensions are chosen to coincide with the number of features in the corresponding regressor. We have tested with other numbers of directions, too, and the performance does not change much when the number of directions is increased (data not shown here; see Hardoon et al. 2007).
The above regression scheme is performed by pooling the eye movements from all users who completed the same query. The size of training set used to adapt the model to a particular search topic (set T2 in Algorithm 1) is 30–50 documents depending on the topic (3–5 test subjects performed the same query and each of them saw ten doc- uments). About half of them were relevant. If the users employ very different reading strategies it might make more sense to handle them separately. Therefore we also test a model that is otherwise identical to Weye+text(26) but instead of pooling the data each session is handled as a separate, smaller training set consisting of only ten documents. This model is referred to as Wus(26) (for “user specific”) in Table 3.
Figure 3 shows a user’s eye movement trajectory on a document, and the term weights on the same document inferred by the Weye+text (26) regressor from eye move- ments of all test subjects who were searching for documents about Dinosaurs.
1 3
Implicit queries from gaze patterns
323
 Table 3
Random (no feedback)
6.0
The mean average precisions for biased data
 TFIDF Wlin (impl. (no feedback) feedback)
73.4 37.8
Wtext (4) (impl. feedback)
19.9
Weye+text (26) (impl. feedback)
46.3
Wus (26) (impl. feedback)
31.3
SVMex SVM-2K (impl. (impl. feedback) feedback)
75.1 77.0
    Fig. 3 On the left, eye movement trajectory. The document is about Dinosaurs, which is also the topic the reader was interested in. The circles mark the fixations, and the radius of a circle is proportional to the duration of the fixation. On the right, weights inferred from eye movements. The thickness of the underline denotes the magnitude of the weight. Gray words do not appear in the dictionary
4.2.2 Explicit feedback models
For reference we also tested how much the performance can be improved if explicit relevance feedback is available. The explicit feedback can be used alone or combined with the information from the eye movement measurements. If the eye movements are excluded, only the TFIDF vectors of the documents are available. In this case we used a standard SVM classifier that was trained to discriminate between relevant and non-relevant samples (Sect. 3.2.5). The resulting model is called SVMex. To combine explicit feedback and eye movements we used the SVM-2K model of Sect. 3.2.6. This model is referred to as SVM-2K below.
4.2.3 Baseline models
We compare the results to two baseline models that do not utilize any relevance feed- back at all. The simpler one just ranks the test documents randomly.
The second baseline model ranks the test documents by the average similarity of their textual content with the training documents. The training and testing documents are presented as 2-norm normalized TFIDF vectors. We compute mean cosine distance (which equals to dot product for normalized vectors) of the test documents to the train- ing documents. More formally, for each training set s with documents y j , j ∈ T r (s ), and for each test document xi we compute
g ( x i ) = 1   x iT y j , |Tr(s)| j∈Tr(s) ||xi||||yj||
  1 3
324 A. Ajanki et al.
 where Tr(s) is the set of all (both positive and negative) training documents in the training set s and the document lengths are taken into account by normalizing the vectors. The test documents are then sorted according to the cosine distance, in the order of similarity with the training documents.
4.2.4 About bias in the learning set
In a typical information retrieval setup, the proportion of relevant documents the user receives during a search session will be larger than on average in the corpus. This happens because an information retrieval system naturally aims for a good precision, or proportion of relevant documents of all results. This creates a bias: the proportion of relevant documents is larger in the learning set (seen documents) than in the test set (unseen documents). If the precision of the search results is good enough, then as a result of the bias even the learning document set as such is a reasonably good query—without any explicit or implicit relevance feedback! This has a direct effect on the performance of any information retrieval algorithm. Hence we will inspect the performance of our methods as a function of the bias.
In our experiments, half of the documents in a session were positive on average and others were randomly sampled from the negative topics. Because the positive docu- ments form a large cluster while negative documents are likely to be more scattered, even a simple TFIDF baseline model can perform well in this setup as the mean vector is drawn near to the large positive cluster. On the other hand, in a more realistic setup we can’t assume such a large fraction of the seen documents would be relevant, and therefore models using just the textual content of the documents are likely to perform much worse. Implicit or explicit feedback can help the search engine to identify the relevant documents even when the bias is low.
To test our algorithm under smaller bias we created new training sets by re-balancing the data in the original sessions. We constructed new artificial sessions by dividing the documents (and their corresponding eye movement measurements) anew. The new sessions have only one positive document, and therefore the positive topic will not stand out from the rest. We also limited the proportions of the topics in the test set to be the same as those in the training set. This way there is no extra information available in the setup that could bias the results, and the random ranker is a fair baseline.
Each session was divided into n new sets by taking one of the n positive and all negative documents in the original session. If two or more negative documents hap- pened to belong to the same topic, all possible one-document-per-topic combinations were used as the new training sets constructed from that session. Unlike in the original setup, where we always used all test documents for testing, we now construct a sep- arate test set for each training set by taking only the test documents from the same topics that appear in the training set. Thus, the proportion of the topics in the training and testing is the same but the size of the test set depends slightly on the training set. We call these resampled sets unbiased training sets.
There were some documents where the eye tracker had failed to capture any eye movements at all. A likely reason is that the user was probably sitting too far or too close to the monitor while reading these documents. When constructing the unbiased
1 3
Implicit queries from gaze patterns 325
 sessions we left out all sessions which would have included at least one of these doc- uments without any eye movements. In total, we get 694 unbiased sessions which had 88 distinct pairs of user and topic.
Training of the classifiers for the unbiased case is done identically to the training in the biased case in Sect. 3.2. The learning algorithm again iterates over all sessions and learns the regression parameters from data collected from all other topics except the left-out topic. The training set (denoted by T1 in Sect. 3.2.1) again consists of features collected from all the sessions except for the left-out topic. Because T1 is the same for all new sessions which were constructed from a single original session, also the regression parameters will be the same for those sessions. The training set T2 for learning the term weights wt contains (et , st ) feature pairs from the documents in the left-out session. The sessions with the same topic are not pooled, as was done in the biased case, because doing so would alter the amount of bias. The size of T2 is 4–8 documents depending on the session. The model is learned and the predictions are made the same way as before.
In the testing phase the performance of the learned models is tested on the cor- responding test sets. To get a single performance index for each user/topic pair, we computed averages of the performance figures over sessions which had the same com- bination of users and topics.
We compare the performance of the algorithms on the biased and the unbiased data sets.
4.3 Results
4.3.1 Performance measure
To compare the goodness of the rankings of the unseen test documents produced by the different methods we computed mean average precisions (MAP).
Average precision measures how well the positive documents are positioned in a given ranking. It is calculated as a mean of average value of precision at the positive ranks:
AVGPREC=1 R i, R i=1 ri
where R is the number of positive examples in the test set and the ri are the rankings of the positive examples, ordered such that ri < ri+1. The mean average precision is the mean of average precisions of different information needs or, our case, sessions.
We inferred a query w with each method for each session and ranked the test doc- uments according to their predicted similarity to the inferred query by computing the discriminant function values gw(d) for each test document d. The discriminant functions were described in Sect. 3.3. The quality of the ranking was established by computing the average precision. The final MAP value is the mean of the average precisions of all sessions. We computed MAP for all methods both in the biased and unbiased setting. The full results for both cases are in Appendix B.
  1 3
326
A. Ajanki et al.
 Table 4
Random (no feed- back)
25.7
The mean average precisions for unbiased data
 TFIDF (no feedback)
25.9
Wlin (impl. feedback)
28.1
Wtext(4)
(impl.
feedback) feedback) feedback)
24.8 27.4 80.0
4.3.2 About statistical tests
The performances of the models reported in Tables 3 and 4 are significantly different when tested with non-parametric ANOVA ( p ≪ 0.0001 (biased case) and p = 0.018 (unbiased case) for differences between all baseline and implicit feedback models, Friedman’s Test). This indicates that there is always at least one method for which the performance differs significantly from the others. However, since we are not inter- ested in comparing all methods, but only certain pre-defined pairs, it is sufficient to use pairwise tests.
In the biased case it is clear that TFIDF performs well and random performs poorly. Instead of comparing to those, we are interested to see if the eye movement features bring in additional information compared to using just the text features. If yes, we expect that a model which takes eye movement information into account starts to perform better than text-only models when the bias is reduced. Therefore, we test the significance of Weye+text (26) versus Wtext (4). We additionally investigate the need for user-specific models versus pooling all data together, by comparing Wus(26) versus Weye+text (26).
In the unbiased case, in addition to testing the difference between eye movement and text-only models, we compare the performance of the eye movement models to the random ranking, which is a fair baseline after the bias is removed.
We use Wilcoxon Signed Rank Test to compare the classification performances of the pairs of algorithms (Weye+text(26) versus Wtext(4); and Weye+text(26) versus Wus(26)). We use the non-parametric Wilcoxon Signed Rank Test instead of paired t -test because the average precision values are not normally distributed. Wilcoxon test is known to be less stringent than the t-test. In comparisons with the random model, however, the Wilcoxon Signed Rank Test is not applicable, because the random model does not have a single ranking of the test set documents, but a distribution of rankings. In comparison with the random model (Wlin or Weye+text(26) vs. random) in the unbi- ased case in Sect. 4.3.5 we used a standard permutation test for which we sampled 100,000 random rankings.
4.3.3 Baseline performance
The performance of the random baseline model in each session was tested by sampling 100,000 random rankings of the test documents and computing average precisions for each. To summarize the performance of the random model in Table 3, we first com- puted session-wise mean average precisions over the random rankings and finally took average of them. The MAP is 6.0% in the biased case and 25.7% in the unbiased case.
1 3
Weye+text (26) (impl.
SVMex
SVM-2K (impl. feedback)
80.0
(impl.

Implicit queries from gaze patterns
327
    90
80
70
60
50
40
30
20
10
0
Wtext(4) Weye+text(26)
                                                                                                          Average precisions of Wtext(4) and Weye+text(26) in different topics in the biased case. The topics are sorted according to the performance of Weye+text model
The baseline TFIDF model can often identify the correct topic in the biased setting because the positive documents form the largest cluster among the training documents. This results in achieving a MAP of 73.4%, which is the highest of all tested models in the biased case. In the unbiased case in Table 4, the positive document does not stand out from the rest of the training set, and therefore the performance of the TFIDF model (25.9%) is close to random.
4.3.4 Regression models on biased data
In the biased case (Table 3) the difference in average precision between the non-linear regression model Weye+text(26), which combines eye movement and textual features, and the textual feature model Wtext(4) is statistically significant (46.3 vs. 19.9%, p = 0.00005, Wilcoxon Signed Rank Test). This implies that the eye movement fea- tures contain information that helps in the complex task of inferring the hidden query. Figure 4 shows the difference in performance of the two models. The availability of eye movement information improves the performance clearly on several topics, but in two cases the text-only model outperforms the eye movement model.
Weye+text(26), which was trained pooling together the data from all users who completed the same query, is significantly better than Wus(26), which used the user specific average precisions (46.3 vs. 31.3%, p = 0.0001, Wilcoxon Signed Rank Test).
Fig. 4
1 3
Average precision
Languages Ball games
Dinosaurs Postal system
Olympics Printing
Elections Space exploration
Natural disasters Cities
Education Internet
Government Speeches
Optical devices Court systems
Film Television
Sculpture Transportation
Writing systems Family
Astronomy Literature
Music
328 A. Ajanki et al.
 This result suggests that the method benefits from the larger training sets created by pooling the data form different users.
We next tested how much the results depend on the specific topics, that is, how universal the results are over the choice of topics. For this, we resampled with replace- ment topics among the original 25 topics. To study the effect of the number of topics, we repeated the analysis for different set sizes between 2 and 25 topics. For each topic set size, we draw 10,000 random topic sets, and computed the difference in average precision between Wtext(4) and Weye+text(26) in each replicate. We computed BCa corrected estimates of the 95% confidence intervals (DiCiccio and Efron 1996) for the difference between the two methods based on the obtained bootstrap distribution. To keep the computational workload down we employed the regression parameters we had already learned on the full corpus instead of retraining them on the sampled, smaller topic sets. Note that all training and testing has still been done on completely sepa- rate sets. The model using eye movement features was better (the confidence interval included only positive values, i.e. values for which the average precision of the eye movement model was better than that of the text-only model) on all sets larger than three topics. In other words, the model performs reliably regardless of which topics are chosen.
4.3.5 Regression models on unbiased data
The results for the unbiased data are shown in Table 4. The linear regression model Wlin attains the highest MAP of all implicit feedback models with the non-linear Weye+text(26) following closely. Note that the results in Table 4 are not directly com- parable to the results on biased data in Table 3, because in the unbiased case the training sets are much smaller (4–8 documents in the unbiased case vs. 30–50 documents in the biased case) and the measurements from different users are not pooled as is discussed in Section 4.2.4.
Combination of eye movement features and textual features gives better perfor- mance than the textual features alone (Weye+text(26) vs. Wtext(4), p = 0.028). To test our assumption that the implicit feedback can improve over a random ranking when the artificial bias of our training set is removed, we compared the results of Weye+text(26) to random. As discussed earlier in Sect. 4.3.2 the observed Weye+text(26) MAP val- ues are compared to the distribution of the MAP value of the random model using a permutation test. We randomly permuted the relevance labels of the test documents 100,000 times in each session, computed the mean average precision, and counted the proportion of the permutations that had at least as high MAP as the eye movement models. Only 2.7% of the permuted samples had at least as high MAP as Weye+text(26) (27.4% vs. 25.7%, p = 0.027, Permutation Test), and 0.26% were at least as good as Wlin (28.1 vs. 25.7%, p = 0.0026, Permutation Test).
We studied the effect of the choice of topics on the performance using a similar bootstrap approach as in Sect. 4.3.4. We sampled the topics with replacement and con- structed sets consisting of 2 to 25 topics. We generated 10,000 samples for each set size and estimated the 95% confidence intervals for the difference in average precision between Wtext(4) and Weye+text(26). The model that uses both text and eye move- ment features performed significantly better than the text-only model (assessed by the
1 3
Implicit queries from gaze patterns
329
    70
60
50
40
30
20
10
0
random Weye+text(26)
                                                                   ...
                           Fig. 5 Average precisions of ten best performing (left-hand side) and ten worst performing sessions (right-hand side) in the unbiased case. The session are sorted according to the p-value of the Permuta- tion Test between random ranking and Weye+text(26)
confidence interval including only positive values, i.e. values for which the eye movement model was better) with 21 or more topics. The variance of the bootstrap estimate gets larger as the number of topics and thus also the number of average precision samples decreases. At least 21 topics are required for getting statistically significant results with the current amount of training data; it is likely that with more test subjects, or more read documents per topic, fewer topics would be needed for a statistically significant difference.
To get further insight into the performance of the model we took a look at the individual sessions. The average precisions of the best and worst performing sessions (sorted according to the p-value of the Permutation Test between random ranking and Weye+text(26)) are shown in Fig. 5. In 10 out of 88 sessions the average pre- cision of Weye+text(26) is significantly better than the random model (p ≤ 0.05, Permutation Test). The average improvement in MAP over random in these que- ries was 24.5, which is considerably higher than the overall change in MAP for all sessions 1.7 (= 27.4−25.7). Some topics appear very frequently among the top-10 sessions; Olympics occur three times, and Court systems, Languages, and Printing occur two times each. All of these topics were queried in three sessions in total, except for Languages which was the positive topic in four sessions. The fact that they appear so frequently among the highest performing sessions indicates that there
1 3
Average precision
Printing (user 10) Olympics (user 4)
Court systems (user 1) Olympics (user 9)
Languages (user 10) Languages (user 5)
Ball games (user 9) Printing (user 6)
Olympics (user 3) Court systems (user 4)
Literature (user 10) Internet (user 9)
Optical devices (user 5) Astronomy (user 5)
Writing systems (user 2) Astronomy (user 8)
Internet (user 2) Family (user 2)
Internet (user 3) Family (user 6)
330 A. Ajanki et al.
 exists queries for which the implicit feedback works well. On the other hand, because the overall MAP of the implicit feedback model is so close to the MAP of random ranking there clearly are other queries which do not benefit from the eye movement feedback.
4.3.6 Feature selection
The eye movement features (Sect. 3.2.4) used in the experiments have been previously proposed in the context of various psychological studies. There is no evi- dence that all of them are needed for the current task of predicting the interest of a user. It is an important open question what kind of features are best suited for this task.
To get some insight on the relative importance of the features, we analyzed the linear regression model Wlin for which such analysis is straightforward. We used the t-test to find out which regression coefficients differed significantly from zero. There were three eye movement features and one text feature that had Bonferroni corrected
p-values less than 0.05 in all sessions, both in the biased and unbiased case. These features were “saccade length before first fixation to the word,” “did a regression ini- tiate from the following word,” “duration of the first fixation to the word divided by the total fixation durations on the document,” and “relative position of the word on the document.” In addition to these, two features were significant in a subset of sessions either in the biased or the unbiased setting. The feature “duration of the next fixation after leaving the word” was significant in 9 out of 25 sessions in the biased case and 25 out of 694 sessions in the unbiased case, and “duration of a regression starting from this word” was significant in 2 out of 25 biased sessions.
The performance of the linear regression model in the biased setting increased from 37.8 to 40.9% when only the five features with significant contribution in at least nine sessions were selected. On the other hand, the same subset is not optimal for the non- linear regression model. The MAP of Weye+text (26) decreases from 46.3 to 39.9% with the set of five selected features. In the unbiased case, switching from the full feature set to the set of five important features actually impairs the linear regression model. The MAP decreases from 28.1 to 26.8%. The MAP of Weye+text(26) increases a little bit from 27.4 to 27.7%. In summary, the extracted features are clearly significant but probably not completely sufficient.
4.3.7 Performance of eye movements combined with explicit feedback
The average precision of the explicit feedback classifier SVMex, which was discussed in Sect. 3.2.5, is significantly higher than any of the eye movement models. This is, of course, to be expected because explicit feedback gives much more accurate information than eye movements. As a side note, the fact that the TFIDF baseline model attains almost as high performance without any feedback as the SVMex with known relevance labels shows how substantial the effect of the bias really is in our setup.
Our initial assumption was that combining eye movements with the explicit rel- evance feedback improves overall performance over using explicit feedback only.
1 3
Implicit queries from gaze patterns 331
 Comparing SVMex and SVM-2K results for the biased setting in Table 5 shows that this is not true for all search topics. Nevertheless, Table 3 shows that the overall precision for the biased setting is improved slightly by combining the two sources of information. The situation is even more clear in the unbiased setting (Table 4), where the SVM-2K and SVMex models have equal MAP. The equality of performance is consistent across all categories in Table 6 leading us to believe that the selection of eye movement features used in this study does not improve the overall performance when combined with textual information using SVM-2K. It is apparent that the explicit feedback is sufficient to learn the discrimination in the unbiased case.
5 Discussion
We addressed the extremely hard task of constructing a query in an information retrieval task, given neither an explicit query nor explicit relevance feedback. Only eye movement measurements for a small set of viewed snippets, and the text content of the snippets were available. This is a prototype of a task where the intent or interests of the user are inferred from implicit feedback signals, and used to anticipate the users’ actions.
We presented a proof-of-concept solution for this new problem based on the assump- tion that there is a link between eye movements and the relevance, and that the link is independent of the query. We were able to learn a “universal predictor of rele- vance predictors” from a collected database of queries, their relevant and irrelevant documents, and the corresponding eye movements. “Universal” here means inde- pendent of the actual query in the domain of Wikipedia. We validated the proposed solution on a real dataset, and showed that the predictions were better than those of a simple model which utilized only the textual content of the documents for new queries.
There may be a dependence between the gaze pattern and the topic of interest, even within our set of Wikipedia topics. A topic-specific predictor could therefore perform better for that particular topic, but it would be unable to predict the relevance of yet unseen topics. Our goal was to make universal, or topic-independent, predictor of relevance. We accomplished this by explicitly constructing our predictor such that it is invariant with respect to the permutation of the words—that is, the predictor does not take the semantic meaning of the words into account. The fact that we were still able to reach a statistically significant average prediction results on topics unseen in the training data shows that there is a link between the gaze patterns and interests. The statistically significant features, discussed in Sect. 4.3.6, give hints of the nature of this link. The exact nature or interpretation of this connection needs to be left as a topic of further study.
Notice that any method used to discriminate between categories, based on the BOW representation of the documents, such as ours, requires the categories to have different term frequency distributions. If the term frequency distributions of the cate- gories are too similar to each other the BOW methods are expected to perform worse. In our experiments we used Wikipedia categories which have a relatively good sep- aration in BOW representations. However, it remains an open question whether the
1 3
332 A. Ajanki et al.
 eye movements could effectively be used to find finer distinctions, that is, to find most discriminative words even if the relevant and irrelevant categories were very similar to each other.
We also addressed the issue of ‘bias’, which is likely to occur in a real world information retrieval situation, where the user is likely to view proportionally more relevant than irrelevant documents. More specifically, when browsing to look for inter- esting documents, there is no bias, and the bias increases as a function of the length of a focused search or browsing session. Both extremes are important for a practical proactive information retrieval system.
The bias complicates the evaluation of eye tracking results. We showed that taking the eye movement measurements into account improves the precision of the infor- mation retrieval, both in the presence and absence of the bias. Eye tracking is more important when there is no bias, since then content-based searches do not help at all. With heavy bias the relative improvement is much smaller. In percentage units the improvements are rather modest, as eye movements are a quite noisy indirect indicator of relevance. Nevertheless, assuming eye tracking signal is cheaply available, it would be a useful source to complement alternative implicit feedback sources. This holds in particular for applications with little bias towards read documents being relevant, such as in browsing applications.
We further experimented with a model where the textual content of the documents and explicit relevance feedback given by the user (whether or not the user thinks the document is relevant to the search topic) were assumed available. As expected, the pure explicit feedback improved the precision significantly to 75.1%. Our results show that also in the biased case, taking the eye movements into account we can further improve the precision by about two of percentage units.
In this paper, we studied whether eye movements are at all suitable as a source of implicit feedback for learning an universal predictor of relevance. The method is still a long way from being usable in practical applications, and there are several important extensions to be studied in the future. One interesting research topic is replacing the general-purpose machine learning components we used here with ones that are better tailored to the task. There are also problems on the practical level: in order to reliably connect gaze location to the correct word using current eye tracker technology, the size of the font has to be fairly large, which restricts the amount of text on the screen. Another factor that would be worth studying is what kind of eye movement features are optimal for the task.
We conclude that in constructing a query, eye movements provide a useful implicit feedback channel. As expected, the feedback obtained from eye movements is less informative than relevance feedback typed in by the user, but nonetheless this implicit feedback can be exploited. The eye movements may be the only available source of relevance information in the unbiased case where nothing can be deduced from the textual content. In practical applications all available feedback channels, in addition to the eye movements, should of course be utilized; the practical implication of this study is the finding that it is a good idea to include eye movement data if they are cheaply available. The future challenge is to improve the gain in precision obtained from the eye movements, in addition to making this new feedback channel more practically applicable.
1 3
Implicit queries from gaze patterns 333
 Acknowledgments AA and SK belong to Adaptive Informatics Research Centre, a centre of excellence of the Academy of Finland. The authors thank Wray Buntine from the Complex Systems Computation Group at University of Helsinki for providing the Wikipedia data, and Craig Saunders and Steve Gunn from the ISIS Research Group at University of Southampton for many fruitful discussions. This work was supported in part by the IST Programme of the European Community, under the PASCAL 2 Network of Excellence, and in part by TKK MIDE programme, project UI-ART. This publication only reflects the authors’ views. All rights are reserved because of other commitments.
Appendix
A Optimization of SVM-2K
SVM-2K combines KCCA-projection and a regular SVM by introducing the con- straint of similarity between two 1-dimensional projections. The extra constraint is chosen slightly differently from the 2-norm that characterizes KCCA, which typically finds a (varying in number) sequence of projection directions that then can be used as the feature space for training an SVM. Denote the two projections (views) of a training sample xi by φA(xi) and φB(xi). In order to obtain sparse set of projection directions we take an ε-insensitive 1-norm using slack variables to measure the amount by which points fail to meet ε similarity:
|⟨wA,φA(xi)⟩+bA −⟨wB,φB(xi)⟩−bB|≤ηi +ε,
wherewA,bA (wB,bB)aretheweightandthresholdofthefirst(second)SVM,andηi are slack variables. Combining this constraint with the usual 1-norm SVM constraints and allowing different regularization constants gives the following optimization prob- lem:
min
suchthat
1 2 1 2 A  l A B  l B  l
L = 2∥wA∥ + 2∥wB∥ +C ξi +C ξi + D i=1 i=1
ηi i=1
  |⟨wA,φA(xi)⟩+bA −⟨wB,φB(xi)⟩−bB|≤ηi +ε yi (⟨wA,φA(xi)⟩+bA)≥1−ξA
yi (⟨wB,φB(xi)⟩+bB)≥1−ξB i
ξA ≥0, ξB ≥0, ηi ≥0 allfor 1≤i ≤l. ii
The final SVM-2K decision function is then h(x) = sign( f (x)), where
f (x)=0.5 wˆA,φA(x) +bˆA + wˆB,φB(x) +bˆB =0.5(fA(x)+ fB(x)). (1)
B Supplementary results
See Tables 5 and 6.
i
1 3
334
Table 5 Results in the biased setting
TFIDF Wlin Astronomy 53.8 13.5
Wtext (4) 33.3
48.7 15.1 19.0 24.0 17.7 29.2 12.0 11.0
8.9
9.1 23.8 7.2 10.4 29.1 10.0 26.4 11.5 42.2 9.8 6.4 19.4 33.4 26.6 14.3 19.9
Weye+text (26) 15.8
81.2 46.2 36.0 77.9 46.1 68.8 16.8 33.4 41.5 44.6 86.7 12.1 11.7 66.3 70.2 38.7 76.9 69.0 31.6 67.6 40.7 32.3 27.5 18.0 46.3
Wus (26) 20.0
37.9 37.8 44.3 42.6 38.6 34.5
5.7 17.7 25.4 11.7 52.6 11.5 11.7 34.6 45.3 23.2 66.3 63.7 25.7 42.8 19.2 28.0 18.0 23.6 31.3
A. Ajanki et al.
SVMex SVM-2K
58.0 57.4 100.0 100.0 100.0 100.0
74.8 71.3 86.0 87.4 73.3 82.4 76.0 83.6 65.0 66.7 68.6 59.2 55.5 53.4 49.7 55.0 95.0 95.0 26.8 34.2 72.4 74.2 99.1 98.3 76.4 80.0 85.2 91.6 94.3 95.0 86.2 85.5 80.1 79.5 88.5 89.7 84.8 84.8 71.2 73.8 66.1 66.5 45.4 60.3 75.1 77.0
   Ball games Cities
Court systems Dinosaurs Education Elections Family
Film
Government Internet Languages Literature
Music
Natural disasters Olympics
Optical devices Postal system Printing Sculpture
Space exploration Speeches Television Transportation Writing systems Average
100.0 83.2 93.5 32.0 71.6 44.0 99.1 59.7 82.6 30.6 81.1 61.3 44.0 6.7 56.1 16.7 54.8 30.8 47.0 44.8 90.6 60.2 40.3 15.5 68.4 8.0 81.9 62.9 88.6 53.4 93.3 26.6 90.6 50.5 84.2 63.4 69.5 34.9 88.4 49.5 79.0 26.5 63.5 26.8 61.8 25.6 51.6 17.2 73.4 37.8
 These have been computed on a test set of 250 documents, with ten documents being positive
Table 6 Results in the unbiased setting
 User Topic
1 Ball games
1 Dinosaurs
1 Space exploration 1 Literature
1 Government
1 Court systems
1 Cities
1 Film
Pos/test Random TFIDF Wlin set size
Wtext (4) 27.0
30.4 22.0 15.6 18.5 30.0 20.9 18.8
Weye+text SVMex (26)
30.2 99.0 29.3 88.6 29.3 76.0 18.3 59.0 26.4 67.5 51.3 72.1 40.3 82.6 16.5 77.3
SVM-2K
99.0 88.7 76.1 59.3 67.8 71.9 82.6 77.1
 10/50 25.7 10/50 25.7 10/50 25.7 10/60 21.9 10/50 25.7 10/60 21.9 10/50 25.7 10/60 21.9
30.7 24.3 35.9 34.5 34.1 31.9 12.3 16.3 22.4 29.9 50.1 56.4 34.8 42.1 23.3 16.3
 1 3
Implicit queries from gaze patterns Table 6 continued
335
SVMex SVM-2K
77.7 78.1
88.4 88.5
86.1 86.2
87.1 86.9
85.7 85.5
89.0 89.1
57.2 57.2
76.8 76.8
77.7 77.7
86.0 85.9
94.7 94.7
99.0 99.0
55.8 55.4
93.5 93.7
77.0 77.0 89.0 89.1
57.2 57.2 76.8 76.8 77.7 77.7 86.0 85.9 94.7 94.7 99.0 99.0 83.5 83.6 89.0 89.1 72.7 72.7 81.8 81.5 74.1 74.1 88.1 88.4 84.0 84.1 91.3 91.2 81.1 81.2 92.6 92.7 86.0 86.1 94.5 94.4 67.8 67.7 99.9 99.9 84.9 84.8 89.3 89.3 81.3 81.1
  User Topic
1 Sculpture
1 Natural disasters
2 Education
2 Printing
2 Writing systems
2 Optical devices
2 Internet
2 Family
2 Television
2 Speeches
2 Postal system
2 Languages
3 Music
3 Olympics
3 Astronomy
3 Optical devices
3 Internet
3 Family
3 Television
3 Speeches
3 Postal system
3 Languages
4 Music
4 Olympics
4 Literature
4 Dinosaurs
4 Internet
4 Natural disasters 4 Court systems
4 Education
4 Speeches
4 Space exploration
5 Writing systems
5 Postal system 5 Transportation 5 Languages
5 Cities
5 Elections
5 Astronomy
Pos/test Random TFIDF Wlin set size
Wtext (4) Weye+text (26)
15.8 16.4
13.5 16.8
23.5 30.8
33.0 34.1
15.6 15.2
32.7 23.0
12.0 12.4
24.5 17.8
23.7 17.6
38.1 33.8
22.0 26.7
51.3 28.2
14.3 26.2
30.9 43.0
24.3 18.4 31.7 23.8
12.0 11.3 24.9 27.4 23.8 23.9 37.8 47.2 22.2 24.4 53.1 30.7 20.0 25.6 19.6 53.6 22.5 25.7 23.0 19.3 26.2 31.6 18.2 24.3 23.7 49.1 18.9 29.1 15.9 15.2 23.9 35.6 32.7 33.2 17.4 16.0 21.8 27.7 74.1 64.7 20.1 18.5 51.7 43.5 18.9 19.2
 10/60 21.9
10/60 21.9
10/40 31.3
10/40 31.3
10/50 25.7
10/40 31.3
10/60 21.9
10/40 31.3
10/50 25.7
10/40 31.3
10/60 21.9
10/70 19.1
10/60 21.9
10/50 25.7
10/50 25.7 10/40 31.3
10/60 21.9 10/40 31.3 10/50 25.7 10/40 31.3 10/60 21.9 10/70 19.1 10/40 31.3 10/60 21.9 10/40 31.3 10/70 19.1 10/40 31.3 10/50 25.7 10/40 31.3 10/70 19.1 10/60 21.9 10/40 31.3 10/40 31.3 10/70 19.1 10/50 25.7 10/30 40.2 10/50 25.7 10/30 40.2 10/40 31.3
14.8 23.0
20.3 17.5
28.0 21.6
46.6 34.0
17.0 15.5
23.2 25.1
12.8 11.8
26.7 17.5
17.0 18.1
26.1 35.9
24.1 21.7
35.4 31.4
26.9 20.8
39.1 54.4
19.2 20.5 23.2 21.9
12.8 11.5 26.7 28.9 17.0 21.7 26.1 45.0 24.1 24.2 35.4 44.1 28.8 24.1 30.0 56.8 25.4 25.2 14.6 23.9 22.2 30.0 19.8 23.0 38.9 43.5 19.3 24.3 14.6 17.9 29.0 30.0 23.2 27.0 22.8 13.7 24.4 28.9 69.7 66.0 19.3 21.4 54.0 41.2 17.3 18.1
 1 3
336
Table 6 continued
User Topic
5 Government
5 Optical devices
5 Sculpture
6 Speeches
6 Family
6 Education
6 Printing
6 Space exploration
6 Writing systems
6 Music
7 Transportation
7 Ball games
8 Optical devices
8 Astronomy
8 Television
8 Film
8 Literature
8 Dinosaurs
8 Space exploration 8 Transportation
8 Sculpture
8 Government
9 Television
9 Optical devices 9 Olympics
9 Internet
9 Speeches
9 Ball games
9 Film
9 Cities
9 Education
9 Natural disasters
10 Court systems
10 Elections
10 Printing
10 Languages
10 Writing systems 10 Literature
10 Music
Pos/test Random TFIDF Wlin set size
Wtext (4) Weye+text (26)
35.9 26.5
19.4 16.5
31.4 26.2
23.3 38.6
14.8 12.9
27.0 39.9
17.8 38.3
18.0 24.9
17.3 21.4
11.6 17.5
20.5 20.4
40.1 22.7 13.6 14.7
14.3 10.8 28.5 32.5 20.2 27.1 26.9 37.1 21.3 24.8 14.6 29.4 36.7 26.5 13.6 18.5 18.0 24.7 21.1 21.3 12.0 18.7 25.6 51.3 17.1 17.0 13.9 14.2 40.1 40.1 21.0 37.1 17.0 16.0 37.8 28.2 20.9 21.6 19.7 18.4 42.7 18.7 36.6 66.9 22.1 44.2 17.4 14.9 13.9 14.1 47.6 34.8
A. Ajanki et al.
SVMex SVM-2K
60.3 60.1
79.2 79.4
85.8 85.8
87.5 87.5
78.3 78.3
88.9 89.0
91.4 91.0
67.6 67.6
52.3 52.1
71.6 72.0
73.1 73.1
98.7 98.7 85.4 85.3
67.4 67.4 79.9 79.9 69.3 69.4 72.1 72.1 95.9 96.0 87.1 87.2 74.6 74.5 70.5 70.5 76.3 75.9 65.8 66.0 56.4 56.4 90.5 90.6 64.2 64.8 89.0 89.2 93.5 93.2 79.5 79.5 71.5 71.5 77.9 78.4 70.5 70.4 80.6 80.5 88.9 88.9 92.4 92.3 94.9 94.9 70.9 70.7 41.0 40.8 81.3 81.3
   10/60 21.9
10/50 25.7
10/40 31.3
10/40 31.3
10/50 25.7
10/50 25.7
10/60 21.9
10/70 19.1
10/60 21.9
10/80 16.9
10/50 25.7
10/50 25.7 10/60 21.9
10/70 19.1 10/40 31.3 10/50 25.7 10/30 40.2 10/50 25.7 10/70 19.1 10/40 31.3 10/70 19.1 10/50 25.7 10/50 25.7 10/70 19.1 10/50 25.7 10/50 25.7 10/60 21.9 10/60 21.9 10/40 31.3 10/60 21.9 10/50 25.7 10/50 25.7 10/60 21.9 10/60 21.9 10/50 25.7 10/60 21.9 10/70 19.1 10/60 21.9 10/30 40.2
24.0 33.2
26.7 16.5
26.2 24.9
29.7 54.3
13.7 13.0
44.7 36.7
18.1 46.9
33.8 30.6
13.4 23.8
11.8 16.9
18.6 20.5
43.8 22.3 12.5 14.5
14.0 15.2 27.0 31.9 21.0 25.1 27.5 37.8 28.1 22.3 24.3 38.9 21.6 27.6 12.2 19.0 14.0 21.8 16.2 19.8 11.9 15.8 49.3 44.6 26.0 19.5 14.1 16.7 30.2 46.9 31.1 43.0 14.9 15.0 24.5 35.9 25.1 21.8 41.9 16.4 32.9 16.6 40.3 67.4 37.7 58.7 11.9 17.2 12.6 14.8 48.2 32.3
 1 3
Implicit queries from gaze patterns 337 Table 6 continued
  User Topic
10 Astronomy 10 Postal system
Average
Pos/test Random TFIDF Wlin Wtext(4) Weye+text SVMex SVM-2K
set size
10/60 21.9 10/40 31.3
16.8 16.2 24.5 26.3 30.1 30.9
(26)
15.5 44.5 44.4
39.6 93.5 93.5
27.4 80.0 80.0
 25.7 25.9
28.1 24.8
 The size of the test set and the proportion of positive documents in it vary from session to session (third column)
References
Budzik, J. Hammond, K.J.: User interactions with everyday applications as context for just-in-time information access. In: 5th International Conference on Intelligent User Interfaces, pp. 44–51. ACM, New Orleans (2000)
Claypool, M., Le, P., Wased, M., Brown, D.: Implicit interest indicators. In: 6th International Conference on Intelligent User Interfaces, pp. 33–40. Santa Fe (2001)
Conati, C., Mertena, C.: Eye-tracking for user modeling in exploratory learning environments: an empirical evaluation. Knowl. Based Syst. 20, 557–574 (2007)
Czerwinski, M., Dumais, S., Robertson, G., Dziadosz, S., Tiernan, S., van Dantzich, M.: Visualizing implicit queries for information management and retrieval. In: SIGCHI Conference on Human Factors in Computing Systems, pp. 560–567. Pittsburgh (1999)
Dhanjal, C., Gunn, S.R., Shawe-Taylor, J.: Sparse feature extraction using generalised partial least squares. In: IEEE International Workshop on Machine Learning for Signal Processing, pp. 27–32. Maynooth (2006)
DiCiccio, T.J., Efron, B.: Bootstrap confidence intervals. Stat. Sci. 11, 189–228 (1996)
Dumais, S., Cutrell, E., Sarin, R., Horvitz, E.: Implicit queries (IQ) for contextualized search. In: 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, p. 594.
ACM, Sheffield (2004)
Farquhar, J.D.R., Hardoon, D.R., Meng, H., Shawe-Taylor, J., Szedmak, S. : Two view learning: SVM-
2K, theory and practice. In: Weiss, Y., Schölkopf, B., Platt, J. (eds.) Advances in Neural Information
Processing Systems, pp. 355–362. MIT Press, Cambridge (2006)
Fono, D., Vertegaal, R.: EyeWindows: evaluation of eye-controlled zooming windows for focus
selection. In: SIGCHI Conference on Human Factors in Computing Systems, pp. 151–160. ACM
Press, Portland (2005)
Fox, S., Karnawat, K., Mydland, M., Dumais, S., White, T.: Evaluating implicit measures to improve web
search. ACM Trans. Inf. Syst. 23, 147–168 (2005)
Granaas, M.M., McKay, T.D., Laham, R.D., Hurt, L.D., Juola, J.F.: Reading moving text on a CRT screen.
Hum. Factors 26, 97–104 (1984)
Hardoon, D.R., Szedmak, S.R., Shawe-Taylor, J.R.: Canonical correlation analysis: an overview with
application to learning methods. Neural Comput. 16, 2639–2664 (2004)
Hardoon, D.R., Ajanki, A., Puolamaki, K., Shawe-Taylor, J., Kaski, S.: Information retrieval by inferring
implicit queries from eye retrieval by inferring implicit queries from eye movements. In: 11th Intelligence and Statistics. San Juan. Electronic proceedings at http://www.stat.umn.edu/~aistat/ proceedings/start.htm (2007)
Howard, D.L., Crosby, M.E.: Snapshots from the eye: towards strategies for viewing biblographic cita- tions. In: Savendy, G., Smith, M.J. (eds.) Human-Computer Interaction: Software and Hardware Interfaces, pp. 488–493. Elsevier, Amsterdam (1993)
Joachims, T., Granka, L., Pan, B., Hembrooke, H., Gay, G.: Accurately interpreting clickthrough data as implicit feedback. In: 28th Annual International ACM SIGIR Conference on Research and Develop- ment in Information Retrieval, pp. 154–161. ACM, Salvador (2005)
Kelly, D., Teevan, J.: Implicit feedback for inferring user preference: a bibliography. ACM SIGIR Forum 37, 18–28 (2003)
1 3
338 A. Ajanki et al.
 King, L.: The relationship between scene and eye movements. In: 35th Annual Hawaii International Conference on System Sciences, p. 136b. Big Island (2002)
Levy-Schoen, A., O’Regan, K. : The control of eye movements in reading. In: Kolers, P.A., Wrolstad, M.E., Bouma, H. (eds.) Processing of visible language, pp. 7–36. Plenum Press, New York (1979)
Maglio, P.P., Campbell, C.S.: Attentive agents. Commun. ACM 46, 47–51 (2003)
Maglio, P.P., Barrett, R., Campbell, C.S., Selker, T.: SUITOR: an attentive information system. In: Interna-
tional Conference on Intelligent User Interfaces, pp. 169–176. ACM, New Orleans (2000)
Meng, H., Hardoon, D.R., Shawe-Taylor, J., Szedmak, S.: Generic object recognition by distinct features combination in machine learning. In: 17th Annual Symposium on Electronic Imaging, pp. 90–98.
San Jose (2005)
Puolamäki, K., Kaski, S. (eds.): Proceedings of the NIPS 2005 Workshop on Machine Learning for Implicit
Feedback and User Modeling. Helsinki University of Technology, Otaniemi. http://www.cis.hut.fi/
inips2005/ (2006)
Puolamäki, K., Salojärvi, J., Savia, E., Simola, J. Kaski, S.: Combining eye movements and collaborative
filtering for proactive information retrieval. In: 28th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval, pp. 146–153. Salvador, (2005)
Rafter, R., Smyth, B.: Passive profiling from server logs in an online recruitment environment. In: Workshop
on Intelligent Techniques for Web Personalization, pp. 35–41. Seattle (2001)
Rayner, K.: Eye movements in reading and information processing: 20years of research. Psychol. Bull.
124, 372–422 (1998)
Rosipal, R., Trejo, L.J.: Kernel partial least squares regression in reproducing kernel Hilbert space. J. Mach.
Learn. Res. 2, 97–123 (2001)
Salojärvi, J., Kojo, I., Simola, J., Kaski, S.: Can relevance be inferred from eye movements in informa-
tion retrieval?. In: Workshop on Self-Organizing Maps. Kyushu Institute of Technology, Hibikino,
pp. 261–266. Kitakyushu (2003)
Salojärvi, J., Puolamäki, K., Kaski, S.: Implicit relevance feedback from eye movements. In: Duch, W.,
Kacprzyk, J., Oja, E., Zadrozny, S. (eds.) Artificial Neural Networks: Biological Inspirations—ICANN
2005, Lecture Notes in Computer Science 3696, pp. 513–518. Springer, Berlin (2005a)
Salojärvi, J., Puolamäki, K., Simola, J., Kovanen, L., Kojo, I. Kaski, S.: Inferring relevance from eye move- ments: feature extraction. Technical Report A82, Helsinki University of Technology, Publications
in Computer and Information Science. http://www.cis.hut.fi/eyechallenge2005/ (2005b)
Salton, G., McGill, M.J.: Introduction to Modern Information Retrieval. Vol. 1, McGraw-Hill,
New York (1983)
Turpin, A., Scholer, F.: User performance versus precision measures for simple search tasks. In: 29th
Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,
pp. 11–18. Seattle (2006)
Ward, D.J., MacKay, D.J.C.: Fast hands-free writing by gaze direction. Nature 418, 838 (2002)
Author Biographies
Antti Ajanki is a Ph.D. candidate in Computer Science at Helsinki University of Technology. He received his M.Sc. (Tech.) degree in Computer Science from Helsinki University of Technology in 2006. His research interests include machine learning and information retrieval. Previous research has included work in the field of bioinformatics.
Dr. David R. Hardoon is a research fellow at the University College, London. He is currently working on projects that are focused on learning the structure of music, medical analysis, multilingual and multi-modal integration. He has a keen interest in multi-view learning, kernel methods, regression, and sparsity. He has previously worked on various research projects in the fields of Taxonomy, Image analysis, classification and content based retrieval systems. David received his first class B.Sc. Hons. in Computer Science with Artificial Intelligence from the Royal Holloway, University of London and his PhD in Computer Science in the field of Machine Learning from the University of Southampton. He has also received the PhD PASCAL label award from his active participation in the PASCAL network (More information about him can be found at http://www.DavidRoiHardoon.com/).
Implicit queries from gaze patterns 339
 Prof. Samuel Kaski is Professor of Computer Science at Helsinki University of Technology, vice director of the Adaptive Informatics Research Centre, a center of excellence of the Academy of Finland, and group leader in Helsinki Institute for Information Technology HIIT. Dr. Kaski received his M.Sc. and D.Sc. (PhD) degrees in Computer Science from Helsinki University of Technology. His main research field is statisti- cal machine learning, with applications in bioinformatics and information retrieval. He has authored 130 refereed papers in these fields.
Dr. Kai Puolamäki is a lecturing researcher (assistant professor) in the Department of Computer Science at the Helsinki University of Technology. He completed his Ph.D. in 2001 in theoretical physics from the University of Helsinki. His primary interests lie in the areas of data mining, machine learning and related algorithms, and especially their application in ecology and user modelling.
Prof. John Shawe-Taylor has been the Director of the Centre for Computational Statistics and Machine Learning at University College, London since July 2006. He obtained a PhD in Mathematics at Royal Holloway, University of London in 1986. He subsequently completed an MSc in the Foundations of Ad- vanced Information Technology at Imperial College. He was promoted to Professor of Computing Science in 1996. He has published over 150 research papers. He led the ISIS research group at the University of Southampton from 2003 to 2006. He is the scientific coordinator of the EC funded Network of Excellence PASCAL.
    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond
GEORG BUSCHER, ANDREAS DENGEL, RALF BIEDERT, and LUDGER VAN ELST, German Research Center for Artificial Intelligence (DFKI)
  Reading is one of the most frequent activities of knowledge workers. Eye tracking can provide information on what document parts users read, and how they were read. This article aims at generating implicit relevance feedback from eye movements that can be used for information retrieval personalization and further applications.
We report the findings from two studies which examine the relation between several eye movement measures and user-perceived relevance of read text passages. The results show that the measures are generally noisy, but after personalizing them we find clear relations between the measures and relevance. In addition, the second study demonstrates the effect of using reading behavior as implicit relevance feedback for personalizing search. The results indicate that gaze-based feedback is very useful and can greatly improve the quality of Web search. The article concludes with an outlook introducing attentive documents keeping track of how users consume them. Based on eye movement feedback, we describe a number of possible applications to make working with documents more effective.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval—Relevance feedback; H.1.2 [Models and Principles]: User/Machine Systems—Human informa- tion processing
General Terms: Experimentation, Human Factors, Measurement
Additional Key Words and Phrases: Relevance feedback, eye movement measures, personalization, attentive documents
ACM Reference Format:
Buscher, G., Dengel, A., Biedert, R., and Van Elst, L. 2012. Attentive documents: Eye tracking as implicit feedback for information retrieval and beyond. ACM Trans. Interact. Intell. Syst. 1, 2, Article 9 (January 2012), 30 pages.
DOI = 10.1145/2070719.2070722 http://doi.acm.org/10.1145/2070719.2070722
1. INTRODUCTION
Forty years ago, Herbert Simon pointed out that attention is a scarce resource in an information-rich world [Simon 1969, 1971]. Therefore, the way we do utilize our “attention budget” reflects to a certain degree our own preferences: We try to focus on these pieces of information that we think are most relevant, interesting, or useful to us in our current situation. Obviously, in an attention economy, where attention might be understood as a currency [Davenport and Beck 2001], there may exist many factors
This work was supported by the German Federal Ministry of Education, Science, Research, and Technology (bmb+f): Project Mymory, grant 011WF01; Project Perspecting, grant 011W08002.
Authors’ addresses: G. Buscher (contact author), Microsoft Corporation, One Microsoft Way, Redmond, WA 98052; email: georg@gbuscher.com; A. Dengel, R. Biedert, L. Van Elst, Knowledge Management Department, German Research Center for Artificial Intelligence (DFKI), Trippstadter Strasse 122, 67663, Kaiserslautern, Germany.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212) 869-0481, or permissions@acm.org.
⃝c 2012 ACM 2160-6455/2012/01-ART9 $10.00
DOI 10.1145/2070719.2070722 http://doi.acm.org/10.1145/2070719.2070722
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.
9

    9:2 G. Buscher et al.
that distract us from a self-directed utilization of our “attention budget.” The tight coupling of Web search and advertising is just one example. Either way, the extent to which a piece of information is interesting or relevant to us influences how we pay attention to it and how we perceive it.
This is especially true for textual information, which, in its various manifestations, has become an important source of knowledge in professional as well as in private settings. For example, we read emails, reports, papers, blogs, presentation slides, etc., in order to acquire information, establish our opinions, and base our decisions. What we read is influencing what we think and what we know; and how we read reflects, amongst other cognitive dispositions, the handling of our attention resources. For example, on the one hand, we may skim parts of a document being of little interest or of which the content is already known to us. On the other hand, we may intensively read text if it seems relevant or interesting. In some cases, we may get stuck or ignore text that is too difficult to understand or too detailed for us to follow.
While we do not have direct access to attention and cognitive activity during reading, there is evidence that eye movements are tightly coupled with these processes in our brains [Liversedge and Findlay 2000] and may serve as measurable indicators for such processes. Therefore, by analyzing eye movements in detail, we might be able to infer how the reader perceived a text he or she has read, that is, whether it was relevant, irrelevant, too difficult to understand, etc.
Technology for measuring eye movements has evolved rapidly during the last couple of years. Nowadays, eye tracking devices are relatively unobtrusive to use and have sufficiently high precision to be useful for analyzing eye movements during reading. Whereas good commercial eye trackers are still too expensive to be widely employed at office workplaces, a boost in the development of low-fidelity, open-source eye-tracking systems and software could be observed.1 It is conceivable that they will further develop so that eye tracking systems will get more wide-spread in the future.
All in all, affordable eye tracking technology together with methods to interpret that data in terms of a reader’s attention not only gives us the opportunity to analyze how humans deal with that scarce resource. It also has the potential to facilitate the development of information systems that are optimized in this regard and to design new kinds of applications. For example, documents may become attentive in the sense that they record the way they are consumed by the reader. Such usage information could be stored in association with the documents so that the documents do not forget when, how, and what parts of them the reader paid attention to. These attentive documents could, for example, form the basis for improving and personalizing search, making the reading process more effective, and supporting collaborative work.
The purpose of this article is to explore how useful eye tracking is as an interactive method to generate implicit feedback while reading. Within the scope of this article we focus on interpreting and using eye movements as implicit relevance feedback in Web search scenarios. We analyze eye gaze data in order to determine what parts of a doc- ument were read, and which of these parts were also relevant to the reader. Therefore, we implemented an algorithm that analyzes raw gaze data from the eye tracker and detects reading behavior. Based on eye movements during reading, we compute several gaze-based measures and explore their relation to individually perceived relevance of read text in two eye tracking studies. We show that the amount of reading and skim- ming changes with relevance and topicality of the viewed text and demonstrate that one of the most expressive measures for relevance is coherently read text length, that is, the length of text the user has read line by line without skipping any part.
1 http://www.gazegroup.org/.
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:3
We then apply eye tracking together with the studied measures of relevance in order to determine which parts of a text the reader has read, how intensely these parts have been read, and finally to estimate how relevant they have been to him or her. Finally, we demonstrate the usefulness of the reading detection algorithm and the studied eye gaze measures in the application domain of Web search. Here we show that implicit relevance feedback from eye tracking can greatly improve the quality search engine result rankings by over 25% compared to the original ranking, and by over 8% compared to baseline reranking methods based on nongaze implicit feedback.
The remainder of the article is structured as follows: First, we give an overview of related research (Section 2) and explain the foundations for our basic reading detection method (Section 3). We then explore the relation of several eye movement measures to user-perceived relevance of read text. To this end, we report on the results of two eye tracking studies employing two different reading scenarios (Sections 4 and 5). We demonstrate the value of gaze-based implicit relevance feedback for personalization in information retrieval (Section 5), and finally give an outlook of further applications for attentive documents that seem possible in the future.
2. BACKGROUND AND RELATED WORK
In the human-computer interaction domain, eye tracking has been applied primar- ily in diagnostic applications to learn how interfaces are used [Cutrell and Guan 2007; Buscher et al. 2009a] and directly in interactive applications such as eye typing [Majaranta and Ra ̈iha ̈ 2002].
In our work, we aim at interactive gaze-based applications, however, not based on direct control from the eyes. Since the eyes naturally did not evolve as tools but rather as perceptual organs and since many eye movements are unconscious, it is often difficult to use gaze for direct control. However, since eye movements are relatively tightly coupled with cognitive processes [Liversedge and Findlay 2000], there is a great deal of implicit information that may be inferred by analyzing eye movements in our daily work. This implicit information, for instance, about how and where we paid attention is very valuable feedback that can be used to support the individual user’s work. Therefore, we aim at using gaze information in a passive, indiscernible way so that the computer system can assist the user.
In the following, we review related work first with respect to reading-related eye gaze feedback. Then we particularly focus on the application of implicit feedback in the information retrieval domain, how implicit feedback is commonly assessed, and how eye tracking could be of use here.
2.1. Towards Attentive Documents
There is only very limited research focusing on the broader idea of what we call atten- tive documents, that is, documents actively keeping track of how they are consumed by users and applications based on this data.
One of the first occurrences of a similar idea can be found in a paper by Hill et al. [1992]. They introduce the interesting concept of computational wear for digital doc- uments in the style of wear for physical documents. Documents actively keep track of user activities with respect to reading (i.e., read wear) and editing (i.e., edit wear). This is done by measuring display time of different document parts and by recording editing interactions. In their implemented version, wear (i.e., the amount of usage of document parts) is visualized on the scrollbars for a document. The authors argue that such information can be very helpful for professional document work. For example, read wear could be used to show the user which parts of a document have been most interesting to him/her before. In that way, it can provide support for improved refinding and recognition. Edit wear could be useful to get informed about document parts that
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    9:4 G. Buscher et al.
have been edited by coauthors. The latter concept can now be found in most modern word processing software applications.
Ohno [2004] develops the concept of read wear further and applies an eye tracker to keep track of eye gaze traces on documents. The central idea is to improve document browsing by making it easier to find and recognize information read before in long documents. The main conceptual advancement of our method over the system imple- mented by Hill et al. is the employment of measures derived from gaze traces in order to determine the intensity of reading. Therefore, the author uses fixation duration on document parts as a direct and simple measure for the approximation of reading intensity.
Xu et al. [2009] also apply eye tracking to record how much a user pays attention to different document parts. Similar to Ohno, they use fixation duration on document terms as a direct measure of reading intensity (or “interestingness”). However, in contrast to Ohno, they use this kind of information as implicit feedback in order to generate personalized document summaries. Such personalized summaries emphasize these parts and terms of a document that have been read intensively before. The authors show that summaries generated in this way are more reflective of a user’s reading interest and summary preference.
In a more interactive way, Hyrskykari et al. [2003] analyze eye movements on the fly while a user is reading and tries to automatically detect comprehension difficulties on foreign language texts. The basic idea is to use the total gaze duration on a word in order to decide whether help for this word or the entire sentence should be provided. Overall, the work shows that it is difficult to detect understanding difficulties on the fly. However, the general idea is intriguing since the text being read plays an active role in the reading process because it actively observes and helps the reader understand it.
2.2. Implicit Feedback for Information Retrieval
Implicit relevance feedback for information retrieval is an important area of research and has been tackled by many different researchers (see Kelly and Teevan [2003] for an early overview). It has been shown that incorporating explicit relevance feedback, that is, asking the user directly for relevance judgments of documents for a given query, can drastically improve the quality of search result rankings [Rocchio 1971; Salton and Buckley 1990]. However, explicitly asking users for relevance judgments imposes an additional burden on the user and causes additional cognitive load. Therefore, work about implicit relevance feedback generally aims at analyzing user context or inter- preting user interactions in order to generate relevance feedback for search implicitly, without causing any additional cognitive load for the user. Most of the research in this area is based on implicit feedback from user context or user interactions not related to eye tracking. However, there is also some recent work based on gaze data.
2.2.1. Non-Gaze-Based Feedback. One of the most frequently researched implicit feed- back data sources is display time, that is, the duration a document is visible on the screen. As one of the earliest, Morita and Shinoda [1994] analyzed reading time for doc- uments and its correlation to explicit relevance judgments for these documents. They found a strong tendency of users to spend more time on interesting documents than on uninteresting ones; a finding that has also been reported by Claypool et al. [2001] and Fox et al. [2005]. However, in more complex and naturalistic settings, Kelly and Belkin could not find any general relationship between display time and the users’ explicit relevance judgments for documents [Kelly and Belkin 2001, 2004. But when taking different task types into account White and Kelly [2006] found clear signals in display time and showed that retrieval performance could be improved. Buscher et al. [2009b] analyze display time in greater detail and measure the duration different document
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:5
parts have been visible on the screen. They report considerable improvements when employing such fine-grained implicit feedback for the improvement of search result ranking. In addition, Gyllstrom [2009] shows that re-finding information in documents on the desktop can be sped up when incorporating information about which document parts had been visible on the screen before.
In addition to display time for documents, it has been found that the amount of scrolling on a Web page [Claypool et al. 2001], click-through for documents in a browser [Fox et al. 2005; Joachims et al. 2007], and exit type for a Web page [Fox et al. 2005] are good indicators of interest. Agichtein et al. [2006] have shown that search result ranking can be considerably improved when combining implicit feedback from several implicit relevance indicators. In important finding by Melucci and White [2007] is that individually personalizing such measures leads to additional improvements.
Most of the previously cited work was based on implicit feedback acquired for entire documents. However, there is only little work focusing on implicit feedback on the segment level, that is, implicit feedback for document parts. [Golovchinsky et al. 1999] recorded highlighting and underlining behavior of users working with documents and also analyzed notes in margin of a document in order to infer individually perceived relevance of different document parts. Using this kind of implicit feedback, the authors showed that the quality of search can be significantly improved. A somewhat similar approach has been examined in Ahn et al. [2008] where users could copy and paste interesting document parts in a personal notebook. They also reported improvements of search engine ranking.
2.2.2. Gaze-Based Feedback. There is some research focusing on gaining implicit rele- vance feedback from eye movements. Balatsoukas and Ruthven [2010] compute basic eye movement measures from eye movements on search engine results pages and inves- tigate their expressiveness with regard to a variety of relevance criteria. They report that result entries that differ in topicality have strongest effects on eye movement measures, but interestingly criteria like familiarity with the information content also play a role. Moe et al. [2007] use a qualitative and exploratory approach and identify eye movement measures that are correlated with explicit relevance ratings for the read text. While most of their tested measures were inconclusive, they found that the amount of reading behavior is informative with respect to relevance of the read text. Loboda et al. [2011] inspected the signal in eye movement measures for inferring relevance of single words and found that sentence-terminal words in particular attract more and longer fixations. Brooks et al. [2006] also focused on determining what gaze-based measures are most helpful in estimating relevance of single paragraphs in documents. In a preliminary study, they found that relevant text parts caused a higher number of fixations and regressions. Furthermore, Puolama ̈ ki et al. [2005] combined a number of eye tracking measures and trained HMM-based classifiers to predict relevance for pre- viously read text. They found that predicting relevance by analyzing eye movements is possible to some extent. However, they did not examine which gaze-based measures were most correlated with relevance. Ajanki et al. [2009] further built on this work and aimed at automatically inferring queries from eye movements while reading. They trained support vector machines based on gaze data to determine text parts that have been relevant to the user.
Buscher et al. [2008b] used eye movement measures to detect parts of longer doc- uments that have been read intensively. They used these read parts of documents as implicit feedback for query expansion and re-ranking in an information retrieval scenario and reported considerable improvements of search engine result quality as measured by explicit user ratings. Cole et al. [2010] further investigated the effect
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    9:6 G. Buscher et al.
of different tasks on reading behavior and found that switches between reading and skimming behavior are implicit indicators of the current task.
In summary, gaze is an excellent data source for providing information about how much attention the user paid to what locations and contents on the screen. Therefore, it seems to be well suited to provide information about which documents and document parts have been interesting or relevant to the user. As previous literature already indicates to some extent, this knowledge can be very valuable as implicit relevance feedback for information retrieval applications.
In this article, we examine the suitability of different eye movement measures for in- ferring relevance feedback on the passage level in detail. First, we review findings from research in reading psychology. Motivated by these findings, we introduce a reading detection method from Buscher et al. [2008a] and describe how we compute a variety of different eye movement measures. Next, we report results from two eye tracking user studies aiming at finding most useful features for detecting relevance. Finally, to show the applicability of the results from the first two studies, we report on extended analy- sis of a study previously partly published in Buscher et al. [2009b] demonstrating the value of gaze-based implicit relevance feedback in an information retrieval scenario.
3. EYE TRACKING
Interpreting eye tracking data can be tricky and requires careful consideration. Gener- ally, eye movements are relatively tightly coupled with cognitive processes [Liversedge and Findlay 2000]. Therefore, they contain valuable information making it possible to infer information about processes in the brain. However, it has to be born in mind that eye movements are often unconscious in nature and there are likely to be a large number of unknown factors influencing them. Hence, gaze data is generally very noisy. In addition, eye trackers introduce further inaccuracy when estimating the eyes’ focal point.
Most previous research employs gaze-based measures in a rather simple and direct way. For example, a fixation on a word is usually interpreted as user interest in or relevance of that word. However, gaze data is noisy and fixations can have different causes depending on the cognitive state of the user. Therefore, a fixation on a word may not always be meaningful. To reduce noise in gaze data, our approach is to detect reading behavior first based on specific spatial and temporal patterns in eye gaze traces. Then, we only focus on gaze data during reading behavior and ignore everything else. While in general, the focal points of our eyes are not always related to the point of visual attention, there is strong evidence that they match during reading behavior [Rayner 1998]. Hence, the gaze data we are relying on for our analysis is recorded only while users are reading, that is, while they are actively consuming and thinking about textual contents.
3.1. Reading Psychology
Automatically detecting reading behavior by analyzing eye gaze patterns is one of the key steps we take to reduce noise from eye trackers. Implementing a robust reading detection method is possible when taking into account existing knowledge from the area of reading psychology. Here, a great deal of research has been done during last one hundred years concerning eye movements while reading. When reading silently, as summed up in Rayner [1998], the eye shows a very characteristic behavior composed of fixations and saccades. A fixation is a time of about 250ms on average during which the eye is steadily gazing at one point. A saccade is a rapid, ballistic eye movement from one fixation to the next. A typical left-to-right saccade is 7–9 letter spaces long. Approximately 10–15% of the eye movements during reading are regressions, that is, movements to the left along the current line or to a previously read line. Visual
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:7
and saccades. A fixation is a time of about 250ms on average when the eye is steadily gazing at one point. A saccade is a rapid, ballistic eye movement from one fixation to the next. The mean left-to-right saccade size is 7-9 letter spaces. It
Fig. 1. Typical eye movement pattern while reading. Circles mark fixations; connecting lines depict saccades.
information can only be perceived during fixations and not during saccades. Words can be identified only up to approximately 7–8 letter spaces to the right of the fixation point. However, the total perceptual span, where at least some useful information about the text can be extracted, extends about 14–15 letter spaces to the right of the fixation point.
There is high variability of the aforementioned average values both with respect to individual differences between readers as well as with respect to document-induced differences for the same reader. For example, the fixation durations for the same reader can vary between 100–500ms, while saccade sizes can range from 1 to 15 characters. Among many other factors, this variability is influenced by the difficulty of the read text, word predictability, background knowledge, and reading strategy of the reader [Rayner 1998].
3.2. Reading Detection Method
We used these insights about typical eye movements while reading to implement a reading detection method. The method works in several steps. First, fixations are detected by grouping together nearby gaze coordinates for a duration of at least 100ms. Second, the transitions from one fixation to the next, that is, the saccades, are classified according to their distances and directions. Classification of saccades is based on a set of heuristics derived from previous research in reading psychology [Rayner 1998]. In that way, typical saccades belonging to reading behavior can be detected and differentiated from unrelated eye movements. Third, using some simple heuristics, it is determined whether a sequence of saccades over a line of text is more characteristic for reading or for skimming behavior based on the lengths and the composition of the saccades. Figure 1 shows an example for the typical placement of fixations and saccades while reading. More details about the reading detection method can be found in Buscher et al. [2008a].
3.3. Eye Movement Measures
Eventually, one of our goals for this work was to detect what parts of a document have been interesting or relevant to a user and to use this knowledge for the personalization of information retrieval methods. Most previous research applying eye tracking for information retrieval used gaze data directly to infer interestingness or relevance of single terms in a text. For example, Ohno [2004] and Xu et al. [2009] computed very simple gaze-based measures like fixation duration on every term in a document and then determined the “best,” for instance, most looked at, terms for further use.
However, the contents and the relations a text conveys are mostly based on specific combinations of many words in sentences and paragraphs rather than single isolated terms. For example, regressions on a word in a text do not necessarily mean interest or relevance of that word but can rather be caused by difficulties in understanding the word or the entire sentence. However, what the regression can tell us is that the reader pays close attention to the text and tries to understand it. Therefore, in order to detect relevance of document parts, we do not apply gaze-based measures on single terms directly, but aggregate them on the level of sentences and paragraphs. Overall, we aim to determine relevance at the level of paragraphs. The basic assumption is that
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    9:8 G. Buscher et al.
if, for example, a paragraph is interesting or relevant to the reader, then this will be reflected in his or her eye movement pattern over that text part.
Hence, all of the gaze-based measures described in the following are computed as average values of fixations and saccades on entire paragraphs of text. Note, that we only include fixations and saccades belonging to reading or skimming behavior for the computation of the measures. For the following studies, we are focusing on 5 different gaze-based measures.
—Average fixation duration is computed as the sum of the durations of all fixations on a paragraph divided by the number of fixations on that paragraph. There is abundant evidence that fixation duration is influenced by the text is currently being fixated [Rayner 1998]. Some previous work uses this measure directly as an indicator of relevance [Ohno 2004; Xu et al. 2009]. However, it is not clear yet to what extent it relates to relevance.
—Average forward saccade length is the average length of left-to-right saccades. Sac- cade size is also known to be influenced by characteristics of the text [Rayner 1998]. —Regression ratio is computed as the number of regressions divided by the total num- ber of saccades on a paragraph. There is some indication in previous research stat- ing that higher regression ratios signify relevance of the read text [Moe et al. 2007;
Brooks et al. 2006].
—Thorough reading ratio is computed as the length of text that has been detected
as read by our reading detection method divided by the length of read or skimmed text. Thus, it is a measure for the reading intensity of a user: the more parts of a paragraph read instead of skimmed, the higher the value of this measure. A similar measure has been found to be related to relevance of read text [Moe et al. 2007].
—Coherently read text length measures the length of text in characters that has been read coherently without skipping any text in between. The assumption underlying this measure is that users may start and quickly stop reading a paragraph if the contained information is irrelevant. Then, they may skip the irrelevant paragraph and jump to the next one to continue. In contrast, if the information seems relevant, users may continue reading line by line without skipping any text in between.
3.4. Personalization of the Measures
As stated in Rayner [1998], there is high variability of most eye movement measures both within as well as between readers. Therefore, it is difficult to build methods estimating relevance of read text based on absolute values of gaze-based measures (e.g., compare Moe et al. [2007]). However, when individually personalizing such measures, they become more expressive so that precise implicit relevance feedback is easier to achieve as previous research suggests for non-gaze-based feedback data [Melucci and White 2007].
We personalize each of the recorded gaze-based measures as follows. First, we deter- mine the distribution of a measure for an individual user by analyzing all of his or her recorded eye movement data during reading. Then, we compute the upper and lower whiskers (limits) concerning the measure’s value distribution as it is typically done for generating box plots (i.e., lower whisker = max(minimum value, lower quartile − 1.5 * interquartile range); upper whisker = min(maximum value, upper quartile + 1.5 * interquartile range); see Figure 2) [Wilcox 2005]. The upper and lower whiskers define a user-specific interval containing most of the measured values. Outliers do not fall within such intervals and, hence, do not distort them.
Next, the computed absolute values of the eye movement measures are normalized with respect to the individual whisker-intervals. This results in a percentage for each absolute value stating its relative position in the appropriate interval. Outliers that
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:9
 maximum
minimum
Value distribu on (1)
outlier
       1.5* interquar le range
interquar le range contains 50% of the data
1.5* interquar le
                        Fig. 2.
Personalization of a measure by mapping absolute values into a [0,1]-interval.
range
Whisker determina on (2)
1 (upper whisker) 0.9
0.5
0.2
0 (lower whisker)
do not lie within a whisker-interval are mapped to 0 or 1 depending on whether they are smaller or greater than the lower or upper whiskers, respectively. Figure 2 shows an example for a measure’s value distribution, the respective determination of the whiskers, and the normalization of the measured absolute values.
4. STUDY 1: RELEVANCE ESTIMATION BASED ON EYE MOVEMENTS
The first study is of exploratory nature and analyzes eye movement measures while reading texts with varying degrees of difficulty, novelty, and topicality. Specifically, we 1. examine the relation between reading behavior and explicit user judgments, and 2. analyze the effect of measure personalization.
4.1. Methods
4.1.1. Experimental Design and Procedure. We designed an eye tracking experiment where 19 participants had to read through a number of short documents and judge their relevance with respect to a research task we provided.
The task. The same task was given to all participants. They were told to prepare a presentation about technical aspects of renewable energies. Therefore, we provided them with documents their presentation should be based on. Their task was to read through all of the documents and judge them according to their individual assessments of usefulness for the presentation.
In order to make the task more realistic, we told the participants that the exper- iment was composed of two different parts. The first part described above, that is, looking through and judging the usefulness of each document, was just to categorize the documents for later use. They were told that during the second part, that is, pre- sentation writing, the documents would be presented to them again in an order based on their own categorization from the first part. However, this imaginary second part was made up only to let the task seem more realistic. An experiment has always been stopped after the judgment phase was finished.
The documents. We provided all participants with the same set of 16 documents from Wikipedia or newspapers in randomized order. 14 of the documents were on topic and dealt with different aspects of renewable energies, for instance, characteristics of renewable resources, technical explanations of machines, political opinions, and
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.
Normalized interval (3)

    9:10 G. Buscher et al.
examples of power plants. The remaining two documents were completely off topic (one article was about theory of history; one was about a famous castle).
The 14 on-topic documents were carefully selected to cover a broad range of difficulty and novelty. Some were rather easy to understand, containing only facts that could be assumed to be known by most participants, and some contained rather difficult and complicated contents, for instance, one document explained chemical and physical processes in photovoltaic solar panels on the atomic level. Also, some documents were closer and some more distantly related to the main task about technical aspects of renewable energies. For example, some documents dealt with technical explanations whereas others covered political aspects.
All of the documents were approximately one screen page long and contained about 350 words on average. A font size of 12pt was chosen. A line of text was about 720 pixels wide, which corresponded to approximately 120 characters. The documents were written in German and consisted only of plain text.
Explicit judgments. We provided the participants with the following categorization scheme for their explicit judgments.
—“Useful for the presentation” → label: “useful”
—“Useful, but contents completely known” → label: “known” —“Probably useful, but too difficult to understand” → label: “difficult” —“Rather useless” → label: “useless”
This categorization scheme for explicit judgments needs some discussion. Re- searchers in the domain of information retrieval usually employ one-dimensional rel- evance judgment scales with the two poles “relevant” and “irrelevant”. However, there is a multitude of factors and dimensions influencing how users perceive the quality (or relevance) of a document for solving their current task. For example, as summed up by Chen and Xu [2005], relevance is influenced by topicality, novelty, reliability, understandability, and scope of the contents.
The contents of documents can be characterized in all of these dimensions indepen- dently. All of these independent characteristics can cause different behavior in the reader, and different eye movement patterns in particular. For example, if a document is perfectly on topic but already completely known to the reader, he or she may read it differently than a document that is only partly on topic but that contains many new and interesting facts. Probably the reader will come to the same personal explicit judgment for both documents in the end with respect to a one-dimensional relevance judgment scale. However, we expect to see differences in reading behavior on both documents due to the differences in novelty and understandability.
Out of the five dimensions of relevance named above, novelty and understandability are presumably most dependent on individual users. Individual characteristics like knowledge, background, vocabulary, and intelligence influence both individually per- ceived novelty as well as understandability. Since we expected to see differences with respect to both dimensions in the recorded eye movements, we decided against the typical one-dimensional judgment scale but decided for the four point categorization scheme including the two entries “known” and “difficult.” Now, the differences between the four categories have to be clarified: The category “useful” is used for documents that are relevant on all (or most) of the five dimensions of relevance. “Known” documents are relevant on all of the relevance dimensions except for novelty. Likewise “difficult” documents are relevant on all dimensions except for understandability. “Useless” doc- uments do not fit into the three other categories, e.g., because they are off topic.
Procedure. An experimental run proceeded as follows. First, the eye tracker needed to be calibrated using a 9-point calibration method. Next, the participant read through
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:11
  80% 70% 60% 50% 40% 30% 20% 10%
0%
skimmed read
useless & off topic (N=38)
 23%
52%
useful
(N=134) (N=46)
difficult (N=33)
useless & on topic (N=53)
31%
43%
known
23%
43%
Explicit judgment
26%
34%
18%
16%
Fig. 3. Percentage of read text for documents broken down by explicit judgments.
the task description informing about the first and the imaginary second phase of the experiment, and also about the explicit categorization scheme. After this introduction, the participant had to look through each of the 16 documents sequentially and provide his or her explicit judgment. A typical experimental run took around 30 minutes.
4.1.2. Apparatus. The experiment was performed on a Tobii2 1750 eye tracker at a screen resolution of 1280x1024 pixels. The eye tracker has a tracking frequency of 50Hz and an accuracy of 0.5◦ of visual angle. For calibrating the device we used the software ClearView from Tobii. To analyze the eye movements, detect reading behavior, and compute the gaze-based measures, we applied our own implemented software using the Tobii SDK. All documents were displayed by the Firefox3 browser for which we implemented a plug-in asking for explicit judgments every time a document was exited.
4.1.3. Participants. Nineteen participants (10 female) took part in the experiment and produced valid eye tracking data. All of them were undergraduate or graduate stu- dents attracted by advertisements for the study, and they had German as their native language. They ranged in age from 22 to 32 years (mean = 24.2, σ = 3.2) and had a variety of different majors.
4.1.4. Measures. Using our reading and skimming detection method, we analyzed the participants’ eye movement patterns while reading the assigned documents. Based on the eye movements belonging to reading behavior (as detected by our method), we computed the previously described gaze-based measures. Therefore, the data for the analysis had the following structure: for every document and participant we had an explicit judgment and a set of gaze traces that were used to compute gaze-based measures.
4.2. Results
4.2.1. Distribution of Reading Behavior. Figure 3 shows the amount of reading and skim- ming behavior on documents broken down by explicit judgment. The number N of documents in each category is given in the category descriptors.
2 http://www.tobii.com.
3 http://www.mozilla.com/firefox/.
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.
     % of document read or skimmed
    9:12
G. Buscher et al.
Table I.
Means and standard deviations for the measures “average forward saccade length”, “thorough
reading ratio”, and “coherently read text length” split by explicit judgment
   relevance judgment
           mean forw. sacc. length mean σ
   thorough reading ra o mean σ
     coher. read text length mean σ
   relevant
known
difficult
useless & on topic
9.36 le ers* 9.99 le ers 9.80 le ers* 10.41 le ers
      2.84 3.20 3.03 3.57
0.67* 0.56 0.62* 0.54*
     0.36 0.30 0.39 0.43
465 le ers* 409 le ers 426 le ers* 317 le ers*
430
381
438
347
 useless & off topic
10.95 le ers
3.23
0.41
0.42
233 le ers
277
               It should be noted that the category “useless” is further split into “useless & on topic” and “useless & off topic” where on topic refers to documents that have at least something to do with the main topic of the task (14 of the 16 documents), and where off topic refers to the 2 completely off-topic documents. Topicality was determined by the authors, not by the participants. All participants judged the two off-topic documents and some additional on-topic documents as useless.
The differences in the amount of read plus skimmed text of each document are clearly statistically significant between the category “useless & off topic” and all remaining categories at the 0.01 level (e.g., t-test for “useful” vs. “useless & off topic”: t(170) = 9.6, α < 0.01). However, the differences in the amount of reading plus skimming between the categories “useful” and “known” and between “difficult” and “useless & on topic” are not statistically significant.
The ratio between reading and skimming behavior is different for the 5 categories in Figure 3. Documents being on topic were mostly read instead of skimmed. On the contrary, useless off topic documents lead to slightly more skimming than reading behavior (however, not statistically significant).
Discussion. The combined amount of reading and skimming behavior on the 14 on- topic documents is surprisingly similar. Participants read or skimmed only 15% less of on-topic documents judged useless than of documents judged useful. Also, there does not seem to be a difference in the amount of reading or skimming on useful and known documents. Only the difference to useless off-topic documents is clear and statistically significant.
The findings show that documents being on topic in some way do induce reading behavior of a similar amount. It seems that users keep reading or skimming if docu- ments are on topic and if they can be expected to contain useful information. A reason for this is to a great extend the characteristic of the task. Since users were asked to assess entire documents, they tended not to skip much text. In contrast to topicality, the factors novelty and understandability seem to have only slight influence on the amount of reading behavior.
The ratio between reading and skimming behavior while viewing documents seems to be related to the participants’ final explicit judgments. One of our measures, the thorough reading measure, is computed as this ratio and will be analyzed in more detail in the following.
4.2.2. Gaze Measures to Estimate Relevance.
Absolute measures. Table I presents means and standard deviations for the measures “average forward saccade length,” “thorough reading ratio,” and “coherently read text length” across participants and documents. An asterisk signifies statistical significance of the differences to the category “useless & off topic” (student’s unpaired, two-sided t-test using a significance level of α < 0.01).
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:13
        20 15 10
5
0
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45
par cipant 1 Regression ra o par cipant 2
16 14 12 10
8 6 4 2 0
    Count
Count
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
par cipant 1 Personalized regression ra o par cipant 2
        Fig. 4. Absolute and personalized value distribution of “regression ratio” for two participants.
Table II.
Absolute and personalized means and standard deviations for the measures “regression
ratio” and “average fixation duration” split by explicit judgment
       regression ra o average fixa on dura on
  relevance judgment
relevant
known
difficult
useless & on topic useless & off topic
absolute personalized absolute mean σ mean σ mean σ
personalized mean σ
9.55* 0.22 0.52 0.23 0.52 0.22 0.50 0.25 0.48 0.27
                                  0.16 0.13 0.16 0.11 0.16 0.13 0.17 0.16 0.14 0.18
0.42* 0.27 0.44* 0.34 0.42* 0.26 0.39 0.30 0.34 0.33
219ms 67 228ms 75 202ms 71 215ms 68 208ms 81
        Concerning the two measures “average fixation duration” (overall mean = 216ms, σ = 71) and “regression ratio” (overall mean = 0.16, σ = 0.14), we could not determine any significant differences with respect to the five relevance categories.
Personalization. We were surprised not to see any significant differences for fixation duration and regression ratio since previous literature used these measures to estimate relevance [Ohno 2004; Xu et al. 2009; Moe et al. 2007; Brooks et al. 2006]. Therefore, we analyzed the individual distributions of the measured values for each participant.
It turned out that there are big individual differences. Figure 4 (left) shows the absolute value distribution for the measure ”regression ratio” for two participants. Participant 1 typically performs more regressions during reading (mean = 29%) than participant 2 (mean = 17%). As we know from the literature [Rayner 1998], these differences do not necessarily mean that one of the participants found the documents generally more relevant than the other. Rather, these types of individual differences are highly dependent on the typical reading strategy and expertise of the reader.
In order to achieve comparability of the measured values across participants, we per- sonalized them by applying the personalization method from Section 3.4. The resulting distributions for “regression ratio” for participants 1 and 2 are presented in Figure 4 (right).
Personalized measures. Table II presents absolute as well as personalized means and standard deviations for the measures “regression ratio” and “average fixation duration,” calculated across participants and documents. Again, an asterisk signifies statistical significance of the difference to the category “useless & off topic” (determined by a student’s unpaired, two-sided t-test using a significance level of α < 0.01).
While none of the differences concerning the category “useless & off topic” are sta- tistically significant with respect to the absolute values for both measures, some of
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    9:14 G. Buscher et al.
the differences are significant after personalizing them. Particularly for the measure “regression ratio” a clear trend can be observed. For “average fixation duration,” only a slight tendency can be found.
Discussion. The most expressive measure with respect to perceived relevance is based on the length of coherently read text. Participants read about 4 coherent lines of text (corresponding to 465 characters in out setting) in paragraphs that belonged to documents judged useful. In contrast, they only read 2 coherent lines of text (233 characters) in paragraphs belonging to useless off-topic documents. This difference shows how much users tend to skip text if it does not appear to be relevant.
There is evidence that mean forward saccade length and thorough reading ratio are related to relevance of viewed text. On the one hand, the length of left-to-right saccades decreases with perceived relevance of text. On the other hand, the percentage of read (vs. skimmed) text increases with perceived relevance.
With respect to “regression ratio” and “average fixation duration” we find that the measures are not very discriminative when considering values on their absolute scale. There exist great differences across participants with respect to the individual dis- tributions for these measures. But after individually personalizing the measures, we see clear signals for “regression ratio”: the percentage of regressions during reading generally increases with perceived relevance. However, concerning “average fixation duration”, we only see a very slight trend that duration increases with perceived rele- vance. But there is little evidence for it.
The results show that mean forward saccade length, thorough reading ratio, and regression ratio are all influenced by perceived relevance of read text. However, the finding that fixation duration is the least expressive measure is somewhat surprising since it is most frequently used in previous research as a simple indicator for relevance [Ohno 2004; Xu et al. 2009].
4.3. Conclusion
The study shows that reading and skimming behavior is strongly influenced by rel- evance and especially by topicality of text. Whether a document is on or off topic is clearly indicated by the amount of reading behavior. Therefore, the amount of reading behavior alone gives a good first approximation of whether some text has been relevant to the user.
Relevance, and topicality in particular, are also reflected in most of our analyzed eye movement measures. “Coherently read text length,” “thorough reading ratio,” and “regression ratio” increase with perceived relevance of the read text. “Mean forward saccade length” decreases with perceived relevance. Interestingly, we did not find strong evidence that fixation duration is influenced by relevance.
Even in our rather homogeneous set of participants (i.e., young students at univer- sity) we found large individual differences across participants. Some eye movement measures (particularly “regression ratio”) had different, individually specific distri- butions and mean values. Therefore, any signals in these measures with respect to relevance were overwhelmed by the noise that was introduced by individual differ- ences. However, these individual differences seem to be systematic, so that personaliz- ing and normalizing the measures greatly increased their expressivity with respect to relevance.
The found relationships of eye movement measures with perceived relevance of the text may be very useful for generating relevance feedback on the fly for improving and personalizing information retrieval. The next study will further validate the relation- ships and measure their impact on information retrieval personalization.
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:15
5. STUDY 2: READING BEHAVIOR AS IMPLICIT FEEDBACK FOR SEARCH
The first study demonstrates that detecting reading behavior is very valuable and useful, and that there clearly are signals in eye movements with respect to individually perceived relevance. However, the experimental scenario was very specific in that the assigned documents were all either completely on or off topic and that they were all very short and homogeneous. In addition, the given task, that is, assessing the relevance and usefulness of documents, could have had a high impact on the observed reading behavior. In more realistic settings, tasks can be much more directed to finding specific information, and documents can be much longer and can contain some parts that are relevant and other parts that are irrelevant to the task. Also, the first study comprised a document assessment task which presumably leads to just one specific type of reading behavior. It can be expected that reading behavior on such structurally different documents and for a different task is more diverse.
For this second study we had two fundamental goals. First, we wanted to validate the findings from the previous study for different task and document characteristics, and second, we wanted to prove that gaze-based feedback is not only full of potential for estimating relevant document parts, but that it is actually very useful as implicit relevance feedback on the segment level for information retrieval.
In specific, concerning validating study 1 results and estimating relevant document parts, the aims for this study are (1) to study reading behavior in general on long documents that contain relevant as well as irrelevant parts, (2) to find out whether the relations between eye movement measures and relevance found in the previous study are also valid for different document structures and a more goal-directed task, and (3) to determine the influence of an additional factor on reading behavior, that is, familiarity with the document.
For the second fundamental goals, in order to assess the effect of such gaze-based relevance feedback on information retrieval, we compare it to baseline implicit feed- back methods in a search personalization scenario comprising query expansion and re-ranking of search results. (Results for this part of the study have partly been pub- lished in Buscher et al. [2009b] before.) The baseline methods do not comprise any information from eye tracking at all. They rather create implicit feedback incorporat- ing information from the user query and the contents of the documents the user has viewed immediately before querying the search engine. To quantify the effect of our implicit feedback methods on search result re-ranking, we also compare them with the original ranking from the commercial Web search engine Live Search.4
The information retrieval scenario we are using to evaluate such implicit relevance feedback is as follows. We assume that a user has an information need in mind and looks through some documents in order to find relevant information. Next, he or she queries a search engine to find more related information. At this point, we aim at personalizing the result ranking that is produced by the search engine since we know which documents the user has viewed before and how he or she has viewed them. Thus, we use the user’s viewing behavior as implicit feedback for the next query and can either expand the query with additional relevant terms or re-rank the result list generated by the search engine. The search engine should then return more relevant results to the user.
5.1. Methods
5.1.1. Experimental Design and Procedure. We designed an eye tracking experiment where participants had to look through four long documents in order to read up on
4 http://www.live.com/.
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    9:16 G. Buscher et al.
a given topic (the reading phase), and then search for more related information using a search engine and provide us with explicit relevance ratings for the returned search results (the searching phase).
The Tasks. The two topics we gave to each participant were
(1) perception (about the variety of perceptual organs of animals and how they work) and
(2) thermoregulation (about thermoregulation mechanisms of animals used to control their body temperature).
In order to read up on each topic, the participants had to look through the four provided long documents. They were not asked to provide any explicit relevance judgments of any kind while viewing the documents.
After viewing the four documents with one of the topics in mind they had to pose three different queries to the search engine (searching phase). The first query should always find more related information about the general topic of the task. The two remaining queries should return information about two specific subtopics. For example concerning the topic perception, the participants should find more information about visual and magnetic perception in particular. The general topics for the queries were told orally to the participants. They were free in formulating them.
For each of their queries, the participants were asked to provide explicit relevance judgments for the top 20 of the results on a 6 point relevance scale ranging from “– – –” to “+++”. To judge a result, they were told not rely on the search result abstract visible on the search engine results page but to open the document and look through it. In contrast to study 1, this more common relevance rating scale was employed to be able to measure the performance of different implicit feedback methods using standard metrics.
In order to keep them focused on the respective topic of the task, they had moderate time pressure, that is, about 15 minutes for the reading phase and an additional 15 minutes for the searching phase for each task.
The Documents for the Reading Phase. For reading up on the two topics concerning both tasks during the reading phase, the participants had to use the same four assigned documents. Each document was taken from the German version of Wikipedia5 and was about a different animal species, i.e., about snakes, bees, dogs, and seals. The original Wikipedia articles were modified slightly so that all documents had a comparable length of around 4200 words and so that approximately 6% of the contained text was relevant for each of the two task topics. Each article dealt with a great variety of aspects concerning the respective animal species. However, each article contained some relevant subsections with respect to both topics. We placed these subsections in mostly random positions of the document, with the exception that they never occured in the very beginning or the very end of a document. Since we knew about the positions of the relevant sections, we did not need explicit relevance judgments for text passages from the participants.
The documents were mostly composed of plain text at a font size of 12pt. All of the documents contained some pictures. We removed the table of contents from the original Wikipedia articles and any other means of navigating within the document. Section headings were kept, but not in bold font.
Reranking Procedures. The Web search interface they had to use was modified by us so that we could analyze personalization techniques based on implicit feedback collected from the reading phase.
5 http://de.wikipedia.org.
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:17
To assess the effect of the implicit feedback methods on the final ranking of the search results, we applied two different procedures: query expansion and reranking. The two procedures were not applied simultaneously but we rather split the participants of the study in two separate groups. The first group of 17 participants used a query interface that comprised reranking. For the second group of 15 participants we applied query expansion. However, the search interfaces looked exactly alike and it was not possible for the participants to identify which group they belonged to.
Task Procedure. To make the experiment look as realistic as possible to the partici- pants, the story and the task protocol were designed as follows. The participants had to imagine being journalists having to write articles for a newspaper. We provided them with a simulated email from their imaginary advisor stating the topics (i.e., perception and thermoregulation) they had to write their next newspaper article on. The emails contained our four preselected documents as attachments which should help them to get started with the topic. After having looked through the four documents within about 15 minutes of time (reading phase), the participants had to employ our search interface to find more related information for another 15 minutes (searching phase).
After having finished with the reading phase and the searching phase for one topic, the participants had to repeat the exact same procedure for the other topic. Using the same four documents for both topics guaranteed that the participants were familiar with the documents’ structure for the second reading session. However, the Web search session in between the two reading sessions ensured that the participants could not exactly remember the structure of the documents anymore. Half of the participants started the experiment using the topic perception and the other half started with thermoregulation.
Completing the task for both topics together usually took one hour per participant. The eye tracker was used during the reading phase only.
In summary, the procedure looked as follows from the participants’ viewpoint.
(1) They were informed about the task and the current topic in a simulated email.
(2) They had to look through four preselected long documents that contained some
parts relevant to their topic.
(3) They had to query a Web search interface three times to find more related infor-
mation.
(4) Foreachquerytheyhadtoproviderelevanceratingsforthetop20returnedresults.
This procedure was repeated twice, that is, for both topics.
5.1.2. Implicit Feedback Methods. For personalization purposes of search engine result rankings, we analyze and compare five different implicit feedback methods. Overall, we evaluated implicit feedback methods based on all investigated eye movement features. However, since they turned out to have very similar performance, we report only the results from the three most interesting such implicit feedback methods. In all cases, implicit feedback is generated based on the documents the participant viewed during the reading phase, that is, immediately before querying the search engine. All of the implicit feedback methods extract a list of terms most characteristic to describe the user context based on their feedback source. These terms are then either used to expand the user query or to re-rank results from the Web search engine.
Method Reading. This method is based on eye tracking and reading detection. It first concatenates all documents viewed by the user resulting in one large context document d. Second, it creates a filtered context document dP by removing all text parts from
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    9:18 G. Buscher et al.
d that have neither been read nor skimmed. Third, most characteristic terms are extracted from document dP simply based on their t f × idf scores.6
Method ReadLength(l). This method is an extension of the previous method. It is not solely based on the reading and skimming detection method but also incorporates information from the measure “coherently read text length” to decide whether a certain part of read text should be counted as positive or negative relevance feedback. As demonstrated in the previous two studies, length of coherently read text is strongly related to relevance. Therefore, terms occurring in long coherently read text parts should be better descriptors of the user’s topic of interest than terms in short read text parts.
In more detail, the method uses the same large context document d as in the previous method. It creates two filtered context documents dP and dN . Document dP contains all parts of d that have been read or skimmed and that have a “coherently read text length” of at least l characters. On the contrary, dN contains all parts of d that have been read or skimmed but whose “coherently read text length” has been smaller than l characters. Next, scores for all terms w in dP are computed using the following formula.6
tf(w,dP) ×idf(w). (1) tf(w,dP)+tf(w,dN)
t f (w, dX) denotes the term frequency of term w in document dX.
Based on this computation, terms that are very specific for long read text parts will
get higher scores than terms that also appear in short read text parts.
Method ReadExtremes(l1, l2). This method is an immediate extension of the previous method. It also uses the same large context document d and creates two filtered context documents dP and dN. However, document dP contains all parts of d that have been read or skimmed and that have a “coherently read text length” of at least l2 characters, and dN contains those that are smaller than l1 characters. Scores for all single terms included in dP and dN are computed as in the previous method.
The difference to the method ReadLength(l) is that only these read text passages are used as positive or negative feedback that are close to both extremes of the distribution in terms of coherently read text length. Read text passages with an intermediate length are ignored.
Method FullDocument. As a simple baseline, this method just uses information about what documents have been viewed before issuing a query. Therefore, it just extracts the most characteristic terms of the four documents we provided to the participants. The documents are not filtered any further. Most characteristic terms are determined according to their t f × idf values.6
Method QueryFocus. This method can be seen as a pseudorelevance feedback method on the segment level of documents. It takes into account which documents the partici- pant has viewed before issuing the query, and it also considers the issued query itself. The query is used to find these parts of the previously viewed documents that are relevant to the query at hand.
In more detail, the context document d (as before) is split into a set of paragraphs. For each paragraph, a ranking score is determined with respect to the user query using Lucene7 and a BM25 ranking function. Then, the best paragraphs are selected, that is, the paragraphs achieving a ranking score not worse than half of the ranking
6For the computation of idf values Wikipedia was used as a document corpus. 7 http://lucene.apache.org/.
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:19
score achieved by the best matching paragraph. A virtual document dP is created concatenating all selected paragraphs. As before, the most characteristic terms are then extracted from dP based on their t f × idf scores.
5.1.3. Personalization Procedures. There were mainly two reasons why we decided to examine the effect of the implicit feedback methods using two procedures, i.e., re- ranking and query expansion.
First, reranking has the technical advantage that implicit feedback methods (i.e., the method ReadLength(l) with different parameter settings l can be examined even after an experimental run is over. All the data needed to compute the quality of feedback methods can be stored offline: eye tracking data for generating feedback, and relevance judgments for the top k results of the original user query. This is not possible using the query expansion procedure since every new feedback method results in a new unique query. Every unique query can result in a completely different set of result documents for which we needed to ask the participant again to provide relevance judgments.
Second, query expansion has the potential advantage of higher personalization im- pact. Since every expanded query is unique, it can retrieve new documents from the Web and can exclude others compared to the nonexpanded user query. Therefore, we assumed that expanding the original user query with additional terms would have much larger effects on the quality of the final ranking than just reranking the top k results for the original user query.
Reranking Procedure. For this procedure, each user query was first submitted to the Live Search engine using their SDK in order to retrieve the top 20 result Web pages. The pages were automatically downloaded and stored locally. The results were presented to the participants in randomized order (whom we informed about the randomization). After collecting relevance judgments for all 20 results from the participant, we had all necessary data for offline posteriori reranking:
—the original user query,
—the top 20 resulting Web documents,
—relevance judgments for these documents,
—eye tracking data specifying how the user has viewed the four assigned documents
during the reading phase of an experimental session.
To re-rank the top 20 results we performed several steps: First, Lucene8 was used to generate an index over the top 20 result documents. Second, each implicit feedback method to evaluate was executed on the four provided, previously read documents resulting in a list of characteristic terms paired with their achieved scores. Third, the original user query was expanded by the extracted terms weighted by their scores. Weights were also added to the original user query terms so that the weight of all expansion terms together accounted for 60% of all term weights. Overall, an expanded query contained at most 19 terms (due to technical reasons). Fourth, Lucene was applied to rerank the top 20 original result documents using the expanded query.
Query Expansion Procedure. With this procedure we aimed at comparing some of the feedback methods from above. Based on the results we got from the reranking procedure (which was analyzed first; see the result section for further details), we decided to consider the methods Reading as a representative gaze-based feedback method and QueryFocus as the best non-gaze-based method. We further wanted to compare the results to the original ranking from the Web search engine Live Search for the original user query.
8 http://lucene.apache.org/.
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    9:20 G. Buscher et al.
To expand and evaluate the queries, we proceeded as follows. First, each implicit feedback method to evaluate extracted a list of most characteristic terms from the four assigned documents. Second, each user query was expanded with the top m extracted terms so that the resulting expanded query had the following syntax:
termU1...termUn (termE1 OR...ORtermEm)
Hence, the original user search terms termUi appeared in a space-separated list whereas the extracted terms termEi were connected with OR operators. Due to technical reasons of the Web search engine Live Search, a query contained at most 19 terms again, that is, n + m ≤ 19. Third, each expanded query was submitted to Live Search. Fourth, the top returned results from the original user query and all expanded query variants were merged together and presented to the participant in randomized order. All presented documents were then judged by the participant so that we could compute and compare the quality of the returned result lists from the original and the expanded queries.
5.1.4. Apparatus. The experiment was performed using the same Tobii 1750 eye tracker that was applied before in the first study (Section 4.1.2). The screen resolution was kept at 1280x1024 pixels. Before starting an experimental run, the device was calibrated using ClearView and a 9-point calibration method. To detect reading, we used the same implemented methods and techniques as before. The documents and the search interface were presented in a Firefox browser.
5.1.5. Participants. The study was performed by 32 participants (6 female). Most of them were undergraduate students, some were graduates taking a variety of different majors. All of them had German as their native language. Their age ranged between 20 and 28 years (mean = 22.7, σ = 1.7).
A first group of 17 participants was assigned to the re-ranking procedure. The re- maining 15 participants were assigned to the query expansion procedure.
5.1.6. Measures. Reading-related measures. As in the first study, we applied our read- ing and skimming detection method to classify gaze traces as either belonging to read- ing, to skimming, or to non-reading-related behavior. Based on the eye movements belonging to reading or skimming, we computed the same gaze-based measures as be- fore on the level of paragraphs in the documents. Hence, the structure of the data used for the analysis was as follows: for every participant and every paragraph in each doc- ument we had a set of gaze traces and corresponding eye movement measures and we knew whether the respective paragraph was relevant to the topic of the task at hand.
Information retrieval measures. To measure the quality of the different search result lists generated based on the different implicit feedback methods, we computed three well accepted information retrieval metrics, each focusing on different aspects:
—Precision at K. P(K) computes the fraction of relevant documents within the top K results. However, the order of the top K documents does not matter. Since this measure needs binary relevance judgments, we split the 6-point relevance scale that was used in the experiment into two groups – positive and negative.
—DCG at K. The Discounted Cumulative Gain [Ja ̈rvelin and Keka ̈la ̈inen 2000] DCG(K) is different from P(K) in that it considers the order of the top K results and in that it is not bound to binary relevance judgments. It is computed as
DCG(K) =  K  2r( j) − 1 /log(1 + j) (2) j=1
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:21
 100% 80% 60% 40% 20% 0%
84%
read text as a frac on of relevant text
3%
read text as a frac on of irrelevant text
  100% 80% 60% 40% 20% 0%
72%
relevant text as a frac on of read text
28%
irrelevant text as a frac on of read text
  Fig. 5. Distribution of reading behavior. Left: read text as a fraction of relevant/irrelevant text. Right: relevant/irrelevant text as a fraction of read text.
Table III. Means and Standard Deviations for All Eye Movement Measures from the Second Study
r( j) corresponds to the relevance score of the jth result entry. In our case, the judg- ments + + +, ++, + led to relevance scores of 3, 2, and 1, respectively, whereas all negative judgments got a relevance score of 0.
—MAP. Mean Average Precision is a single value metric to measure the quality of a set of result lists. It is calculated as the mean of average precision values for all result lists in the set. Concerning one result list, average precision is computed as the mean of the P(K) values after each relevant document was retrieved.
5.2. Results: Reading Behavior
5.2.1. Validation: Distribution of Reading Behavior. Figure 5 shows the distribution of read- ing behavior on the four assigned documents with respect to relevance of the read text parts. In general, 84% of the text that was relevant has been read (Figure 5 left) and 72% of all text that has been read was relevant (Figure 5 right).
Discussion. The results shows that reading behavior is very focused on the relevant parts of the text. Irrelevant parts are largely ignored. Compared to the previous study, the tendency to skip irrelevant parts is much greater (i.e., 34% of irrelevant off-topic documents have been read or skimmed in the previous study, while only 3% or irrelevant parts have been read or skimmed in this study). One of the reasons for this tendency is the different document structure in this study: most of the document parts were irrelevant to the topic of the task. In contrast, most of the documents in the previous study were completely on topic and most of them were judged relevant.
Almost 3/4 of all text that has been read or skimmed was relevant. This shows that reading behavior alone is already a good indicator for relevance.
5.2.2. Gaze-Based Measures. Table III gives an overview of mean values and stan- dard deviations for the measures that were also discussed in the first study. “Regres- sion ratio” and “average fixation duration” are both computed in their individually
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.
 measure
    relevan  mean σ
            rrelevant mean σ
   mean forward saccade length * thorough reading ra o * coherently read text length * regression ra o (personalized) * average fixa on dura on (personalized)
  7.8 le ers 0.53 351 le ers 0.41 0.51
         3.62 0.35 278 0.24 0.22
11.8 le ers 0.40 121 le ers 0.30 0.49
 7.68 0.45 163 0.33 0.25
       % (± SEM)
% (± SEM)
    9:22 G. Buscher et al.
  100% 80% 60% 40% 20% 0%
86% 83%
read text as a frac on of relevant text
first view second view
5.4% 0.9%
read text as a frac on of irrelevant text
   100% 80% 60% 40% 20% 0%
85%
relevant text as a frac on of read text
first view
second view 41%
15%
irrelevant text as a frac on of read text
 59%
 Fig. 6. Distribution of reading behavior split by first and second view of each document. Left: read text as a fraction of relevant/irrelevant text. Right: relevant/irrelevant text as a fraction of read text.
personalized forms, as before. An asterisk signifies that the differences of the means on relevant and on irrelevant text parts are statistically significant (student’s unpaired, two-sided t-test using a significance level of α < 0.01).
Compared to the results from the first study, we see the exact same trends in this study: The length of saccades is typically smaller on relevant text sections. “Thorough reading ratio,” “coherently read text length,” and “regression ratio” are increased on relevant parts. Like in the previous study, we could not determine any statistically significant difference in “average fixation duration” with respect to relevance.
However, when comparing the specific mean values of the measures for relevant and irrelevant parts between the two studies, then some differences are evident. “Mean forward saccade length” has a much wider value range in this study. The values for “thorough reading ratio” are more spread in the previous study. “Coherently read text length” is generally shorter in this study. In contrast, the value distribution of personalized “regression ratio” has very similar characteristics in both studies.
Discussion. In general, the results verify and confirm the findings from the first study. They show that reliable signals can be found in the four measures “mean forward sac- cade length,” “thorough reading ratio,” “coherently read text length,” and “regression ratio” with respect to relevance. Again, “coherently read text length” is the most ex- pressive measure.
However, the mean values for most of the measures seem to depend on further factors related to task and document structure and are different in both studies. Therefore, it might be difficult to find generic classifiers for predicting relevance that work in a variety of different settings since most of the measures are not robust enough and are not independent from task and document structure. In this respect, personalized “regression ratio” seems to be the most robust measure considering both study settings.
5.2.3. Differences Between First-Time and Second-Time Reading. Every document has been presented twice to every participant, once for each of the two topics. Figure 6 shows the distribution of reading behavior again, but now split by first and second view of each document. While a similar amount of the relevant document parts were read during both views, 6 times less irrelevant text was read during the second view (Figure 6, left). This is also reflected in the amount of relevant or irrelevant text as a fraction of all read text (Figure 6, right).
We also analyzed whether there were differences concerning the value distribution of the measures with respect to first and second view of a document. We found a significant difference concerning coherently read text length being longer for first views than for second views, both for relevant and irrelevant text parts (to determine significance we
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.
    % (± SEM)
% (± SEM)
    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:23 Table IV. MAP and DCG at K = 10 for Different Parameter Settings l for the Methods
ReadLength(l) and ReadExtremes(l1, l2)
used Bonferroni correction of α = 0.001). Beyond that, we could not find any further sig- nificant differences between first and second view concerning the remaining measures.
Discussion. The results demonstrate that reading behavior becomes much more fo- cused on relevant document parts if users have already seen a document and are familiar with its structure. This means that reading behavior alone is already very precise in pointing out relevant parts of known documents (with a precision of 85% and a recall of 83%).
Interestingly, most of the eye movement measures are very robust and do not show significant differences with respect to document familiarity. This is important to know when trying to build generic classifiers to determine relevance based on eye movements.
5.3. Results: Implicit Feedback
All 32 participants together issued 192 queries and gave relevance judgments for 3497 result entries. We describe our findings in this section as follows: First, we evalu- ate the best parameter settings for the implicit feedback methods ReadLength(l) and ReadExtremes(l1,l2). Second, we compare the quality of the result lists produced by the different implicit feedback methods, both using the re-ranking as well as the query expansion procedure.
5.3.1. Evaluating Parameter Settings. Based on the data from the re-ranking procedure, we could test several values for the parameter l of the method ReadLength(l) and the parameters l1 and l2 of the method ReadExtremes(l1, l2). Table IV shows MAP and DCG scores at K = 10 after re-ranking the top 20 search results from the Web search engine based on the two implicit feedback method. The best rankings could be achieved with a parameter l = 50 characters, and l1 = 100, l2 = 400 characters.
Discussion. Given the results from the previous analyses, it is not surprising that we can see slight improvements of the ranking quality when filtering read text by coherently read text length. Regarding the method ReadExtremes(l1,l2) we find that the best parameters are close to the mean values for coherently text lengths on relevant and irrelevant texts, and overall this method seems to be superior to ReadLength(l). This shows that read text passages of lengths between about 100 and 400 characters introduce a considerable amount of noise and should not be used as positive or negative relevace feedback.
5.3.2. Comparison of Method Performance. The diagrams in Figure 7 show the per- formances of the different implicit feedback methods with respect to the reranking procedure as well as the quality of the original ranking from the Web search engine. In addition, Table V presents MAP and DCG scores at K = 10 both for the reranking and the query expansion procedure. Asterisks denote statistical significance of the
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.
  Variant
  MAP
DCG
ReadLength(0 chars)
0.747
9.14
ReadLength(50 chars)
0.749
9.18
ReadLength(100 chars)
0.733
9.17
ReadLength(150 chars)
0.736
9.15
ReadLength(200 chars)
0.734
9.06
ReadLength(250 chars)
0.740
9.26
ReadLength(300 chars)
0.743
9.18
ReadLength(350 chars)
0.737
9.14
 Variant
 MAP
 DCG
ReadExtremes(100,300)
0.752
9.23
ReadExtremes(100,350)
0.748
9.26
ReadExtremes(100,400)
0.756
9.28
ReadExtremes(100,450)
0.753
9.31
ReadExtremes(150,400)
0.755
9.27
ReadExtremes(50,400)
0.753
9.27

 9:24
0.80
0.75
0.70
0.65
P(K)
0.60 0.55 0.50 0.45
0.60 0.50 0.40 0.30
10
Reading
ReadLength(50) 9 ReadExtremes(100,400) FullDocument 8 QueryFocus
Original
G. Buscher et al.
Reading ReadLength(50) ReadExtremes(100,400) FullDocument QueryFocus
Original
1 2 3 4 5 6 7 8 9 10
P(K)
0.70 0.60 0.50
1 2 3 4 5 6 7 8 9 10
Reading ReadExtremes(100,400) QueryFocus
ReadLength(50) FullDocument Original
1.00
0.90
0.80 QueryFocus
KK
Fig. 7. Precision and DCG at recall levels K for the re-ranking procedure.
1 2 3 4K5 6 7 8 9 10
Fig. 8. Precision at different recall levels K for poorly performing queries (left) and highly performing
queries (right).
Table V. MAP and DCG at K = 10 for the Reranking and Query Expansion Procedure
Variant
(1) Reading
(2) ReadLength(50)
(3) ReadExtremes(100,400) (4) FullDocument
(5) QueryFocus
(6) Original
MAP
0.748 *4,5 0.749 *4,5 0.756 *4,5 0.697 0.709 0.729
DCG
9.17 *4,5 9.18 *4,5 9.28 *4,5 8.83 9.06 8.66
7
DCG(K)
6 5 4 3
P(K)
Original
Re-Ranking
Query Expansion MAP DCG
0.762 8.58 *6
0.751 8.21 0.751 8.05
differences to the respective numbered methods (determined by a student’s paired, two-sided t-test using a significance level of α < 0.05).
While the gaze-based feedback methods produced significantly better rankings than the non-gaze-based feedback methods, none of the four implicit feedback methods led to significantly better or worse rankings compared to the original ranking from the Web search engine. Hence, inspired by Agichtein et al. [2006] we further explored the effect of implicit feedback by dividing all user queries into two groups: poorly performing queries where the Web search engine produced rankings with MAP scores ≤ 0.7, and highly performing queries (MAP > 0.7). The former group contained 38, the latter group 64 queries.
Figure 8 shows precision-recall diagrams for poorly and highly performing queries with respect to the reranking procedure. Table VI provides respective MAP and DCG values at K = 10. Asterisks denote statistical significance of the differences as before.
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.
1 2 3 4K5 6 7 8 9 10
Reading ReadLength(50) ReadExtremes(100,400) FullDocument
    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:25
Table VI. MAP and DCG at K = 10 for Queries with Poor and High Original Performance
   Variant
    MAP
MAP of original ranking
<= 0.7 > 0.7
DCG MAP DCG
      (1) Reading
      0.63 *6
6.64 *4,6
      0.82 *4,5
10.67
 (2) ReadLength(50)
0.63 *4,6
6.63 *4,6
0.82 *4,5
10.69
 (3) ReadExtremes(100,400)
0.63 *4,6
6.49 *6
0.83 *4,5
10.95 *5
 (4) FullDocument
0.58 *6
6.00
0.76
10.52
 (5) QueryFocus
0.62 *6
6.80 *1,2,4,6
0.76
10.40
 (6) Original
0.50
5.18
0.86 *4,5
10.73
                   Discussion. Largest gains were achieved by method ReadExtremes(100,400), that is, a 8.5% gain in MAP compared to the simple non-gaze-based feedback method FullDocument, and a 7.2% gain in DCG compared to the original ranking from Live Search. All three gaze-based feedback methods lead to overall improvements compared to the original ranking from the Web search engine. In contrast, the two non-gaze-based methods could also lead to impairments, especially in the re-ranking scenario.
This difference in quality between the three gaze-based and the two non-gaze-based methods becomes particularly evident with regard to their performance on highly per- forming queries. The methods FullDocument and QueryFocus significantly worsened retrieval performance. In contrast, we could not find significant impairments caused by the gaze-based methods. However, with respect to poorly performing queries, all implicit feedback methods greatly improved the quality of the ranking, that is, up to 27% in MAP.
5.4. Conclusion
Overall, the results of this study verify and confirm the findings from the first study and demonstrate the value and usefulness of gaze-based feedback as implicit relevance feedback in information retrieval.
Concerning reading-related measures we found the same relationships as in the previous study, even in a different setting with long documents containing relevant and irrelevant parts, and for a more goal-directed task: First, the amount of reading behavior is strongly influenced by and very focused on relevant text. Second, the ana- lyzed gaze-based measures show the same trends concerning increases or decreases on relevant or irrelevant text. However, the distributions of their absolute values are dif- ferent from the previous study (i.e., their means). They seem to depend on the general experimental setting including task and document structure. Third, “fixation duration” seems to be indifferent to relevance again; there were again no significant differences to detect in this respect.
The factor of document familiarity has considerable effects on how users view documents. If users have previous knowledge about the document structure, then they are much more focused while reading. But interestingly, document familiarity had no noticeable effects on eye movement measures with respect to relevance. It did not influence the general trends concerning increases or decreases of the measures on relevant text.
An important insight that can be drawn from both studies is that classifiers to detect relevance based on eye movement measures will not be trivial to build and need to consider relative differences within their value distributions. Classifiers that are just based on absolute values of the measures are unlikely to work for two reasons. First, there are great individual differences. They can be accounted for when personalizing
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    9:26 G. Buscher et al.
the measures. Second, there are differences with respect to the general setting, that is, task and document structures. How these setting-induced differences can be accounted for is not yet clear; further research is needed here.
Applying gaze-based feedback as relevance feedback in information retrieval turns out to be very useful. Compared to the non-gaze-based feedback methods, it leads to considerable improvements of the search result list quality both using a reranking and a query expansion procedure. Interestingly, the gaze-based methods are even signif- icantly better than QueryFocus, which is the only method incorporating information about the current user query and therefore may use document parts directly related to the query as feedback. However, document parts that have been read or skimmed before are evidently more useful as implicit feedback.
Compared to the original result list quality from the Web search engine, the effects of the feedback methods are less distinct. With respect to queries that lead to poor result list qualities from the Web search engine, all implicit feedback methods lead to great improvements. Particularly QueryFocus performs at a comparable level as the gaze- based methods. However, when focusing on queries already yielding good quality result lists then both non-gaze-based methods entail considerable impairments. In contrast, the gaze-based methods do not worsen the result list quality much.
Surprisingly, we could not determine great differences between the three gaze-based methods. We expected to see much larger improvements from the method ReadLength(l) and ReadExtremes(l1,l2) since they additionally apply the advanced measure “coher- ently read text length” which has been proven to be most expressive with respect to relevance. This leads to the conclusion that a bit more irrelevant text as context (used by the method Reading) does not hurt retrieval performance much. Hence, reading and skimming behavior alone without considering any more advanced gaze-based mea- sures is already a very effective source for implicit relevance feedback for information retrieval.
However, it has to be kept in mind that this is an exploratory study with specific assumptions on task and document characteristics. First, the documents the partic- ipants could read had a very similar structure in that their subsections dealt with largely distinct subtopics (as it is typically the case with Wikipedia articles). Relevant and irrelevant text sections may be less differentiable in other types of documents so that the identification of relevant read text sections may be more difficult, and thus, the relevance measures would be more noisy. Second, whereas the goal-directed task of reading up on a topic for information gathering and then searching for more in- formation on the Web is fairly frequent and generalizeable, time pressure can differ vastly. This may have a significant effect on reading behavior that remains to be es- timated in future work. Third, as we demonstrated in this study, reading behavior changes considerably with document familiarity. While our study showed that expres- sive relevance-related eye movement measures are not much affected by changes in short-term document familiarity, there may be considerable effects coming from long- term document familiarity, that is, if the user is familiar with the document for a long time through multiple re-readings.
Yet overall, this study demonstrates the usefulness of gaze-based feedback for infor- mation retrieval personalization. There are many areas of application for such rather simple implicit feedback, for instance, as a search filter in order to refind previously read document parts, as a document interaction measure that can be used to highlight document parts the user has paid most attention to, as a source for creating topical con- texts for a user that can be applied as implicit feedback for information retrieval, etc. In the following, we give an outlook of a potential future set of applications: attentive documents.
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:27
[Rayner 1998], the eye shows a very characteristic behavior composed of fixations and saccades. A fixation is a time of about 250ms on average when the eye is steadily gazing at one point. A saccade is a rapid, ballistic eye movement from one fixation to the next. The mean left-to-right saccade size is 7-9 letter spaces. It depends on the font size and is relatively invariant concerning the distance between the eyes and the text.
An enormous amount of research has been done during last one hundred years concerning eye movements while reading. When reading silently, as summed up in [Rayner 1998], the eye shows a very characteristic behavior composed of fixations and saccades. A fixation is a time of about 250ms on average when the eye is steadily gazing at one point. A saccade is a rapid, ballistic eye movement from one fixation to the next. The mean left-to-right saccade size is 7-9 letter spaces. It depends on the font size and is relatively invariant concerning the distance between the eyes and the text.
                                                                       Annotation (Read) Delete author: Georg
start date: 07.12.2009 10:46:08 End date: 07.12.2009 10:46:12 length: 226 chars
mean fixation duration: 217ms mean saccade length: 9.4 chars regression ratio: 13.9%
 Fig. 9.
tations containing several eye movement measures (bottom).
An attentive Wiki document records what the user is reading (top) and automatically creates anno-
6. OUTLOOK: ATTENTIVE DOCUMENTS
Attentive documents are documents that keep track of how they are used. This is not only restricted to editing processes which can already be tracked by modern commercial word processing systems. Since users generally read more than they write, information about how documents and their parts are consumed (and read in particular) can be even more important. Therefore, information about what document parts have been read and how they have been read should be stored in association with the documents. As we demonstrated, eye tracking is overall very well suited to detect reading behavior and to determine differences in reading behavior, e.g., with respect to user-perceived relevance.
We built a prototype system described in detail in van Elst et al. [2008] and Kiesel et al. [2008] illustrating how attentive documents could work. The prototype system itself is a Wiki that allows for semantic annotations of text parts in documents. It can be used to manage the personal information space on the user’s desktop. When a user is reading some part of a Wiki document, our reading detection method detects reading or skimming behavior on the fly, computes appropriate gaze-based measures, and then sends this information to the Wiki. Subsequently, the Wiki creates an annotation for the read text passage containing the values from the gaze-based measures. Figure 9 shows an example of such an annotation. Of course, these annotations can be displayed to the user, but they are not visible per se.
Such attentive documents make it possible to design new and innovative applications. For example in the search domain, it is conceivable to use feedback from reading directly as implicit relevance feedback. From this, Web search can effectively be personalized (as demonstrated in study 2). Furthermore, when trying to refind information on the desktop, search filters could be employed that focus and search only in these document parts that have been viewed before by the user. Also, since a system managing attentive documents (such as the local Wiki) knows what the user has read or skipped, it could point out information that is relevant and new to the user, that is, information that has not been read before.
Apart from search, new kinds of applications can be imagined when focusing on the reading process of one particular document. For instance, when a user opens a document that he or she has read a while ago, the system could highlight the parts of
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    9:28 G. Buscher et al.
the document that have been most relevant before. This could help the user to refresh his or her memory. Because contextual clues help to find and reconstruct the thoughts one had before, such highlighting of formerly read text parts can speed up the process of recontextualizing. Furthermore, to make reading itself more efficient, the system could notice when the user is just scanning a document very quickly in order to get a rough idea of its contents. In this case, words in the text that do not convey much content (e.g., stop words) can be grayed out so that the scanning process gets more focused and effective [Biedert et al. 2010b]. Additionally, it is also conceivable that the system can detect difficulties of understanding during reading, e.g., on words in a foreign language. Then, it could automatically provide translations or further explanations [Hyrskykari et al. 2003]. Applications aiming at reading entertainment are also imaginable [Biedert et al. 2010a].
The sketched use-cases are just some compelling applications for gaze-based feedback from reading documents. An almost arbitrary number of further applications can be imagined.
7. SUMMARY AND CONCLUSION
In conclusion, it can be stated that gaze-based feedback about what has been read and how it has been read is very valuable to determine whether viewed document parts have been relevant to an individual user. Furthermore, when using this information as implicit relevance feedback it can greatly improve and personalize information retrieval methods.
Reading behavior is very focused on relevant parts of documents, especially when users are working with long documents, and there is strong evidence that individually perceived relevance of read text influences eye movement measures like the number of regressions during reading, the typical length of saccades, etc. We determined the re- lations between relevance and gaze-based measures and validated them in two studies using different task types and document structures. Interestingly, the popular measure “fixation duration” does not seem to be related to perceived relevance.
We found good relations between gaze-based measures and user-perceived relevance, as well as a great variation in the measures caused by individual differences and dif- ferences in task and document structure. Individually personalizing the measures can greatly improve their expressivity and can account for individual differences. However, how variation caused by task and document structure can be accounted for is not clear yet and stays for further research.
Since users typically read in a very focused way, information about what document parts have been read can be used as an indicator of relevance. We demonstrated its usefulness as implicit relevance feedback for personalizing Web search in a further study. The results of the study show that gaze-based feedback is much more effective than non-gaze-based relevance feedback baselines.
Finally, in an outlook, we sketched potential innovative applications based on at- tentive documents, i.e., documents that keep track of how they have been read by the user.
ACKNOWLEDGMENTS
We thank Tristan King for his help improving the style of the paper.
REFERENCES
AGICHTEIN, E., BRILL, E., AND DUMAIS, S. 2006. Improving web search ranking by incorporating user behavior information. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’06). ACM, New York, NY, 19–26.
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    Attentive Documents: Eye Tracking as Implicit Feedback for Information Retrieval and Beyond 9:29
AHN, J. W., BRUSILOVSKY, P., HE, D., GRADY, J., AND LI, Q. 2008. Personalized web exploration with task models. In Proceedings of the 17th International Conference on World Wide Web (WWW’08). ACM, New York, NY, 1–10.
AJANKI, A., HARDOON, D., KASKI, S., PUOLAMA ̈ KI, K., AND SHAWE-TAYLOR, J. 2009. Can eyes reveal interest? Implicit queries from gaze patterns. User Model. User-Adapt. Interact. 19, 307–339.
BALATSOUKAS, P. AND RUTHVEN, I. 2010. The use of relevance criteria during predictive judgment: An eye tracking approach. Proc. Amer. Soc. Info. Sci. Techn. 47, 1, 1–10.
BIEDERT, R., BUSCHER, G., AND DENGEL, A. 2010a. The eyebook – using eye tracking to enhance the reading experience. Informatik-Spektrum 33, 3, 272–281.
BIEDERT, R., BUSCHER, G., SCHWARZ, S., HEES, J., AND DENGEL, A. 2010b. Text 2.0. In CHI’10: Extended Abstracts on Human Factors in Computing Systems. ACM Press, New York, NY, 4003–4008.
BROOKS, P., PHANG, K. Y., BRADLEY, R., OARD, D., WHITE, R., AND GUIMBRETIERE, F. 2006. Measuring the utility of gaze detection for task modeling: A preliminary study. In Proceedings of the International Conference on Intelligent User Interfaces (IUI’06). (Workshop on Intelligent User Interfaces for Intelligence Analysis).
BUSCHER, G., CUTRELL, E., AND MORRIS, M. R. 2009a. What do you see when you’re surfing?: using eye tracking to predict salient regions of web pages. In Proceedings of the 27th International Conference on Human Factors in Computing Systems (CHI’09). ACM, New York, NY, 21–30.
BUSCHER, G., DENGEL, A., AND VAN ELST, L. 2008a. Eye movements as implicit relevance feedback. In CHI’08: Extended Abstracts on Human Factors in Computing Systems. ACM, New York, NY, 2991–2996.
BUSCHER, G., DENGEL, A., AND VAN ELST, L. 2008b. Query expansion using gaze-based feedback on the subdoc- ument level. In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’08). ACM, New York, NY, 387–394.
BUSCHER, G., VAN ELST, L., AND DENGEL, A. 2009b. Segment-level display time as implicit feedback: a comparison to eye tracking. In Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’09). ACM, New York, NY, 67–74.
CHEN, Z. AND XU, Y. 2005. User-oriented relevance judgment: A conceptual model. In Proceedings of the 38th Annual Hawaii International Conference on System Sciences (HICSS’05). IEEE Computer Society, Los Alamitos, CA, 101.2.
CLAYPOOL, M., LE, P., WASED, M., AND BROWN, D. 2001. Implicit interest indicators. In Proceedings of the 6th International Conference on Intelligent User Interfaces (IUI’01). ACM Press, New York, NY, 33–40. COLE, M. J., GWIZDKA, J., BIERIG, R., BELKIN, N. J., LIU, J., LIU, C., AND ZHANG, X. 2010. Linking search tasks with
low-level eye movement patterns. In Proceedings of the 28th Annual European Conference on Cognitive
Ergonomics (ECCE’10). ACM, New York, NY, 109–116.
CUTRELL, E. AND GUAN, Z. 2007. What are you looking for?: an eye-tracking study of information usage in web
search. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI’07).
ACM Press, New York, NY, 407–416.
DAVENPORT, T. H. AND BECK, J. C. 2001. The Attention Economy: Understanding the New Currency of Business.
Harvard Business School Press.
VAN ELST, L., KIESEL, M., SCHWARZ, S., BUSCHER, G., AND LAUER, A. 2008. Contextualized Knowledge Acquisition
in a Personal Semantic Wiki. In Proceedings of the 16th International Conference on Knowledge Engi- neering and Knowledge Management (EKAW’08). Springer, Lecture Notes in Computer Science vol. 5268, 172–187.
FOX, S., KARNAWAT, K., MYDLAND, M., DUMAIS, S., AND WHITE, T. 2005. Evaluating implicit measures to improve web search. ACM Trans. Inform. Syst. 23, 2, 147–168.
GOLOVCHINSKY, G., PRICE, M. N., AND SCHILIT, B. N. 1999. From reading to retrieval: freeform ink annotations as queries. In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’99). ACM Press, New York, NY, 19–25.
GYLLSTROM, K. 2009. Passages through time: chronicling users’ information interaction history by recording when and what they read. In Proceedings of the 13th International Conference on Intelligent User Interfaces (IUI’09). ACM, New York, NY, 147–156.
HILL, W. C., HOLLAN, J. D., WROBLEWSKI, D., AND MCCANDLESS, T. 1992. Edit wear and read wear. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI’92). ACM Press, New York, NY, 3–9.
HYRSKYKARI, A., MAJARANTA, P., AND RA ̈IHA ̈, K.-J. 2003. Proactive response to eye movements. In Proceedings of the International Conferece on Human-Computer Interaction (INTERACT’03). 129–136.
JA ̈RVELIN, K. AND KEKA ̈LA ̈INEN, J. 2000. Ir evaluation methods for retrieving highly relevant documents. In
Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’00). ACM, New York, NY, 41–48.
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.

    9:30 G. Buscher et al.
JOACHIMS, T., GRANKA, L., PAN, B., HEMBROOKE, H., RADLINSKI, F., AND GAY, G. 2007. Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search. ACM Trans. Inform. Syst. 25, 2.
KELLY, D. AND BELKIN, N. J. 2001. Reading time, scrolling and interaction: exploring implicit sources of user preferences for relevance feedback. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’01). ACM, New York, NY, 408–409.
KELLY, D. AND BELKIN, N. J. 2004. Display time as implicit feedback: understanding task effects. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’04). ACM Press, New York, NY, 377–384.
KELLY, D. AND TEEVAN, J. 2003. Implicit feedback for inferring user preference: a bibliography. SIGIR Fo- rum 37, 2, 18–28.
KIESEL, M., SCHWARZ, S., VAN ELST, L., AND BUSCHER, G. 2008. Using attention and context information for annotations in a semantic wiki. In Proceedings of the 3rd Semantic Wiki Workshop (SemWiki’08).
LIVERSEDGE, S. P. AND FINDLAY, J. M. 2000. Saccadic eye movements and cognition. Trends Cogn. Sci. 4, 1, 6–14.
LOBODA, T. D., BRUSILOVSKY, P., AND BRUNSTEIN, J. 2011. Inferring word relevance from eye-movements of readers. In Proceedings of the 16th International Conference on Intelligent User Interfaces (IUI’11). ACM, New York, NY, 175–184.
MAJARANTA, P. AND RA ̈IHA ̈, K.-J. 2002. Twenty years of eye typing: systems and design issues. In Proceedings of the Symposium on Eye Tracking Research & Applications (ETRA’02). ACM, New York, NY, 15–22.
MELUCCI, M. AND WHITE, R. W. 2007. Discovering hidden contextual factors for implicit feedback. In Proceed- ings of the CIR’07 Workshop on Context-Based Information Retrieval (in conjunction with CONTEXT’07). MOE, K. K., JENSEN, J. M., AND LARSEN, B. 2007. A qualitative look at eye-tracking for implicit relevance feedback. In Proceedings of the 2nd International Workshop on Context-Based Information Retrieval.
B.-L. Doan, J. Jose, and M. Melucci, Eds., 36–47.
MORITA, M. AND SHINODA, Y. 1994. Information filtering based on user behavior analysis and best match text
retrieval. In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR’94). Springer, 272–281.
OHNO, T. 2004. Eyeprint: support of document browsing with eye gaze trace. In Proceedings of the 6th
International Conference on Multimodal Interfaces (ICMI’04). ACM, New York, NY, 16–23.
PUOLAMA ̈KI, K., SALOJA ̈RVI, J., SAVIA, E., SIMOLA, J., AND KASKI, S. 2005. Combining eye movements and collabo- rative filtering for proactive information retrieval. In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’05). ACM Press, New
York, NY, 146–153.
RAYNER, K. 1998. Eye movements in reading and information processing: 20 years of research. Psych.
Bull. 124, 3, 372–422.
ROCCHIO, J. 1971. The SMART Retrieval System: Experiments in Automatic Document Processing. Prentice
Hall, 313–323.
SALTON, G. AND BUCKLEY, C. 1990. Improving retrieval performance by relevance feedback. J. Amer. Soc. Inf.
Sci. 41, 4, 288–297.
SIMON, H. A. 1969. The Sciences of the Artificial. MIT Press.
SIMON, H. A. 1971. Designing organizations for an information rich world. In Computers, Communications
and the Public Interest. Johns Hopkins Press, 38–51.
WHITE, R. W. AND KELLY, D. 2006. A study on the effects of personalization and task information on implicit
feedback performance. In Proceedings of the 15th ACM International Conference on Information and
Knowledge Management (CIKM’06). ACM, New York, NY, 297–306.
WILCOX, R. 2005. Introduction to Robust Estimation and Hypothesis Testing, 2nd Ed. Elsevier Academic
Press.
XU, S., JIANG, H., AND LAU, F. C. 2009. User-oriented document summarization through vision-based eye-
tracking. In Proceedings of the 13th International Conference on Intelligent User Interfaces (IUI’09). ACM, New York, NY, 7–16.
Received December 2010; revised June 2011; accepted August 2011
ACM Transactions on Interactive Intelligent Systems, Vol. 1, No. 2, Article 9, Pub. date: January 2012.
Effect of Presentation on Reading Behaviour
Leana Copeland
Research School of Computer Science Australian National University Canberra, Australia leana.copeland@anu.edu.au
ABSTRACT
Eye tracking is a useful tool for investigating how people read and the attention that they give to certain words and phrases. Eye tracking is used to investigate how different presentation formats of the same learning material affect learning performance, eye movements, and reading behaviour. We show that different presentation formats induce different eye movements and that reading behaviour is subject to the goals placed on the reader. We also observe that the presentation format affects not only their learning performance but also how they perceive their performance. Finally, we show that different formats and question types can induce specific reading behaviour such as thorough reading. This can be used to influence how students interact with the learning environment as well as how they learn the material. The purpose of this investigation is to be able to make informative decisions about designing adaptive eLearning environments.
Author Keywords
Eye gaze; eye tracking; adaptive eLearning; presentation format
ACM Classification Keywords
H5.m. Information interfaces and presentation (e.g., HCI): Miscellaneous.
INTRODUCTION
Educational material is being offered through online mediums more frequently in part due to increased accessibility and availability of computer technologies. This is especially true for tertiary education, where face- to-face education is now heavily supplemented with material that is available through online learning environments, such as Moodle and Blackboard. It has become common for universities to offer online/off- campus degrees where students may have little or no face-to-face interaction with their instructors or other students. Eye tracking has been shown to be an effective way of analysing human behaviour, particularly reading (Rayner, 1998). Given that a primary form of educational material is text, this raises the question of how eye
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org.
OzCHI '14 , December 02 - 05 2014, Sydney, NSW, Australia Copyright 2014 ACM 978-1-4503-0653-9/14/12...$15.00 http://dx.doi.org/10.1145/2686612.2686648
Tom Gedeon
Research School of Computer Science Australian National University Canberra, Australia tom.gedeon@anu.edu.au
tracking can be used to make the learning process more effective when an instructor has little or no direct interaction with students.
The question of how electronic text (eText) can be used to support learning and comprehension is long standing (Anderson Inman and Horney, 2007). There is much potential in providing support to readers and much research on how to do so in particular situations, but no consensus on how to do so in a generalised way and for large variety of readers and situations. In particular, research has often tended to focus on readers with difficulties or disabilities (Anderson Inman and Horney, 2007). Our investigation contributes to the wider research on how to produce specific eText supports in an eLearning environment context, with the use of eye tracking to predict when support is required. The support investigated is the use of quiz questions and their relation to the presented text. This is an evaluation support, whereby materials and prompts are designed to assess student learning from the text.
To do so, the presentation of educational reading material is investigated to assess how eye movements, answer- seeking behaviour, and learning performance are affected. A user study was conducted to compare four presentation formats. These presentation formats are manipulations of the order of which text and quiz questions are shown to a student. The hypotheses for this investigation are that the presentation format of the text and comprehension questions will: 1) have an effect on students' performance, in terms of time and quiz score, and perceived understanding of the text; 2) cause differences in eye movements; and 3) induce different reading behaviour.
This paper is organized into the following sections: background information; user study methodology; results and discussion; recommendations based on the results; finishing with conclusions and further work.
BACKGROUND
Electronic Text (eText)
Electronic text, eText, is the general term for digital presentation and storage of text. eText is read through some kind of computer device, such as a computer screen, tablet, phone, eReader, etc. The advent of smartphones, tablet devices, and eReaders has meant that eText is becoming more prevalent.
Initially a large amount of research went into comparing reading digital to paper based texts (see review by Dillon (1992)). Many studies are centred on the outcomes of reading, such as speed and comprehension. In general, the
 230
literature has shown that there is little evidence to support claims that one method of display is better than another in terms of improving comprehension (Dillon and Gabbard, 1998).
Paper offers advantages over digital presentation, which has been studied to provide design suggestions for better reading technologies (O'Hara and Sellen, 1997). These include supporting annotation, quick and easy navigation as well as control of spatial layout. Meanwhile, eText does itself have advantages over paper that include increased accessibility, easy storage and retrieval, ubiquity, and flexibility. It is the flexibility that eText provides that gives rise to the potential to support learning. Flexibility refers to the ability to dynamically change the way text is read. Changes can be simple, such as font size, colour, or typeface or they can be more complex, such as verbalisations of the text, embedded definitions and links to background information (Anderson Inman and Horney, 2007). The reader controls such simple changes; conversely, the eText can be transformed to promote learning and comprehension. Horney and Anderson-Inman (1999) produced a typology of resources for supported eText. These include presentational, navigational, translational, explanatory, illustrative, summarising, enrichment, instructional, notational, collaborative, and evaluation resources. The typology is a list of ways in which eText can be supported; they vary vastly in method and purpose. Perhaps for this reason there is no consensus which supports should be provided (Anderson Inman and Horney, 2007).
Many studies have considered navigation through eTexts as it is considered a non-trivial text to accomplish in electronic form (Dillon, 2004). Studies have investigated navigation in ebooks (McKay, 2011) and periodicals (Marshall and Bly, 2005) as well as the impact of screen size on document triage (Loizides and Buchanan, 2010).
Dillon (1996) proposed a framework for building electronic texts so that the issues relevant to their design are considered. The framework, known as TIMS, was designed to be a representation of the cognitive behaviours humans exhibit whilst reading documents.
We focus on small pieces of text material that are presented to the reader in a structured fashion. Within the limits of the eLearning environment, the reader is free to choose where in the quiz they would like to read but the structure of the tutorial is such that there is a linear progression through the content. In this way the eText can be considered supported eText whereby the addition of an evaluation resource and the format in which it is presented are intended to influence learning.
Eye Movements and Reading Comprehension
Tracking a reader’s eye has looked has long been used to measure the reading process (see review by Rayner (1998)). During the process of reading the eye moves in well-studied ways that can be broadly characterised as fixations and saccades. A fixation is where the eye remains relatively still to take in visual information. A saccade is a rapid movement that transports the eye to
another fixation. This behaviour is due to the anatomy of the eye. At the centre of the retina is a special part of the eye called the fovea that sees in fine detail. The foveal region of the eye is very small, being only about 0.2mm in diameter. Around the point of fixation, visual acuity extends about 2° (Rayner, 1998). Humans see very little in detail at any fixation so the eye must move around rapidly so that it can compose a more detailed view of the environment.
When reading English fixation duration ranges anywhere between 60-500 milliseconds and is generally about 250 milliseconds (Liversedge and Findlay, 2000). Saccadic movement is between 1 and 15 characters with an average of 7-9 characters. The majority of saccades are to transport the eye forward in the text when reading English; however, a proficient reader exhibits backward saccades to previously read words or lines about 10-15% of the time. Backward saccades are termed regressions. Short regressions can occur within words or a few words back and may be due to problems in processing the currently fixated word, overshoots in saccades, or oculomotor errors. However, longer regressions occur due to comprehension difficulties, as the reader tends to send their eyes back to the part of the text that caused the difficulty (Frazier and Rayner, 1982).
Eye movements can be used to understand the ongoing cognitive processes that occur during reading (Liversedge and Findlay, 2000). Comprehension of text can have significant effects on eye movements (Rayner et al., 2006). A number of studies have shown there are many variables based upon comprehension functions that can influence eye movements during reading. These variables include: semantic relationships between words, anaphora and co-reference, lexical ambiguity, phonological ambiguity, discourse factors, stylistic conventions, and syntactic disambiguation (see review by Rayner (1998)). These variables have different effects on eye movement, causing them to deviate from the default reading process. For example, syntactically ambiguous sentences induce regressions to resolve the comprehension problems (Frazier and Rayner, 1982). Eye movements have been shown to reflect global text difficulty as well as inconsistencies within text (Rayner et al., 2006). More difficult text causes more fixations, more regressions, and longer fixation duration time. Eye movement has also been shown to indicate reading comprehension and reading skill (Underwood et al., 1990).
Whilst eye movements are a good way of measuring the observable part of the reading process it is important to note that the limitation in the context of HCI research is that the researcher cannot tell what the reader is thinking or doing at the time of reading (Dillon, 2004).
Eye Tracking in Adaptive eLearning
As mentioned previously, eText can be modified to support learning. One way of supporting learning is the use of eye tracking. Eye gaze patterns can be used to detect what kind of task the participant is performing (Iqbal and Bailey, 2004) or whether a person is reading or not (Campbell and Maglio, 2001) as well as if they are reading or skimming (Buscher et al., 2008).
231
The use of eye tracking in adaptive learning systems (ALS) is not novel and has been approached in a number of ways. An example is iDict, a reading aid designed to help readers of a foreign language (Hyrskykari et al., 2000). iDict uses eye gaze to predict when a reader is having comprehension difficulties. If the user hesitates whilst reading a word then a translation of the word is provided with a dictionary meaning. Another example is The Reading Assistant (Sibert et al., 2000) that uses eye gaze to predict failure to recognise a word. The Reading Assistant then provides auditory pronunciation of the word to aid reading. These applications work on the assumption that the user pauses on a problematic word, and then the system provides feedback about that word. They do not look at overall text comprehension or provide feedback about the overall comprehension of that text.
Separate from direct applications of eye tracking in ALSs are the investigations of eye movements within learning systems such as analysing how multiple choice questions are answered (Nugrahaningsih et al., 2013; Tsai et al., 2012), or using eye movements to predict student comprehension of physics concepts when presented as text or images (Chen et al., 2014).
Prior Work
This study presents further analysis and a follow-up to a previous user study (Copeland and Gedeon, 2013). In the initial user study participants’ eye gaze was recorded as they completed a tutorial and quiz. The tutorial was composed of 9 screens of textual content each covering a specific area about the main topic of the tutorial (Web Search). After each text screen, participants were required to answer two questions to measure their comprehension. Whilst answering the questions, participants were given the opportunity for a second read-through of the content to assist answering the questions.
Importantly, the study showed that there were distinctively different reading behaviours observed when the text was shown alone compared to when it was shown with the questions. When the text was shown alone, ‘normal’ reading behaviour is observed. When text was shown with the questions then a behaviour we term answer-seeking is observed; this is a targeted search, read, and confirm behaviour. Given the discrepancies in observed behaviours we postulate that by manipulating the way in which the text and the questions are presented will alter not only the reading behaviour observed but also learning outcomes. The sequences the text and questions are presented in are therefore strategically manipulated in this our study to assess this hypothesis.
METHOD
Design
A user study was conducted to collect participants’ eye gaze as they read a tutorial and completed a quiz based on the tutorial’s content. The tutorial and quiz are coursework from a first year Computer Science course taken at the Australian National University. The tutorial and quiz is composed of 9 screens of textual content, each covering a specific area about the main topic of the
tutorial (Web Search). Each screen is 400 words long and has an average Flesch Kincaid Grade readability level of 11.5. This indicates that participants need around a 12th grade education level. As the slides are targeted at a first year university students this is a suitable readability level. For each screen there were two comprehension questions; one of the questions was multiple-choice and the other was cloze (fill-in-the-blanks). These two types of questions were used because they can be used to assess different forms of comprehension (Fletcher, 2006).
The tutorial and quiz was presented to participants in four formats to measure the effect of presentation on participants’ eye gaze and answering behaviour. The presentation formats are based on when the quiz questions are presented to the participants in relation to the text. These presentation formats are described below:
Format A: The tutorial text slide (T) Figure 1 is first shown to participants followed by a slide with both questions and the tutorial text (Q&T) Figure 2. Since there are 9 topics there are 18 slides in total displayed in this part of the study.
In this format participants are required to read the text before being able to read the questions relating to it. Whether the participants understand it or not they have knowledge about the concepts in each of the paragraphs. When they reach the questions, participants can either answer them straight away or search the text to look for the answers, i.e. answer-seeking behaviour.
Figure 1: Example of tutorial text only slide
Format B: The questions and tutorial text slide (Q&T) is shown to participants. An example of this is seen in Figure 2. Since there are 9 topics there are 9 slides in total displayed in this part of the study.
In this format participants are no longer required to read the text before they see the questions. Our question is, will participants read the text completely or will answer seeking behaviour be observed? Additionally, is there a difference in quiz performance when participants can immediately answer the questions without reading the text?
Format C: The tutorial slide (T), shown in Figure 1, is first shown to participants followed by the questions slide (Q) but no access to the content again, see Figure 3. Since
 232
there are 9 topics there are 18 slides in total displayed in this part of the study. This format can be considered to be a control presentation method. In this format the reference text is removed from the questions slide so the participants are forced to answer the questions from understanding and memory. We expect that the worst comprehension scores will be observed for this format.
Figure 2: Example of comprehension questions and text
Figure 3: Example of the questions only slide
Format D: The last presentation consisted of displaying a slide with only the questions (Q) on it, as seen in Figure 3, followed by the tutorial text slide (T) Figure 1, and then again presenting them with the questions slide (Q) as in Figure 3. Since there are 9 topics there are 27 slides in total displayed in this part of the study. The reasoning for this format is to mimic a situation where the participants knew what the comprehension questions are but no access to them as they read. The hypothesis is that participants will read the text differently than for formats A and C.
Experiment Setup
The tutorial text was accessible via the online learning environment used at ANU, called Wattle (a Moodle variant). The study was displayed on a 1280x1024 pixel Dell monitor. Eye gaze data was recorded at 60Hz using Seeing Machines FaceLAB 5 infrared cameras mounted at the base of the monitor. The study involved a 9-point calibration prior to data collection for each participant. As the data recorded is a series of gaze points, EyeWorks Analyze was used to pre-process the data to give fixation
points. The parameters used for this were a minimum duration of 60milliseconds and a threshold of 5 pixels.
Demographic Information
The study used a total of 33 participants that were divided into the four presentation categories. The experiment used a between-subjects design so that each participant was presented with only one presentation type.
The initial study used format A and can be split into two demographics, COMP1710 students and others. In this analysis only the COMP1710 student subgroup is consider so that a comparison to the follow-up studies can be made, as the remaining presentation formats only used COMP1710 students. The choice of participants is based on the target user group of the eventual online learning environment, which are university students. Further, the COMP1710 students had been exposed to these tutorials throughout the duration of the course and therefore have some experience and familiarity not only using the online learning environment but also with this type of tutorial.
Format A: There were 9 participants (2 female, 7 male) in this group with an average age of 21.3 years (standard deviation 4.7 years, range 17-31 years). English was not their first language for 4 of the participants.
Format B: There were 8 participants (1 female, 7 male) in this group with an average age of 21.8 years (standard deviation 7.9 years), age range 18-41 years. English was not the first language for 3 of the participants.
Format C: There were 9 participants (2 female, 7 male) in this group with an average age of 22.8 years (standard deviation 6.4 years, range 18-37 years). English was not the first language for 5 of the participants.
Format D: There were 7 participants (1 female, 6 male) in this with an average age of 20.1 years (standard deviation 2.8 years, range 17-24 years). English was not the first language for 3 of the participants.
Data Pre-processing
The raw eye gaze data consists of x,y-coordinates recorded at equal time samples (60Hz). Fixation and saccade identification was performed on the eye gaze data. From this point many other eye movement measures are derived. The measures used in this analysis are:
Number of fixations: The sum of fixations recorded for each tutorial page. The number of fixations can be affected by the reading behaviour, text difficulty, and reading skill (Rayner, 1998).
Maximum fixation duration (seconds): The maximum duration of the longest fixation recorded for a tutorial page. Longer fixations can be an indicator of difficulties in processing particular words or due to linguistic and/or comprehension difficulties (Rayner, 1998).
Average fixation duration (seconds): The sum of the duration of all fixations on a paragraph divided by the number of fixations on that paragraph. This measure has been used to predict reading comprehension (Underwood et al., 1990).
  233
Total fixation duration (seconds): The sum of all fixations on complete text. This measure is useful in global text processing analysis (Hyona et al., 2003) because this measures immediate as well as delayed effects of comprehension.
Number of regressions and regression ratio: The number of regressions divided by the total number of saccades on a paragraph. There is evidence that when reading more difficult text more regressions are observed (Rayner et al., 2006).
Reading analysis: Using our combination of two reading detection algorithms (Buscher et al., 2008, Campbell and Maglio, 2001), this is the percentage of saccades classified as being part of reading (read ratio), skimming (skim ratio), and scanning/searching (scan ratio).
RESULTS AND DISCUSSION
This section outlines the analysis performed on the collected eye gaze data. After the gaze points have been converted to fixations and saccades, a number of eye movement measures were calculated and used for this analysis.
First, a comparison of the performance (in terms of quiz results and time taken) observed between the formats is presented. This is in relation to the first hypothesis, that the presentation format will have an effect on these performance measures. The second two hypotheses are dealt with in following subsections. Finally, the effect of different questions on reading behaviour is shown.
Effect of Presentation on Overall Performance
The first aspect analysed is how the participants performed on the tutorial and quiz in each format. The hypothesis is that the different presentation formats will affect how well the participants score, how long it takes them, and also will affect how they perceive their performance. The averages these performance measures are presented in Table 1.
Conversely, if participants read the text before seeing the questions then they should have little need to re-read the text when re-presented with it, instead they would just have to re-read to confirm that they have the correct answers. This is roughly the same as thinking about what the answer is and fully considering it to be correct which is the situation in format C. It would then be expected that the time taken should be similar to that of format C. Given that both formats B and C took significantly less time on average to complete than format A these assumptions do not stand. Instead the participants are re- reading the text so that it increases the total time taken to complete the quiz. There is no difference in time taken to complete the quiz for formats B, C or D.
There is no significant difference between the quiz scores obtained from format A and B. This indicates that reading the tutorial text before being presented with the questions did not improve comprehension scores. Format A has significantly higher quiz scores compared to formats C and D (p=0.005<0.05 and p=0.043<0.05, respectively). This is also true for format B, where the quiz scores from format B are higher than those from formats C and D (p=0.009<p=0.05 and p=0.073<p=0.1, respectively). In formats C and D, the participants did not have access to the content as they answered the questions. Participants therefore had to rely on short-term memory and their understanding of the material. This could account for the discrepancy in results. There is no significant difference between the quiz scores obtained from formats C and D. Interestingly, the knowledge of the questions before reading the text did not significantly improve their quiz score or perceived comprehension.
The Pearson's correlation coefficients (r) between the performance measures are shown in Table 2. Notably, in format C there is a very strong positive correlation between the quiz scores and the subjective comprehension scores (r=0.9). This indicates that in this presentation format participants are more accurately able to estimate their comprehension level compared to other formats. This relationship is closely followed by the medium positive correlation (r=0.63) between quiz scores and subjective scores for format A. When considering formats B and D the participants seem unable to interpret their own comprehension levels and in the case of presentation format B there is even a small negative correlation (r=-0.15) between the quiz scores and the subjective comprehension scores. Students should be able to interpret their knowledge level rather than over or underestimate their comprehension level. Over estimation of comprehension could lead to students not learning the material properly as they think they know the answers. In format C the questions are asked after the participants have read the content and they cannot refer back to the material. The participants can seemingly gauge whether they know the answers or not. This effect is still observed to some extent for format A even though participants are able to re-read the tutorial text to answer the questions. The participants are still able to interpret that they have a decreased level of comprehension. In format B the participants only have access to the text whilst answering the questions; this may induce "laziness" in students
   Format
Time (minutes)
  Quiz Score
  Subjective Score
   A
(T-Q&T)
38.4±9.1
  16.2±1.4
  7.7±1.8
   B
(Q&T)
26.0±8.0
  16.1±1.6
  7.9±1.0
   C
(T-Q)
28.2±10.9
  11.9±3.6
  6.8±2.9
  D
(Q-T-Q)
23.9±7.8
   14.1±2.2
  7.3±0.95
      Table 1: Comparison of means of time taken, quiz score and subjective scores for each format.
Format A took significantly longer to complete than formats B, C and D (p=0.009<0.05, p=0.046<0.05 and p=0.004<0.05, respectively, all two-sided, unpaired t- test). In format A, participants were requested to read the tutorial text and then move on to answering the questions where they also had the option to re-read the content. If a participant did not read the text when it is first shown to them and rather waited for the questions to read through the text to find the answers, then the time taken to complete the quiz should be similar to that of format B.
234
whereby they do not fully read the content and thus fail to find key concepts in the text. In format D participants read the questions before reading the tutorial text but then still need to answer the questions without access to the text. In both cases the participants have a false sense of confidence in their answers.
learnt the most from the material in these formats compared to C and D. Since a key aim of any educational material is to promote learning, formats A and B are optimal in this respect.
In conclusion, the analysis of performance measures shows that the initial hypothesis was correct. The different presentation formats have an effect on students’ performance, in terms of time and quiz scores, as well as their perceived understanding of the text. The presentation format can therefore be manipulated to optimise the performance outcomes of students, thereby increasing their understanding.
Effect of Presentation on Eye Movements and Reading Behaviour
In this part of the investigation the two central differences in presentation are analysed separately. That is, the tutorial text when shown without the questions versus the tutorial text when shown with the questions. This addresses the final two hypotheses, that different eye movements will be observed for each format, and that the formats will induce different reading behaviours.
Text Page Only
The eye movement measures observed for reading the tutorial text slide are shown in Table 3. Formats A, C and D present the tutorial text slide on its own and these are compared to look at the reading and eye movement behaviour in this scenario.
   Format
 Time & Quiz Score
 Time & Subjective score
  Quiz Score & Subjective Score
   A
(T-Q&T)
 -0.65
 -0.46
  0.63
   B
(Q&T)
 -0.63
 0.45
  -0.15
   C
(T-Q)
 -0.43
 -0.25
  0.90
  D
(Q-T-Q)
 -0.16
  -0.29
   0.22
      Table 2: Correlation analysis (Pearson’s R) between performance measures for each format.
There are medium negative correlations between times taken to complete the quiz and actual quiz scores for all formats except D. This correlation is strongest for format A and B. These presentation formats show the questions with the content. The longer the participants spend completing the quiz the lower their quiz score is observed to be. This indicates that the more uncertain a student is the longer they will spend looking for the answers to the questions and therefore may not understand the material well enough to identify the correct answer. In format C participants are presented with the material and then with the questions so participants spending longer trying to answer the questions could account for the correlation between time taken and their quiz score.
There is a medium positive correlation (r=0.45) between times taken and subjective scores for format B. The longer that the participants took the more they thought they understood the material. Yet there is a negative correlation between times taken and quiz score for this format so in fact the opposite is true.
For the rest of the formats there was a negative correlation between the time taken and the subjective score. These results are consistent with the correlations between times taken and quiz score for these formats. The longer that the participants took, the poorer they understood the material but were also unaware of their lack of understanding unlike, for format B.
Under-estimation of understanding can lead to students wasting time on material already understood instead of using the time to learn more material. On the other hand, overestimation of understanding will results in students not learning what they need to and not realising their lack of understanding. In this respect, presentation formats A and C are the optimal format methods for learning.
Additionally, the results from this study show that the formats A and B have the highest quiz scores. For these formats participants were able to consult the text whilst answering the questions to ensure that they understood it before answering. The results show that participants
Table 3: Comparison of eye movement measures
Two types of behaviour are hypothesised; the first is that participants presented with format C will take more care reading the text, as they know they cannot refer to it again whilst answering the comprehension questions. In format D, participants have already seen the questions that they will need to answer. The second hypothesis is that for format D, participants will not (as) thoroughly read the text but rather skim the text to find the paragraphs where they believe the answers are located and read only those paragraphs thoroughly.
The average number of fixations and total fixation time for format C are higher than those from both formats A and D. The difference in number of fixations is statistically significant between formats A and D as well as formats C and D (both p=0<0.05). Meanwhile, the difference in total fixation duration time is statistically significant between formats A and C (p=0.001 0.05) and formats C and D (p=0.0002<0.05). Although the participants are showing no difference in the number of fixations whilst reading the text in formats A and C they
   Format
Number of fixations
  T otal fixation duration (s)
  Number of regressions
   A
(T-Q&T)
278.4±107.3
  51.1±32.3
  85.8±29.5
   C
(T-Q)
299.7±122.9
  68.21±33.1
  92.8±42.7
  D
(Q-T-Q)
190.2±98.8
   45.88±37.4
   67.3±28.8
     235
are spending longer reading the text in format C according to the fixation time.
There is a significant difference in the average number of regressions observed between formats A and D (p=0.0002<0.05) as well as C and D (p=0<0.05). Once again, there is no difference in the average number of regressions between formats A and C. It was expected that for format C participants would read the text more thoroughly. Supportive of this, format C has the highest number of regressions, which is indicative of participants regressing back to re-read words or sentences. Given that participants show higher on average numbers of fixations and total fixation duration time for format C, the hypothesis is supported. Since the text is the same in all formats, participants intentionally reading the text more carefully can explain this increase. Furthermore, format D has the lowest number of fixations, lowest total fixation duration, and lowest number of regressions, which is indicative of skimming behaviour. There is no difference in the text between formats, so participants reading less thoroughly can explain the difference in eye movements.
In support of the above evaluation of eye movement measures, and to address the final hypothesis, the reading behaviour is compared between the three formats. The reading behaviour is determined using the reading detection algorithm described in (Buscher et al., 2008) but modified to include scanning behaviour as described by Campbell and Maglio (2001). The percentage of fixation transitions (i.e. saccades and regressions) classified as being part of reading, skimming and scanning are compared between the formats; results are shown in Table 4.
Table 4: Comparison of reading behaviour of tutorial text
Format A has the highest on average reading ratio and lowest skim and scan ratios. These measures are significantly different from both format C and D (all p=0<0.05).
As stated above, the hypothesis was that participants will read the text more thoroughly when presented in format C. Format C has the highest number of regressions, which is indicative of re-reading parts of the text, which supports this conclusion. Longer regressions may be part of skimming and scanning behaviour to find the appropriate location from which to re-read. These results align with those above where participants show less skimming and scanning in format A; they are only reading the text not thoroughly studying it.
As expected, format D has significantly lower reading ratios compared to format A and C (p=0<0.05 and p=0.0001<0.05, respectively). The skimming and scanning ratios are also significantly higher than those
observed for format A (both p=0<0.05). The hypothesis was that participants would search the text in format D to find the answers, and only read those parts of the text. Furthermore, the scanning ratio is also significantly higher in format D than it is for C (p=0.0001<0.05. Although the skimming ratio is higher for format D than it is for C this is a weak difference (p=0.06>0.05).
In conclusion, the eye movement and reading behaviours that are observed for the formats A, C and D are quite different. This difference reflects the participants’ overall intentions in reading the text and the goals set for the participants. The purpose of this analysis was to assess the hypotheses that the different presentation formats would affect the eye movements observed and therefore the reading behaviours observed. These hypotheses have been shown to be true. The implications of these findings can be used to support design decisions for eLearning environments. That is, if the teacher wants to promote thorough reading, the goals placed on the reader should not be targeted at certain parts of the text as in format D. Instead, thorough reading is observed for format C where the goal was to understand the text overall.
Questions and T ext
Now the slide with the questions and text is considered. Format A consists of two presentations of the text, first on its own and second with the questions. It is hypothesised that the first read through of the text in format A will help participants answer the question and less reference to the text will be needed compared to format B.
Table 5: Comparison of eye movement measures
As seen in Table 5, we observe the average numbers of fixations, number of regressions, and total fixation times are significantly lower for format A compared to format B (all p=0<0.05). This indicates that participants spend far less time reading the question and text (Q&T) compared to the participants in format B.
The reading behaviour is compared between the formats; shown in Table 6.
Table 6: Comparison of reading behaviour of questions and tutorial text
There is a significantly higher ratio of skimming behaviour observed for format B compared to format A (p=0.01<0.05). However, none of the other ratios are
   Format
 Number of Fixations
T otal Fixation Duration (s)
   Number of Regressions
   A
(T-Q&T)
  224.4±171.2
40.4±35.8
   95.7±66.9
  B
(Q&T)
 374.2±167.8
 77.2±43.7
   157.5±66.4
     Format
  Read Ratio
Skim Ratio
   Scan Ratio
   A
(T-Q&T)
 0.66±0.23
0.15±0.11
  0.19±0.12
   C
(T-Q)
  0.52±0.21
0.21±0.09
   0.28±0.15
  D
(Q-T-Q)
 0.36±0.27
 0.24±0.13
   0.40±0.24
         Format
 Read Ratio
 Skim Ratio
   Scan Ratio
   A
(T-Q&T)
0.39±0.21
 0.18±0.11
  0.44±0.17
  B
(Q&T)
0.35±0.19
  0.22±0.10
    0.43±0.14
    236
significantly different. This is a surprising finding given the observations from Table 5. A higher read ratio for format B compared to format A was predicted. The reason for this may be that the participants have not read the material in format B, so it is expected that participants presented with format B would read the material more carefully than those in format A. Instead what is seen is the same reading, skimming and scanning behaviour for both the formats. The participants are most likely skimming and scanning through the text quite a lot to find where the answers to the questions may be and only reading the parts that they find most relevant as in format A.
Comparing the reading behaviour to format D (as shown in Table 4), there is also no difference between any of the ratios when comparing format B to D and no difference between the read and scan ratios when comparing formats A to D. There is statistical difference between the skim ratios for formats A and D (p=0.0009<0.05). Essentially, the reading behaviour observed for format D can be likened to that observed when the questions are present.
As stated, formats A and B provide participants with the ability to check the content whilst answering the questions. This behaviour is defined as answer seeking (Copeland and Gedeon, 2013). In format A, participants are requested to read the content before moving on to answer the comprehension questions. Participants should have some idea about the answers to the questions as well as have some idea where to find the answers in the text.
In summary, the eye movement behaviours that are observed for formats A and B are considerably different. This difference reflects the fact that participants are requested to read the material before moving on to answer the questions in format A, and participants do not have to do this in format B. Although being asked to read the tutorial text does not improve comprehension results (Table 1), reading the text before answering the questions reduces the effort, measured by number of fixations and total fixation time required for finding and answering the questions.
The analysis in this subsection was designed to investigate the final two broad hypotheses that different eye movements will be observed for each format and that the formats will induce different reading behaviours. There were several sub-hypotheses that were also addressed in this section that broke down the broad hypotheses. Overall both broad hypotheses are validated; the different formats affect the eye movements observed and therefore the reading and answer seeking behaviour observed.
RECOMMENDATIONS FOR DESIGNING ETEXT IN ELEARNING ENVIRONMENTS
The investigation so far has compared how different presentation formats of the text and comprehension questions affects performance, in terms of time and quiz score, and perceived understanding of the text, as well as affects eye movements, and reading behaviour. Conclusions have been made throughout the analysis in regards to these metrics, however, this section will
summarise these conclusions to make recommendations for 1) educators designing courseware in an eLearning environment and 2) design considerations for developers of eLearning environments.
Foremost, the analysis has established that the presentation of text and evaluation resources, such as quiz questions, can have a great impact on the learning process and outcomes. The presentation format can be manipulated to optimise the performance outcomes of students, thereby increasing their understanding. Format A has been shown to be the optimal presentation format to do so. The reasoning for this is now summarised. Format A and C were shown to promote accurate self- assessments of comprehension, which in turn minimises both under- and over-estimation of knowledge. Formats A and B have the highest quiz scores. Given that the aims of any educational material is to promote learning, as well as accurate self assessment format A is thus optimal.
The differences in eye movement measures and reading behaviours reflect the overall purpose and goals placed on the reader. If an educator wants to promote thorough reading, the goals placed on the reader should not be targeted with the use of quiz questions. In this case, students only read the parts of the text that they think contains the answers. However, not showing the text with the questions means that the students have to rely too heavily on short term memory and impacts on their quiz scores. The happy medium is format A where the students are requested to read the text and then move on to answer the compression questions. Of course this raises the question of how to actually have students read the text before moving on to the questions and text page. This is where eye tracking can be utilised. The eye tracker can be integrated into the learning environment so that it can monitor reading behaviour. Once the student has read the text then the learning environment will allow the student to move on to the questions.
As can be seen, the common denominator for optimal presentation formats is format A. Educators should consider using this method of presentation when designing text based learning material. In addition, the inclusion of eye tracking into eLearning environments would be beneficial in monitoring the reading behaviours of students.
CONCLUSIONS AND FURTHER WORK
Different presentation formats of text and comprehension questions were investigated to see the effect on students' learning performance, eye movements, and reading behaviour. The purpose is to support informative decisions about how adaptive eLearning environments should present text-based learning material in combination with evaluation resources such as quizzes, as well as to gain insight into how presentation formats affect eye movement and reading behaviour.
Three hypotheses were investigated; the presentation sequence of text and comprehension questions will: 1) have an effect on students’ performance, in terms of time and quiz score, as well as their perceived understanding of the text; 2) cause different eye movements to be
237
observed; and 3) induce different reading behaviours. The hypotheses were validated through the investigation and the conclusions are summarised to give recommendations to educators and developers of eLearning environments.
The presentation format can affect a student’s ability to self-identify their comprehension level. A learning environment should present material in such a way that students can accurately perceive their own comprehension so as to alleviate the potential of under or overestimation of their own knowledge.
Importantly, the investigations shows that different presentation formats induce different eye movements and reading behaviours. The differences are due to the goals placed on the reader. In particular, giving the participant the goal of understanding the text to the point that they subjectively believe they understand it causes the participant to read the text more thoroughly then when specific questions are asked of the reader about the contents of the text. Requesting the participants read the text with this intention and then asking them comprehension questions with access to the text again does promote thorough reading behaviour, as well as decrease the need to refer back to the text whilst answering the questions.
The overall conclusion is that the optimal presentation format is format A; presentation of the text on its own followed by presentation of the questions and the text for reference. This format optimised the learning outcomes in addition to promoting thorough reading behaviours.
A limitation of this study is that a relatively small sample size was used. It would be beneficial to run this experiment will a greater number of student participants so that the results can be more generalized. Another limitation is that the majority of the participants are male. This is because the participants were volunteers from a first year computer science course that is predominantly made up of male students. In eLearning environments it has been shown that there is no difference between the observations (Tsianos et al., 2009), further study should be performed to investigate whether this is true for this study. Liu and Huang (2008) showed that there is a significant difference in preference for reading digital text and printed text based on gender so could potentially impact results.
There are only two types of questions that are investigated in this analysis, being multiple choice and cloze questions. These are commonly used question type but not the only types generally available in eLearning environments, so further research should include to see what effect other question types have on the observed behaviour.
Further exploration on how presentation formats on mobile devices would be beneficial given the prevalence of this technology, as this study only considers reading from a computer screen in a University setting.
An area of interest is to investigate the relationship between eye movements and subjective comprehension. Although we touch on it in this investigation we want to
investigate this relationship further. In a follow-up study we propose recording eye gaze from participants as they once again read through a tutorial and quiz. After each tutorial slide we can ask participants for their subjective comprehension score as well as other factors regarding how they read and perceived the text.
ACKNOWLEDGMENTS
Thank you our proofreaders and all the participants of the study. Thank you to the reviewers for your thoughtful comments and recommendations.
REFERENCES
Anderson-Inman, L. & Horney, M. A. Supported eText: Assistive Technology Through Text Transformations. Reading Research Quarterly, (2007) 42, 153-160.
Buscher, G., Dengel, A. and Elst, L. V. Eye Movements As Implicit Relevance Feedback. CHI '08 Extended Abstracts On Human Factors In Computing Systems. Florence, Italy: ACM. (2008), 2991-2996.
Campbell, C. S. and Maglio, P. P. A Robust Algorithm For Reading Detection. Proceedings Of The 2001 Workshop On Perceptive User Interfaces, ACM, (2001), 1-7.
Chen, S.-C., She, H.-C., Chuang, M.-H., Wu, J.-Y., Tsai, J.-L. and Jung, T.-P. Eye Movements Predict Students' Computer-Based Assessment Performance Of Physics Concepts In Different Presentation Modalities. Computers & Education, (2014), 74, 61- 72.
Copeland, L. and Gedeon, T. Measuring Reading Comprehension Using Eye Movements. Cognitive Infocommunications (CogInfoCom), IEEE 4th International Conference On, 2013. IEEE, (2013), 791-796.
Dillon, A. Reading From Paper Versus Screens: A Critical Review Of The Empirical Literature. Ergonomics, (1992) 35, 1297-1326.
Dillon, A. TIMS: A Framework For The Design Of Usable Electronic Text. In: Van Oostendorp, H. & De Mul, S. (Eds.) Cognitive Aspects Of Electronic Text Processing. Norwood, NJ: Ablex. (1996)
Dillon, A. Designing Usable Electronic Text: Ergonomic Aspects Of Human Information Usage, CRC Press (2004)
Dillon, A. & Gabbard, R. Hypermedia As An Educational Technology: A Review Of The Quantitative Research Literature On Learner Comprehension, Control, And Style. Review Of Educational Research, (1998) 68, 322-349.
Fletcher, J. M. Measuring Reading Comprehension. Scientific Studies Of Reading, (2006), 10, 323-330. Frazier, L. and Rayner, K. Making And Correcting Errors
During Sentence Comprehension: Eye Movements In The Analysis Of Structurally Ambiguous Sentences. Cognitive Psychology, (1982), 14, 178-210.
Horney, M. A. & Anderson-Inman, L. Supported Text In Electronic Reading Environments. Reading & Writing Quarterly, (1999) 15, 127-168.
Hyona, J., Lorch Jr, R. F. and Rinck, M. Chapter 16 - Eye Movement Measures To Study Global Text Processing. In: Hyona, J., Radach, R., H. Deubela2 -
238
J. Hyona, R. R. & Deubel, H. (Eds.) The Mind's Eye.
Amsterdam: North-Holland. (2003)
Hyrskykari, A., Majaranta, P., Aaltonen, A. and Räihä,
K.-J. Design Issues Of Idict: A Gaze-Assisted Translation Aid. Proceedings Of The 2000 Symposium On Eye Tracking Research & Applications, ACM Press, (2000), 9-14.
Iqbal, S. T. and Bailey, B. P. Using Eye Gaze Patterns To Identify User Tasks. The Grace Hopper Celebration Of Women In Computing (2004).
Liu, Z. & Huang, X. Gender Differences In The Online Reading Environment. Journal Of Documentation, (2008) 64, 616-626.
Liversedge, S. P. and Findlay, J. M. Saccadic Eye Movements And Cognition. Trends In Cognitive Sciences, (2000) 4, 6-14.
Loizides, F. & Buchanan, G. R. Performing Document Triage On Small Screen Devices. Part 1: Structured Documents. Proceedings Of The Third Symposium On Information Interaction In Context, ACM, (2010), 341-346.
Marshall, C. C. & Bly, S. Turning The Page On Navigation. Digital Libraries, 2005. JCDL'05. Proceedings Of The 5th ACM/IEEE-Cs Joint Conference On, IEEE, (2005), 225-234.
Mckay, D. A. Jump To The Left (And Then A Step To The Right): Reading Practices Within Academic Ebooks. Proceedings Of The 23rd Australian Computer-Human Interaction Conference. Canberra, Australia: ACM. (2011), 202-210
Nugrahaningsih, N., Porta, M. and Ricotti, S. Gaze Behavior Analysis In Multiple-Answer Tests: An Eye Tracking Investigation. Information Technology Based Higher Education And Training (Ithet), 2013 International Conference On, IEEE, (2013) 1-6.
O'Hara, K. & Sellen, A. A Comparison Of Reading Paper And On-Line Documents. Proceedings Of The ACM SigCHI Conference On Human Factors In Computing Systems. Atlanta, Georgia, Usa: ACM. (1997), 335- 342.
Rayner, K. Eye Movements In Reading And Information Processing: 20 Years Of Research. Psychological Bulletin, (1998), 124(3), 372-422.
Rayner, K., Chace, K. H., Slattery, T. J. and Ashby, J. Eye Movements As Reflections Of Comprehension Processes In Reading. Scientific Studies Of Reading, (2006), 10, 241-255.
Sibert, J. L., Gokturk, M. and Lavine, R. A. The Reading Assistant: Eye Gaze Triggered Auditory Prompting For Reading Remediation. Proceedings Of The 13th Annual ACM Symposium On User Interface Software And Technology. California, USA: ACM, (2000) 101- 107.
Tsai, M.-J., Hou, H.-T., Lai, M.-L., Liu, W.-Y. and Yang, F.-Y. Visual Attention For Solving Multiple-Choice Science Problem: An Eye-Tracking Analysis. Computers & Education, (2012), 58, 375-385.
Tsianos, N., Germanakos, P., Lekkas, Z., Mourlas, C. & Samaras, G. Eye-Tracking Users' Behavior in Relation to Cognitive Style within an E-learning Environment. Advanced Learning Technologies,
2009. ICALT 2009. Ninth IEEE International
Conference on, (2009), 329-333.
Underwood, G., Hubbard, A. and Wilkinson, H. Eye
Fixations Predict Reading Comprehension: The Relationships Between Reading Skill, Reading Speed, And Visual Inspection. Language And Speech, (1990), 33, 69-81.
239
Leana Copeland
Research School of Computer Science
Australian National University Canberra, Australia leana.copeland@anu.edu.au
Tom Gedeon
Research School of Computer Science
Australian National University Canberra, Australia tom.gedeon@anu.edu.au
Sabrina Caldwell
Research School of Computer Science
Australian National University Canberra, Australia sabrina.caldwell@anu.edu.au

Effects of Text Difficulty and Readers on Predicting Reading Comprehension from Eye Movements
Abstract—The task of predicting reader state from readers' eye gaze is not trivial. Whilst eye movements have long been shown to reflect the reading process, the task of predicting quantified measures of reading comprehension has been attempted with unsatisfactory results. We conducted an experiment to collect eye gaze data from participants as they read texts with differing degrees of difficulty. Participants were sourced as being either first or second English language readers. We investigated the effects that reader background and text difficulty have predicting reading comprehension. The results indicate that prediction rates are similar for first and second language readers. The best combination is where the concept level is one level higher than the readability level. The optimal predictors are ELM+NN and Random Forests as they consistently produced the lowest MSEs on average. These findings are a promising step forward to predicting reading comprehension. The intention is to use such predictions in adaptive eLearning environments.
Keywords—first language reader (L1); second language reader (L2); reading comprehension prediction; eye tracking; adaptive eLearning
I. INTRODUCTION
Online learning is now ubiquitous in tertiary education. Universities frequently offer online and / or off-campus degrees where students may have little or no face-to-face interaction with their instructors or other students. Even for university courses that deliver material traditionally, absenteeism from lectures is prevalent despite the fact it has been shown to negatively affect learning [1, 2]. As a result there is growing importance in designing effective eLearning environments. Yet eLearning for the most part is one size fits all. For the eLearning environments that provide personalization this is often done explicitly by the learner. Adaption can be provided by various methods including questionnaires [3, 4], skill level [5], motivation [6], and eye gaze [7-10].
Eye tracking has proven to be a powerful tool for investigating how humans interact with computer interfaces. With eye tracking becoming increasingly more precise and cheaper, the use of such technology in adaptive eLearning is becoming more of a reality. Eye movements can reveal much about the cognitive processes behind human behaviors. Eye movements have long been shown to reflect the reading process (see review by [11]), as they are unique in reading. Eye
movements can reveal when readers encounter difficulties in reading [12] as well as text difficulty and comprehension [13]. Yet the task of predicting quantified measures of reading comprehension has been attempted with poor results [14, 15].
We investigate two factors that influence prediction performance of reading comprehension; text difficulty and first versus second language readers. To perform this investigation we conducted a study to collect eye gaze from participants as they read and completed an online tutorial and quiz. The tutorial consisted of sections of text with varying degrees of difficulty. Participants were sourced as being either first (L1) or second (L2) language English readers. We hypothesize that there will be a difference in predictive performance between L1 and L2 readers; furthermore, we predict that text difficulty will affect prediction performance. The goal is to find ways of optimizing prediction of reading comprehension.
The intended use of reading comprehension prediction from eye gaze is in the design of adaptive eLearning environments that use eye gaze to predict reading comprehension. This would allow for the omission of some assessment questions as well as to differentiate between actual understanding and accidental choice of the correct answer. We contribute to CogInfoCom research by utilizing knowledge of human cognition to develop information transfer techniques between humans and infocommunications systems that are primarily based on Internet technology and for the purpose of education. This paper is organized into the following sections: background information; user study methodology; results and analysis; implications; finishing with conclusions and further work.
II. BACKGROUND
A. Eye Tracking to Analyse the Reading Process
During the process of reading the eye moves in well- studied ways that can be broadly characterized as fixations and saccades. A fixation is where the eye remains relatively still to take in visual information. A saccade is a rapid movement that transports the eye to another fixation. This behavior is due to the anatomy of the eye. At the center of the retina is a special part of the eye called the fovea. This is the part of the eye that sees in fine detail, however the foveal region is very small, being only about 0.2mm in diameter. Around the point of fixation, visual acuity extends about 2° [16]. This means that
978-1-4673-8129-1/15/$31.00 ©2015 IEEE

humans see very little in detail at any fixation so the eye must move around rapidly so that it can compose a more detailed view of the environment.
When reading English, fixation duration ranges anywhere between 60-500 milliseconds and is generally about 250 milliseconds [17]. Saccadic movement is between 1 and 15 characters with an average of 7-9 characters. The majority of saccades are to transport the eye forward in the text when reading English; however, a proficient reader demonstrates backward saccades to previously read words or lines about 10- 15% of the time. Backward saccades are termed regressions. Short regressions can occur within words or a few words back and may be due to problems in processing the currently fixated word, overshoots in saccades, or oculomotor errors. However, longer regressions occur due to comprehension difficulties, as the reader tends to send their eyes back to the part of the text that caused the difficulty [12].
1) Eye movements and Reading Comphrension
Eye movements can be used to understand the ongoing cognitive processes that occur during reading [17]. Comprehension of text can have significant effects on eye movements [13]. A number of studies have shown there are many variables based upon comprehension functions that can influence eye movements during reading. These variables include: semantic relationships, anaphora and co-reference, lexical ambiguity, phonological ambiguity, discourse factors, stylistic conventions, and syntactic disambiguation (see review by [11]). These variables have different effects on eye movement, causing them to deviate from the default reading process. For example, syntactically ambiguous sentences induce regressions to resolve the comprehension problems [12]. Eye movements have been shown to reflect global text difficulty as well as inconsistencies within text [13]. More difficult text causes more fixations, more regressions, and longer fixation duration time. Eye movement has also been shown to indicate reading comprehension and reading skill [14, 18].
B. ProvidingAdaptivityineLearning
The use of eye tracking to make eLearning adaptive is not new. The advantage of using eye tracking is that eye movements are an implicit behavior and can reveal underlying cognitive behavior [17]. Eye gaze patterns can be used to detect what kind of task the participant is performing [19] or whether a person is reading or not [20] as well as if they are reading or skimming [21]. More recently, eye gaze has been used to investigate parts of text that readers are failing to comprehend [22]. Results from this investigation indicate that eye gaze features such as number and duration of fixations can be used to identify reading incomprehension. A classic example of the use of eye tracking in eLearning is AdeLE (Adaptive e- Learning with Eye-Tracking). The AdeLE project sets out a structure for how an adaptive eLearning environment could be constructed using eye tracking data such as blink rate and how open the eye lid is [7].
Eye tracking has been shown to be an effective way of identifying learner style. Learner style is a common way of adapting learning material to suit different students [3, 4]. Eye
tracking has been shown to be a potential way of identifying visual and verbal learners [8]. Eye movements in areas of interest on the page were related to measures of learner style in that investigation. Similar uses of eye tracking have been used to investigate learning behaviors between novice and advanced students when learning SQL [23]. Indeed more advanced students looked at the database schema more that novices. Studies such as this are useful for identifying such difference in order to provide more help for novice students.
One aspect that has about learning that is frequently investigated is engagement. Eye tracking has been used to identify aspects of a student’s emotional state, such as stress and arousal, and adapt the material based on the identified state. An example of this is e5Learning (enhanced exploitation of eyes for effective eLearning) which uses eye gaze metrics such as fixation statistics and pupil diameter to identify the students emotional state [10]. Gaze Tutor uses eye gaze to determine the user's level of stimulation to alter the environment to stimulate the user [9]. An interesting approach to identifying students engagement comes from the use of type- 2 fuzzy logic based system [24]. This novel method gauges degree of engagement to adapt the learning environment. Results show that using the system to adapt material there is significant improvement in average scores compared to other methods of adaption and no adaption.
Eye tracking is also used to analyze reading in eLearning environments. One example is iDict, a reading aid designed to help readers of a foreign language that uses eye gaze to predict when a reader is having comprehension difficulties [25]. If the user hesitates whilst reading a word then a translation of the word is provided along with a dictionary meaning. Similarly, the Reading Assistant [26] uses eye gaze to predict failure to recognize a word. The Reading Assistant then provides an auditory pronunciation of the word to aid reading. Adaption of reading material has been shown to be beneficial to young students [27]. Adaptive eBooks involves detection of reading difficulty, currently based on measures such as out load reading speed, and dynamically simplifying the text for the students. The system is designed for year 4 students and an initial study shows that such modifications can improve reading performance. However, the authors note that the reading problem detection currently used is in the system not sufficient and should be replaced, noting also that eye tracking would be a good solution.
A gap still remaining is whether reading comprehension can be quantifiably predicted from eye gaze data. Whilst there has been progress on the matter, it has been shown to be difficult to predict quantified reading comprehension measures [14, 15] .
III. USER STUDY METHODOLOGY
A. ParticipantsandDesign
The eye gaze of 70 participants (47 male, 23 female) was recorded. Participants had an average age of 25 years with standard deviation of 9 year (age range of 18 to 60 years).
Participants’ eye gaze was tracked as they read and completed a tutorial on the topic of Digital Images. The tutorial

TABLE I.
READABILITY GROUPS

was taken from a first year computer science course on Web Development and Design offered at the Australian National University (ANU). The tutorial was composed of 9 texts of approximately 240 words in length. Given that there are 70 participants and 9 texts there is a total of 630 eye gaze sets for the prediction analysis. Due to problems in collected data 12 of these eye gaze sets had to be removed resulting in 618 eye gaze data sets for the prediction analysis.
After each text, 2 comprehension questions were asked, each scored out of 1. Each text has a readability level in one of three classes, easy, medium or hard. The readability levels are calculated using Flesch Kincaid Grade level and are described in Table I. Each increase in readability level is approximately 3 years of education.
1) Inputs : Eye movements measures
a)Normalised Number of fixations: The sum of
fixations recorded for each page is divided by the number of words on the page. The number of fixations can be affected by reading behavior, text difficulty, and reading skill [11].
b) Maximum fixation duration (seconds): The maximum duration of the longest fixation recorded for a tutorial page. Longer fixations can be an indicator of difficulties in processing particular words or due to linguistic and/or comprehension difficulties [11].
c) Average fixation duration (seconds): The sum of the duration of all fixations is divided by the total number of fixations. This measure has been used to predict reading comprehension [18].
d)Normalised total fixation duration (seconds): The sum of all fixations divided by the number of words in the text. This measure is useful in global text processing analysis [28] because this measures immediate as well as delayed effects of comprehension.
e) Regression ratio: The number of regressions divided by the total number of saccades on a paragraph. There is evidence that when reading more difficult text more regressions are observed [13].
f) Percentage of fixations and duration in text area and out of text area: The number of fixations as well as the duration of fixation recorded in the text area divided by the total number of fixations. The text area is shown in light blue in Fig. 1. Additionally, the inverses were calculated as inputs, that is the number of fixations and fixations duration out of the text area.
g) Number of distractions: A count of the number of times the readers eyes exited the text area to look at another part of the page.
h)Reading analysis: Using our combination of two reading detection algorithms [20, 21], the percentage of saccades classified as being part of reading (read ratio), skimming (skim ratio), and scanning/searching (scan ratio).
2) Output : Reading comprehension score
The outcome variables are in the form of the participants’ reading comprehension scores. After each piece of text the participant was asked two comprehension questions. The minimum score is 0 and the maximum that could be obtained is 2 however there was part marks that could be obtained for each of the questions depending on the answer provided so that reading comprehension scores are continuous from 0 to 2.
IV. RESULTS: READING COMPREHENSION PREDICTIONS
In this investigation we consider two factors that influence prediction performance of reading comprehension, text difficulty and the difference between L1 and L2 readers. The results section is divided into three subsections that analyze the reader types separately and then combined. In each subsection we investigate the effects of text difficulty. In all of the analyses five prediction methods are trialed; regression trees, boosted regression trees, random forests, artificial neural networks trained using backpropagation (FFNN) as well as the
   Readability Grade
     Readability Level
  Easy
 Medium
  Hard
 Flesch Kincaid Grade Level
 11.3 SD 1.0
 14.8 SD 0.9
   18.4 SD 0.8
  B. MaterialsandProcedure
The tutorial quiz was accessible via the online learning environment used at ANU, called Wattle (a Moodle variant). The study was displayed on a 1280x1024 pixel Dell monitor.
Fig. 1.
Example of text shown in Wattle eLearning Environment
 Eye gaze data was recorded at 60Hz using Seeing Machines FaceLAB 5 infrared cameras mounted at the base of the monitor. The study involved a 9-point calibration prior to data collection for each participant. As the data recorded is a series of gaze points, EyeWorks Analyze was used to pre-process the data to give fixation points. The parameters used for this were a minimum duration of 60 milliseconds and a threshold of 5 pixels.
C. Data Pre-processing
The raw eye gaze data consists of x,y-coordinates of where the participants eye was looking. Fixation and saccade identification was performed on the eye gaze data. From this data many other eye movement measures are derived. For each piece of text the following eye movement measures are calculated:


extreme learning machine (ELM-NN) algorithm. The activation function used for the ELM+NN training is Hard- limit. The backprogation neural networks were trained using the Levenberg Mardqaurt method and a topology of [16 8 4] was used as this has proven to be an effective topology for prediction reading comprehension from eye gaze using neural networks [29]. All analyses were performed using Matlab R2013b.
A. PredictionofreadingcomprehensionforL1readersonly
The half of the data set that was collected from L1 readers is used to predict reading comprehension using the predictions methods. Mean MSEs from 5-fold cross validation are shown in Table II.
TABLE II. MSE FROM PREDICTOR FOR L1 READERS
The italicized are the lowest MSEs for each combination of text properties. The results show that the lowest MSEs come from the combinations where the concept level is one level higher than the readability. More specifically when the readability is Easy and the concept is Intermediate the MSE is on average 0.23 using both ELM-NN and Random Forest predictors, and when the readability is Moderate and the concept is Advanced the MSE is on average 0.18 using both ELM-NN and Random Forest predictors again. This highlights another key finding that both ELM-NN and Random Forest predictors tend to generate the lowest MSE values.
B. PredictionofreadingcomprehensionforL2readersonly
The half of the data set that was collected from L2 readers is used to predict reading comprehension using the predictions methods. Mean MSEs from 5-fold cross validation are shown in Table III.
TABLEIII. MSEFROMPREDICTORFORL2READERS
The italicized are the lowest MSEs for each combination of text properties. The MSEs predicted for the L2 data set are higher on average than from the L1 data set indicating that it is
harder to make predictions from the L2 data set. In this case, primarily the Random Forest predictor produces the lowest MSEs on average. Again the lowest average MSE of 0.26 is recorded from where the readability is Moderate and the concept is Advanced.
C. Prediction of reading comprehension for both reader types combined
The full data set that was collected is used to predict reading comprehension using the predictions methods. Mean MSEs from 10-fold cross validation are shown in Table IV. Note that in this case 10-fold cross validation is used as the data set is much larger.
TABLE IV. MSE FROM PREDICTOR FOR BOTH READERS
The italicized are the lowest MSEs for each combination of text properties. The MSEs generated from the full data set are similar to those produced from the L1 data set. This indicates that adding the L2 data does not decrease prediction power. Once again the lowest MSEs come from the combinations where the concept level is one level higher than the readability. When the readability is Easy and the concept is Intermediate the MSE is on average 0.26 using the ELM-NN predictor, and when the readability is Moderate and the concept is Advanced the MSE is on average 0.22 using the ELM-NN predictor again. Both the ELM-NN and Random Forest predictors generated the lowest MSE values.
V. IMPLICATIONS
From previous attempts to classify eye movement measures it has been established that making predictions of reading comprehension is not a trivial task [15]. Overall the goal of this investigation was to find ways of optimizing prediction performance of reading comprehension. Two factors regarding prediction performance of reading comprehension are considered in this investigation, namely, text difficulty and L1 versus L2 readers. The results from the study indicate that there is a difference in predictive performance between L1 and L2 readers, as we hypothesized. However, this difference is not large and when the two groups are combined there is not a negative impact on prediction outcomes (Table IV). This is a somewhat surprising result given that past research has shown that while L1 and L2 readers have the same comprehension they do have different eye movements during reading, as well as taking longer to read [30]. It was therefore hypothesized that this difference would negatively impact the predictions from L1 and L2 readers combined given that they have different eye movements. The implication of this in the design of a predictor
  Text Properties
   ELM- NN
     FFNN
   RegTree
Boosted RegTree
   Random Forest
       Readability
   Concept
 1
 1
  0.31
 0.32
 0.48
0.62
  0.31
1
 2
  0.26
 0.32
 0.41
0.68
  0.27
1
 3
  0.33
 0.55
 0.60
0.77
  0.33
2
 1
  0.51
 0.64
 0.74
0.70
  0.47
2
 2
  0.52
 0.45
 0.49
0.56
  0.37
2
 3
  0.22
 0.29
 0.38
0.68
  0.23
3
 1
  0.40
 0.59
 0.48
0.53
  0.34
3
 2
  0.53
 0.60
 0.65
0.67
  0.51
3
  3
  0.39
  0.71
  0.64
 0.75
   0.43
         Easy
Text Properties
ELM- NN
Boosted RegTree
Random Forest
             Readability
Concept
 FFNN
RegTree
           Basic
0.39
0.33
0.41
0.64
0.34
           Easy
Interm.
0.23
0.26
0.34
0.70
0.23
           Easy
Adv.
0.39
0.49
0.45
0.72
0.33
           Mod.
Basic
0.48
0.75
0.68
0.66
0.42
           Mod.
Interm.
0.36
0.55
0.69
0.66
0.34
           Mod.
Adv.
0.18
0.44
0.24
0.67
0.18
           Difficult
Basic
0.34
0.54
0.72
0.54
0.37
           Difficult
Interm.
0.44
0.63
0.78
0.71
0.48
         Difficult
Adv.
0.46
0.42
0.78
0.83
0.45
      Text Properties
   ELM- NN
     FFNN
   RegTree
Boosted RegTree
   Random Forest
     Readability
  Concept
 1
 1
  0.49
 0.67
 0.52
0.62
  0.44
 1
 2
  0.83
 0.75
 0.75
0.75
  0.43
 1
 3
  0.78
 0.64
 0.48
0.76
  0.32
 2
 1
  0.93
 0.72
 0.76
0.82
  0.62
 2
 2
  0.57
 0.40
 0.63
0.70
  0.51
 2
 3
  0.26
 0.52
 0.48
0.66
  0.29
 3
 1
  0.29
 0.35
 0.54
0.65
  0.34
 3
 2
  0.67
 0.68
 0.86
0.70
  0.58
 3
 3
  0.98
 0.59
  0.74
 0.73
   0.40


for reading comprehension in intelligent eLearning is that different predictors do not have to be created for each of the different reader types. This makes the prediction process simpler and less intrusive to the reader since explicitly asking them about their first language is not needed.
Our second hypothesis was that text difficulty affects prediction performance. We found that texts with combinations of difficulty where the concept level is one level higher than the readability produced the best prediction outcomes. In particular when readability is Easy and the concept is Intermediate and when the readability is Moderate and the concept is Advanced. The implication of this in the design of intelligent eLearning is that to obtain the best prediction results using this combination of text difficulty.
Finally, the investigation showed that the most effective predictor of the data set are ELM+NN and Random Forests as they consistently produced the lowest MSEs on average. These predictors should be considered when constructing a predictor of reading comprehension from eye movement measures.
1) Use Case: Adaptive learning paths
The goal of reading comprehension detection is to incorporate eye tracking into eLearning environments and use this data as a form of adaption. In this way the content and the presentation of content can be altered to reflect the student’s current state. The product of reading comprehension prediction is twofold; first, if students are given text to learn, instead of formatively assessing their comprehension, eye tracking could be used to assess their understanding thus reducing time, workload and potentially stress or anxiety of the students. Following on from this, predicting students’ comprehension using eye tracking would allow the learning environment to adapt 1) assessment questions about the content, and 2) the content difficulty to reflect the students’ current understanding levels.
If a student has read some learning materials but does not understand them altering the learning path could be used to increase understanding. This could be achieved by modifying the questions to be easier, perhaps covering more superficial understanding of the content. Text with more explanation of the content that was not understood could then be given, after which they are assessed on the original comprehension questions. This could otherwise be achieved by not asking comprehension questions at all, the text with more explanation could be provided to the student. Previous studies have shown that simplifying text can improve reading performance [27].
The inverse case where a student has an extremely high level of understanding, as in the case when the student has prior knowledge on a certain topic, this student may become frustrated or bored by being presented with easy content and unchallenging questions. Either the questions or the content could be altered to present these students with hard subject matter and questions that require much more thought and insight than the student with a lower level of understanding.
2) Implications for CogInfoCom
CogInfoCom is the investigation of links between cognitive science and information communication technology
(ICT). The role of this is to support development and analysis of the co-evolution of infocommunications and the capabilities of the human brain [31]. The pervasive use of Internet technologies and increasing ubiquity of online learning environments means that there is an inextricable link between learning and these technologies. We contribute to CogInfoCom by investigating the inter-cognitive communication between students and online learning environments. Students learn differently now due to the existence of online learning and so our proposition is that online learning environments must evolve to take into consideration the changes in learners’ needs. This is achieved through multi-modal interaction, namely eye tracking technology, between the student and the learning environment to dynamically adapt to the learners’ implicit behaviours.
VI. CONCLUSION AND FURTHER WORK
We investigated the effects that reader type and text difficulty have on predicting reading comprehension from eye movements. The overall goal is to provide insight into optimizing prediction performance for reading comprehension as it has been established as being a non-trivial task [15].
Whilst predictions from L2 readers are worse compared to L1 readers, in a combined data set there is not a significant negative impact on prediction outcomes. This implies that L1 and L2 reader can use the same predictor without negatively impacting prediction performance. Additionally, text difficulty affects prediction performance. The best combination is where the concept level is one level higher than the readability. Finally, the optimal predictors are ELM+NN and Random Forests as they consistently produced the lowest MSEs on average.
Further improvements on predictions should be explored by investigating the effectiveness of additional physiological data to see if this also increases classification results. Proposed physiological data could be in the form of pupil dilation, galvanic skin response (GSR), electrocardiogram (ECG) and electroencephalogram (EEG). Another area of investigation is breaking down the eye movement measures to different levels of granularity.
ACKNOWLEDGMENT
Thank you to all of the participants who took part in the study. Many thanks go to the proofreaders for your helpful comments.
REFERENCES
[1] D. Romer, "Do Students Go to Class? Should They?," The Journal of Economic Perspectives, vol. 7, pp. 167-174, 1993.
[2] R. Woodfield, D. Jessop, and L. McMillan, "Gender differences in undergraduate attendance rates," Studies in Higher Education, vol. 31, pp. 1-22, 2006/02/01 2006.
[3] H. D. Surjono, "The design of adaptive e-Learning system based on student’s learning styles," International Journal of Computer Science and Information Technology (IJCSIT), vol. 2, pp. 2350-2353, 2011.
[4] H. D. Surjono, "The Evaluation of a Moodle Based Adaptive e- Learning System," International Journal of Information and Education Technology, vol. 4, 2014.


[5] C.-M. Chen, "Intelligent web-based learning system with personalized learning path guidance," Computers & Education, vol. 51, pp. 787-814, 9// 2008.
[6] F. Kareal and J. Klema, "Adaptivity in e-learning," A. Méndez- Vilas, A. Solano, J. Mesa and JA Mesa: Current Developments in Technology-Assisted Education, vol. 1, pp. 260-264, 2006.
[7] C. Gütl, M. Pivec, C. Trummer, V. M. García-Barrios, F. Mödritscher, J. Pripfl, et al., "Adele (adaptive e-learning with eye- tracking): Theoretical background, system architecture and application scenarios," European Journal of open, Distance and E-learning (EURODL), vol. 2, 2005.
[8] T. J. Mehigan, M. Barry, A. Kehoe, and I. Pitt, "Using eye tracking technology to identify visual and verbal learners," in Multimedia and Expo (ICME), 2011 IEEE International Conference on, 2011, pp. 1-6.
[9] S. D'Mello, A. Olney, C. Williams, and P. Hays, "Gaze tutor: A gaze-reactive intelligent tutoring system," International Journal of Human- Computer Studies, vol. 70, pp. 377-398, 2012.
[10] C. Calvi, M. Porta, and D. Sacchi, "e5Learning, an e-learning environment based on eye tracking," in Advanced Learning Technologies, 2008. ICALT'08. Eighth IEEE International Conference on, 2008, pp. 376- 380.
[11] K. Rayner, "Eye movements in reading and information processing: 20 years of research," Psychological Bulletin, pp. 372-422, 1998.
[12] L. Frazier and K. Rayner, "Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences," Cognitive Psychology, vol. 14, pp. 178-210, 1982.
[13] K. Rayner, K. H. Chace, T. J. Slattery, and J. Ashby, "Eye movements as reflections of comprehension processes in reading," Scientific Studies of Reading, vol. 10, pp. 241-255, 2006.
[14] P. Martínez-Gómez and A. Aizawa, "Recognition of understanding level and language skill using measurements of reading behavior," in Proceedings of the 19th international conference on Intelligent User Interfaces, 2014, pp. 95-104.
[15] L. Copeland, T. Gedeon, and S. Mendis, "Predicting reading comprehension scores from eye movements using artificial neural networks and fuzzy output error," Artificial Intelligence Research, vol. 3, p. p35, 2014.
[16] K. Rayner and J. H. Bertera, "Reading without a fovea," Science, vol. 206, pp. 468-469, 1979.
[17] S. P. Liversedge and J. M. Findlay, "Saccadic eye movements and cognition," Trends in cognitive sciences, vol. 4, pp. 6-14, 2000.
[18] G. Underwood, A. Hubbard, and H. Wilkinson, "Eye fixations predict reading comprehension: The relationships between reading skill, reading speed, and visual inspection," Language and speech, vol. 33, pp. 69-81, 1990.
[19] S. T. Iqbal and B. P. Bailey, "Using Eye Gaze Patterns to Identify User Tasks.," The Grace Hopper Celebration of Women in Computing 2004, 2004.
[20] C. S. Campbell and P. P. Maglio, "A robust algorithm for reading detection," in Proceedings of the 2001 workshop on Perceptive user interfaces, 2001, pp. 1-7.
[21] G. Buscher, A. Dengel, and L. v. Elst, "Eye movements as implicit relevance feedback," presented at the CHI '08 Extended Abstracts on Human Factors in Computing Systems, Florence, Italy, 2008.
[22] S. Martinez-Conde, "Fixational eye movements in normal and pathological vision," Progress in brain research, vol. 154, pp. 151-176, 2006.
[23] Z. Liu, "Reading behavior in the digital environment: Changes in reading behavior over the past ten years," Journal of documentation, vol. 61, pp. 700-712, 2005.
[24] A. Paramythis and S. Loidl-Reisinger, "Adaptive learning environments and e-learning standards," in Second European Conference on e-Learning, 2003, pp. 369-379.
[25] A. Hyrskykari, P. Majaranta, A. Aaltonen, and K.-J. Räihä, "Design issues of iDICT: a gaze-assisted translation aid," in Proceedings of the 2000 symposium on Eye tracking research & applications, 2000, pp. 9- 14.
[26] J. L. Sibert, M. Gokturk, and R. A. Lavine, "The reading assistant: eye gaze triggered auditory prompting for reading remediation," presented at the Proceedings of the 13th annual ACM symposium on User interface software and technology, San Diego, California, United States, 2000.
[27] A. Dingli and C. Cachia, "Adaptive eBook," in Interactive Mobile Communication Technologies and Learning (IMCL), 2014 International Conference on, 2014, pp. 14-19.
[28] J. Hyona, R. F. Lorch Jr, and M. Rinck, "Chapter 16 - Eye Movement Measures to Study Global Text Processing," in The Mind's Eye, J. Hyona, R. Radach, R. R. H. DeubelA2 - J. Hyona, and H. Deubel, Eds., ed Amsterdam: North-Holland, 2003, pp. 313-334.
[29] L. Copeland, T. Gedeon, and S. Mendis, "Fuzzy Output Error as the Performance Function for Training Artificial Neural Networks to Predict Reading Comprehension from Eye Gaze," in The 21st International Conference on Neural Information Processing 2014, 2014.
[30] H. Kang, "Understanding online reading through the eyes of first and second language readers: An exploratory study," Computers & Education, vol. 73, pp. 1-8, 2014.
[31] P. Baranyi and A. Csapo, "Definition and synergies of cognitive infocommunications," Acta Polytechnica Hungarica, vol. 9, pp. 67-83, 2012.
UBICOMP/ISWC '15 ADJUNCT, SEPTEMBER 7–11, 2015, OSAKA, JAPAN
 Lima Sanches Charles
Osaka Prefecture University 1-1 Gakuen-cho, Naka, sakai, Osaka, Japan
Ecole Nationale Superieure de l’Electronique et de ses Applications
6, avenue du Ponceau 95014 Cergy-Pontoise, France charles-ls@hotmail.fr
Kise Koichi
Osaka Prefecture University 1-1 Gakuen-cho, Naka, sakai, Osaka, Japan kise@cs.osakafu-u.ac.jp
Augereau Olivier
Osaka Prefecture University 1-1 Gakuen-cho, Naka, sakai, Osaka, Japan augereau.o@gmail.com
Abstract
Eye tracking data has been widely used to analyze our reading behavior. Usually, experiments are carried out with head fixations or by analyzing eye tracking data in large ar- eas such as paragraphs. But if we want to analyze the eye gaze line by line or word by word with a non invasive appa- ratus, we have to face the mislocation of the recorded eye gaze. The lack of accuracy involves a difficult analysis of the small eyes movements during reading. This paper pro- poses a method to match lines of gazes with corresponding text lines, using three different methods. We will show that the Dynamic Time Warping is a promising way to measure similarity between a line of gaze and a text line.
Author Keywords
eye tracker; gaze analysis; sequence alignment; reading understanding
ACM Classification Keywords
H.5.m [Information interfaces and presentation (e.g., HCI)]: Miscellaneous; H.5.2 [User Interfaces (D.2.2, H.1.2, I.3.6)]: Input devices and strategies
1 Introduction
Reading is an everyday task we perform all day long. Many researches have been carried out on our reading behavior. It is now possible to estimate English skill level [3], to sum-
Eye gaze and text line matching for reading analysis
 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
UbiComp/ISWC ’15 Adjunct, September 7-11, 2015, Osaka, Japan.
Copyright 2015 © ACM 978-1-4503-3575-1/15/09...$15.00. http://dx.doi.org/10.1145/2800835.2807936
1227
marize a text [9] or to detect skimming behavior [1] by ana- lyzing the way our eyes are moving during reading. Accord- ing to [8], eye movements during reading are divided in two states: fixations and saccades. Fixation is when the eyes stare at a word during reading and last for about 250ms. Saccade is the quick movement of the eyes between two points of fixation.
During reading analysis, researchers have to deal with in- accuracy or miscalibration of an eye tracker [1]. Usually this issue is compensated by setting up specific conditions of experiment such as head fixation [6] or by analyzing our reading behavior statistically (using the average number
of words per line of the document) [4]. Another strategy is to analyze our reading behavior on a paragraph scale [7]. But it could be interesting to analyze our reading behavior line by line or word by word using a non invasive method. However in such conditions, these problems lead to a mis- location of the recorded eye gaze as illustrated in Figure 1. In such case, reading analysis will be difficult to perform since the gap between the recorded eye gaze and the text is greater than the line spacing. For a given line of fixations it is not obvious to know the corresponding text line. Several tasks in reading analysis process, such as counting read words or detecting difficult part in a text, would be much easier if it were possible to match the recorded eye gaze with the text because it would be possible to know which line or which word is read.
This paper proposes an algorithm to find the accordance between lines of gazes and lines of text. Our approach is based on the use of an eye tracker designed for the general public. First we will present the different steps of our algo- rithm and propose different alternatives to do the matching. Second we will present the experiment and the correspond- ing results. We will show we can use a sequence alignment
Figure 1: The mislocation of the eye gaze variate with the conditions of recording. If the mislocation is greater than the line spacing, the reading analysis cannot be done precisely.
algorithm to match the lines of gazes with the lines in the text and get an accuracy of 60% for the matching. Then we will conclude and discuss about ways to improve the algo- rithm.
2 Algorithm overview
Our algorithm is divided into four steps. The input of the algorithm is the raw eye gaze as in the example of Figure 1.
1. To detect fixations during reading.
2. To detect line breaks to gather fixations into lines in order to compare them with text lines.
3. To rate the similarity between a line of fixations and a line of the text. In this step we will present three different methods of rating.
4. The line matching where the line of fixations is matched with the corresponding text line.
1228
UBICOMP/ISWC '15 ADJUNCT, SEPTEMBER 7–11, 2015, OSAKA, JAPAN

WORKSHOP
  Figure 2: Output of the fixation detection algorithm
2.1 Fixation detection
Our eye movements during reading is not a continuous mo- tion but a succession of fixations and saccades. However the output of the eye tracker is a continuous recording. We use the fixation detection algorithm from [2] to extract fix- ations from the raw eye gaze. In this algorithm, the gazes which are near each others are gathered into fixations. The output of the algorithm is shown in Figure 2
2.2 Line break detection
When fixations are detected, we gather them into lines of fixations in order to compare them with the text lines. To find lines of fixations we try to detect the line breaks. A line break happen when we finish to read one line and start to read a new one. It can be detected when a large regres- sion occurs. Considering xf (i) as the x-coordinate of the fixation i, if
xf (i + 1)   xf (i) <  L < 0 (1)
then a line break is detected. L is chosen to detect the re- gression which correspond to a line break. The output of this algorithm is shown in Figure 3. Errors in segmentation can occur in case of rereading (going backward could be detected as a line break regression even though we are
Figure 3: Line break detection, each new color is a different line
rereading the same line), or in case of one or two words text lines (the regression will not be detected and the line we are reading will be merged with the next one).
2.3 Fixations line and text line comparison
In this section we will talk about three ways to compare a line of fixations with a line in the text. The output of each method of comparison is a score we will use in the final matching step.
2.3.1 Fixation number
First, we try a basic idea to match a line of fixations with a line of the text. For a given line of fixations we compare the number of fixations in this line Nf with the number of words in a line of the text Nw. Then, we compute the score
s=|Nf  Nw| (2) The line of fixations and the line in the text are similar if
there are as many fixations as words.
2.3.2 Line length
Another basic idea to match a line of fixations with a line in the text is to compare the length of these two lines. In this
1229
algorithm words were replaced by their center and the x- coordinate of the word is defined as the x-coordinate of its center. For a given line of fixations, we compute the dis- tance between the last and the first fixation, Lf and we compute the distance between the last and the first word in the line of the text Ll. Then we define the score
s=|Ll  Lf| (3) The line of fixation and the line in the text are similar if they
have the same length.
2.3.3 Sequence alignment
The principle is to find the best alignment between two sequences. We choose to align the x-position of words in the text with x-position of fixations by using Dynamic Time Warping (DTW) algorithm. Again, in this algorithm the x- coordinate of a word is defined as the x-coordinate of the center of the word. So we have two align two sequences:
x1 x2 x3 M= x01 0 1 1
x02 1 e f x03 1 g h
Figure 4: M matrix with for instance h = |x3-x’3| + Min (e,f,g)
as follows. The first line is filled with infinite value, the first column is also filled with infinite values. M (0, 0) is filled with the value 0. Then for each position in the matrix M a score is defined as
M[i, j] = d + min(M[i   1, j], M[i, j   1], M[i   1, j   1]) (7)
This methodology is maintained until we reach the coef- ficient [n, m] of the matrix M (low right corner). This last value represents the score of the matching between the line of fixation Xf and the line in the text Xt.
s = M[n,m] (8)
2.4 Line matching
The point of this step is to find the best matching between one line of fixations and several lines of text. This match- ing is based on the scores from the previous step. For a given line of fixation we will compare it with the nearest n text lines and try to find which one correspond to this fix- ations line. The mislocation due to the inaccuracy of the eye tracker can vary but is not greater than a couple of cen- timeters. According to this measure, we choose n = 5. The y-coordinate of a line is considered as the center of
UBICOMP/ISWC '15 ADJUNCT, SEPTEMBER 7–11, 2015, OSAKA, JAPAN
     and
Xf =x1,x2,x3...xn (4) Xt =x01,x02,x03...x0m (5)
In the DTW algorithm it is necessary to define a "cost of alignment".
In our case the cost of alignment between a fixation xiand a word x0j is defined as
d = |xi   x0j | (6)
which is the distance in pixel between the fixation i and the word j. With these two sequences we compute a matrix M (n, m) as in Figure 4. And the matrix M has to be filled
1230
WORKSHOP
 Algorithm
  % of good line matching
 Number of fixations
  39%
 Line length
  56%
 DTW
  60%
 its bounding box. Then we look at the pair (fixations line- text line) with the minimum score (according to the previous step) to find the best matching.
3 Experiment and Results
For the experiment, 8 participants were asked to read 3 different texts. The distance between the screen and the subjects was between 60 and 70 centimeters. Readers were asked to read carefully forward the text. In order to avoid errors due to errors of segmentation during the line break detection step, we selected only the well segmented lines as input of the algorithm. Then we manually labelled the position of each fixations line. The number of selected lines is 138. A good matching is detected when the line of fixations is matched with the corresponding line in the text according to the ground truth. The eye tracker employed in this experiment is a Tobii EyeX Controller 1.
The accuracy of the algorithm is computed as the percent- age of all good matches on a total amount of 138 lines. Comparison between the three methods of matching is shown in Table 1. The accuracy of the DTW alignment method is better than comparing the number of fixations with the number of lines. It is slightly better than comparing the length of line of fixations with the length of the line in the text. We can see a visualization of an alignment with a high score in Figure 6 and a visualization of an alignment with a low score in Figure 5. Both using DTW algorithm.
3.1 Results analysis
Comparing the number of fixations with the number of words is not very accurate because there is not necessarily one fixation per words. We tend to skip small words during reading which can affect this method of comparison. More- over confusions can occur when successive lines contains
1[http://www.tobii.com/]
Table 1: Results of the algorithm with three different methods of matching
Figure 5: Result of an alignment with a low score using DTW algorithm.
the same number of words.
Comparing the length of the fixations line with the length of the text line is more accurate, but if the text is justified all the lines will have the same length. Then it will be very hard to match a line of fixations with the corresponding text line using this method.
Using DTW for the matching is more accurate, because the number and the position of the fixations are both taken into account. Even if some small words does not have any corresponding fixation or some long words have more than one fixations, these differences can be absorbed by the elasticity property of the DTW. But this is true only for small changes and have its limits. And, still, some text lines can be very similar to each other and introduce some confu- sions for the DTW.
4 Conclusion and discussion
We have presented a way to match recorded fixations with the text using the DTW. Our algorithm can match a line of
      1231
Figure 6: Result of an alignment with a high score using DTW algorithm.
fixations with the corresponding text line with an accuracy of 60%. In this paper we focused on finding the correct matching between one line of fixations with several line of text. This algorithm can be used for estimating and cor- recting the global mislocation for one specific document. We know that a majority of fixation lines are successfully matched to the correct text lines with the presented algo- rithm. So, the corresponding vertical translation can be ap- plied to adjust all the fixations of the document closer to the correct lines.
A way to enhance our results is to use psychological read- ing behavior. For instance, according to [8] we only read 80% of contents words and 35% of function words in a whole text. Moreover, the x-position of a word in the text
is defined as the x-position of the center of the word. But according to [5], the fixation are not exactly located at the center of the word.
Another different perspective is to use the DTW for applying an horizontal alignment. As we can see in Figure 6, it might be possible to match each fixation with the corresponding word. But for now we can’t evaluate it because no ground truth is available at the word level.
5 REFERENCES
1. Ralf Biedert, Jörn Hees, Andreas Dengel, and Georg Buscher. 2012. A robust realtime reading-skimming classifier. In Proceedings of the Symposium on Eye
Tracking Research and Applications. ACM, 123–130.
2. Georg Buscher, Andreas Dengel, and Ludger van Elst. 2008. Eye movements as implicit relevance feedback. In CHI’08 extended abstracts on Human factors in computing systems. ACM, 2991–2996.
3. Kai Kunze, Hitoshi Kawaichi, Kazuyo Yoshimura, and Koichi Kise. 2013a. Towards inferring language expertise using eye tracking. In CHI’13 Extended Abstracts on Human Factors in Computing Systems. ACM, 217–222.
4. Kai Kunze, Hitoshi Kawaichi, Kazuki Yoshimura, and Kenji Kise. 2013b. The wordometer–estimating the number of words read using document image retrieval and mobile eye tracking. In Document Analysis and Recognition (ICDAR), 2013 12th International Conference on. IEEE, 25–29.
5. George W McConkie, Paul W Kerr, Michael D Reddix, and David Zola. 1988. Eye movement control during reading: I. The location of initial eye fixations on words. Vision research 28, 10 (1988), 1107–1118.
6. Takehiko Ohno. 2004. EyePrint: Support of document browsing with eye gaze trace. In Proceedings of the 6th international conference on Multimodal interfaces. ACM, 16–23.
7. Ayano Okoso, Kai Kunze, and Koichi Kise. 2014. Implicit gaze based annotations to support second language learning. In Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication. ACM, 143–146.
8. Keith Rayner. 1998. Eye movements in reading and information processing: 20 years of research. Psychological bulletin 124, 3 (1998), 372.
1232
UBICOMP/ISWC '15 ADJUNCT, SEPTEMBER 7–11, 2015, OSAKA, JAPAN

WORKSHOP
9. Songhua Xu, Hao Jiang, and Francis Lau. 2009. User-oriented document summarization through vision-based eye-tracking. In Proceedings of the 14th international conference on Intelligent user interfaces. ACM, 7–16.
 Psychological Bulletin Copyright 1998 by the Americi i Psychological A ssociation,Inc.
1998, V ol. 124, No. 3, 372-422
Eye Movementsin Reading and Information Processing: 20 Years of Research
Keith Rayner
U niversity of M assachusetts at A m herst
Recent studies of eye movements in reading and other information processing tasks, such as music reading, typing, visual search, and scene perception, are reviewed. The major emphasis of the review is on reading as a specific example of cognitive processing. Basic topics discussed with respect to reading are (a) the characteristics of eye movements, (b) the perceptual span, (c) integration of information across saccades, (d) eye movement control, and (e) individual differences (including dyslexia). Similar topics are discussed with respect to the other tasks examined. The basic theme of the review is that eye movement data reflect moment-to-moment cognitive processes in the various tasks examined. Theoretical and practical considerations concerning the use of eye movement data are also discussed.
0033-2909/98/$3.00
M any
processes have appeared over the past 20 years. In an earlier review, I (Rayner, 1978b) argued that since the mid-1970s we have been in a third era of eye movement research and that the success of research in the current era would depend on the ingenuity of researchers in designing interesting and inform ative studies. It w ould appear from the vast num ber of studies using eye movement data over the past 20 years that research in this third era is fulfilling the promise inherent in using eye movement behavior to infer cognitive processes. The first era of eye move- ment research extended from Javal's initial observations con- cerning the role of eye m ovem ents in reading in 1879 (see Huey, 1908) up until about 1920. D uring this era, m any basic facts about eye m ovem ents w ere discovered. Issues such as saccadic suppression (the fact that we do not perceive information during an eye movement), saccade latency (the time that it takes to initiate an eye m ovem ent), and the size of the perceptual span (the region of effective vision) were of concern in this era. The second era, w hich coincided w ith the behaviorist m ovem ent in experim ental psychology, tended to have a m ore applied focus, and little research w as undertaken w ith eye m ovem ents to infer cognitive processes. Although classic work by Tinker (1946) on reading and by Buswell (1935) on scene perception was carried out during this era, in retrospect, m ost of the w ork seem s to have focused on the eye movements per se (or on surface aspects of the task being investigated). Tinker's (1958) final review ended on the rather pessim istic note that alm ost every-
Preparation of this article was supported by a Research Scientist Award from the National Institute of Mental Health (MH01255) and by Grants HD 17246 and HD 26765 from the National Institutes of Health. Thanks are extended to Ken Ciuffreda, Charles Clifton, David Irwin, and Alexander Pollatsek for their helpful comments on prior versions of this article.
Correspondence concerning this article should be addressed to Keith Rayner, Department of Psychology, University of Massachusetts, Am- herst, Massachusetts 01003. Electronic mail may be sent to rayner@ psych.umass.edu.
studies
using eye m ovem ents to investigate cognitive
thing that could be learned about reading from
(given the technology at the time) had been discovered. Perhaps that opinion was widely held, because between the late 1950s and the mid-1970s little research with eye movements was undertaken.
The third era of eye movement research began in the mid- 1970s and has been m arked by im provem ents in eye m ovem ent recording system s that have allow ed m easurem ents to be m ore accurate and more easily obtained. It is beyond the scope of the present review to detail all of the technological advancements that have been made. Numerous works have dealt with methods of analyzing eye movement data (see Kliegl & Olson, 1981; Pillalam arri, Barnette, Birkmire, & Karsh, 1993; Scinto & Bar- nette, 1986), and much has been learned about the characteris- tics of various eye-tracking systems (see Deubel & Bridgeman, 1995a,1995b;MulletCavegn,d'Ydewalle,&Groner,1993). M ore im portant, the era has yielded trem endous technological advances that have made it possibleto interface laboratory com-
puters with eye-tracking systems so that large amounts of data can be collected and analyzed. These technological advances have also allow ed for innovative techniques to be developed in which the visual display is changed contingent on the eye posi- tion. In the eye-contingent display change paradigm (M cCon- kie, 1997; M cConkie & Rayner, 1975; Rayner, 1975b; Reder, 1973), eye movements are monitored, and changes are made in the visual display that the reader is looking at, contingent on when the eyes move (or at some other critical point in the fixation). Finally, the development of general theories of lan- guage processing has m ade it possible to use eye m ovem ent records for a critical exam ination of the cognitive processes underlying reading.
In the present article, recent studies of eye m ovem ents in reading and other information processing tasks are examined. Since the last review in this journal (Rayner, 1978b), there have been many reviews of eye movement research (Kennedy, 1987; LeVy-Schoen & O'Regan, 1979; O'Regan, 1990; Pollatsek, 1993; Rayner, 1984, 1993, 1995, 1997; Rayner & Pollatsek, 1987, 1992; O. Underwood, 1985). However, none of them are
372
eye m ovem ents
 com prehensive: Som e deal only w ith reading (or scene percep- tion), and most of them advocate a particular point of view. The goal of the present article is to provide a comprehensive review of eye movement research over the past 20 years. Since 1978, m any eye m ovem ent studies have appeared in edited books. Because the quality of the research reported in such books is somewhat variable, the focus of the current review is on studies that have appeared in peer-reviewed journals.
Basic Characteristics of Eye Movements in Information Processing
When we read, look at a scene, or search for an object, we continually make eye movements called saccades. Between the saccades, our eyes remain relatively still during fixations for about 2 0 0 -3 0 0 m s. There are differences in these tw o m easures as a function of the particular task (see Table 1). Saccades are rapid m ovem ents of the eyes w ith velocities as high as 500° per second. Sensitivity to visual input is reduced during eye movements; this phenomenon is called saccadic suppression (Matin, 1974) and has been the topic of considerable debate. We do not obtain new information during a saccade, because the eyes are m oving so quickly across the stable visual stim ulus that only a blur w ould be perceived (U ttal & Sm ith, 1968). More important, however, masking caused by the information available before and after the saccade makes it such that we do not perceive any type of blurring effect (Brooks, Impelman,& Lum, 1981; Campbell & Wurtz, 1979; Chekaluk & Llewellyn,
1990). However, some suppression is found even when masking is elim inated (R iggs, M erton, & M orton, 1974), w hich suggests that there is a central inhibitory contribution to saccadic sup- pression as w ell.
The velocity of the saccade is a monotonic function of how far the eyes move; it rapidly rises during the saccade to a maxi- mum that occurs slightly before the m idpoint of the m ovem ent and then drops at a slightly slower rate until the target location is reached. The duration of a saccade is also influenced by the distance covered; a 2° saccade typical of reading takes around 30 ms, whereas a 5° saccade, typical of scene perception, takes around 40-50 ms (R. A. Abrams, Meyer, & Kornblum, 1989; Rayner, 1978a).
An issue that has received considerable attention recently is
whether or not cognitive processing activities are suspended during a saccade (Boer & van der Weijgert, 1988; W.Hansen & Sanders, 1988; tw in & Carlson-Radvansky, 1996; Irwin, Carl- son-Radvansky, & Andrews, 1995; Matin,Shao, & Boff, 1993; Sanders & Houtmans, 1985; Sanders & Rath, 1991; Van Duren, 1993; V an Duren & Sanders, 1992,1995). Some of these studies have found evidence to suggest that some cognitive activities are suppressed during saccades. Because the tasks used in most of these studies are relatively sim ple, it w ill be interesting to determ ine w hether thinking is suspended during saccades in more complex tasks, such as reading and scene perception. Cer- tainly, people are not aware of pauses in mental activity during eye movements, but because saccade durations are so brief, any disruptions might not be particularly salient. Recently, Irwin (1998) reported some studies demonstrating that lexical pro- cessing is not suppressed during saccades.
Saccades need to be distinguished from three other types of eye movements: pursuit, vergence, and vestibular eye move- ments. Pursuit eye movements occur when our eyes follow a moving target; the velocity of pursuit eye movements is mark- edly slower than saccades and, if the target is moving quickly across our visual field, w e often m ake saccades to catch up w ith the target (White, 1976). Vergence eye movements occur when we move our eyes inward, toward each other, in order to fixate on a nearby object. Vestibular eye movements occur when the eyes rotate to com pensate for head and body m ovem ents in order to m aintain the sam e direction of vision. A lthough pursuit, vergence, and vestibular eye m ovem ents are im portant and ex- tensively studied (see Ciuffreda & Tannen, 1995, and Leigh & Zee, 1991, for more details), saccadic eye movements are more relevant in typical information processing tasks.'
Finally, three types of small movements of the eyes need to be m entioned: nystagm us, drifts, and m icrosaccades. A lthough researchers interested in eye m ovem ents in inform ation pro- cessing tasks typically discuss fixations as the period of time when the eyes are still, the term fixation is something of a misnomer. The eyes are never really still, because there is a constant tremor of the eyes called nystagmus. Such tremors of the eyes are quite small, and their exact nature is somewhat unclear, though it is often assumed that the movements are re- lated to perceptual activity and help the nerve cells in the retina to keep firing. D rifts and m icrosaccades tend to be som ew hat
1
Reading, Visual Search, Scene Perception, Music Reading, and Typing
T ask
duration
(m s)
Silent reading
Oral reading
V isual search
Scene perception
M usk reading 375 Typing 400
2 (about 8 letters) 1.5 (about 6 letters) 3
4
M ean fixation
M ean saccade size (degrees)
1 (about 4 letters)
Note. Values are taken from a number of sources and vary depending on a number of factors (see Rayner, 1984).
225 275 275 330
EYE MOVEMENTS IN READING 373
From a number of studies, it is known that the coordination of horizontal and vertical saccades is very accurate in adults when the ApproximateMeanFixationDurationandSaccadeLengthin stimulusisavisuallypresentedtarget(Bains,Crawford,Cadera,&Vilis, 1992; Collewijn, Erkelens, & Steinman, 1988a, 1988b). It is typically assumed that the eyes move conjugately during tasks like reading. How- ever, Collewijn et al. (1988a) found that the movements of the abducting (temporally moving) eye are somewhat larger than the corresponding movements of the adducting (nasally moving) eye in simple scanning tasks. Similar observations were reported by Heller and Radach (1995) for reading and other complex tasks. Bassou, Pugh, Granie, and Morucci (1993) and Ygge and Jacobson (1994) reported that although the eyes of fifth-grade readers tended to be well-coordinated spatially, they were not always closely conjugated spatially. Henriks (1996) recently exam- ined vergence movements of skilled readers and found that the eyes converge during reading. As she noted, this finding is in contrast to earlier reports (E. A. Taylor, 1966) in which the eyes were found to
diverge during reading.
Table 1
 374 RAYNBR
larger m ovem ents than the nystagm us m ovem ents. A lthough the reasons for these movements are not completely clear, it appears that the eyes occasionally drift (i.e., make small, slow move- m ents) because of less-than-perfect control of the oculom otor system by the nervous system. When this happens, there is often a small microsaccade (a much more rapid movement) to bring the eyes back to w here they w ere. M ost experim enters interested in reading assume that these small movements are "noise" and adopt scoring procedures that ignore them. For example, most researchers lump together successive fixations that are on adja- cent characters as a single fixation. Another alternative is a more sophisticated procedure in which fixations are pooled if the two fixations are on adjacent characters and one is short (i.e., less than 100ms).
Saccade Latency
provided by the fixation-point offset; visual offset is more effec- tive than a neutral (auditory) cue, which in turn is more effective than onset of visual stimulation at the fixation point (Findlay, 1992). This pattern of results suggests that some process that may be termed relinquishing of attention is involved in generating a saccade. Interest in these results has intensified recently because of the suggestion that there is a separate category of saccades, called express saccades, with very short latencies (Cavegn & d'Ydewalle, 1996;B. Fischer, 1992;B. Fischer & Boch, 1983; B. Fischer & Rampsperger, 1984; B. Fischer & Weber, 1993). At the moment, there is some debate about (a) whether or not there is a bim odal distribution of saccade latencies (w ith express saccades representing one peak of the distribution and normal saccades representing the other) and (b) the phenomenon in general (Find- lay, 1992;Kingstone & Klein, 1993b; A. B. Sereno, 1992). Even if there were no controversy surrounding express saccades per se, there are questions about the functional utility of such short- latency saccades for normal information processing tasks (M. H. Fischer & Rayner, 1993): Inhoff, Topolski, Vitu, and O'Regan (1993) found no evidence for express saccades or a bimodal distribution of fixation durations during reading.
The Visual Field and Acuity
W e m ake saccades so frequently because of acuity lim itations. As we look straight ahead, the visual field can be divided into three regions:foveal, parafoveal, andperipheral. Although acu- ity is very good in the fovea (th e central 2° of vision), it is not nearly so good in the parafovea (which extends out to 5° on either side of fixation), and it is even poorer in the periphery (the region beyond the parafovea). Hence, we move our eyes so as to place the fovea on that part of the stimulus we want to see clearly. Of course, characteristics of the stimulus in parafo- veal or peripheral vision influence whether or not a saccade needs to be made to identify it. For example, if a word of normal- size print is presented in parafoveal vision, it is identified more quickly and accurately when a saccade is made (Jacobs,1986, 1987a; Rayner & M orrison, 1981). However, if an object or large letter is presented as the stim ulus, it can often be identified in peripheral vision without a saccade (Pollatsek, Rayner, & Collins, 1984). Sanders (1993) showed that the visual field can be divided into regions where (a) a stimulus can be identified w ithout an eye m ovem ent, (b) it is necessary to m ake an eye movement to identify the stimulus, and (c) it is necessary to m ake a head m ovem ent to identify the stim ulus.
Eye Movements and Attention
There is a latency
because they are motor movements that require time to plan and execute. Even if uncertainty about when or where to move the eyes is eliminated, saccade latency is at least 150-175 ms (R. A. Abrams & Jonides, 1988; Rayner, Slowiaczek, Clifton, & Ber- tera, 1983; Salthouse & Ellis, 1980; Salthouse, Ellis, Diener, & Somberg, 1981), which suggests that saccade programming is done in parallel with comprehension processes in reading.
Studies dealing with saccade latency are legion, and a complete review is beyond the scope of the present article. However, some im portant facts have been learned about saccade latency that m ay be relevant to understanding eye movement behavior in informa- tion processing tasks (Becker & Jurgens, 1979; Crawford,1996; Findlay, 1992; Findlay & Walker, in press; Heywood & Churcher, 1980). First, there are separate decision processes involved in com puting w hen and w here to m ove the eyes (A slin & Shea, 1987; Becker & Jurgens, 1979). Second, although saccades in simple reaction-time experiments are often characterized as re- flexive, there is also evidence that cognitive processes can influ- ence the latency (Deubel, 1995). For example, in the antisaccade paradigm (Hallett, 1978), saccades are voluntarily directed away from a peripheral target, and latencies increase. Likewise, when
a number of saccades are planned in a sequence, latencies of the initial saccade increase (Crawford, 1990; Inhoff, 1986; Zingale & Kowler, 1987); as eccentricity of the target increases, latencies can increase (Kalesnykas & Hallett, 1995). Third, increasing the saccade latency generally leads to increased accuracy in locating
a target (Jacobs, 1987a; Nazir & Jacobs, 1991). Fourth, when saccades are made to targets consisting of two elements, inrea- sonably close proximity, the first saccade goes to some intermedi- ate location. This is referred to as the global or center of gravity effect (Deubel, Wolf, & Hauske, 1984; Findlay, 1982; Ones,Van Ginsbergen, & Eggermont, 1984). If one element is larger or more intense (or brighter), then the saccade tends to land closer to that target in comparison to a condition in which the two elem ents are identical. Instructions to be m ore careful influence where the eyes land (CoSfftS & O'Regan, 1987; Kowler & Blaser, 1995) but also increase saccade latency. Fifth, when a fixation point disappears prior to the appearance of a target, latency de- creases (M. E. Cohen & Ross, 1977;Kingstone & Klein, 1993a; L. E. Ross & Ross, 1980;S. M. Ross & Ross, 1981). This speeding up (called the gap effect) relates to the tem poral w arning
period
associated
w ith
m aking a
saccade,
A lthough it is often necessary to m ove
objects in our environment, we can move attention without mov- ing our eyes (Posner, 1980). The relationship between attention and eye movements has been extensively investigated (M. H. Fischer, in press; Klein, 1980;Klein, Kingstone, & Pontefract, 1992; Rafal, Calabresi, Brennan, & Sciolto, 1989; Remington, 1980; Reuter-Lorenz & Fendrich, 1992;Shepherd, Findlay, & Hockey, 1986)but is beyond the scope of the present review. However, with complex stimuli, it is more efficient to move our eyes than to move attention (He & Kowler, 1992;Sclingensie-
our eyes to
identify
 pen, Campbell, Legge, & Walker, 1986). Furthermore, there is evidence suggesting that attention precedes a saccade to a given location in space (Hoffman & Subramaniam, 1995; Kowler, Anderson, Dosher, & Blaser, 1995; Rayner, McConkie, & Ehr- lich, 1978; Remington, 1980) and that attentional movements and saccades are obligatorily coupled (Deubel & Schneider, 1996; but see Stelmach, Campsall, & Herdman, 1997, for con- flicting data). Although we can easily decouple the locus of attention and eye location in simple discrimination tasks (Posner, 1980), in complex information processing tasks such as reading, the link between the two is probably quite tight.
Developmental Changes in Eye Movements
The characteristics of children's eye movements differ some- what from those of adults. Preschool children exhibit more fre- quent small saccades and drifts during maintained fixation; sac- cadic latency is usually longer, and saccade accuracy is usually less precise in preschool children than in adults when scanning a scene (Kowler & Martins, 1985). However, the shapes of the frequency distributions of fixation durations for children, adults, and even infants are quite similar (Hainline, Turkel, Abramov, Lemerise, & Harris, 1984; Harris, Hainline, Abramov, Lemer- ise, & Camenzuli, 1988). Although frequency distributions of fixation durations of elderly adults look like those of younger adults, saccade latency increases with age (Abel, Troost, & Dell'Osso, 1983; Pirozzolo & Hansch, 1981).
Measuring Eye Movements
Eye movements are monitored in many different ways. Eye- tracking systems are currently in use that rely on (a) surface electrodes (which are fairly good at measuring saccade latency but not good at measuring location), (b) infrared corneal re- flections, (c) video-based pupil monitoring, (d) infrared Pur- kinje image tracking, and (e) search coils attached like contact lenses to the surface of the eyes. Although there has been some discussion concerning the measurement, evaluation, and re- porting of eye movement data (Harris, Abramov, & Hainline, 1984; Heller, 1983; Inhoff & Radach, 1998; McConkie, 1981; McConkie, W olverton, & Zola, 1984; Nodine, Kundel, Toto, & Krupinski, 1992), no measurement standards have been adopted, and many methodological issues remain unaddressed or unresolved (see Inhoff & Radach, 1998, for a good discussion of these issues). Despite this fact, mostof the important findings discussed in this review have been replicated across different labs.
Eye Movements in Reading
Eye movements differ somewhat for reading silently versus aloud (see Table 1). When reading aloud, or when reading silently while listening to a voice reading the same text, mean fixation durations are longer than in silent reading, and the eyes tend to get ahead of the voice; consequently, there are many fixations in which the eyes appear to be holding in place so as to not get too far ahead of the voice (Levy-Schoen, 1981). The vast majority of the research on reading reviewed here is for silent reading.
When reading English, eye fixations last about 200-250 ms and the mean saccade size is 7-9 letter spaces (see Table 1). Letter spaces are the appropriate metric to use, because the number of letters traversed by saccades is relatively invariant when the same text is read at different distances, even though the letter spaces subtend different visual angles (Morrison, 1983; Morrison & Rayner, 1981; O'Regan, 1983; O'Regan, Levy-
2
Schoen, & Jacobs, 1983) . The primary function of a saccade
is to bring a new region of text into foveal vision for detailed analysis, because reading on the basis of only parafoveal or peripheral information is difficult to impossible (Rayner & Bert- era, 1979; Rayner, Inhoff, Morrison, Slowiaczek, & Bertera, 1981). Whereas a majority of the words in a text are fixated during reading, many words are skipped so that foveal pro- cessing of each word is not necessary. For example, content words are fixated about 85% of the time, whereas function words are fixated about 35% of the time (Carpenter & Just, 1983; Rayner & Duffy, 1988). Function words are fixated less frequently than content words, because they tend to be short, and there is a clear relationship between the probability of fixating a word and its length: As length increases, the probability of fixating a word increases (Rayner & McConkie, 1976); 2-3 letter words are only fixated around 25% of the time, whereas words 8 letters or longer are almost always fixated (and often fixated more than once).
Although most saccades in reading English are made from left to right, readers do not relentlessly go forw ard: A bout 10- 15% of the saccades are regressions (right-to-left movements along the line or movements back to previously read lines). Many regressions tend to be only a few letters long and could be due to the reader making too long of a saccade, in which case a short saccade to the left may be necessary for reading to proceed efficiently. Short within-word regressive saccades may also be due to problems that the reader has processing the currently fixated word. Longer regressions (more than 10 letter spaces back along the line or to another line) occur because the reader did not understand the text. In such cases, good readers are very accurate in sending their eyes to that part of text that caused them difficulty (Frazier & Rayner, 1982; Kennedy, 1983; Kennedy & M urray, 1987a, 1987b; M urray & Kennedy, 1988), whereas poor readers engage in more backtracking through the
text (M urray & Kennedy, 1988).
On return sweeps from the end of one line to the beginning
of the next, readers often undershoot and make small corrective movements to the left. Because corrective saccades are often made following return sweeps, it should not be assumed that readers place their fixation to correspond to the beginning of a line. Rather, the first and last fixations on a line are generally 5-7 letter spaces from the ends of a line. Thus, about 80% of the text typically falls between the extreme fixations. The first fixation on a line tends to be longer than other fixations (Heller,
1982; Rayner, 1977), and the last is shorter (Rayner, 1978b).
2
However, it should be noted that fixation rime increases somewhat when the visual angle subtended by a letter is appreciably smaller than in normal reading. This is because the letters are more difficult to dis- crim inate when the text is further away from die eyes. Thus, letter spaces are the appropriate metric of saccade size for relatively normal-sized print; if the text is too small or too large, the principle would not hold.
EYE MOVEMENTS IN READING 375
 376 RA YNER
Furthermore, readers tend to not fixate in the blank spaces be- tween sentences (S. G. Abrams & Zuber, 1972; Rayner, 1975a). A lthough average values for fixation duration, saccade length, and frequency of regression are cited above, there is consider- able betw een-reader variability for all three m easures. M ore important, there is variability for a given reader within a single passage of text so that fixation durations range from under 100 ms to over 500 ms, and saccades vary from 1 to over 15 letter spaces (see Figure 1). A ctually, fixations as short as 50 m s som etim es appear in the eye m ovem ent record during reading. Saccades as long as 15 letter spaces are quite rare and often occur im m ediately follow ing a regression in w hich readers typi- cally make a long saccade to place the eyes ahead of where they
were prior to making the regression.
Eye m ovem ents are also influenced by textual and typographi-
cal variables. For exam ple, as text becom es conceptually m ore difficult, fixation duration increases, saccade length decreases, and the frequency of regressions increases (Jacobson & Dod-
well, 1979; Rayner & Pollatsek, 1989). If the text looks fairly norm al, typographical variables tend to have a relatively m inor influence. However, factors such as the quality of the print (vari- ations in fonts), line length, and letter spacing (Kolers, Duch- nicky, & Ferguson, 1981; Morrison & Inhoff, 1981) influence eye movements. Characteristics of the writing system also in- fluence eye movements (Osaka, 1989; Peng, Orchard, & Stern, 1983; Sun, Morita, & Stark, 1985).
A crucial point that has emerged recently is that eye move- m ent m easures can be used to infer m om ent-to-m om ent cogni- tive processes in reading (Just & Carpenter, 1980; M cConkie, Hogaboam, W olverton, Zola, & Lucas, 1979; Rayner, 1978b; Rayner, Sereno, Morris, Schmauder, & Clifton, 1989) and that the variability in the measures reflects on-line processing. For exam ple, there is now abundant evidence that the frequency of a fixated word influences how long readers look at the word (Inhoff & Rayner, 1986; Rayner & Duffy, 1986). Thus, proper- ties of the fixated word modulate the fixation time and result in variability in fixation tim es. There is, of course, a purely m otoric com ponent to this variability (K ow ler & A nton, 1987), because when spatial and temporal uncertainty about where and when to move the eyes is eliminated, there is still variability in the latency of eye movements (Rayner, Slowiaczek, et al., 1983; Salthouse & Ellis, 1980). Similarly, there is variability in where the eyes land on a target (Cogfte & O'Regan, 1987; Findlay,
1982). A lthough this noise of m otoric variability m akes it diffi- cult to interpret the cognitive signal in the eye m ovem ent record, it is clear that the signal is there, and great strides have been made over the past 20 years in understanding the relationship between reading and eye movements. One simple point consis- tent with this claim is that, in reading, there is no correlation between fixation duration and saccade length (Rayner & McConkie, 1976), whereas in nonreading situations, in which the necessity of linguistic processing is eliminated, there is a correlation: the longer the saccade, the longer the next fixation (Kapoula, 1983; Nattkemper & Prinz, 1986). The difference between these two situations suggests that the difficulty of on- line language processing associated with reading wipes out any simple correlation. However,it is important to note thatalthough there is no correlation between fixation duration and saccade length across extended text, correlations can be obtained locally between a given fixation duration and saccade length (Pollatsek, Rayner, & Balota, 1986), indicating that processing difficulty influences both eye m ovem ent variables.
What Is the Best Measure of Processing Time?
A major issue concerns how to best summarize the eye move- ment record to understand cognitive processing. This issue is particularly relevant to the question of how to m easure the tem - poral processing associated with a given region of text. If the unit of analysis is larger than a w ord, then the total first-pass fixation tim e on that unit is generally used as the prim ary m ea- sure of interest. It is im portant, w hen analyzing such regions, to distinguish between first-pass (i.e., the initial reading con- sisting of all forw ard fixations) and second-pass (i.e., rereading) reading time for the region. There is some controversy about how best to analyze a region w hen readers m ake regressions (see Altmann, 1994; Rayner & Sereno, 1994b, 1994c). For example,
14-
12-
10-
)
I- i
• 6-
4-
2-
12-1
no 8
100
200 300 400 Fixation Duration (ms)
5 10 15 20 25 Saccade Length
Figure 1. Frequency distributions for fixation durations and forward saccade lengths. Fixation durations are in milliseconds, and saccade length is in character spaces.
500 600
 Rayner and Sereno (1994c) noted that, when readers enter a
region and then quickly make a regression out of that region,
the first-pass tim e is very short in com parison to w hen the
reader does not regress. How best to analyze such regressions
is uncertain. Some researchers simply record first-pass reading
time and then examine the pattern of regressions out of the
target region. Others have argued for regression-path durations
analyses (Konieczny, Hemforth, Scheepers, & Strube, 1997) or
cumulative region reading time analyses (Brysbaert & Mitchell,
1996). W ith these analyses, reading tim e represents the sum of
all fixations starting with the first fixation in a region and ending
with the first forward saccade past the region under consider-
3
ation (see Liversedge, Paterson, & Pickering, 1998, for further
discussion of these issues).
Issues have also arisen concerning the fact that comparisons
often are made between regions of text that contain different words, with some regions being longer than others. One proce- dure is to divide the reading time (first pass or otherwise) by the number of letters in the region to yield a mittisecond- per-character reading time. However, this procedure has been dem onstrated to yield a nonlinear function of w ord length on reading time, especially in short regions (Trueswell, Tanen- haus, & Garnsey, 1994). Thus, when the length of a region differs, a m ore appropriate procedure is to analyze the deviations from expected reading tim es as determ ined by the best linear fit for the reading time as a function of the number of letters in the region (Rerreira & Clifton, 1986; Trueswell et al., 1994).
W hen a w ord is the unit of analysis, then the appropriate m easure to use is also controversial (Inhoff & R adach, 1998). If readers always were to make one and only one fixation on a word, then there would be little problem: The fixation duration on the word would be the measure of processing time.However, words are sometimes fixated more than once, and sometimes they are skipped. The problem of multiple fixations has led to different solutions. Using the mean fixation duration on a word is clearly inadequate, because it underestim ates the tim e the eyes are actually on the word (i.e., a 200-ms fixation and a 150-ms fixation on the same word would yield a mean fixation duration on the word of 175 m s). Likewise, the strategy of including only words that are fixated once {single fixation duration) is problematic, because some words are fixated more than once, and some are skipped altogether; such a strategy may result in the elimination of too much data. Thus, the two most frequently used measures are the first fixation duration and the gaze dura- tion on a word. Gaze duration represents the sum of all fixations made on a word prior to a saccade to anotherword.Firstfixation duration is the duration of the first fixation on a word regardless of whether it is the only fixation on a word or the first of multiple fixations on a word.
The argument over which measure is best to use as an index of processing time partly depends on what is being examined. Inhoff (1984) argued that first fixation duration and gaze dura- tion measure different processes. In his data, first fixation dura- tion and gaze duration both were affected by word frequency, but only gaze duration was affected by the predictability of the word in the context. He thus posited that first fixation is a measure of lexical access, whereas gaze duration reflects text integration processes as well. However, it now appears that this distinction does not hold up in general and that for much of
the tim e, although not alw ays, first fixation duration and gaze duration yield similar results. Rayner and Pollatsek (1987) ar- gued that the data suggest that if a cognitive operation is very fast it affects first fixation duration, and if it is a bit slower it may affect gaze duration. Assuming that the gaze versus first fixation duration problem were solved, there is still the problem of trying to assess the average time spent processing a word. The problem is that words are processed when they are not fixated. Fisher and Shebilske (1985) showed this by having readers read text as their eye movements were monitored. They then had another group read the same text after deleting the words that the first group skipped over (function words and other short words). The second group had a difficult time under- standing the text.
Just and Carpenter (1980) assigned a value of 0 ms to words that were not fixated in their gaze duration analysis. However, this procedure really does not solve the problem, because words that are not fixated are clearly processed by the reader (generally on the fixation prior to the skip). Subsequently, as a means of dealing with word skipping, Carpenter and lust (1983) proposed a conditionalized gaze duration, which is the mean gaze dura- tion, given that the word has been fixated for at least 50 ms. The problem with this measure is that it ignores the time spent in a fixation processing words to the right of the fixated word. As discussed in detail later in this review, when the word to the right of fixation is not identified, some initial parafoveal processing occurs. Rayner and Pollatsek (1987) suggested that conditionalized gaze duration would be better if conditionalized on fixating the word for 50 ms and not skipping the next word. A second solution to the problem is the read to the right of gaze (RRG) algorithm (Blanchard, 1985; Hogaboam & McConkie,
1981). The RRG measure sums fixations on words receiving more than one fixation; when words are skipped, the fixation time is equally distributed between the last word fixated and the word that was skipped. Both the conditionalized gaze and the RRG measures assume that (except for skipping) a word is processed only when it is fixated. However,asjust noted, parafo- veal information is extracted from a word on most fixations and facilitates identification on the subsequent fixation. Rayner and Pollatsek (1987) suggested that perhaps this preview benefit should be added to the fixation time on the word and should be subtracted from the tim e spent on the prior w ord. A ll of this assumes that these processes can be precisely identified and that words are processed in series. If some processes overlap, then the calculation of processing times would be even more com- plex. In addition, there is the problem that the processing of a word is not always completed by the time the eyes move, as there are spillover effects: Time processing a word can "spill over" onto the next word (Rayner & Duffy, 1986; Rayner, Sereno, et al., 1989). To the extent that the processing of a word spills over onto subsequent fixations (McConkie, Zola, & Blanchard, 1984; Rayner, Sereno, et al., 1989), these measures are in error.
It thus appears that any single measure of processing time per word is a pale reflection of the reality of cognitive processing.
'R ayner and Duffy (1986) and Duffy, M orris, and Rayner (1988) used a virtually identical procedure to establish the time needed to understand the inform ation in a region.
EYE MOVEMENTS IN READING 377
 378 RAYNER
Therefore, the strategy of analyzing large amounts of text with a single measure of processing is likely to be of limited value in measuring on-line processing. Just and Carpenter (1980) adopted such a strategy when they used multiple regression techniques to analyze eye m ovem ents over large am ounts of text. Their procedure has been criticized by a number of researchers (Fisher & Shebilske, 1985; Hogaboam & McConkie, 1981; Kliegl, Olson, & Davidson, 1982; Slowiaczek, 1983). Those criticisms are not repeated here, but many of them follow be- cause any such procedure m ust m ake unjustifiable sim plifying assumptions about the relationship between the measure being used (in this case, gaze duration) and cognitive processing dur- ing reading. An alternative strategy that many researchers have adopted is to select target locations in texts for careful analysis (usually on the basis of theoretical considerations) and examine many different measures (such as first fixation duration, single fixation duration, gaze duration, probability of fixating a target word, number of fixations on the target word, saccade length to and from the target word, and spillover effects). By doing so, it is possible to draw some reasonable inferences about reading processes (Rayner, Sereno, et al., 1989; Schmauder, 1992).
SaccadeDurationsandGazeDuration
A recent controversial issue concerns whether or not saccade durations should be included in the computation of gaze dura- tion. Most researchers have followed the lead of Just and Car- penter (1980) and have used only fixation duration values in computing gaze durations. However, Irwin (1998) recently ar- gued that lexical processing continues during saccades and that saccade duration should be added into the gaze computation (see also Inhoff & Radach, 1998). This is a reasonable sugges- tion. On the other hand, because intraword saccade durations are quite brief and because the frequency of refixating a word before moving to another word is relatively low, any effect of adding saccade duration into the gaze duration measure is quite minimal. For example, as noted previously, readers' gaze dura- tions are longer on low-frequency words than on high-frequency words. When saccade duration is added into the gaze measure, the size of the effect (which is typically around 50 ms) increases by roughly 3-8 m s. In addition, if saccade durations are to be added to gaze durations, it is unclear what should be done with saccade durations for interword saccades. Should they be added to the word just fixated or the word that is the target of the next saccade? Thus, there are unresolved issues regarding what to do with saccade durations. For gaze durations on single target words, researchers would be wise to determine if adding saccade duration has an effect in a given study. A t this point, it appears that it typically does not. On the other hand, when regions larger than a single word represent the unit of analysis, effects of saccade duration are larger and hence are more likely to aid in revealing differences between conditions.
Eye M ovem ents and Perceptual Processes in Reading
The Acquisition of Information During Reading
With respect to the sequence of saccades and fixations, when is information acquired from the text during reading?W olverton
and Zola (1983) used the eye-contingent display change tech- nique to replace an entire line of text with (a) another line of text, (b) random letters, or (c) a row of X s for a 20-ms period during either a saccade or a fixation. After the 20-ms period, the normal text reappeared. If the text was altered at any point during the fixation (including the first 20 ms of the fixation), the change was noticed and disrupted reading. However, if the change took place during the saccade, it was not noticed and did not disrupt reading (see also Ishida & Ikeda, 1989). Thus, new inform ation is acquired from the text only during fixations.
Rayner et al. (1981; see Ishida & Ikeda, 1989; Slowiaczek &
Rayner, 1987) presented a masking pattern at various points in a
fixation and found that reading proceeded quite smoothly if the
text was presented for 50 ms or more prior to the onset of the
4
mask. On the other hand, Blanchard, McConkie, Zola, and Wolv-
erton (1 9 8 4 ) argued that visual inform ation m ay be acquired within the fixated region at any point during a fixation as needed by comprehension processes. They replaced a given target word (such as tombs) in a sentence with another word (such as bombs) at various points during a fixation and had readers make forced- choice decisions about whether or not they had seen these words (as well as foils) during reading. Their basic finding was that the longer the word was present in the text, the higher the probability of indicating that it was present. However, words presented for only 50 ms (at either the beginning or end of a fixation) were sometimes responded to positively. Blanchard et al. argued that visual information is acquired whenever needed during a fixation. Actually, the results of these two sets of experiments are really not so contradictory; both of them emphasize the flexibility that readers have in dealing with situations in which masks suddenly appear or words suddenly change during a fixation. For the sake of parsimony, it seems most likely that readers typically acquire the visual information necessary for reading during the first 50- 70 ms of a fixation, but when one word changes to another they are aware of it. Both studies are consistent in providing evidence against the notion that there is a serial letter-by-letter scan within a fixation. The same conclusion was reached in a study by Inhoff, Pbllatsek, Posner, and Rayner (1989), in which readers read text that was transformed in some way. By requiring readers to read mirror-image text (or rotated or reversed text) and varying (a) the direction of letters within words (left to right vs. right to left) and (b) the direction of reading (left to right vs. right to left), Inhoff et al. showed that congruency of letter order and direction of reading is not critical for reading (Kowler & Anton, 1987) and that a serial scan of letters in foveal vision does not occur.
The Perceptual Span in Reading
How much useful information can a reader obtain during eye fixations? Many different techniques have been used to estimate the size of the effective visual field or perceptual span in read- ing. However, most of them have severe limitations that are not discussed here (see Rayner, 1975b, 1978b for discussion). Rather, the focus is on studies using eye-contingent display tech-
" Ishida and Ikeda (1989) concluded that the visual sensitivity that is suppressed during a saccade recovers only partially during the initial part of a fixation and that it is fully recovered approximately 70 ms after the beginning of the fixation.
 during
XXXXXX
XXXXXX
during
during
during
during
Figure 2.
niques (see Figure 2) developed by McConkie and Rayner (1975) and Rayner (1975b). This research has provided the most definitive information concerning the perceptual span.
In the moving window technique (M cConkie & Rayner, 1975), the text is perturbed except in an experimenter-defined window region around the point of fixation. Wherever the reader looks, the text is visible, while outside of the window area the text is perturbed in some way. Readers are free to move their eyes whenever and wherever they wish, but the amount of useful information that is available on each fixation is controlled by the experim enter. Each tim e the eyes m ove, a new region of text is exposed while the region previously fixated is perturbed. In some cases, the window is defined in terms of letter spaces, whereas, in other cases, the window coincides with word bound- aries. Sometimes the spaces between words outside of the win- dow are preserved, and other tim es they are filled in, and som e- times the text is perturbed outside the window only on selected fixations. The assumption with this technique is that when the window is as large as the region from which the reader can obtain inform ation, there is no difference betw een reading in
that situation and when there is no window.
The moving mask technique (Rayner & Bertera, 1979) is like
the moving window technique except that, wherever the reader fixates, a m ask obscures the text around fixation w hile the nor- m al text is presented beyond the m ask region. Just as w ith the
5
m oving w indow technique, the size of the m ask can be varied. In the boundary technique (Rayner, 1975b), a single critical
Normal Text
Moving Window
Foveal Mask
Boundary
EYE MOVEMENTS IN READING
379
a saccade because the eyes are moving so *
X XXXcade because the XXXX XXX XXXXXX XX
X XXXXXXX XXXXXse the eyes are mXXXXX XX
a saccade XXXXXXX the eyes are moving so *
a saccade becausXXXXXXXyes are moving so
a saccade because the dogs are moving so
a saccade because the eyes are moving so
Examples of the moving window, foveal mask, and boundary
a normal line of text with the fixation location marked by an asterisk. The
of two successive fixations with a window of 17 letter spaces (with other
spaces between words preserved). The next two lines show an example of two successive fixations with a 7-letter mask. The bottom two lines show an example of the boundary paradigm. The first line shows the text prior to a display change with fixation locations marked by asterisks. When the reader's eye movement crosses an invisible boundary location (the letter e in the), an initially displayed word (dogs) is replaced by the target word (eyes). The change occurs during the saccade so that the reader is not aware of the
paradigms. The first line shows next two lines show an example letters replaced with Xs and the
target word is initially replaced by another word or by a non- word. W hen the reader's saccade crosses over an invisible pre- specified boundary location in the text, the initially displayed stimulus is replaced by the target word. The assumption with this technique is that if a reader obtains information from the initially presented stimulus, any inconsistency between what is available on the fixation after crossing the boundary and with what was processed on the prior fixation (when information about the initial stimulus was processed) is registered in the fixation time on the target word. In a related technique (McCon- kie & Zola, 1979), a single word in a given target location alternates between two different words with each saccade. Fi- nally, in another variation of the boundary technique (McCon- kie & H ogaboam , 1985), text is m asked (or sim ply rem oved) following a saccade and the reader must report the last word that was read.
Studies using these techniques (DenBuurman, Boersma, & Gerrissen, 1981; Ikeda & Saida, 1978; McConkie & Hogaboam, 1985; McConkie & Rayner, 1975; O'Regan, 1979,1980; Pollat- sek et al., 1986; Rayner, 1975b, 1986; Rayner & Bertera, 1979; Rayner et al., 1981; Rayner, W ell, Pollatsek, & Bertera, 1982;
N. R. Underwood & McConkie, 1985; N. R. Underwood &
s
This situation creates an artificial foveal scotoma and eye movement behavior if the situation is quite similar to the eye movement behavior of patients with real scotomas (Whittaker, Cummings, & Swieson, 1991; Zihl, 1995).
 380 RAYNER
Zola, 1986) have been very consistent in indicating that the size of the perceptual span is relatively small; for readers of alphabetical orthographies (e.g., English, Prench, and Dutch) the span extends from the beginning of the currently fixated
6
word but no more than 3-4 letters to the left of fixation (M cConkie & Rayner, 1976a; Rayner, W ell, & Pollatsek, 1980; N. R. Underwood & McConkie, 1985) to about 14-15 letter spaces to the right of fixation (DenBuurman et al., 1981; McConkie & Rayner, 1975; Rayner, 1986; Rayner & Bertera,
7
1979; Rayner et al., 1981). Thus, the span is asymmetric to
the right for readers of English. However, for orthographies, such as Hebrew,that are printed from right to left, the span is asymmetric to the left of fixation (Pollatsek, Bolozky, W ell, & Rayner, 1981).
Characteristics of the writing system influence not only the asymmetry of the span but also the overall size of the perceptual span. Ikeda and Saida (1978) found that the perceptual span for readers of Japanese was about 13 character spaces (6 charac- ter spaces to the right of fixation), which is considerably smaller than that of readers of English. Osaka (1987, 1992) found that the span was larger for text consisting of a combination of kanji (an ideographic script) and kana (a phonetic-based script) than for text consisting only of kana characters. For the combined kana-kanji script (which is typical of Japanese text), the span extended 7 character spaces to the right of fixation, whereas for the kana script it extended 5 character spaces to the right of fixation. These studies involved horizontal reading, but Japanese can also be printed (and read) vertically. Osaka and Oda (1991) found that the span was 5—6 character spaces in the vertical direction of the eye movement. Subsequent experiments by Osaka (1993a) revealed that the span was asymmetric when reading horizontally and vertically. Osaka (1993b) also found (using the moving mask technique) that readers of Japanese had a great deal of difficulty when 4 or 6 characters in the center of vision were masked; this result is quite consistent with results with readers of English (Rayner & Bertera, 1979), except that the mask needed to be larger for English to cause the same type of disruption (which is consistent with the finding that more characters are processed per fixation in English than in Japa- nese). Recently, Inhoff and Liu (1998) found that readers of Chinese have an asymmetric perceptual span extending from 1 character space left of fixation to 3 character spaces to the right. In addition to the characteristics of the writing system influencing the size of the perceptual span, reading skill influ- ences it. Rayner (1986) found that beginning readers had a smaller span (about 12 letter spaces to the right of fixation) than skilled readers (14-15 letter spaces) but that it was asym- metric to the right of fixation.
The results of a number of studies are quite consistent in indicating that, for skilled readers of alphabetic w riting system s, the perceptual span extends about 14-15 character spaces to the right of fixation. However, this does not mean that words can be identified that far from fixation; indeed, word length information is acquired further to the right of fixation than is letter inform ation (Ikeda & Saida, 1978; M cConkie & Rayner, 1975; Rayner, 1986). The word identification span (or area from which words can be identified on a given fixation) is smaller than the total perceptual span (Rayneret al., 1982; McConkie &
Zola, 1987; N. R. Underwood & McConkie, 1985) and generally does not exceed 7-8 letter spaces to the right of fixation.
Is the size of the perceptual span better thought of in terms of letters or words? The answer appears to differ to the left and right of fixation. Rayner, W ell, and Pollatsek (1980) found that the left boundary of the span was primarily defined by the beginning of the fixated word (though it did not extend more than 4 letter spaces to the left of fixation). In contrast, Rayner et al. (1982) found that the right boundary of the span was prim arily defined in term s of letters. W hen they com pared read- ing performance when the window was defined in terms of the number of letters available to the right of fixation with reading performance when the window was denned by the number of words to the right of fixation, there was no difference between "w ord" and "letter windows" when they were roughly equiva- lent in size. M ore critically, detailed analyses revealed that per- formance in the word window conditions (in which word integ- rity was maintained) could be predicted very accurately from knowing the number of letters available on each fixation, whereas performance in the letter window conditions could not be predicted from know ing the num ber of w ords available on each fixation. Thus, the perceptual span should be defined in terms of the number of letters available to the right of fixation.
A nother interesting issue is w hether or not readers are able to acquire inform ation from below the line that they are reading. Inhoff and Briihl (1991; Inhoff & Topolski, 1992) addressed this issue by asking readers to read a line from a target passage while ignoring a distracting line (from a related passage) as their eye movements were recorded. When readers completed a line, they pushed a button, and the next line (with the corre- sponding distracting line of text directly below the target text) was presented. Readers' answers to multiple-choice questions suggested that they had obtained inform ation from both attended and unattended lines. However, a detailed examination of their eye movements showed that they occasionally fixated unat- tended text. When such fixations were excluded, there was no indication that readers obtained useful sem antic inform ation from the unattended text. Pollatsek, Raney,LaGasse, and Rayner (1993) more directly examined the issue by using a moving window technique. The line the reader was reading and all lines above it were normal, but the text below the currently fixated line was altered. The lines below the currently fixated line con- sisted of (a) the original text, (b) lines from another passage (a semantically different condition), (c) Xs, (d) visually similar letters, or (e) dissimilar letters replacing the letters from the original text. Pollatsek et al. found that the passages were read most easily when the normal text was below the line and when there were Xs below the line. None of the other conditions
b
are some circumstances under which readers process more information
to the left of fixation. For example, when a word is skipped, attention
may often be directed to the left of the fixation following the skip.
7
It is important to note that the moving window technique yields a maximum span estim ate rather than an absolute value for each fixation (Well, 1983). Thus, although the size of the perceptual span has been estimated as being 14-15 letter spaces to the right of fixation, there is variability in the size of the actual perceptual span from fixation to fixation, as some studies reviewed in a later section have demonstrated.
Binder, Pollatsek, and Rayner (in press) recently found that there
 differed from each other, which suggests that readers do not obtain semantic information from below the line of text. How- ever, when the task was to find a target word in the passage, readers were occasionally able to obtain information below the fixated line (see also Prinz, 1984).
A final important finding is that the size of the perceptual span is not constant but varies as a function of text difficulty (Inhoff et al., 1989; Rayner, 1986). Rayner found dial the size of the span was smaller when text was difficult to read. W hen fourth-grade children were given reading-level-appropriate text, the size of their perceptual span was similar to that of adult readers. However, when they were given college-level text, their span became much smaller.
In summary, it appears that the perceptual span for readers of English generally extends from 3-4 letter spaces to the left of fixation to about 1 4 -1 5 spaces to the right. W hen the reading material is difficult, the size of the span tends to be smaller than this. Typically, readers focus their attention on the currently fixated line, and information below the line of text is not ob- tained, unless the task (e.g., visual search) or characteristics of the orthography demand it (e.g., reading vertically arranged ideographic systems such as Chinese or forms of Japanese). for readers of languages printed from right to left, the perceptual span is asymmetric to the left of fixation (Pollatsek et al., 1981). The size of the span also varies as a function of orthography and is smaller for more densely packed writing systems.
Acquiring Information to the Right of Fixation
W hat kind of inform ation is acquired to the right of fixation when reading English? Rayner et al.'s (1982) finding that indi- vidual letters to the right of fixation are more critical than word integrity and that the acquisition of letters is not tied to words being intact suggests that readers acquire partial w ord inform a- tion from the parafovea. Further evidence for this conclusion comes from another experiment reported by Rayner et al. (1982), in which readers read text when (a) only the fixated word was available and all other letters to the right of fixation were replaced by another letter (a one-word window), (b) the fixated word and the word to the right of fixation were available and all other letters were replaced by another letter (a two-word window), or (c) the fixated word as well as partial information about the word to the right of fixation were available. In the third condition, 1, 2, or 3 letters of the word to the right of fixation w ere available on each fixation. W hen the first 3 letters of the word to the right of fixation were available and the remain- der of the letters were replaced by visually similar letters, read- ing rate was not too different from when the entire word to the right was available; if the remainder of the word was replaced with visually dissimilar letters, reading was not as good aswhen the word to the right was available, suggesting that more infor- mation is often processed than just the 3 letters at the beginning of the next word. Lima and Inhoff (1985) and Lima (1987) also found that the time a reader looked at a word decreased when the first 3 letters of a word were available on the prior fixation (compared to when they were not available on the prior fixation).
Why are the first 3 letters of the next word to be fixated important in facilitating reading? One obvious reason is that
they are used in initiating lexical access processes. Another possibility is that they are important because they are closer to fixation than the letters further into the next word. That this is not the cause of the facilitation is clear from experiments by Inhoff (1987, 1989a, 1989b, 1990; Inhoff, Bohemier, & Briihl, 1992; Inhoff & Tousman, 1991), in which words were either printed normally from left to right or in reversed order from right to left (but with letters within the words printed normally from left to right). A s readers fixated on word n - 1, either the entire target word was available or the first 3 letters of the target word were available (with the other letters being replaced by Xs, visually similar, or visually dissimilar). Inhoff found that readers obtained considerable facilitation when the first 3 letters of the parafoveal word were available prior to fixating it when reading from left to right. More importantly, he also found that reading rate was facilitated by having the first 3 letters of the target word available when reading from right to left (where the beginning letters of the word were further away from fixation than the end letters). He also found evidence that readers obtain inform ation about the last 3 letters of 6-letter w ords. W hen the letters where replaced by Xs or dissimilar letters, there was facilitation from having the end letters parafoveally available. However, when the letters were replaced with similar letters, there was virtually no facilitation. Inhoff's results thus show that proximity to the fixation point is not the reason that the initial letters of unidentified parafoveal words are important; rather, they are important because they are useful in initiating lexical access processes, in integrating inform ation across fixa- tions, or in both (see the next section). Although McConkie, Zola, Blanchard, and W olverton (1982) reported an experiment that led them to conclude that information used to identify a word is obtained only on the fixation on which the word is com pletely identified, several experim ents have found signifi- cant effects of partial-word information (Balota, Pollatsek, & Rayner, 1985; Henderson & Ferreira, 1990; Inhoff, 1989a, 1989b; Lima, 1987; Lima & Inhoff, 1985; Pollatsek et al., 1986; Rayner et al., 1982). Thus, me use of partial-word information from the word to the right of fixation is likely to be a general phenomenon.
In addition to partial-w ord inform ation being obtained para- foveally, w ord-length inform ation is acquired parafoveally and used in computing where to look next (O'Regan, 1979, 1980; Morris, Rayner, & Pollatsek, 1990; Pollatsek & Rayner, 1982; Rayner, 1979a; Rayner & Morris, 1992; Rayner, Sereno, & Ra- ney, 1996). It is clear from the fact that readers skip some words (S. F. Ehrlich & Rayner, 1981; O'Regan, 1979, 1980; Rayner & W ell, 1996) that som etim es w ords to the right of fixation can be fully identified and skipped. In such cases, the duration of the fixation prior to the skip is inflated (H ogaboam , 1983; Pollat- sek et al., 1986). W ord-length information may be used not only in determining where to fixate next but also in how parafoveal information is used. That is, enough parafoveal letter informa- tion may be extracted from short words so that they can be identified and skipped, whereas partial-word information (the first3letters)extractedfromlongerparafovealwordsmayrarely allow full identification of them but facilitate subsequent foveal processing. An experiment by Blanchard, Pollatsek, and Rayner (1989) documented this pattern. A parafoveal preview enabled readers to skip short words (1-3 letters) significantly more of
EYE MOVEMENTS IN READING 381
 382 RAYNER
the time, whereas it had a negligible effect on how often long words (6-10 letters) were skipped. On the other hand, preview shortened the gaze durations on the long words but had a much smaller effect on the gaze durations on the short words.
To what extent do parafoveal words influence the processing of the fixated word? The evidence on this issue is somewhat mixed. Kennedy (1998); Inhoff, Briihl, and Starr (1998); and Murray (1998; Murray & Rowan, 1998) have reported that the characteristics of the word to the right of fixation can influence the processing of a fixated word. For example, M urray (1998) found that when readers were fixated on the end of word n, if word n + 1 resulted in an implausible reading, the fixation on word n was inflated even when word n + 1 was subsequently fixated. On the other hand, other studies (Carpenter & Just, 1983; Henderson & Ferreira, 1993; Rayner, Fischer, & Pollatsek,
1998) have found that the frequency of the word to the right of fixation does not influence the processing of the fixated word; fixation time on word n did not vary when word n + 1 was either a low- or high-frequency word.
Integration of Information Across Saccades
The extraction of partial-word information from the parafo- vea implies that it must be integrated in some way with foveal information from the subsequent fixation. When we read, we do not have the experience of seeing text for about 250 ms followed by a gap or blank period (resulting from the saccade) and then by new text around our new fixation. Rather, the brain smooths out the discrete inputs so that we maintain a stable coherent view of the text we are reading.
W hat kind of inform ation is integrated across saccades? Im - portant data on this question have been obtained both from reading studies (w ith fixation tim e on a target w ord as the primary dependent variable) and from a word-namingtask intro-
8
duced by Rayner (1978c). This naming task is actually a varia-
tion of the boundary paradigm discussed earlier. In the task, during an initial fixation, a word or letter string is presented parafoveally. When the participant makes an eye movement to the parafoveal stimulus, it is replaced by a word, which must be named as rapidly as possible. Integration is assessed by exam ining the effect of the parafoveal stim uli on nam ing tim e. These naming studies (Balota & Rayner, 1983; McClelland & O'Regan, 1981; Rayner, 1978c; Rayner et al., 1978; Rayner, McConkie, & Zola, 1980) have been very influential in assessing what type of information is integrated across saccades, but it is important to note that virtually identical results have been ob- tained from reading studies.
The basic finding from the naming task is that if the first 2- 3 letters of the parafoveal word are the same as for the foveal word (which is present following the saccade), naming is facili- tated when the word is presented either 1°, 2.3°, or 3° from fixation (either to the left of fixation or the right); if the parafo- veal stimulus is presented 5° from fixation, then there is no facilitation. W hen the parafoveal stim ulus is presented 1° from fixation, if the first 2-3 letters are preserved across the saccade, naming is much faster than when these letters change across the saccade but not as fast as when the entire target word is pre- sented parafoveally (and all letters remain the same during the
saccade). However, when the parafoveal stimuli are presented
2.3° or 3° from fixation, there is no difference between the condition in which the first 2 or 3 letters are preserved across the saccade and the condition in which all letters are preserved. It was suggested (McClelland & O'Regan, 1981; Paap & New- some, 1981) that the basic pattern of results is obtained only when there is a limited target set, but Balota and Rayner (1983) demonstrated the robustness of the effect when a large stimulus set was used.
M uch of the original interest in the integration of inform ation across saccades was related to the notion of an integrative visual buffer in reading (McConkie & Rayner, 1976b; Rayner, 1978c) in which gross visual information obtained from a parafoveal word was stored in a buffer and then combined with information available in the fovea about that word following the saccade. According to McConkie and Rayner (1976b), the justification of the information from the two fixations could be based on (a) knowledge about how far the eyes moved and (b) the common- ality of the visual patterns. Although an integrative visual buffer has a certain amount of intuitive appeal, a number of experi- ments found no support for it. First, Rayner et al. (1978) found that the typical pattern in the naming task could be obtained when the saccade was simulated. In the simulation condition, the viewer maintained fixation, the parafoveal stimulus was pre- sented for a period of time approximating the latency of a sac- cade, and then the target word appeared in foveal vision for the viewer to name. Thus, information initially impinged on the parafoveal retina and then on the fovea, just as it did when a saccade was made. The fact that the same pattern of results was obtained under both conditions suggests that knowledge about how far the eyes moved is not necessary for integration. In addition, Le>y-Schoen and O'Regan (1979) and McConkie, Zola, and Wolverton (1980; see also McConkie & Zola, 1987) found that readers are unable to detect small shifts of text during saccades; if the movement of the text was no larger than one fourth the size of the saccade, they were unaware that anything odd occurred (though they did sometimes make small corrective saccades to adjust for the m ovem ent).
The finding that the visual display can be shifted a small amount during the saccade without the reader noticing it is problem atic for an integrative visual buffer. A s indicated above, this result suggests that knowledge of how far the eyes moved is not crucial for integration. Thus, integration would have to be based purely on the commonality of visual patterns from two fixations, and how to align the two images might be difficult. Even more problematic for the integrative visual buffer notion were subsequent experiments that demonstrated that case changes did not affect reading (McConkie & Zola, 1979; Rayner, McConkie, & Zola, 1980). McConkie and Zola had readers read text in alternating case, and each tim e they m oved their eyes, the text shifted from one version of alternated case to another (e.g., cHaNgE shifted to ChAnGe). Readers did not notice that the change was taking place, and their reading behav- ior was not different from a control condition in which they read alternated case in which the letters did not change case from fixation to fixation. If visual codes were important in
8
Other tasks (such as lexical decision, sam e-different m atching, and categorization judgments) have been used, but the bulk of the studies have used nam ing.
 integrating information across saccades, then the change of fea- tures between upper- and lowercase letters should have disrupted reading. Thus, visual information is not combined across sac-
. cades during reading in an integrative visual buffer.
A number of other candidates for the code conveying informa- tion across saccades have been considered. One candidate is some type of phonological code. Pollatsek, Lesch, Morris, and Rayner (1992) found in both the naming task (see also Hender- son, Dixon, Petersen, Twilley, & Ferreira, 1995) and reading that phonological inform ation is used in integrating inform ation across saccades. In Pollatsek et al.'s (1992) study, a homophone of a target word presented as a preview in the parafovea facili- tated processing of the target word seen on the next fixation more than a preview of a word matched with the homophone in orthographic similarity to the target word. Because the ortho- graphic similarity of the preview to the target also plays a part in the facilitative effect of the preview, however, codes other than phonological codes (such as abstract letter codes) are preserved
across saccades.
M orphem es have also been exam ined as candidates for facili-
tating the integration of information. Lima (1987) examined whether prefixes could be extracted as a unit from the beginning of a parafoveal word. She used words that had true prefixes (e.g., revive) and words that were "pseudoprefixed" (e.g., res- cue). If extracting morphemes were a significant part of the benefit of parafoveal preview , then a larger preview benefit (the difference in performance between when a parafoveal preview of the target was present and when the preview was absent) should be observed for the prefixed words. There was equal benefit in the two cases, which suggests that prefixes are not active units in integration across saccades. Another morphemic unit that could be extracted from the parafovea is the first part of a compound word. Inhoff (1989a) compared the preview benefit from seeing the first m orphem e of a com pound w ord such as cow in cowboy with that from the first part of a ' 'pseu- docompound" word, such as car in carpet, and found no differ- ence between the two conditions.
Finally,ithasbeensuggestedthatsemanticpreprocessingof unidentified parafoveal words is a relevant factor in integration across saccades and aids in the later identification of a word (G. Underwood, 1985). Rayner, Balota, and Pollatsek (1986) tested the idea that semantic preprocessing influences subse- quent fixation time in a boundary study. Prior to fixating on a target word (such as tune), the parafoveal preview for that word was either orthographically similar (turc), semantically similar (song), or unrelated (d o o r). W hen the eyes crossed the bound- ary location, the target word replaced the preview word. The semantically similar pairs (song-tune) produced a priming ef- fect in a standard priming experiment. However, in the reading situation, although fixation time on the target word was shorter when the preview was orthographically similar to the target w ord, there w as no difference betw een the sem antically sim ilar and unrelated conditions. Thus, readers apparently do not obtain semantic information from unidentified parafoveal words.
In summary,it appears that abstract letter codes (McConkie & Zola, 1979; Rayner; M cConkie, & Zola, 1980) and phonological codes (Henderson et al., 1995; Pollatsek et al., 1992) are in- volved in integrating information from words across saccades.
No evidence has been obtained to suggest that visual, morpho- logical, or sem antic inform ation is im portant.
The Effect of Lexical and Sentential Constraint
To determine whether lexical constraint can influence parafo- veal processing, Lima and Inhoff (1985) presented sentences in which one of two words appeared in a target location. The target words, for example dwarf and clown, were selected to have equal frequency in the language and be equally predictable in the sentence context. However, the initial letters of the target word in one condition (dwa) were shared by few words in the lexicon, whereas those in the other condition (do) were shared by many words. Because prior studies had demonstrated that seeing the first three letters of the parafoveal word produced a large benefit, L im a and Inhoff reasoned that if lexical constraint were a potent variable in parafoveal processing, then the preview benefit for dwarf should be greater than for clown. In fact, there was equal preview benefit in the two cases, indicating that lexi- cal constraint does not operate on parafoveal information. How- ever, the fixation time on clown was actually less than for dwarf (regardless of whether there was a preview or not), indicating that the frequency of the word-initial-letter sequence influences the time to process a word foveally. In a related vein, Inhoff and Rayner (1986) found that more parafoveal preview benefit was obtained from a high-frequency parafoveal word than from a low-frequency parafoveal word.
The effect of sentential constraint on parafoveal processing was examined by Balota et al. (1985). They varied both the predictability of a target word and the availability of parafoveal information by using the boundary technique. Two interesting findingsemerged. First, the finding that a predictable target word is more likely to be skipped than an unpredictable one (S. F. Ehrlich & Rayner, 1981; O'Regan, 1979) was replicated indicat- ing that sentential constraint influences the usefulness of parafo- veal information. Second, when the target word was not skipped, fixation time on it was shorter when the word was more predict- able. More importantly, the benefit of a parafoveal preview was greater when the target word was predictable, indicating that the extraction of parafoveal information is more efficient when aided by sentential context. Additional analyses indicated that more letters were extracted from the parafovea when contextual constraint w as high.
An interesting corollary to the Balota et al. (1985) study is that, when the difficulty associated with processing the foveal word is high, the extraction of parafoveal information decreases (Henderson & Ferreira, 1990; Inhoff et al., 1989; Kennison & Clifton, 1995; Rayner, 1986; Schroyens, Vitu, Brysbaert, & d'Ydewalle, in press). For example, Henderson and Ferreira (1990) used the boundary technique and m anipulated foveal pro- cessing difficulty while also varying the availability of parafoveal information. They found that difficult foveal processing led to the reader obtaining no parafoveal preview from the word to the right of fixation. These results thus suggest that the amount of information processed on a fixation is somewhat variable and can be influenced by factors such as the length of the currently fixated word and the word to the right of fixation (Rayner, 1979a), sentential context (Balota et al., 1985), and the difficulty of pro- cessing the fixated word (Henderson & Ferreira, 1990).
EYE MOVEMENTS IN READING 383
 384 RAYNER
Display Change Effects?
A final issue to be addressed in this section is the extent to which the results of studies using eye-contingent display changes are due to the display changes per se. Each time a display change occurs in such studies, because of phosphor persistence of letters on the display monitor or the refresh rates of the display monitor (or both), there is a flash on the screen
9
from replacing letters by other letters. One could argue (see
O'Regan, 1990, 1992) that the change per se is somehow influ- encing the results or that the amount of change artifactually influences the data pattern so that the more letters that are re- placed, the m ore disruption there w ill be, or, given that the number of letters that change is held constant, either the location of the changing letters (closer or further from the fixation) or the characteristics of the changing letters (replacing letters with visually similar letters might cause less of a flash than replacing letters with dissimilar letters) might influence the results.
To what extent is this issue a worry? Some of the earliest experiments of the genre (McConkie & Rayner, 1975; Rayner, 1975b) suffered from the fact that the display changes associated with eye movements were relatively slow, so readers undoubt- edly saw some of the letters change. The eye-tracking and com- puter systems currently used in eye-contingent change experi- ments (see Beauvillain & Beauvillain, 1995; M cConkie, W olv- erton, & Zola, 1984; McConkie, W olverton, Zola, & Burns, 1978; Rayner, 1979b; van Diepen, DeGraef, & Van Rensbergen, 1994) are more sophisticated than those used in the early classic experim ents, and the m ore recent research w ith faster display changes have yielded patterns of data very consistent w ith those originally reported. The currently used display change tech- niques generally mean that in moving window experiments the movement of the window is completed within 3-8 ms of the end of the saccade10 and the speed of the display change in studies using the boundary technique guarantees that the change occurs during the saccade. In the classic McConkie and Rayner (1975) experiment, it was argued that the perceptual span ex- tended 15 letter spaces to the right of fixation. Although there were lingering effects for windows larger than 15 letter spaces to the right of fixation, M cConkie and Rayner argued that such effects (which were not significant) were due to readers seeing movement in parafoveal vision fas a result of the display change). As this review has documented, many subsequent ex- periments (with faster display changes) have found the size of the window to be about 15 letter spaces to the right of fixation. Likewise, Pollatsek et al. (1986) obtained results that were quite consistent with those of Rayner (1975b).
Another issue to consider is that readers in moving window and boundary experiments are often not aware that display changes are taking place. If there are Xs outside of a moving window, readers are certainly aware that the Xs are there, be- cause they represent a homogeneous pattern. However, if the letters outside the w indow are replaced w ith other letters and readers have approximately 7-8 normal letters to the right of fixation, they are unaware of the display changes. Even when the window is smaller than this and visually similar letters are outside of the window,readers are aware of reading more slowly than usual, but they do not know why. In boundary experiments, if the display change is slowed down so that readers can see
the change take place, they are aware of changing letters. How- ever, under norm al experim ental conditions, even though they are encouraged to report any strange events as they read, they rarely do so, and even skilled observers who know what is happening in the experiment do not perceive the changes.
It could be argued that it does not matter whether or not readers are aware of the display changes, because there is a change in the stimulus pattern and it is registered by the visual system (even when the change occurs during a saccade) and thus interacts with the manipulations being made. If this is so, then the interaction is very complex and not easily interpretable. The clearest example of this is the situation in which the display change is constant (in terms of the number of letters changing) but some other variable, such as contextual constraint, is varied and is found to interact with the amount of parafoveal informa- tion obtained (Balota et al., 1985). Another factor is that it has consistently been found thatXs outside the window lead to better performance than when letters are outside of the window. This is undoubtedly due to readers trying to incorporate erroneous letters into their reading in the latter case. However, a ' 'flicker'' explanation of the results should lead to the opposite prediction, because there would be more of a flash going from Xs to the norm al text than in going from letters to other letters. A lthough one always has to worry about extraneous factors influencing performance, the evidence in favor of display change artifacts seems quite limited. Indeed, Briihl and Inhoff (1995) and In- hoff, Starr, Liu, and Wang (1998) directly tested the idea that display changes per se influence the outcom e of eye-contingent display change studies. They varied the speed of a display change and the refresh rate of the display monitor and found
•no evidence to suggest that the results of eye-contingent change experiments were artifacts of the paradigm.
The Control of Eye Movements in Reaoling
One factor that dampened interest in using eye movements to study reading at the end of the second era of eye movement
' There is some controversy over the exact amount of phosphor persis- tence due to different types of phosphors (see=Di Lollo, Bischof, Walther- Muller, Groner, & Groner, 1994; Groner, Groner, Muller, Bischof, & Di Lollo, 1993;Irwin, 1994;Westheimer, 1993). Much of the controversy centers on how much persistence remains when a letter is removed from a monitor. However, in most experiments discussed in this section, one letter is replaced by another letter, and readers do not have the impression of overlapping letters. In addition, a number of studies (Kennedy & Baccino, 1995; Kennedy, Brysbaert, & Murray, 1998; Kennedy & Mur- ray, 1991, 1993, 1996; Neary & Wilkins, 1989; Wilkins, 1986) have shown that eye movement control is disturbed by pulsating illumination characteristic of display monitors that are periodically refreshed. In particular, the size of a saccade varies as a function of refresh rate. However, this finding is not directly relevant to the issue under discussion because across experiments in which display changes take place, the refresh rate is constant across the experimental conditions.
10
There is some variability across labs that use eye-contingent display changes in terms of how quickly the display change is made. Thus, attention should be paid to what each investigator reports as to how quickly the change can be made. It should also be noted that some labs use an algorithm whereby the location of the next saccade can be pre- dicted on the basis of the velocity of the saccade. Also, the computations can be made such that the window moves as the eye is moving and does not depend on knowing exactly where the landing position will be.
 research was the belief that eye movements are unrelated to ongoing cognitive processes (Bouma & deVoogd, 1974; Kolers, 1976; Viviani, 1990), Over the past 20 years, abundant evidence has accum ulated indicating that fixation tim e and saccade length are related to aspects of the text currently fixated (Kapoula, 1984; Rayner, 1997; Rayner, Sereno, et al., 1989). In a following section, research related to ongoing language processing is re- viewed. In this section, the focus is on the extent to which how far the eyes move and how long the eyes fixate are determined by the inform ation currently fixated.
The earliest unam biguous dem onstration that the length of the next saccade and the duration of the current fixation are computed on-line was provided by Rayner and Pollatsek (1981). They varied the physical aspects of the text randomly from fixation to fixation and found that the behavior of the eyes mirrored what was seen on a fixation. In one experiment, the size of the window of normal text was randomly varied from fixation to fixation, and saccade length was shown to vary ac- cordingly. In another experiment, the foveal text was delayed after the onset of a fixation by a m ask (w ith the tim e varying randomly from fixation to fixation), and fixation durations var- ied accordingly (see also M orrison, 1984). In addition, the m a- nipulations appeared to affect saccade length and fixation dura- tion independently. Thus, there is reason to believe that the decisions of where and when to move the eyes can be made independently (Rayner & McConkie, 1976). Accordingly, these two topics are discussed separately.
Where to Move the Eyes
inform ation to the right of the fixated w ord w as the space infor- mation delimiting word n + 1 (all the letters to the right of word n were converted to Xs). Despite the fact that word n + 1 was all Xs before it was fixated, readers made a larger saccade off word n, the longer word n + 1 was.
A lthough m ost researchers have found evidence that w ord length is an important cue in deciding where to look next, Epel- boim, Booth, and Steinman (1994, 1996; see also Epelboim, Booth, Ashkenazy, Taleghani, & Steinman, 1997) argued that word length per se is not critical for eye guidance. On the basis of experim ents in w hich readers read text w ith the spaces removed, Epelboim et al. claimed that reading unspaced text is relatively easy. They further argued that the elimination of space information between words primarily disrupts the word recogni- tion process. Subsequent analyses by Rayner and Pollatsek (1996) demonstrated that most readers are slowed down (on average by about 30%) by the absence of space information (see also Spragins, Lefton, & Fisher, 1976), and experiments by Rayner, Fischer, and Pollatsek (1998) demonstrated that both word identification processes and eye guidance are disrupted by the lack of space information (Pollatsek & Rayner, 1982). Rayner et al. (1998) found that (a) the initial landing position in w ords shifted closer to the beginning of a w ord and (b) low - frequency words were harder to identify than high-frequency words when space information was absent. In another recent demonstration, it was found that, when space information is provided for readers of Thai (who are not used to reading with spaces between words), they read more effectively than normal (Kohsom & Gobet, 1997). Likewise, Inhoff, Radach, and Heller (1996) reported that the reading of long German compound words is facilitated by the insertion of spaces, even though this format is ungrammatical and never encountered in normal reading. Thus, it appears safe to conclude that space information is used for guiding the eyes in reading.
Word-length information also plays a clear role in where in the w ord a reader fixates. A lthough there is variability in w here the eyes land on a word, readers tend to make then- first fixation on a word about halfway between the beginning and the middle of a word (Dunn-Rankin, 1978; McConkie, Kerr, Reddix, & Zola, 1988; McConkie, Kerr, Reddix, Zola, & Jacobs, 1989; McConkie & Zola, 1984; O'Regan, 1981; Radach & Kempe,
1993; Rayner, 1979a; Rayner et al., 1996; Rayner, Fischer, et
al., 1998; V itu, 1991a, 1991b, 1991c; V itu, O 'R egan, Inhoff, &
Tbpolski, 1995; Vitu, O'Regan, & Mittau, 1990). Rayner
(1979a) originally labeled this prototypical location as the pre-
ferred viewing location. Subsequently, O'Regan and Levy-
Schoen (1987) distinguished between the preferred viewing lo-
cation and what is now referred to as the optimal viewing posi-
11
W hat influences w here to m ove the eyes
indicated above, one important influence on where to move is the am ount of useful letter inform ation to the right of fixation. Balota et al. (1985) demonstrated that letter information to the right of fixation was important in determining whether a word was skipped. However, most of the research suggests that word boundary information (conveyed by the spaces between words) is the major determinant used in deciding where to move to next; saccade length is influenced by both the length of the fixated word and the word to the right of fixation (O'Regan,
1979,1980; Rayner, 1979a). Such a relationship, however, could
also be a result of the linguistic characteristics of words (be-
cause shorter w ords tend to have a higher frequency in the language). In order to examine the independent effect of word boundary information, text has been presented to readers in
which all of the letters to the right of the window boundary
were replaced by Xs but the spaces were preserved. This condi-
tion was compared to one in which the spaces were also replaced
by Xs (M cConkie & Rayner, 1975; Rayner, 1986). A lthough tion. m ean saccade length decreased som ew hat w hen the letter infor-
mation to the right of fixation was removed, the removal of space inform ation m ore seriously affected saccade length. Pol- latsek and Rayner (1982) also showed that saccade length de- creased m arkedly w hen the spaces betw een w ords w ere filled in, even when the letters were left intact. These experiments suggest that letter information may be of little value in guiding the eye if word boundary information is not present. That word length inform ation is im portant in eye guidance is clear from an experiment by Morris et al. (1990) in which the only useful
The optimal viewing position is the location in a word at w hich recognition tim e is m inim ized. A ccording to O 'R egan and L6vy-Schoen (1987), the optimal viewing position is a bit to the right of the preferred viewing location, closer to the center of the word. Extensive research efforts have examined the consequences of making fixations at locations other than this optimal viewing position (Coe'ffe', 1985; Holmes & O'Regan,
next in
reading?
A s
EYE MOVEMENTS IN READING 385
11
O'Regan (1981) originally called the optimal viewing location the
convenient viewing position.
 386 RA YNER
1987; Nazir, 1991, 1993; Nazir, Heller, & Sussmann, 1992; Na- zir, O'Regan, & Jacobs, 1991; O'Regan & Jacobs, 1992; O'Re- gan, L6vy-Schoen, Pynte, & Brugaillere, 1984; G. Underwood, Clews, & Wilkinson, 1989; Vitu, 1991c, 1993; Vitu & O'Regan, 1988, 1991; Vitu et al., 1990). For words presented in isolation, two general effects have been found. First, there is a reflation effect: The further the eyes are from the optimal viewing posi- tion, the more likely it is that a reflxation will be made on the word. Second, there is a processing-cost effect: For every letter that the participant's fixation deviates from the optimal viewing position, the associated cost amounts to about 20 ms (O'Regan et al., 1984). W hen w ords are in text, however, although the refixation effect remains, the processing-cost effect is greatly attenuated or absent (Rayner et al., 1996; Vitu et al., 1990). This indicates that contextual information overrides low-level visual processing or that in reading text, the information ac- quired about a word before it is directly fixated affects its later fixation location and duration.
In any event, there is a clear tendency for readers to fixate between the beginning and the middle of the word. Evidence that word-length information helps to guide the eye toward the preferred view ing location com es from conditions in w hich there was no parafoveal letter inform ation (M orris et al., 1990). W hen the space indicating the location of word « + 1 was visible in the parafovea, the first fixation on that word was closer to the preferred viewing location than when the parafoveal preview did not contain that space information.
A given fixation location in a word can be viewed not only as a landing position but also as the takeoff point or launch site to the next word, and one launch site is related to the next landing position (McConkie, Kerr, et al., 1988; Radach & Kempe, 1993; Radach & McConkie, 1998; Rayner et al., 1996). Although the average landing position in a word lies between the beginning and middle of a word, this position varies as a function of the distance from the prior launch site. For example, if the distance to a target word is large (8-10 letter spaces), the landing position is shifted to the left. Likewise, if the distance is small (2-3 letter spaces), the landing position is shifted to the right. To explain the launch site effect, McConkie, Kerr, et al. (1988) suggested that something like a range effect might explain these results (see also McConkie, Kerr,& Dyre, 1994). The range effect (Kapoula, 1985; Kapoula & Robinson, 1986) occurs when the eyes saccade toward isolated targets at various eccentricities: they tend to land near the mean of the range of possible eccentricities instead of directly saccading toward the position of the target. Thus, targets presented at a small or a large eccentricity are overshot and undershot, .respectively. However, Vitu (1991b) showed that when words were presented in isolation at different eccentricities from fixation, the eyes' landing position did not differ as a function of eccentricity. Thus, she argued that the range effect cannot account for the launch site data and that a "center of gravity" account (in which readers target the middle of a word for the next saccade) can better explain the data. More recently, Radach and McCon- kie (1998) provided analyses that are more consistent with a range effect explanation than with a center of gravity account.
The location of the first fixation is between the beginning and themiddleofawordforwordsthatare4-10letterslong(either for the first fixation in a word or when only a single fixation is
made). However, with longer words, the effect breaks down (Vitu, 199la), and readers tend to fixate near the beginningof the word and then make a second fixation toward the end of the word (Hyona, Niemi, & Underwood, 1989; O'Regan et al., 1984; Rayner & Morris, 1992; G. Underwood, Bloomfield, & Clews, 1988; G. Underwood, Clews, & Everatt, 1990; G. Un- derwood, Hyona, & Niemi, 1987; Vitu, 1991b). Hyona et al. (1989); O'Regan et al. (1984); Pynte, Kennedy, and M urray (1991); Rayner and Morris (1992); and G. Underwood et al. (1987, 1988; G. Underwood, Clews, et al., 1990) have shown that informational density (or morphological structure) of the word influences how long the fixations are on each half of the word. For example, Hyona' et al. (1989, using Finnish) found that if the word was predictable from the first 6 letters (the words were typically about 12 letters), readers generally made a fixation in the first half of the word and then moved their eyes to the next word; if they made a second fixation on the word it tended to be quite short. However, if the word could only be identified by knowing what the ending was, readers typically m ade a short fixation at the beginning follow ed by a longer fixation on the end of the word.
Although low-level visual information influences where the eyes initially land in a word, it is less clear whether properties other than the spaces between words and the length of a not- yet-fixated word are used in deciding where to fixate next. G. Underwood et al. (1987, 1990; Everatt & Underwood, 1992; Hyona et al., 1989) examined the landing position in long words (10 or more letters) composed of informative and redundant halves. They reported that the eyes initially move further into a word when the informative information is at the end of the word than at the beginning and suggested that sem antic preprocessing of parafoveal words was responsible for the effect. However, neither Raynerand Morris (1992) nor Hyona (1995a) replicated the effect (see also Radach, Krummenacher, Heller, & Hofmeis- ter, 1995). On the other hand, there are demonstrations that orthographic properties of words influence the initial landing position. That is, some studies (Beauvillain & Dore, 1998; Beauvillain, Dor6, & Baudouin, 1996; Dore & Beauvillain, 1997; Hyona, 1995a) have demonstrated that an orthographi- cally irregular letter cluster at the beginning of a word results in the eyes initial landing position deviating toward the begin- ning of the word, although Radach et al. (1995) did not observe such an effect. W hether or not m orphology influences the initial landing position is somewhat unclear. Beauvillain (1996) used prefixed and suffixed French words that were around 10 letters long and found little effect of morphology on initial landing positions. On the other hand, Inhoff, Briihl, and Schwartz (1996) found that the initial fixation was more toward the center of a word for compound words than for derived (or monomor- phemic) words. However, they also found that initial fixations on suffixed and m onom orphem ic w ords w ere virtually identical. Hyona and Pollatsek (in press) varied the length of the initial morpheme of Finnish words, while holding the overall length constant, and found no effect on the initial landing position. They also varied the frequency of the initial morpheme, while holding the overall frequency constant, and obtained a small (about a quarter of a letter position) difference such that the landing position was further into the word for more frequent morphemes. In summary, it is clear that low-level visual factors
 influence where readers fixate next. Furthermore, there appear to be effects of the regularity of the initial letter clusters. How- ever, it is unclear if morphology influences the initial landing position and semantic preprocessing effects do not reliably in- fluence where the eyes land in a word (Liversedge & Un- derwood, 1998).
To this point, the focus has been on forward saccades. How- ever, many saccades do not simply move the reader forward from word to word through the text: Sometimes they regress, sometimes they refixate on the fixated word, and sometimes they skip words. Each of these topics is now discussed.
What Causes Regressions?
Although roughly 10-15% of all fixations are regressions, very little is really known about what causes them. As noted earlier, text difficulty strongly influences the number of regres- sions that readers make. Readers are much more likely to regress to a word on the current line than to words on previous lines (Duffy, 1992; K. Ehrlich & Rayner, 1983). However, when they do regress further back in text, although there are sometimes backtracking m ovem ents readers often have fairly good spatial memories for where they went wrong in comprehension and make fairly accurate saccades to that region of text (Frazier & Rayner, 1982; Kennedy, 1983,1992; Kennedy & Murray, 198ya,
1987b). Frazier and Rayner (1982) also demonstrated thatwhen readers encountered a word indicating that their prior interpreta- tion of the sentence was in error, they often made a regression as soon as they encountered disam biguating inform ation. It is likely that many regressions are due to comprehension failures (Blanchard & Iran-Nejad, 1987; K. Ehrlich, 1983; Frazier & Rayner, 1982; Hyona, 1995b; Just & Carpenter, 1978; Shebil- ske & Fisher, 1983; Vauras, Hyona, & Niemi, 1992). However, many regressions are very short saccades and are probably due to oculomotor errors. It is interesting that regressive saccades are more likely to occur following longer forward saccades (Vitu, McConkie, & Zola, 1998); this suggests that long saccades are less accurate and lead to ill-placed fixations. Because regres- sions are hard to induce experimentally, it is more difficult to examine them than forward saccades.
What Causes Refixations?
A bout 15% of the w ords in text are refixated (i.e., they receive additional fixations before the reader leaves the w o rd ). W hy does this happen? O'Regan and Levy-Schoen (1987; O'Regan et al., 1984) argued that refixations on a word are often caused by originally landing in a "bad" place in a word and that processing of the word is distributed over two or more fixations in such cases. This suggests that (a) most refixations on words should take the eyes to the optimal viewing location and (b) second fixations on a word should take longer than first fixations (because'less information would be obtained when the eyes are in a bad place). When readers make two fixations on a word, in the vast majority of cases, an initial fixation on a word is followed by a rightward movement within the word. By far the most frequent pattern is to fixate near the beginning of a word followed by a fixation near the end of the word (Rayner & Pollatsek, 1987; Rayner et al., 1996). First fixations tend to be
longer than the second fixations (Rayner et al., 1996). Thus, although some second fixations on a word are made because the reader was in a bad place, refixadons are also often made for other reasons. Certainly, when more than two fixations are made on a word, it is as likely that the additional fixations are as attributable to incomplete lexical processes (Pollatsek & Rayner, 1990; Pynte, 1996) as to being in a bad place in the word. It is also the case that frequency affects the first fixation of words that are fixated twice (Rayner et al., 1996; S. C. Sereno, 1992), which is at odds with O'Regan's argument. There is also evidence that contextual variables influence whether a reader will make an additional fixation on a word. Balota et al. (1985) showed that readers were less likely to have their next fixation remain in the currently fixated word if it was predictable in the sentence context.
What Causes Word Skipping?
Contextual constraint has a large effect on skipping: Words that are highly constrained by the preceding context are skipped more frequently than words that are not constrained (Balota et al., 1985; Binder, Pollatsek, & Rayner, in press; S. F. Ehrlich & Rayner, 1981; Rayner & Well, 1996; Schustack, Ehrlich, & Raynei; 1987; Vitu, 1991c). There is also some evidence to suggest that high-frequency w ords are skipped m ore than low - frequency words, particularly if the eyes are close to the target word on the fixation prior to skipping (Radach & Kempe, 1993; Rayner et al., 1996). However, the most important variable in word skipping is word length: Short words are much more likely to be skipped than long words (Brysbaert & Vitu, 1998; Raynei;
1979a). These facts suggest that when readers skip a word, they identify it on the prior fixation (Rayner & Duffy, 1988). Indeed, when words are skipped, the fixation duration prior to the skip is inflated (H ogaboam , 1983; Pollatsek et al., 1986).
In summary,the data suggest that, in addition to word-length inform ation, textual variables appear to influence w here the readers' eyes move to next. W ord length seems to be the primary cue used in the normal sequence in which the word to the right of fixation has not been identified (so that the reader fixates on word n and then moves to word n + 1). Textual variables appear to influence the decision of w hether to m ake the next fixation on the currently fixated word, to skip the next word, or to make a regression back to some earlier part in the text.
When to Move the Eyes
The ease or difficulty associated with processing a fixated word influences when the eyes move; much of this research is reviewed in a later section (see Rayner & Sereno, 1994a, for a complete summary). One variable that obviously influences gaze duration on a word is word length: As word length in- creases, gaze duration increases (Just & Carpenter, 1980; Rayner et al., 1996). Much of this effect is due to the fact that as w ords get longer, the probability that readers refixate it before moving on increases. This section, however focuses on two variables that, when word length is controlled, strongly influence fixation time on a word: word frequency and contextual constraint.
M any studies have dem onstrated that readers look longer (first
EYE MOVEMENTS IN READING 387
 388 RA YNER
fixation, gaze, and single fixation duration) at low-frequency words than at high-frequency words (Altarriba, Kroll, Sholl, & Rayner, 1996; Henderson & Ferreira, 1990, 1993; Hyona & Olson, 1995; Inhoff & Rayner, 1986; Just & Carpenter, 1980; Kennison & Clifton, 1995; Raney & Rayner, 1995; Rayner, 1977; Rayner & Duffy, 1986; Rayner & Fischer, 1996; Rayner & Raney, 1996; Rayner et al., 1996; Rayner, Fischer, et al., 1998; S. C. Sereno, 1992; V itu, 1991c). There are three additional points with respect to this finding. First, there is a spillover effect from fixating low-frequency words as fixation time on the next word is inflated (Rayner & Duffy, 1986). Second, although the duration of fixation n is influenced by word fre- quency, the duration of fixation n — I (when the target word is to the right of fixation) is not (Carpenter & Just, 1983;Hender- son & Ferreira, 1993; Rayner, Fischer, et al., 1998). Third, high- frequency words are skipped more than low-frequency words whenwordsare6lettersorless(O'Regan, 1979;Rayneretal.,
1996).
S. F. Ehrlich and Rayner (1981) found that words that are
constrained by preceding context are fixated for less time and skipped m ore often than unconstrained w ords (see also A ltarriba et al., 1996; Balota et al., 1985; Inhoff, 1984; Rayner & W ell, 1996; Schustack et al., 1987; Zola, 1984). Hyona (1993) subse- quently found that w hen high-constraint target w ords w ere not
i2
of word n + 1 to begin and signals the eye movement system to prepare a motor program for an eye movement to the newly attended location. Once the motor program is completed, it is executed, and the eyes then move to the new word. Because there is a lag between the attention shift and the eye movement due to program m ing latency, inform ation continues to accum u- late from the parafoveal word before it is directly fixated. If the parafoveal word is identified quickly, attention shifts again to the word beyond the parafoveal word (word n + 2) before the eye movement is fully programmed. In this case, the eyessac- cade to word n + 2, skipping word n + 1. Usually, there is a cost in modifying the motor program, and, as noted in a prior section, the duration of the fixation prior to a skip is conse- quently inflated (Hogaboam, 1983; Pollatsek et al., 1986). If the motor program is too far advanced, however, there will be either (a) a short fixation on word n + 1 followed by a longer fixationonwordn+ 2or(b)afixationlocated atanintermedi- ate position between words n + 1 and n + 2. The model thus can explain some rather puzzling aspects of eye movement be- havior in reading such as (a) very short fixations in text (given that saccadic latency in sim ple oculom otor tasks is typically on the order of 175-200 ms) and (b) unusual landing sites (e.g., the spaces between words). Although Morrison did not discuss the issue of where in a word readers fixate, most processing models assume that low-level cues such as word length influence where readers fixate (Rayner et al., 1996).
One problem hi Morrison's original model was that there is no explanation for why words are sometimes refixated. That is, if lexical access is the trigger for attentional shifts (andhence eye movements), words should never be refixated. Some modi- fications of the model (Henderson & Ferreira, 1990; S. C.Ser- eno, 1992) incorporated a deadline for programming an eye movement: If lexical processing has not reached a criterion level by this deadline, attention does not shift from the current w ord and it may be refixated. Another account of why words are refixated proposed by Pollatsek and Rayner (1990) did not involve any type of deadline. Rather, they suggested that the signal to stay on a word may be related to a decision that something does not fully compute; for example, the word that has been accessed m ay not fit into the syntactic or sem antic structure of the sentence being constructed. These models do not deny that lexical effects can occur in the first fixation of refixated words; they merely state that lexical processing may not be complete enough to warrant an attentional shift to the following word.
According to O'Regan's strategy-tactics model (1990, 1992), the eyes' initial landing position in a word largely deter- mines how long to remain fixated and where the following fixa- tion is made. He proposed that readers adopt a global' 'strategy'' (e.g., careful or risky reading) that coarsely influences fixations and saccades. He also proposed that readers implement local, within-word "tactics" that are based on lower level,nonlexical
12
In S. F. Ehrlich and Rayner's (1981) study, the high-constraint target words were identified 80-90% of the time in a cloze task, and the low-constraint words were identified less than 10% of the time. In Hyona's (1993) study, the high-constraint words were identified 65% of the time, and the low-constraint words were identified 32% of the time in a cloze task.
as constrained as those used by S. F.Ehrlich and Rayner,
were no significantdifferences in either fixation time or skipping rates. However, more recently, Rayner and W ell (1996) found that readers fixated low-constraint target words longer than they fixated high- or medium-constraint target words and that readers
skipped high-constraint target w ords m ore medium- or low-constraint target words.
Models of Eye Movement Control
than they did
there
either
The issue of when to move the eyes is at the heart of a recent controversy concerning different types of m odels of eye m ove- ment control in reading. Such models can be classified into two general categories: (a) processing models, which assign lexical processing or other ongoing comprehension processes a major role in influencing when the eyes move, and (b) oculomotor models, which maintain that eye movements are mainly controlled by oculomotor factors and are only indirectly related to ongoing language processing. The first category includes a model pro- posed by Morrison (1984), with various modifications (Hender- son & Ferreira, 1990; Kennison & Clifton, 1995; Pollatsek & Rayner, 1990; S. C. Sereno, 1992), as well as one proposed by Just and Carpenter (1980). The second category includes a model proposed by O'Regan (1990, 1992), as well as proposals of Kowler and A nton (1987) and M cConkie et al. (1989; M cConkie, Kerr, et al., 1988). Because of the importance of the models of Morrison and O'Regan in generating research on the issue of eye movement control, each is discussed.
A ccording to M orrison's m odel, w hich w as influenced by Becker and Jurgens (1979) and McConkie (1979), at the begin- ning of each eye fixation, eye location and visual attention are oriented at the same location: the foveal word (word n). After foveal word processing has reached a criterion level (such as lexical access), attention shifts to the parafoveal w ord (w ord n -I- 1) during the fixation. This shift of attention allows processing
 information available early in a fixation. If the eyes land in a region of a word that is optimal (near the word's middle), there will be a single fixation. However, if they land in a nonoptimal position, a refixation will generally occur. Fixation durations according to this scheme are mainly determined by oculomotor constraints. The probability that a word is reflxated does not depend on its lexical status but on lower level visual factors such as the landing position in that word. Linguistic factors influence the duration of a single long fixation (300 ms or longer; O'Regan, 1992) but only the second of two fixations in a refixated word.
A problem with O'Regan's model is that the optimal viewing position effect was originally obtained in tasks in which words were presented in isolation (Nazir, Heller, & Sussmann, 1992; O'Regan & Jacobs, 1992; O'Regan & Levy-Schoen, 1987; O'Regan et al., 1984). Normal reading, however, involves more than recognizing individual w ords. A s noted previously, al- though the reflxation effect (i.e., readers are more likely to refixate a w ord if the initial fixation is aw ay from the optim al viewing position) was obtained in reading, the processing-cost effect (i.e., a cost of 20 ms per character that the fixation is away from the optimal viewing position) was greatly attenuated or absent (Vitu, 1991c; Vitu et al., 1990).
A number of studies have tried to discriminate between these models. For example, Vitu and O'Regan (1995; see also Schroy- ens et al., in press) presented data that they argued are inconsis- tent with the oculomotor deadline proposed by Henderson and Ferreira (1990); specifically, they found that the duration of a single fixation is longer than the first of two fixations (see also Rayner et al., 1996; Schroyens et al., in press). Models with an oculomotor deadline should predict just the opposite, because when a second fixation on a word takes place, the deadline would be approached by the first fixation. On the other hand, Rayner et al. (1996) argued that data they obtained were incon- sistent with the oculomotor type of model; specifically, they found that when single fixations were made on a word, there was a frequency effect independent of where the reader fixated. In contrast, Reilly and O'Regan (1998) reported a number of simulations that led them to argue that the oculomotor type of model can better explain the details of where readers initially fixate in words. Although Reilly and O'Regan's analyses are quite valuable, because they demonstrate the value of using simulations to account for data patterns, their conclusion is not too surprising given that w here the eyes land w as never a central issue of processing models. Finally, Vitu, O'Regan, Inhoff, and Topolski (1995) examined eye movements when participants scanned transform ed text (in w hich each letter w as replaced with a z) and found that the eye movements were globally quite similar to when the same participants read text. Thus, they ar- gued that the similarity of the pattern suggested that predeter- mined oculomotor strategies were an important factor in eye movement control in reading. However, Rayner and Fischer (1996) showed that there are important differences between the two situations and that eye movements in reading are influenced by immediate processing demands.
Although there are extant data supporting both types of model, Rayner et al. (1996) argued that the evidence is more consistent w ith the processing type of m odel than the oculom o- tor type of model. More critically, they argued that neither type
of model is sufficiently precise for comprehensive testing. Thus, they suggested that future instantiations of models of eye move- ment control must be more explicit, perhaps through the use of mathematical or computational modeling. One such model was proposed by Just and Carpenter (1980) and a number have appeared recently (see Legge, Klitz, & Tjan, 1997; Reichle, Pollatsek, Fisher, & Rayner, 1998; Reilly, 1993; Reilly & O'Regan, 1998; Suppes, 1990, 1994). These models provide the hope of m ore explicit testing of underlying claim s related to eye movement control.
W hile Suppes's stochastic model (1990, 1994) has the virtue of mathematical precision, it cannot account for many effects that are known to influence fixation durations. Mr. Chips, the model proposed by Legge et al. (1997), is based to some extent on work done on the impact of low vision on reading (Legge, Rubin, Pelli, & Schleske, 1985). As such, it is an interesting attempt to generalize from low-vision reading to normal reading. However, many of the premises made by the model seem unreal- istic in term s of real-tim e processing events that occur in norm al reading. The Reader model of Just and Carpenter (1980; see also Thibadeau, Just, & Carpenter, 1982) attempted to account for reading processes from individual eye fixations to compre- hension processes. Although it was intended as a comprehensive model of reading, very few predictions can be derived from the model that are relevant for eye movement control. The £-Z Reader model (Reichle et al., 1998; see also Rayner, Reichle, & Pollatsek, 1998), on the other hand, is a formal instantiation of
13 Morrison's model (but with a number of modifications ) and
applies directly to eye movement control. Reichle et al. com- pared actual eye m ovem ent data w ith sim ulations from the model, and the fit between the model and data was very good. Furthermore, the model has the virtue that it accounts for many findings discussed in prior sections (e.g., preview benefits, fre- quency effects, spillover effects, constraint effects on skipping
4
behavior) w hile also m aking explicit predictions.'
In summary, there is controversy surrounding the issue of eye m ovem ent control, but som e findings and conclusions seem incontroversial. Specifically, where readers look next is related to the length of upcoming words (and where in the current word the reader is fixated), and how long they fixate on a word is related to the ease or difficulty associated with processing that word. It is hoped that more precision will be obtained on these
issues as models such as that of Reichle et al. (1998) appear.
Using Eye Movements to Study Language Processing in Reading
Considerable data have been collected that dem onstrate that eye m ovem ents are intim ately related to the m om ent-to-m om ent
13
For example, in the E-Z Reader model, a familiarity check that precedes lexical access serves as the signal for the initiation of an eye
EYE MOVEMENTS IN READING 3 8 9
m ovem ent, and com pletion covert attention.
14
of lexical
access
is the
signal
for a
shift of
As an example, the model predicted that when a word is skipped, the duration of the fixation prior to the skip and the duration of the fixation following the skip should both be inflated. W hereas the former effect had been previously noted (H ogaboam , 1983; Pollatsek et al., 1986), the latter had not. However, examination of data revealed such an effect.
 390 RA YNER Table 2
Variables and Specific Studies Influencing Eye Movements in Reading
V ariable
Semantic relationships between words
Repetition effects Morphemic units
Study
Carroll & Slowiaczek, 1986; Morris, 1994; S. C. Sereno, 1995; S. C. Sereno & Rayner, 1992
Hyona & Niemi, 1990; Inhoff et al., 1993; Raney & Rayner, 1995; Rayner, Raney, & Pollatsek, 1995 Beauvillain, 1996; Holmes & O'Regan, 1987, 1992; HySna & Pollatsek, in press; Inhoff, Briihl, &
Schwartz, 1996; Lima, 1987
Albrecht & Clifton, 1998; Blanchard, 1987; Duffy & Rayner, 1990; K. Ehrlich, 1983; K. Ehrlich &
Rayner, 1983; Garrod, Freudenthal, & Boyle, 1994; Garrod, O'Brien, Morris, & Rayner, 1990; Kerr & Underwood, 1984; Kennison & Gordon, 1998; O'Brien, Raney, Albrecht, & Rayner, 1997; O'Brien, Shank, Myers, & Rayner, 1988; Paterson, Sanford, Moxey, & Dawydiak, 1998; Vonk, 1984, 1985
Binder & Morris, 1995; Binder & Rayner, 1998; Dopkins, Morris, & Rayner, 1992; Duffy, Morris, & Rayner, 1988; Frazier & Rayner, 1990; Pacht & Rayner, 1993; Rayner & Duffy, 1986; Rayner & Frazier, 1989; Rayner, Pacht, & Duffy, 1994; S. C. Sereno, 1995; S. C. Sereno, Pacht, & Rayner, 1992
Carpenter & Daneman, 1981; Daneman & Reingold, 1993; Daneman, Reingold, & Davidson, 1995; Folk, 1998; Folk & Morris, 1995; Inhoff & Topolski, 1994; Jared, Levy, & Rayner, in press; Lesch & Pollatsek, 1993, 1998; Pollatsek et al., 1992; Rayner, Pollatsek, & Binder, 1998; Rayner, Sereno, Lesch, & Pollatsek, 1995
Birch & Rayner, 1997; Blanchard & Iran-Nejad, 1987; Carrithers & Bever, 1984; Dee-Lucas, Just, Carpenter. & Daneman, 1982; Dentsch, 1998; Fletcher, 1990, 1991, 1993; Frisson & Pickering, 1998; Grabe, Antes, Kahn, & Kristjanson, 1991; Grabe, Antes, Thorson, & Kahn, 1987; Hyona, 1995b; Hyona & Hujanen, 1997; Hyona & Jarvella, 1993; Inhoff, 1985; Kennedy & Pidcock, 1981; Morris & Folk, 1998; Rothkopf & Billington, 1979; Scinto, 1978; Vauras, Hyona, & Niemi, 1992
A naphora and
Dereference
Lexical ambiguity
Phonological am biguity
D iscourse factors conventions
and stylistic
Syntactic
disam biguation
A dam s, C lifton, & M itchell, 1998; A ltm ann, 1994;
Gamham, & Henstra, 1994; Altmann, van Nice,
Garrod, & Rayner, 1992; Brysbaert & Mitchell,
Abney, 1991; Ferreira & Clifton, 1986; Ferreira
1997; Frazier & Clifton, 1998; Frazier & Rayner, 1982, 1987, 1988; Frenck-Mestre & Pynte, 1997; Garnsey, Pearlmutter, Myers, & Lotocky, 1997; Holmes & O'Regan, 1981; Kennedy, Murray, Jennings, & Reid, 1989; Konieczny, Hemforth, Scheepers, & Strube, 1997; Liversedge, Pickering, Branigan, & van Compel, 1998; Mazuka, Itoh, & Kondo, 1997; Murray & Liversedge, 1994; Ni, Grain, & Shankweiler, 1996; Ni, Fodor, G ain, & Shankweiler, 1998; Paterson, Liversedge, & Underwood, in press; Pickering & Traxler, 1998; Rayner, Carlson, & Frazier, 1983; Rayner & Frazier, 1987; Rayner, Garrod, & Perfetti, 1992; Rayner & Sereno, 1994b, 1994c; Schmauder,
1991; Schmauder & Egan, 1998; Speer & Clifton, 1998; Traxler, Bybee, & Pickering, 1997; Traxler & Pickering, 1996a, 1996b; Trueswell, Tanenhaus, & Garnsey, 1994; Trueswell, Tanenhaus, & Kello, 1993; Zagar, Pynte, & Rativeau, 1997
cognitive processing activities of readers. T w o variables, w ord frequency andcontextualconstraint,werediscussedinaprior section. Table 2 lists a number of other variables that influence fixation tim e on a w ord or the pattern of eye m ovem ents. Briefly, with respect to these variables, the findings are as follows. First, w ords that are sem antkally related and in close proxim ity to each other produce priming effects. Thus, the word king in the same clause results in shorter fixation time on queen than if an unrelated word is present (Carroll & Slowiaczek, 1986; M orris, 1994). Second, when text is reread, fixation duration decreases (Hyona & Niemi, 1990; Inhoff et al., 1993); when high- and low-frequency words are encountered a number of times within a passage, fixation times on the words decrease, with the effect being more pronounced for low-frequency w ords" (Rayner, Raney, & Pollatsek, 1995). Third, readers look longer at prefixed w ords than pseudoprefixed w ords (L im a, 1987) and longer at m orphem es in long w ords that are m ore inform ative w ith respect to the overall meaning of the word (Hyona' & Pollatsek, in press). Fourth, fixation tim e in the region of a pronoun varies as a function of how easy it is to make the link between the pronoun and its antecedent (K. Ehrlich & Rayner, 1983), and w ords that invite the reader to m ake an elaborative inference
are fixated for less tim e than w hen such an inference is not made.Forexample,O'Brien,Shank,Myers,andRayner(1988) found that when the phrase stabbed her with his weapon was read early in a passage, readers subsequently fixated no longer on the target word knife than when the earlier phrase read stabbed her with his knife; in the weapon case, the reader pre- sumably inferred that the weapon was a knife. By contrast, when the original phrase read assaulted her with his weapon, readers fixated longer on the subsequent occurrence of knife. Fifth, fixation times on lexically ambiguous words are modu- lated by the characteristics of the word and the prior context. The basic finding is that when the preceding context is neutral
15
A curious fact is that during the first and second era of eye move- ment recording, researchers did not report effects of linguistic variables. For the most part, such effects were not examined, but they may not have been noticed because the tendency was to record eye movements while readers read passages of text. In such passages, words would often be repeated throughout the passage. Because the repetition effect observed by Inhoff et al. (1993) and Rayner et al. (1995) results in pretty much eliminating the word frequency effect, effects of word frequency would not be apparent.
Altmann, Garnham. & Dennis, 1992; Altmann, Gamham, & Henstra, 1998; Britt, Perfetti,
1996; Clifton, 1992, 1993; Clifton, Speer, &
& Henderson, 1990, 1993; Ferreira & McClure,
 (so that it does not bias either meaning of the word), readers look no longer at a biased ambiguous word (one with a highly dom inant m eaning) than a control w ord (m atched on length and frequency); however, they do look longer at a balanced ambiguous word (with two equally likely meanings) than at a control word. On the other hand, when the context biases one meaning, readers look no longer at a balanced word than the control; however, if the context biases the subordinate meaning of the word, they look longer at a biased word than at a control word (Duffy, Morris, & Rayner, 1988). Sixth, when a word is replaced by a hom ophone, readers look at it longer (D anem an & Reingold, 1993) unless the context is highly constraining of the correct word (Rayner, Pollatsek, & Binder, 1998). Seventh, stylistic conventions such as focus and dovetailing (the final content word of a sentence and the first content word of the next one have the same referent) influence eye movements (Birch & Rayner, 1997; Kennedy & Pidcock, 1981). Finally, if an incor- rect interpretation of a syntactically ambiguous phrase is made, fixation times on the disambiguating word increase or readers make an immediate regression (Frazier & Rayner, 1982).
The above findings raise some important issues with respect to eye movements and reading. First, what exactly controls fixa- tion time on a word? Some of the effects listed above are un- doubtedly related to postlexical processing, so is the decision to m ove off of a w ord based on lexical or postlexical processing? The answer to this question is not immediately obvious.Itcould be argued that the following variables influence lexical access: word frequency, contextual constraint, semantic relationships between words, lexical ambiguity, and phonological ambiguity. However, it seems less plausible that syntactic disambiguation effects are due to lexical access processes: When readers en- counter a disambiguating word, the lengthened fixation time must surely be due to postaccess integration processes. Like- wise, coreference and anaphoric processing consists of integra- tion processing. The best answer to the question (Carroll & Slowiaczek, 1987; Hyona, 1995b; Pollatsek, 1993; Rayner & Morris, 1990) is that lexical access (or familiarity check; Reichle et al., 1998) is the trigger to program a new eye move- ment but that other higher level (postaccess) processes intervene when something does not compute. Thus, fixation times primar- ily reflect lexical processing, but postaccess integration effects can have an effect as w ell.
A second general question concerns w hether effects of experi- mental manipulations on fixation time measures are produced by sim ilar effects in all relevant cases or by com binations of unequal effects in subsets of the data. McConkie (see McCon- kie, Reddix, & Zola, 1992; McConkie, Underwood, W olver- ton, & Zola, 1988; McConkie, Zola, & W olverton, 1985) has discussed the theoretical and m ethodological im plications of this "frequency-of-effects" issue and has shown that when 20% of the fixations are affected by a particular manipulation with the size of the effect being 100 ms, even though the rest of the fixations are not affected there w ill still be a 20-m s effect. O n the basis of such arguments, some researchers (Rayner, 1995; Rayner & Fischer, 1996) have shown frequency distributions for the effect being investigated to dem onstrate that the distribution shifted and was not due to a subset of the data.
A final question that is relevant with respect to the findings discussed in this section concerns the extent to which eye move-
m ent data can be m im icked by other laboratory tasks. O ne great virtue of eye movement data is that readers are engaged in the task of normal reading and can proceed at their own rate. Many years ago, Tinker (1939) demonstrated that reading rate and com prehension w ere the sam e w hen readers read in an eye movement laboratory (with their heads fixed) and when they read in an easy chair. Given the cost and effort that is required to obtain eye m ovem ent data, m any researchers have opted for tasks that are thought to simulate eye movement data. The two primary techniques that have been used are rapid serial visual presentation (R S V P ) of text and self-paced reading. In the RSVP task, words are presented at a set rate (such as every 100-200 ms) in the same spatial location. Comparisons of this task to normal reading have typically revealed that with short sentences, results that are typical of normal reading can be obtained. However,when longer passages are presented, the pro- cessing system quickly gets overloaded, and comprehension breaks dow n (see M asson, 1983). W ith self-paced reading, text is presented a word or two at a time and readers advance the text by pushing a button. There are a num ber of variations of this technique: Some allow readers to look back in text (cumulative methods), some do not (so that only 1-2 words are tin the screen at a tim e); and in som e, w ords are presented sequentially in the same location, whereas in others, words are presented spatially as in normal text. There are clear trade-offs associated with these techniques. One virtue when only 1 word is presented is that a processing-tim e m easure is associated w ith each w ord in the text (i.e., words cannot be skipped). However, the most natural method, the self-paced cumulative moving window, in- volves cumulative presentation of words so that, if a reader looks back in the text before pushing the button, there is no way of knowing this. For this reason, this technique is seldom used. The alternative, self-paced noncumulative moving window, has the problem that readers are unable to reread words they did not understand (or misanalyzed).
A general problem with all of the variations of the self-paced reading technique is that reading rate is about half as fast as it is normally. The fact that readers are slowed down by the de- mands of the task (because the reaction time of the eyes is faster than the reaction time of the fingers) suggests that some caution has to be exerted in generalizing from these tasks to normal reading (Rayner, Sereno, et al., 1989). There have been many com parisons of R S V P and self-paced reading tasks to norm al reading (luola, W ard, & McNamara, 1982; lust, Carpenter, & W oolley, 1982; Kennedy & M urray, 1984; Rubin & Turano, 1992). In many cases, similar results have been obtained, but this is not always the case (Just, Carpenter, & Woolley, 1982; Kennedy & Murray, 1984; Magliano, Graesser, Eymard, Ha- berlandt, & Gholson, 1993). Researchers interested in the topic of syntactic ambiguity and parsing have been particularly keen to show that similar results are obtained when self-paced reading and eye movement data are compared (see Ferreira & Clifton, 1986; Garnsey, Pearlmutter, Myers, & Lotocky, 1997; Ken- nedy & Murray, 1984; Trueswell, Tanenhaus, & Kello, 1993); indeed many results are quite similar across the two situations. Perhaps it is not surprising that, when dealing with syntactic ambiguity where readers are "garden-pathed" and have diffi- culty getting the appropriate meaning, self-paced reading and eye m ovem ent data are quite sim ilar because processing has
EYE MOVEMENTS IN READING 391
 392 RAYNER
been disrupted and readers must use some type of problem- solving strategy to compute the correct meaning. Given that they are going slow ly in both situations, the results m ight w ell be fairly comparable. Even if this is the case, caution is war- ranted in generalizing from self-paced reading to normal reading because of the depressed reading rate.
How well do standard laboratory tasks like naming and lexical decision compare with eye movement data? Everatt and Un- derwood (1994) found little correlation between eye fixation times and lexical decision times. However, Schilling, Rayner, and Chumbley (1998) compared naming and lexical decision times for words presented in isolation to fixation times on the same words inserted in neutral sentence contexts and found reasonably good correlations between the measures. The size of the word-frequency effect (i.e., the difference in reaction times for high- and low-frequency words) was fairly equivalent for naming and gaze duration (with the size of the effect a bit smaller for first fixation and single fixation duration). The size of the frequency effect was exaggerated in the lexical decision times compared with the eye fixation times. These results sug- gest that researchers can have some confidence that results ob- tained with standard naming and lexical decision tasks general- ize to word recognition processes while reading.16
Another procedure that has been adopted recently is to use the same stimulus materials across different tasks to obtain con- verging evidence on a topic of interest (see Altarriba et al., 1996; Inhoff, Briihl, et al., 1996; O'Brien, Raney, Albrecht, & Rayner, 1997; Schustack et al., 1987). Im portant inform ation can be gleaned both when the results are consistent (e.g., the results are generalizable across tasks) and when they are incon- sistent. In the latter case, differences across tasks can enlighten our understanding not only of the specific tasks but of the pro- cesses involved.Recently, studies (Raney & Rayner, 1993; S. C. Sereno, Rayner, & Posner, 1998) have been reported with event- related potential data in conjunction with eye m ovem ent data, which makes it possible to address issues that would be other- wise difficult to examine. For example, S. C. Sereno et al. (1998) examined the time line of lexical access and saccade program m ing.
Finally, a technique called fast priming (S. C. Sereno & Rayner, 1992) combines priming techniques with eye movement paradigms. In this technique, a random string of letters initially occupies a target location in a sentence. When the reader's eye movement crosses a boundary location, the random letter string is replaced by a prime word that is presented briefly (30-40 ms) at the onset of the fixation and then changes to the target word. Studies using this technique have examined semantic priming (S. C. Sereno, 1995; S. C. Sereno & Rayner, 1992), phonological priming (Lee, Binder, Kim, Pollatsek, & Rayner, in press; Rayner, Sereno, Lesch, & Pollatsek, 1995), and ortho- graphic priming (Lee et al., in press; Rayner, Sereno, et al., 1995).
Eye Movements When Reading and Looking at Pictures
Carroll, \bung, and Guertin (1992) found that when viewers looked at cartoons, mean fixation durations on the cartoon were longer than on the caption. There was not a great deal of consis- tency in where viewers looked first. They did not move back
and forth repeatedly between cartoon elements, and the picture frequently was not given a full inspection until the caption was read. Similar results were obtained when complex mechanical diagrams in text (Hegarty, 1992a, 1992b) and subtitled TV pro- grams (d'Ydewalle & Gielen, 1992) were used.
Eye Movements and Individual Differences in Reading
There are well-known individual differences in eye movement measures as a function of reading skill: Fast readers make shorter fixations, longer saccades, and fewer regressions than slow read- ers (Everatt, Bradshaw, & Hibbard, 1998; Everatt & Un- derwood, 1994; Rayner, 1978b; G. Underwood, Hubbard, & W ilkinson, 1990). Likewise, bilingual readers make shorter fix- ations, longer saccades, and fewer regressions in their dom inant language (Altarriba et al., 1996; Chincotta, HyonS, & Un- derwood, 1997), and children who stutter make more fixations and regressions than nonstutterers (Brutten, Bakker. Janssen, & VanderMeulen, 1984). In characterizing the eye movement pat- terns of dyslexic readers, Olson, Kliegl, Davidson, and Foltz (1985) categorized such readers as plodders and explorers; plodders made relatively short forward saccades and very few regressions, whereas explorers showed more frequent word skipping, longer forward saccades, and more regressions. It is likely that skilled readers can be divided into similar styles (Rothkopf, 1978). Table 3 shows eye movement characteristics of 10 skilled readers with good comprehension. It is clear that there is a considerable am ount of difference been them . In the rem ainder of this section, studies dealing w ith speedreading, developmental changes in eye movements, and eye movements of dyslexic readers are discussed.
Speedreading
Unfortunately, there is very little good data on the characteris- tics of the eye movements of speedreaders. Rayner and Pollatsek (1989) reviewed some early studies but pointed out that the studies were disappointing because of inadequate control condi- tions or because comprehension tests were not taken. The only good recent report of the characteristics of speedreaders' eye movements was provided by Just, Carpenter, and Masson (1982; also reported in Just & Carpenter, 1987). They monitored the eye movements of speedreaders (reading at approximately 600- 700 words per minute [wpm]) and normal readers (reading
16
Not every variable that affects isolated word recognition influences eye movements in reading. For exam ple, Rayner and Duffy (1986) found no effect of verb complexity on fixation times; Inhoff, Lima, and Carroll (1984) found little effect of whether or not a literal or metaphoric interpretation was associated with a target word (though regression frequency was affected); Perea and Pollatsek (1998) found effects of neighborhood frequency on only spillover fixations (see Pollatsek, Perea, and Binder, in press, for earlier affects; and see Grainger, O'Regan, Jacobs, & Sequi, 1992, for how initial fixation location and neighborhood frequency interact for isolated words); and Inhoff and Tbpolski (1994) found that the effects of orthographic regularity were rather transient in the eye movement record. However, S. C. Sereno, Rayner, and Posner (1998) found a regularity effect for low-frequency words, whereas, consistent with word recognition studies, there was no effect for high- frequency w ords.
 Table 3
Mean Fixation Duration, Mean Saccade Length,Percentage Regressions, and Words per Minute (WPM)
for 10 Skilled Readers
Fixation Saccade Regressions
found that children in their 1st year of reading show the same landing position patterns as adults; like adults, they send their eyes to the middle of a word. McConkie et al. (1991) also concluded that one of the prim ary differences betw een the eye movements of children and adults is the frequency with which words are refixated before the reader moves to another word; adults refixated 5 letter words 15% of the time, whereas first- grade children refixated 5 letter w ords 57% of the tim e. A s noted earlier, Rayner (1986) found that the perceptual span of beginning readers was smaller than skilled readers and that the span was asymmetric for beginning readers. Apparently, 1 year of reading is sufficient for the developm ent of the asym m etry of the span. Finally, the effect of aging on eye movements in reading is not com pletely clear. Solan, Feldm an, and Tujak (1995) found that elderly adults read more slowly and made more fixations (and regressions) than younger readers. However, they also found thatreading efficiency training greatly improved perform ance. A lthough Solan et al. did not exam ine fixation durations, it is highly likely that they increase, because saccade latency in elderly adults is longer than younger adults (Abel et al., 1983).
Eye Movements, Poor Readers, and Dyslexia
Poor readers and dyslexic readers, like beginning readers, make longer fixations, shorter saccades, more fixations, and more regressions than normal readers. These facts have been known for some time and have been reinforced by many recent studies (Adler-Grinberg & Stark, 1978; Eden, Stein, W ood, & W ood, 1994; Elterman, Abel, Daroff, DeU'Osso, & Bernstein,
1980; Lefton, Nagle, Johnson, & Fisher, 1979:Martos & Vila, 1990). Lefton et al. (1979) found that normal developmental gains made by children (wherein fixation duration decreases, saccade length increases, and the frequency of regressions de- creases) are not shown by dyslexic readers.
No one would dispute that the eye movements of dyslexic readers are different from norm al readers. O ne critical question that reemerged recently concerns the extent to which eye move- ments per se represent the cause of reading problems. Certainly, people with low vision (Bowers & Reid, 1997; BuHimore & Bailey, 1995; Legge, Rubin, Pelli, & Schleske, 1985), oculomo- tor disturbances such as saccade intrusions (Ciuffreda, K en- yon, & Stark, 1983), or congenital jerk nystagm us (Ciuffreda, 1979) have difficulty reading (see Ciuffreda, 1994, and Ciuf- freda & T annen, 1995, for m ore extensive discussion of the eye movements of patients with oculomotor disturbances). The more critical issue is w hether or not reading disability in the absence of obvious oculom otor problem s can be attributed to faulty eye movements. If eye movements were a causative factor in reading disability, then the problem could easily be diagnosed with a sim ple eye m ovem ent test, and oculom otor training w ould result in improved reading. Although there have been some demonstra- tions that oculom otor training im proves reading perform ance (see Solan, 1985), Tinker (1946, 1958) and Rayner (1985b) argued quite strongly that eye m ovem ents w ere generally not a cause of reading disability but were a reflection of other underly- ing problems.
This issue was brought back into the spotlight by Pavlidis (1978, 1981, 1983, 1985, 1991). He reported experiments in
R eader duration length
KB 195 9.0 JC 227 7.6 AK 190 8.6 TP 196 9.5 TT 255 7.7 GT 206 7.9 GB 205 8.5 BB 247 6.7 LC 193 8.3 JJ 241 7.2
M 216 8.1
(% ) WPM
Note. Fixation duration is in milliseconds, and saccade length is in character spaces (4 characters = 1° of visual angle). The text was fairly easy reading. If the text w ere m ore difficult, for each reader, the m ean fixation duration would likely increase, the mean saccade length would likely decrease, the percentage of regressions would likely increase, and the reading rate would decrease.
From "Eye Movements in Reading and Information Processing" by K. Rayner, 1978b, Psychological Bulletin, 85, p. 627. Copyright 1978 by the A m erican Psychological A ssociation.
around 250 wpm). In addition, the normal readers were asked to "skim " the text to produce rates around 600-700 w pm . W hen tested after reading, the speedreaders did as w ell as the normal readers (when reading at their normal speed) on general comprehension questions or questions about the gist of the pas- sage. However, when tested about the details of the text, speedreaders could not answer questions if they had not fixated on the region where the answer was located. The normal readers, whose fixations were much denser than the speedreaders, were able to answer the detail questions relatively well. When the normal readers were asked to skim the text, both their eye move- ment patterns and comprehension measures were quite similar to those of the speedreaders. In general, the results of research on the characteristics of speedreaders' eye movements are con- sistent with the idea that they are skimming the text and not really reading every word (see Rayner & Pollatsek, 1989,for more details).
Developmental Changes in Eye Movements
For over 70 years (Buswell, 1922), it has been known that there are developm ental trends in eye m ovem ents during read- ing: A s reading skill increases, fixation duration decreases,sac- cade length increases, the number of fixations decreases, and the frequency of regressions decreases. Table 4 shows summary data from Buswell (1922), S. E. Taylor (1965), Rayner (1985b), and McConkie et al. (1991) on different measures. A lthough eye m ovem ents w ere recorded in very different w ays, the trends in the data are remarkably consistent.
McConkie et al. (1991) examined children's eye movement behavior and found that they tended to have more variability in their eye movement indices than adults. However, they also
EYE MOVEMENTS IN READING 393
6
12 251 11 348 15 382 19 244
4 335
6
11 308
347 1 257 20 314 14 230
378
 394 RAYNER Table 4
Developmental Characteristicsof Eye Movements During Reading
A rticle and characteristic
6 A dult
270 240 120 90 21 17
236 252 87 75 21 8
242 239 110 92 24 9
248 243 200 —
132 135 118 —
S. E. Taylor (1965) Fixation duration (m s) Fixations per 100 w ords Frequency of regressions
Buswell (1922)
Fixation duration (m s) Fixations per 100 words Frequency of regressions
Rayner (1985b)
Fixation duration (m s) Fixations per 100 words Frequency of regressions
McConkie et al. (1991) Fixation duration (m s) Fixations per 100 words Frequency of regressions
M
Fixation duration (m s) Fixations per 100 words Frequency of regressions
Note. Dashes indicate that
(%)
(%)
(%)
(%)
(%)
—
122 — 25
which normal children, backward readers (whose reading dis- ability could be accounted for by intelligence, motivation, or socioeconomic factors), and dyslexic readers followed a dot that moved across a screen. He reported that when the dot moved from left to right, dyslexic readers made many more right-to- left saccades than either the normal or backward readers. Thus, he argued that faulty eye movements or some type of central tem poral ordering problem could be diagnostic of dyslexic read- ers (Pavlidis, 1981). Pavlidis quite rightly pointed out that ex- amining the eye movements of dyslexic readers during reading was inconclusive because one would not be able to tell if eye movements per se or the dyslexic individual's problems with reading accounted for the differences betw een their eye m ove- ment characteristics and those of normal readers. The fact that erratic eye m ovem ent patterns w ere obtained in the absence of text thus suggested to Pavlidis that eye movements are a contributing factor to reading disability. However, a number of attempts to replicate Pavlidis's findings have failed to confirm his results (Black, Collins, DeRoach, & Zubrick, 1984a; B. Brown, Haegerstrom-Portnoy, Adams, et al., 1983; Olson, Con- ners, & Rack, 1991; Olson, Kliegl, & Davison, 1983a; Stanley, 1994; Stanley, Smith, & Howell, 1983a). Furthermore, other studies have failed to find differences between normal and dys- lexic readers' eye movements during nonreading tasks (Adler- Grinberg & Stark, 1978; Black, Collins, DeRoach, & Zubrick, 1984b, 1984c; B. Brown, Haegerstrom-Portnoy, Adams, et al., 1983; B. Brown, Haegerstrom-Portnoy, Tingling, Herron, Galin, & Marcus, 1983; Eskenazi & Diamond, 1983; Fields, Wright, & Newman, 1993; Olson, Conners, & Rack, 1991; Stan- ley, Smith, & Howell, 1983b).
In addition to Pavlidis, others have argued that poor eye m ove- ment control is characteristic of some dyslexic readers. Eden et
al. (1994) found that dyslexic readers had worse stability of
fixation on sm all targets than norm al readers and that dyslexic
readers' vergence and smooth pursuit eye movements differed
from norm al readers. It is not clear w hat to m ake of these
findings because pursuit eye m ovem ents are not characteristic
17
of reading.
by Pavlidis (1981) and Raymond, Ogden, Pagan, and Kaplan (1988). However, Raymond et al. also found that the latency and accuracy of eye movements of dyslexic children did not differ from those of normal readers. Finally, Fischer and col- leagues (Biscaldi, Fischer, & Aiple, 1994; B. Fischer, Bis- caldi, & Otto, 1993; B. Fischer & Weber, 1990) found that some dyslexic readers exhibit m ore express saccades than norm al readers. They argued that express saccades are related to atten- tional processing. Given the failures to replicate Pavlidis's work, it would be premature at this point to arrive at firm conclusions regarding the relationship between eye movements and dyslexia until these results are replicated in other labs.
It has also been suggested that dyslexic readers process less parafoveal inform ation on each fixation than do norm al readers (see Farmer & Klein, 1995). Experiments by Rayner, Murphy, Henderson, and Pollatsek (1989) and N. R. Underwood and Zola (1986) used the moving window technique to investigate this issue. Although N. R. Underwood and Zola found little difference between good and poor readers (fifth-grade children
17
Stein and colleagues (Stein, 1989, 1994; Stein, Riddell, & Fowler, 1988) have presented data to indicate that dyslexic readers have fine binocular control problems, which result in their being unable to hold their eyes steady. On the other hand, Lennerstrand, Ygge, and Rydberg (1994) found no differences between normal and dyslexic readers with respect to binocular control.
1 2
330 300 224 174 23 23
432 364 182 126 26 21
290
— 165
Grade level
3 4 5
280 270 270 155 139 129 22 22 21
316 268 252 113 92 87 20 19 20
276 ——
data were not collected.
—
27 ———
304 268 168 138 34 33
355 306 191 151 28 26
262 125 34
286 131 25
36 36
266 255 121 117 26 26
— 21
249 233 106 94 22 14
The finding of fixation instability was also reported
 reading 1-2.5 years behind their expected reading level), Rayner, Murphy, et al. (1989) found that the perceptual span was smaller for dyslexic readers than for normal readers. How- ever, this finding does not necessarily mean that dyslexic readers process parafoveal inform ation less effectively than do norm al readers (Rayner, Pollatsek, & Bilsky, 1995). As noted in an earlier section, norm al readers obtain less parafoveal inform a- tion when the fixated word is difficult to process (Henderson & Ferreira, 1990; Rayner, 1986). Rayner (1986) showed that when children were given difficult text to read, their perceptual span got smaller. Thus, whereas the perceptual span is somewhat smaller for dyslexic readers than for normal readers, it is proba- bly not the case that dyslexic readers process parafoveal infor- mation less effectively but rather that they have difficulty pro- cessing the fixated word.
Actually, there is some irony with respect to this issue since Geiger and Lettvin (1987) proposed that the cause of dyslexia was that dyslexic readers processed parafoveal information more effectively than normal readers. On the basis of experi- ments in which dyslexic and normal readers had to simultane- ously identify foveal and parafoveal letters, they suggested that dyslexic readers' efficient parafoveal processing interfered with foveal processing, and this is what causes their reading prob- lems. Indeed, Geiger, Lettvin, and Fahle (1994) argued that dyslexic readers could markedly increase their reading ability by cutting a small window in an index card and reading the material inside die window as they moved it across the text. However, there is considerable controversy regarding whether or not dyslexic readers process parafoveal information more effectively than do normal readers. Perry, Dember, W arm, and Sacks (1989) reported results that were consistent with Geiger and Lettvin (1987), but others (Goolkasian & King, 1990; Klein, Berry, Briand, D'Entremont, & Farmer, 1990; Slaghuis, Lovegrove, & Freestun, 1992) were unable to replicate the find- ings. Finally, Rayner,Murphy, et al. (1989) described a dyslexic reader w ho show ed characteristics that w ere som ew hat like those of Geiger and Lettvin's (1987) participants: He could identify parafoveal w ords and letters better than the norm al readers could, and, when reading in a moving window experi- ment, he read better with a small window than with a larger one." Rayner, Murphy, et al. (1989) argued that he had a selec- tive attention deficit, which made it difficult for him to focus attention on the fixated word, but that he was atypical of dyslexic readers.
In concluding this section, the question arises as to why there is so much inconsistency in the results of experiments on dys- lexia and eye movements. Why, for example, did Pavlidis find evidence for erratic eye movements in nonreading tasks while other investigators found no such evidence? There are many reasons why inconsistent results may be obtained with a group of abnormal participants and variability is generally greater with abnormal than normalparticipants.However, one likely possibil- ity (Morris & Rayner, 1991; Pollatsek, 1983; Rayner, 1983, 1985a) is that a subgroup of dyslexic readers have erratic eye movements as found by Pavlidis. Indeed, case studies of dyslexic readers reported by Zangwill and Blakemore (1972) and Piroz- zolo and Rayner (1978, 1979) have suggested that some dys- lexic readers' eye movement characteristics are in close agree- ment with those reported by Pavlidis." Likewise, as noted, the
dyslexic reader reported by Rayner, Murphy, et al. (1989) was consistent with data reported by Geiger and Lettvin (1987). However, it is not necessarily the case that eye m ovem ents per se are the problem. Rather, the evidence suggests that these dyslexic readers have a visual-spatial deficit and that eye move- ments reflect a more serious underlying spatial orientation prob- lem. For the vast majority of dyslexic individuals, the evidence suggests that they have a language processing deficit and that their eye m ovem ents sim ply reflect their difficulty processing language. Three findings are consistent with this conclusion. First, HyOna and Olson (1995) found that dyslexic readers show the typical word frequency effect in which low-frequency words are fixated longer than high-frequency words. Second, Pirozzolo and Rayner (1978) found that when dyslexic readers with a language processing deficit were given text appropriate for their reading level, their eye movements were much like those of normal readers at that particular age level (see also Olson, Kliegl, & Davidson, 1983b); when they were given age appro- priate text, their eye movement characteristics showed the usual differences from those of normal readers. Third, Rayner (1986) showed that normal children's eye movements (saccade lengths, fixation durations, and the size of the perceptual span) could be made to look characteristic of dyslexic readers' eye move- ments when they were given text that was too difficult for them. In sum, the most appropriate conclusion remains that'eye move- ments reflect the difficulties that dyslexic individuals have read- ing and are not the cause of the reading problem.
Eye Movements in Music Reading and Typing
Music Reading
Although some classic studies of eye movements while read- ing music (usually by pianists) were carried out over 50 years ago (see W eaver, 1943), only recently has interest in the topic reemerged. Weaver found that (a) the number of notes per fixa- tion was about 1.5, (b) the average fixation duration varied as a function of the difficulty of the music with more difficult selections leading to longer fixations (460 m s) than easier selec- tions (365 m s), (c) 7-23% of all fixations were regressions, (d) the eye-hand span (how far the eyes are ahead of the hand) was variable (but never exceeded a separation of eight successive notes or chords) and was influenced by the difficulty of the selection, and (e) progression along the musical score was accomplished in two ways: When the music was homo- phonic and chordal, pianists tended to use a vertical fixation sequence in w hich they alternated vertical m ovem ents from one half of the staff to the other half; however, when the music was contrapuntal, there tended to be a series of horizontal saccades followed by a vertical movement and then more horizontal saccades.
18
xs outside the window. If there were other letters outside the window,
he had problem s reading.
19
For example, Pirozzolo and Rayner (1979) reported a case study of a person who could read text better when it was upside-down (and the eye movements going from right to left) than when it was in its normal orientation (see also Larsen & Parlenvi. 1984).
EYE MOVEMENTS IN READING
395
Actually, he read better when the window was small and there were
 396 RAYNER
An issue that has surfaced recently concerns where musicians fixate in the musical score. Goolsby (1994a, 1994b) found that skilled music readers did not fixate directly on each note or item of information that was performed. Rather, they often fixated in blank areas of the musical score. He also found that they seldom fixated on note stems or the bars connecting eighth notes, indi- cating that the inform ation necessary to acquire the rhythm ic aspect of the notation is acquired vertically and horizontally within a fixation. Finally, Goolsby (1994b) reported that melo- dies in w hich the notes and other visual inform ation w ere spaced close together resulted in fewer fixations than less condensed notation. Kinsler and Carpenter (1995) found that as the tempo of performance increased, the average time between saccades decreased but the mean amplitude increased. They also found that musicians' eye movements tended to be unrelated to the execution of the elements of performance. Specifically, some of the eye fixations are "holding" periods when the eyes are wait- ing for the hands to catch up (see also Goolsby, 1989).
Two other topics have received attention recently: (a) individ- ual differences and (b) the perceptual span. W ith respect to the former, Goolsby (1994a) compared 12 skilled and 12 less skilled music readers as they read music and hummed the mel- ody. The two groups were compared on (a) progressive fixation duration, (b) num ber of progressive fixations, (c) progressive saccade length, (d) number of regressive fixations, (e) regres- sive fixation duration, and ( f ) regressive saccade length. Gool- sby found that the two groups reliably differed only in the duration of progressive fixations and that less skilled readers were much more likely to fixate on notes and rests than skilled readers. On the other hand, Polanka (1995) found little differ- ence in looking patterns as a function of expertise. However,in a study in w hich skilled m usicians, less skilled m usicians, and nonmusicians were asked to determine whether two scores were identical or not, W aters, Underwood, and Findlay (1997) found that the skilled readers were able to make the judgment with fewer eye movements and shorter fixations than the other two groups; the nonmusicians made more saccades and longer fixa- tions than the less skilled musicians.
Truitt, Clifton, Pollatsek, and Rayner (1997) recently re- ported a moving window study in which they examined the perceptual span of pianists. They found that when the window included one measure there was little difference from when the entire line of music was available. On the other hand, a half- measure window slowed down performance. Truitt et al. also examined individual differences in eye movements when reading music and found results quite consistent with those reported by Goolsby (1994b): The two groups differed only in average fixation duration. Finally, examination (see also Rayner & Pol- latsek, 1997) of the eye-hand span yielded results that were quite consistent with Weaver's findings: The eye-hand span ranged from the eyes being 2 beats behind the hand to being 12 beats ahead of the hand (however, the eyes were between 1 and 3 beats ahead of the hand 88% of the time).
Typing
Classic studies of eye movements during typing were reported over 50 years ago (Butsch, 1932). More recently, a number of studies have been reported by Inhoff and colleagues (Inhoff,
1991; Inhoff, Briihl, Bohemier, & W ang, 1992; Inhoff, Cala- brese, & Morris, 1986; Inhoff, Chiu, & Wang, 1990; Inhoff & G ordon, 1997; Inhoff, T opolski, & W ang, 1992; Inhoff & W ang, 1992). The primary finding reported by Butsch (1932) was that typists, irrespective of skill level, attem pt to keep the eyes 1 s ahead of the currently typed letter. However, Inhoff's studies have found no support for the 1-s hypothesis. Rather, there was considerable variability in the size of the eye-hand span (M = 2.8 letters), and it was affected by lexical properties of the text. As in music reading, Butsch and Inhoff have both found that there are periods of time when the eyes wait in place for the hand to catch up.
Inhoff and colleagues also examined the size of the perceptual span. Prior research (Salthouse, 1984; Shaffer, 1973) used a moving window paradigm in which the movement of the win- dow coincided with keypresses. However,the eyes do not move one letter space to the right with each keypress. Thus, Inhoff and Wang (1992) and Inhoff, Briihl, et al. (1992) used the eye- contingent moving window paradigm to determine the size of the span in typing. The results indicated that (a) when fewer than seven letter spaces were visible to the right of fixation, saccade size decreased and (b) interkeypress times increased when fewer than three letter spaces were visible. Thus, the per- ceptual span and the region from which letter information is used in typing is about half that of reading.
Eye M ovem ents and V isual Search
In com parison to reading, there have not been nearly as m any
studies dealing with visual search. In fact, most studies of visual
search rely on presenting stimuli for brief exposures, and re-
searchers either assume that viewers do not make eye move-
20
ments or their eye movements are monitored to make sure
they do not move their eyes (Klein & Farrell, 1989). Other researchers seem to assume that understanding eye movement behavior in visual search may not be particularly important. However, many studies (Binello, Mannen, & Ruddock, 1995; Findlay, 1995,1997; Findlay & Gilchrist, 1997; Gilchrist, Find- lay, & Heywood, in press; Gould, 1973; Miller, 1978; Motter & Belky, 1998a, 1998b; Ponsoda, Scon, & Findlay, 1995; Previc, 1996; Scialfa & Joffe, 1998; Scialfa, Thomas, & Joffe, 1994; Scinto, Pillalamarri, & Karsh, 1986; Staller & Sekuler, 1980; Togami, 1984; Williams, Reingold, Moscovitch, & Behrmann, 1997; Zelinsky, 1996; Zelinsky, Rao, Hayhoe, & Ballard, 1997; Zelinsky & Sheinberg, 1997) have indicated that eye movement data yield im portant inform ation about visual search. Indeed, Findlay and Gilchrist (1998) argued that the tradition to pay little attention to eye m ovem ents in search research is m is- guided, and they demonstrated that when viewers are free to move their eyes, no additional covert attentional scanning occurs.
Whereas most studies of eye movements during visual search have examined complex search tasks, some (Findlay, 1997; Ot- tes, Van Gisbergen, & Eggermont, 1985; Viviani & Swensson,
20
Recent studies by Jordan, Patching, and M ilner (1998) and Patching and Jordan (in press) have demonstrated that when fixation location is not ensured by an eye-tracking system, central fixations occurred on only 23% of trials.
 1982) involved presenting a fairly simple array to participants and examining the characteristics of the initial saccade as a function of the array. Both accuracy and latency of the initial saccade were influenced by the characteristics of the array. Vivi- ani and Swensson (1982) had participants locate a star-shaped target among 15 disk-shaped distractors located between 4.1" and 12.7° eccentricity. W hen the target was at a small eccentric- ity, participants located it accurately with a single saccade (see also Findlay, 1997); when the target was more peripheral, wrongly directed initial saccades were common (up to 40% of the time). In free viewing tasks, the tendency to fixate the object closest to fixation is very coercive (Engel, 1977). Zelinsky et al. (1997) found that in a complex search task that participants initially directed their eyes to the center of the global display and then to the centers of recursively smaller groups of objects until the target was acquired.
W hen eye m ovem ents are recorded during extended search, fixations tend to be longer than in reading. However, there is considerable variability in fixation time and saccade length as a function of the particular search task. Specifically, visual search tasks vary widely, and tasks in which eye movements have been monitored consist of at least the following: search (a) through text or textlike material (Rayner & Fisher, 1987a, 1987b), (b) with pictorial stimuli (Boersma, Zwanga. & Adams, 1989; Nodine, Carmody, & Herman, 1979), (c) with complex arrays such as X-rays (Carmody, Nodine, & Kundel, 1980, 1981), and (d) with randomly arranged arrays of alphanumeric characters or objects (Zelinsky, 1996). Because the nature of the search task influences eye movement behavior, any statement about visual search and eye movements needs to be qualified by the characteristics of the search task.
The Perceptual Span
Rayner and Fisher (1987a) used the moving window tech- nique as viewers searched through horizontally arranged letter
21
of a target letter. They used both the moving window technique and the moving mask technique described earlier. The number of items in the array was held constant, but the size of the display was varied (13° X 10°, 6° X 6°, and 5° X 3.5°). In the moving window study, search performance reached asymptote when the window was 5°. The moving mask had a deleterious
strings for a specified target letter.
the span varied as a function of the difficulty of the distractor letters. When the distractor letters were made up of dissimilar letters (e.g., the target letter was b and the distractor letters were v,s,z,c,r,w,andn),theperceptualspanextendedfrom3to 4 letters to the left of fixation to about 15 letters to the right. However, when the distractor letters were similar to the target (h, k, d, and / when the target was b), the span extended only about 12 letters to the right of fixation. Rayner and Fisher (1987b) found that there were two qualitatively different regions within the span: a decision region, where information about the presence or absence of a target is available, and a preview re- gion, where partial letter information is available but where information on the absence of a target is not available. They also found that the decision region varied from 3 to 4 letters when the target was similar to the distractor letters to 5 -6 letters when the target was dissimilar to the distractors. Likewise, they found that the preview region varied from 5 -6 letters (to the right of the decision region) when the target was similar to the distractors to 7-8 letters when the target was dissimilar to the distractors.
Bertera and Rayner (1998) had viewers search through a randomlyarrangedarrayoflettersanddigitsforthepresence
The difficulty of the search task influences eye movements. Zelinsky and Sheinberg (1997; see also Williams et al., 1997) found that fixations were longer, saccades were shorter, and more eye movements were made in serial search tasks (such as searching for a circle among Q-like distractors) than in parallel search tasks (a single Q-like target embedded in a field of circles). A num ber of studies have found that when distractors are sim ilar to targets, fixation tim e increases, m ore fixations are made, and saccade size decreases (Noyes, 1980; R. J. Phillips, 1981; Rayner & Fisher, 1987a, 1987b). Nattkemper and Prinz (1984) also found that fixation duration is affected by the redun- dancy or predictability of the array. Finally, Moffitt (1980) dem- onstrated that the difficulty of the array (complexity and density of the array) generally, though not invariably, influenced the average fixation duration.
Comparisons With Reading
Some studies have compared eye movements when reading with visual search when viewers search either text or letter strings searching for a target letter while moving their eyes from left to right. A lthough V itu et al. (1995) argued that there were many eye movement similarities between the two tasks, Rayner and Fischer (1996) demonstrated that there were important dif- ferences between them. In particular, they found that when view- ers were asked to search for a target word, the word frequency effect disappeared (see also Rayner & Raney, 1996). Thus, although readers looked longer at low-frequency words than high-frequency words, when the task was changed to search, there was no difference in how long the two types of words were fixated. Rayner and Fischer (1996; see Vitu et al., 1995) also found that fixations were longer in search than in reading (when the stimulus was the same in the two situations). As noted earlier, readers do not obtain inform ation from below the line they are reading. However, in search tasks in which viewers search through rows of letters, Prinz (1984; Prinz, Natt- kemper, & Ullann, 1992) found that information is obtained from below the fixated line. A lthough there are some sim ilarities between eye movements in search and reading, there are im- portant differences. Thus, it is hazardous to generalize from search to reading given that the cognitive demands differ in the
21
In Rayner and Fisher's (1987a) experiments, the search array al- ways consisted of strings of letters, with the string size being either 2, 4, 6, or 8 letters. Spaces were inserted between the strings (such as pklmjhtrnbfd).Thestringsweresearchedfromlefttoright.
EYE MOVEMENTS IN READING 397
They found that the size of
effect on search tim e and longer the search tim e.
Search Difficulty
accuracy;
the larger the
m ask, the
 398 RAYNER
Eye Movement Control
V aughan and colleagues (G raefe & V aughan, 1978; V aughan, 1982, 1983; V aughan & G raefe, 1977) tried to determ ine whether fixation durations in visual search are influenced by the nature of currently fixated information or whether they are preprogrammed (i.e., programmed without regard to the cur- rently fixated inform ation). V aughan presented target letters im - m ediately on fixation or else delayed their appearance w ith a mask, as in the Rayner and Pollatsek (1981) experiment. He concluded that whereas some fixations are preprogrammed, oth- ers are influenced by the fixated information. Other results (Hooge & Erkelens, 1996, 1998) have suggested that the com- pletion of foveal analysis is not necessarily the trigger for the subsequent saccade. Indeed, there is some controversy over how much processing of the fixated target occurs before a saccade (see Sanders & van Duren, 1998; Viviani, 1990); the amount of processing that is completed may vary as a function of the nature of the specific search task under investigation. In general, though, it seems reasonable that in a visual search paradigm, the link between eye fixations and cognitive processes might not be as tight as they are in reading. Nevertheless, cognitive load does increase fixation time (Findlay & Kapoula, 1992).
Jacobs (1986, 1987a, 1987b, 1987c, 1991; Jacobs & O'Re- gan, 1987; see also O'Regan et al., 1983) examined the extent to which the visual span influences eye movements. The visual span was defined as the size of the region around fixation in which letters can be recognized with a given accuracy without the use of any contextual information. Jacobs (1986) measured the visual span psychophysically when viewers had to find a target letter in a row of is without moving their eyes; the target letter was presented at different eccentricities from fixation on each trial. The eye movement behavior for the same stimuli was then exam ined and predictions w ere m ade concerning how easy the target was to locate on the basis of the visual span deter- mined for that particular target letter. A bout 80% of the variance of mean saccade length could be accounted for by adjustments of saccades to changes in visual span, whereas less than 50% of fixation duration variance was determined by visual span changes. Jacobs concluded that cognitive factors related to deci- sion processes intervened in the triggering of saccades and that the com putation of their spatial param eters play an im portant role in determining fixation durations in a simple search task. In reading, som ething like lexical access presum ably serves as the trigger to move the eyes from one location to another, but in visual search, a simpler process may well serve as the trigger. Indeed, Rayner (1995) suggested that the trigger to move the eyes in a search task is something like "Is the target present in the decision region?'' If it is not, a new saccade is programmed to move the eyes to a location that has not been examined. As with reading, attention would move to the region targeted for the next fixation prior to the actual saccade. Theeuwes, Kramer, Hahn, and Irwin (1998) recently reported evidence consistent with parallel programming of two saccades: one voluntary, goal- directed eye movement to a search target and one reflexive, saccade elicited by the appearance of a new object.
The decision about where to fixate next is also strongly influ- enced by the characteristics of the specific search task. When searching through text or textlike arrays, the eyes would be
programmed to move just beyond the decision region into the preview region (Rayner & Fisher, 1987b). How far the eyes actually move would be determined by the characteristics of the array (dense, difficult distractors, etc). In situations such as this, the participant would primarily be moving from left to right across the line. Jacobs (1987b) found that when letter strings had spaces between them (as in Rayner & Fisher, 1987b), participants actually moved their eyes a shorter distance than when there were no spaces. He also found that average fixation duration and overall search rate were longer when there were spaces. When the array consists of randomly arranged alphanu- meric characters (Bertera & Rayner, 1998) or colored bars (Zel- insky, 1996) or circles with or without line segments (W illiams et al., 1997; Zelinsky & Sheinberg, 1997), then more effort is needed because a strategy m ust be developed for system atically examining the array.
Eye Movements and Scene Perception
In comparison to visual search, there have been many recent studies exam ining scene perception. A lthough there has not been as much research on this subject as there has been on reading, many of the same questions have been addressed within the context of scene perception.
How Important Are Eye Movements in Scene Perception?
Viewers make many eye movements when looking at a scene. Despite this simple fact, it has often been implied that examining the fine details of eye movements during scene perception is a high-cost, low-yield endeavor. Some experiments using tachisto- scopic presentations (see Biederman, Mezzanotte, & Rabino- witz, 1982) and eye movement recordings (see G. R. Loftus & Mackworth, 1978) have led to the conclusion that participants get the gist of a scene very early in the process of looking, sometimes even from a single brief exposure. Thus, it has been advocated that the gist of the scene is abstracted on the first couple of fixations, and the remainder of the fixations on the scene are used to fill in details. Given these findings, one may question the value of information gleaned from detailed eye movement analyses as people look at scenes.
A lthough this seem s like a reasonable point, it basically as- sumes that the conclusions reached in the studies are valid. However, as Rayner and Pollatsek (1992) argued, it remains to be conclusively demonstrated that tachistoscopic studies reveal a perceptual effect rather than the outcom e of m em ory processes or guessing strategies. Likewise, concerns can be generated about the eye movement studies that have supported the idea that the gist is abstracted very quickly from a scene. One finding is that the eyes are quickly drawn to informative regions of a picture (Antes, 1974; Mackworth & Morandi, 1967). Another prom inent finding is that the eyes quickly m ove to an object that is out of place in a scene (Friedman, 1979; Friedman & Liebelt, 1981; G. R. Loftus & Mackworth, 1978). These findings have been taken as evidence that viewers quickly get the gist of the scene and move their gaze to the object that does not fit. However, Rayner and Pollastek (1992) pointed out that the eyes
 may be quickly drawn to informative areas because they are physically distinctive in the scenes, and the definition of what is informative in these studies often corresponds to a definition of w hat is distinctive. Thus, in studies in w hich the eyes quickly move to an out-of-place object, it is also possible that such objects are physically distinct from the surroundings. In recent experiments, DeGraef, Christiaens, and d'Ydewalle (1990); De- Graef (1998); and Henderson, Weeks, and Hollingworth (in press) controlled for distinctiveness, and none of them replicated the finding that semantically inconsistent objects were fixated earlierthanconsistent objects.Itis,however,clearthatimportant or interesting objects in scenes are fixated more and longer than less im portant objects (C hristiansen, Loftus, Hoffman, & Loftus, 1991; Henderson et al., in press; Kristjanson & Antes, 1989; G. R. Loftus & Mackworth, 1978; E. F.Loftus, Loftus, & Messo, 1987).
A controversial claim regarding the im portance of eye m ove- ments was advocated by Noton and Stark (1971a, 1971b; for more recent work see Choi, Mosley, & Stark, 1995; Ellis & Stark, 1986; Stark & Ellis, 1981; Zangemeister, Sherman, & Stark, 1995) in their "scanpath" theory in which the process of pattern perception is viewed as a serial process with a fixed- order strategy of extraction of information from the scene during an initial view ing period and during subsequent recognition of the scene. Although the sequence of fixations is largely unpre- dictable before view ing begins, N oton and Stark argued that memory for a scene consists of information about the sequence actually used to get the inform ation in the first place. A ccording to their theory, the order of fixations on a scene during initial viewing and later recognition should be similar, and they pre- sented data consistent with this idea. However, although it has been demonstrated a number of times that the initial viewing and subsequent recognition patterns of fixations are often sim ilar (M annen, Ruddock, & W ooding, 1995, 1996, 1997), other stud- ies (L ocher & N odine, 1974; Parker, 1978; W alker-Sm ith, Gale, & Findlay, 1977) indicated that there is no necessity that they be so for accurate recognition to occur.
G. R. Loftus (1972; Christiansen et al., 1991) found that memory for a scene was related to the number of fixations made
22
on the scene: M ore fixations yielded higher recognition scores. L oftus's general finding, along w ith the fact that there is quite a bit of variation in the pattern of eye fixations on a scene as a function of instructions (B usw ell, 1935;Y arbus, 1967), suggests that people continue to abstract im portant inform ation from the scene following the initial few fixations. A ntes (1974) reported a pattern of visual exploration whereby participants initially m ade m any long saccades to fixate on inform ative parts of the scene with short fixations. This behavior gradually evolved into fixating inform ative areas less frequently and exam ining less informative details through short saccades and long fixations. On the other hand, G. R. Loftus (1983) found that the initial fixation on a scene was shorter than subsequent fixations, and Nodine, Carmody, and Kundel (1978) found no evidence that mean fixation duration or saccade length varied over the course of viewing. Fixations were longer, however, on areas rated high in inform ativeness.
Finally, van Diepen, DeGraef, and d'Ydewalle (1995) used the m oving m ask paradigm to investigate how quickly inform a- tion in a scene can be obtained. V iew ers exam ined scenes w ith
a pattern mask presented at fixation either 15, 45, 75, or 120 ms after the onset of a fixation. On a given trial, the mask was either 1.5° wide and 1° high or 2.5° wide and 1.7° high. In a control condition, no mask was presented. Total viewing time was longer when a mask was present than when it was not only in the 15-ms delay condition. Thus, van Diepen et al. concluded that the information needed for object analysis was acquired within the first 45-75 ms of a fixation. However, their fixation duration data led diem to conclude that, as in reading, informa- tion can be acquired throughout the fixation as needed.
Given the existing data, there is fairly good evidence that information is abstracted throughout the time course of viewing a scene (G. R. Loftus, 1981, 1983). Whereas the gist of the scene is obtained early in viewing, useful information from the scene is obtained after the initial fixations. Thus, important conclusions about the tem poral aspects of scene perception can be obtained ty analyses of eye movements.
The Perceptual Span
Rayner and Pollatsek (1992) argued that the data on the perceptual span in scene perception mirrors that for reading with one important difference: Meaningful information can be extracted much further from fixation in scenes than in text. Using a moving mask paradigm, Henderson, M cClure, Pierce, and Schrock (1997) confirmed this conclusion and showed that although the presence of a foveal mask influences looking time, it does not have nearly as serious effects for object identification as a foveal m ask has for reading. A lthough these data confirm that objects can be identified farther from fixation than words, two other lines of research suggest that there are limits to how far from fixation information can be extracted even in scene perception. First, Nelson and Loftus (1980) determined how close to fixation an object had to be for it to be recognized as being in the scene. They found that objects located within about 2.6° from fixation were generally recognized, but recognition depended to some extent on the characteristics of the object. They also suggested that qualitatively different information is acquired from the region 1.5° around fixation than from any region further from fixation. Similar results were reported by Nodine, Carmody,and Herman (1979). Second, Saida and Ikeda (1979) used the moving window paradigm and varied the win- dow size and the exposure duration of a scene; the probability of correct recognition was determined as a function of these two variables. In addition, two picture sizes were used (14.4°
X 18.8° and 10.2° X 13.3°). As would be expected, and consis- tent with G. R. Loftus (1972), recognition accuracy increased with viewing duration. More important, Saida and Ikeda found a serious deterioration in recognition when the window was limited to a small area (about 3.3° X 3.3°) on each fixation. Performance gradually improved as window size became larger, reaching asymptote when it was about 50% of the entire picture. Saida and Ikeda noted that the scene is scanned so that there is considerable overlap of inform ation across fixations.
22 Tversky (1974) found just the opposite effect: Fewer fixations (but longer fixations) were associated with better recall and recognition of pictures.
EYE MOVEMENTS IN READING 399
 400 RAWER
Shioiri and Ikeda (1989) varied window size while the infor- mation outside of the window was degraded by adding visual noise. They tried to find the critical window size beyond which a further increase did not im prove perform ance. They found that the greater the degradation, the larger the window had to be to meet the criterion performance. Thus, at high eccentricities, severely degraded inform ation yielded norm al perform ance. Shioiri and Ikeda concluded that low -resolution inform ation is processed in the more peripheral parts of the visual field, whereas high-resolution information is processed in foveal vi- sion. More recently, Wampers, van Diepen, and d'Ydewalle (1998) used a moving window paradigm (see van Diepen, 1997; vanDiepen,Wampers,&d'Ydewalle,1998)andfoundthathigh
the preview and target were identical. Thus, integration is taking place at a level less abstract than the basic level concept. Another finding that indicated something less abstract than the meaning was involved in the integration process was that a visually simi- lar preview was facilitory (such as a carrot preview for a base- ball bat), whereas a semantically similar preview (such as a baseball preview for a baseball bat) was not (Pollatsek et al., 1984). This latter finding suggests that visual form in some sense is mediating the integration across saccades. hi addition, Henderson and Seifert (in press-a, in press-b) and Pollatsek et al. (1984) found that left-right orientation of an object is re-
23
tained and integrated across a saccade in object identification. A number of studies (Blackmore, Brelstaff, Nelson, & Trosci- anko, 1995; Carlson-Radvansky & tw in , 1995; Henderson, 1992a, 1994; Henderson & Anes, 1994; Henderson et al., 1987; Henderson & Seifert, in press-a; Irwin, 1991, 1992a; Irwin & A ndrew s, 1996; Irw in & G ordon, 1998; Juttner, 1997; Juttner & Rohler, 1993; Palmer & Ames, 1992; Pollatsek et al., 1984, 1990) have converged on the conclusion that transsaccadic inte- gration of real-world objects relies to a high degree on relatively abstract representations. A pparently, the preview im age of an object leads to the computation of an abstract structural descrip- tion of the object (Carlson-Radvansky, in press; Carlson-Rad- vansky & Irwin, 1995; Henderson & Seifert, in press-a). In general, it does not appear that anything resembling precise spatial alignment is needed for combining information across saccades. Virtually the same preview benefit can be obtained if the preview is presented extrafoveally, followed by the target presented foveally after approximately 200 ms (which is the average saccade latency in these experiments), with the partici- pant maintaining fixation throughout (Pollatsek et al., 1990). Thus, the integration process does not depend on spatial align- ment of the two images. This conclusion is also supported by research demonstrating that a scene can be shifted left or right during a saccade, and displacement of the scene is not detected if the eye movement is more than three times larger than the scene displacement (Bridgeman, Hendry, & Stark, 1975;
Bridgeman & Stark, 1979).
Consistent with the latter finding are some interesting experi-
ments by McConkie and Currie (1996; see also Irwin, McCon- kie, Carlson-Radvansky, & Currie, 1994), which have revealed that participants are rem arkably unaw are of changes occurring in scenes when they move their eyes. Thus, if participants are looking at a scene that consists of a house and a front yard (with a tree in the front yard), they are often unaware of the tree changing location or even completely disappearing from the scene. Of course, the disappearance of the tree is noticed when it is important, the target for a saccade, or attention is drawn to it (see Irwin & Gordon, 1998), but McConkie and Currie's finding is consistent with the idea that abstract features of the scene are encoded. McConkie and Currie and Irwin et al. proposed the saccade target theory to account for the results.
" Henderson and Seifert (in press-a) obtained similar results in the naming paradigm and when fixations were monitored during a scanning task. Henderson and Seifert (in press-b) also found that left-right orien- tation mediates the preview benefit only when position remains constant. They also found greater preview benefit when objects remain constant than when they change location.
spatial frequency inform ation is m ore useful in parafoveal peripheral vision than low spatial frequency information.
Integration of Information Across Saccades
and
How information is combined across saccades and how we perceive a visually constant world in spite of the displacement of the light pattern on the retina with each saccade has been the topic of much debate (Bridgeman, van der Heijden, & Velich- kovsky, 1994; Irwin, 1992b, 1993, 1996; Pollatsek & Rayner, 1992). A s w ith reading, it has been suggested that inform ation is integrated in a visual buffer that combines the information available on successive fixations (Breitmeyer, 1983; Feldman, 1985; Jonides, Irwin, & Yantis, 1982). However, research on object perception has found that visual information is not over- lapped in a point-by-point manner in a buffer. The typical para- digm used in this research involves eye contingent display changes (see Pollatsek et al., 1984). Viewers' eye movements are monitored, and an object is presented in extrafoveal vision. Viewers are instructed to move their eyes to the location of the object, and during the saccade, the initially presented object is replaced by another object that the participant m ust nam e (w ith the relationship between the two objects being varied). As in reading, viewers obtain information from an extrafoveal object that aids them in naming the target object. This preview effect is about 100 ms both in the naming paradigm (Henderson, 1992a, 1992b; Henderson, Pollatsek, & Rayner, 1987; Pollatsek et al., 1984; Pollatsek, Rayner, & Henderson, 1990) and when target filiation time is measured (Henderson, Pollatsek, & Rayner, 1989).
A num ber of interesting results have em erged from this para- digm . First, a size change of 10% produced no decrem ent in preview benefit (Pollatsek et al., 1984), and there was preview benefit when the location of the preview and target changed (Pollatsek et al., 1990), indicating that a direct point-to-point mapping was irrelevant for preview benefit. Second, there was som e preview benefit w hen the preview and target shared the same name (e.g., a baseball bat and the animal bat), indicating that phonological codes may be involved in the integration pro- cess (Pollatsek et al., 1984). Third, changing the contour infor- mation of an object from one fixation to another influenced the amount of preview benefit (Henderson, 1997). Fourth, changing the form of the preview but preserving the meaning also altered the preview benefit. That is, the benefit was significantly smaller when one dog (e.g., a collie) was the preview object and a different dog (a German shepherd) was the target than when
 According to this theory, prior to a saccade a target object in a scene is chosen. Once this object is chosen, the goal of the perceptual system is to encode critical features about the object. After the saccade, these features are used by the system to search for the target object in the visual array provided during the new fixation. Finding the object provides the bridge that coordinates perception across saccades. Because the search for the object is limited to a narrow region around the fovea, sac- cadic stimulus shifts are detected if the shift places the target object outside of the limited search region. However, if the target object is close enough to fixation, changes in the scene are not detected (see Deubel, Bridgeman, & Schneider, 1998; Deubel, Schneider, & Bridgeman, 1996, for similar arguments).
When the stimuli do not involve meaningful properties, is low-level visual information integrated across saccades? Some experiments purported to demonstrate such low-level visual in- tegration, but the results were artifactual. In particular, Jonides, Irwin, and Yantis (1982) reported some experiments that were initially taken as evidence for a low-level integrative visual buffer. In these experiments, participants were asked to fixate on a target in the center of vision while in parafovea] vision, 12 dots appeared. The 12 dots appeared to be a random array, but in reality they formed part of a 5 X 5 matrix. Participants were instructed to move their eyes to the array, and when they did so, the 12 initially presented dots were replaced by 12 differ- ent dots. The participants' task was to indicate which dot from the 5 X 5 array was missing. Clearly, this task could be done only if participants were holding the first 12 dots in a buffer and integrating that information with the second 12 dots available following the saccade. Jonides et al. found that the location of the missing dot was reported with an extremely high degree of accuracy. In contrast, performance in a control condition (in which participants held fixation while the first 12 dots were presented parafoveally and then the second 12 dots were pre- sented foveally) was at chance level. These results thus seemed like good evidence for a low-level integrative visual buffer. Other studies (Breitmeyer, Kropfl, & Julesz, 1982; Ritter, 1976; Wolf, Hauske, & Lupp, 1978, 1980) dealing with low-level visual processing reported findings consistent with the integrative vi- sual buffer theory. However, a number of studies (Bridgeman & Mayer, 1983; Irwin, Yantis, & Jonides, 1983; Jonides, Irwin, & Yantis, 1983; O'Regan & Levy-Schoen, 1983; Rayner & Pollat- sek, 1983) demonstrated that the original results were due to screen persistence in the display monitor used to present the stimuli. When appropriate controls were used, the effect was small or nonexistent. Some careful experiments by Irwin (1991,
1992a; Irwin, Zacks, & Brown, 1990) demonstrated that the results of Breitmeyer et al., Ritter,and Wolf et al. were artifactual because of (a) inadequate control over stimulus presentation, (b) screen persistence on the displays used to present stimuli, and (c) comparison with inappropriate control conditions (see Irwin, 1993).
A phenomenon that would be consistent with an integrative visual buffer is if backward masking occurred across fixations. That is, if a stimulus appeared in a spatial location and a mask appeared in the sam e spatial location on the subsequent fixation, would the mask affect the visibility of the stimulus? Davidson, Fox, and Dick (1973) reported that there was a cross-saccade effect of retinotopic masking (i.e., that a later mask in the same
retinal location inhibited identification of the stimulus) and that the m ask in the sam e spatial location as the target appeared phenomenally to cover it but did not actually degrade the infor- mation. M cRae, Butler, and Popiel (1987) obtained evidence of retinotopic and spatiotopic masking associated with eye move- ments. Later work by Irwin, Brown, and Sun (1988) indicated that the retinal masking phenomenon can be replicated, but that there was little evidence that the mask in the same spatial loca- tion has any effect either inform ationally or phenom enally on the visability of the target stimulus (see also Van der Heijden, Bridgeman, & Mewhort, 1986).
In summary, the evidence suggests that when meaningful in- formation is presented, integration of information occurs at an abstract level of representation rather than at a visual level (see Irwin, 1992b, 1993; Pollatsek & Rayner, 1992). A lthough some very low-level visual information may be integrated across sac- cades in some circumstances (see Hayhoe,Bensinger,& Ballard, 1998; Hayhoe, Lachter, & Feldman, 1991; Hayhoe, Lachtei; & Moeller, 1992), when meaningful processing occurs (as in scene perception), the information that survives the saccade is at a higher level of representation than pure visual information.
Scene Context Effects on Eye Movements
Scene context has an effect on eye movements: Fixation time on an object that belongs in a scene is less than fixation time on an object that does not belong (Antes & Penland, 1981; Friedman, 1979; Henderson et al., in press; G. R. Loftus & Mackworth, 1978). This result demonstrates that eye move- ments reflect on-line processing of the scene. However, it is not clear whether the longer fixations on objects in violation of the scene reflect longer times to identify those objects or longer times to integrate them into a global representation of the scene (it could also reflect am usem ent of the absurdity of the violating object in the given context). Other experiments have therefore attem pted to determ ine w hether context operates at the level of object identification. Boyce and Pollatsek (1992) developed a paradigm in which the participant fixates in the middle of a scene, and then after 75 ms an object wiggles (moves a fraction of a degree of visual angle and then returns to its original position). The participant's task was to name the wiggled ob- ject. Participants invariably moved their eyes to the wiggled object, and it was named more rapidly when it was in a normal scene context than when it was in the wrong scene. DeGraef et al. (1990) used an object decision task in which participants examined a scene and pushed a button whenever they detected a nonobject (a geometric entity that they had not seen before) in the scene. DeGraef et al. found mat scene context affected fixation time on an object, but only after the scene had been viewed for a while (roughly 10 fixations). Thus, it appears that scene context can have an im m ediate effect on object processing (Boyce & Pollatsek, 1992), but it need not.
Eye Movement Control
In comparison to what is known about eye movement control in reading, considerably less is known about the mechanisms that controj eye movements during scene perception. Consistent with reading studies, Shiori (1993) found that the onset of a
EYE MOVEMENTS IN READING 401
 402 RAYNER
visual mask (either a uniform mask or a blurred image of the scene) at the onset of a fixation increased fixation duration proportional to the mask delay (see also van Diepen, 1998b), and van Diepen (1998a, 1998b), using an eye-contingent mask- ing technique, found that sufficient information was encoded in the first 50-75 ms of a fixation for object identification to occur. Also, like reading, Henderson (1993) found that viewers tended to fixate near the center of an object. Landing variability was greater in the direction of the eye movement vector than in the direction perpendicular to the movement. He also found that there was more of a tendency to undershoot the center of the object than to overshoot it. fiirthermore, landing position influ- enced fixation time: The duration of the first fixation on an object decreased and the probability of refixating an object increased as the deviation of the initial landing position from the center of the object increased. Mannen et al. (1995, 1996, 1997) had viewers look at normal scenes or the scenes after they had been either high-pass or low-pass filtered. They found that viewers looked in pretty much the same locations across the three ver- sions of the scene. Because it was virtually impossible to recog- nize the scenes in the nonnormal situations, low-level visual information must be critical in determining where to fixate next (H enderson, 1992b).
With respect to when to move the eyes, low-level variables,
such as luminance and contrast, influence fixation time on a
scene (G. R. Loftus, 1985; G. R. Loftus, Kaufman, Nishi-
moto, & Ruthruff, 1992). However, more critically, some as-
M
on processing the fixated information, it would make sense to leave the job of deciding where to fixate next to a relatively ' 'dum b'' process operating on information in parafoveal and peripheral vision, such as brightness differences or contours, that use little or no central processing capacity.
Eye Movements and Other Information Processing Tasks
In addition to the topics discussed above, eye movements have also been examined in other information processing domains. It is beyond the scope of the present review to describe all of this research in detail, but examples of a number of tasks are described.
Auditory Language Processing
A currently popular method to study auditory language pro- cessing involves recording eye movements as people listen to a story or follow instructions regarding an array they are looking at. Cooper (1974) introduced this method and found that when viewers listen to spoken language with an array in front of them consisting of elements that occur in the discourse that they quickly move their eyes to those elements that are most closely related to the meaning of the words currently heard. Tanenhaus and colleagues (Allopena, Magnuson,& Tanenhaus, 1998; Eber- hard, Spivey-Knowlton, Sedivy, & Tanenhaus, 1995; Tanen- haus & Spivey-Knowlton, 1996; Tanenhaus, Spivey-Knowlton, Eberhard, & Sedivy, 1995, 1996) and others (Meyer, Sleider- ink, & Levelt, 1998) have used the method to examine a number of different issues. Lansing and McConkie (1994) advocated recording eye m ovem ents of deaf speech readers as a m eans of studying the visual perception of spoken language, and Vatiki- otis-Bateson, Eigsti, Yano, and Munhall (1998) examined eye movements during audio-visual speech perception.
Mathematics, Numeral Reading, and Problem Solving
Eye movements have also been recorded as participants solve math and physics problems (De Corte, Verschaffel, & Pauwels, 1990; Hegarty & Just, 1993; Hegarty, Mayer, & Green, 1992; Suppes, 1990; Suppes, Cohen, Laddaga, Anliker, & Floyd, 1983; Verschaffel, De Corte, & Pauwels, 1992), as well as analogies (Bethell-Fox, Lohman, & Snow, 1984; Carpenter, Just, & Shell, 1990; Dillon, 1985). Not surprisingly, more complicated aspects of the problem s typically lead to m ore and longer fixations. Studies dealing with numeral reading (Brysbaert, 1995; Chin- cotta et al., 1997; Gielen, Brysbaert, & Dhondt, 1991; Pynte, 1974) have revealed that fixations on numbers are influenced by the number of syllables in the numeral and by the frequency and magnitude of the numeral. Research on these topics has yielded some interesting observations although the eye move- ment data are usually more descriptive than the data in reading, visual search, and scene perception.
24
It is interesteing that Zangemeister et al. (1995) found that eye movement patterns were fairly similar when viewing expressionist art (in which there are typically no well-defined objects) and scenes.
pect of object recognition is related to fixation tim e,
(a) the eyes move from an object quickly when a semantically related object was just fixated (DeGraef, 1992; DeGraef, De- Troy, & d'Y dew alle, 1992; Henderson, 1992a; Henderson et al., 1987, 1989), (b) incongruous objects in scenes are fixated longer than congruous objects (A ntes & Penland, 1981; Fried- man, 1979; Henderson et al., in press; G. R. Loftus & Mack- worth, 1978), and (c) important objects are fixated longer than less important ones (Christiansen et al., 1991; E. F.Loftus et al., 1987).
Ideally, a model of eye movement control during scene per- ception should specify the mechanisms involved both in when to move the eyes and where to move the eyes. However, be- cause eye m ovem ents in scene perception depend on the task (Buswell, 1935; Rayner & Pollatsek, 1992; Yarbus, 1967), this is not a trivial endeavor. Henderson (1992b; Henderson & Hollingworth, 1998; Henderson et al., in press) and Rayner and Pollatsek (1992) have proposed that a model like Mor- rison's model of reading can account for eye movements dur- ing scene perception. Thus, something like identification of the fixated object (or objects) serves as a trigger to move to another part of the scene. Exactly where to move the eyes is probably driven by low-level information (Findlay, 1981, 1982; Findlay, Brogan, & W enban-Smith, 1993; He & Kowler, 1989; Henderson et al., in press; Mannen et al., 1997; McGo- wan, Kowler, Sharma, & Chubb, 1998), with the center of to- be-fixated objects or other prominent aspects of the scene serving as targets for the next fixation. Certainly, if something moves (as in Boyce & Pollatsek's, 1992, experiments), the movement drives the eyes to that location. However, even with static displays, it is plausible that lower level information usually dominates eye control. Because viewers concentrate
because
 Eye Movements and Dual Tasks
Pashler, Carrier, and Hoffman (1993) examined eye move- ments when viewers were engaged in a dual-task situation. For example, a speeded manual choice response to a tone was made in close proxim ity to an eye m ovem ent. A lthough there w as some slowing of the eye movement, the dual-task situation did not yield the dual-task interference effect typically found. Ja- cobs, Nazir, and Heller (1989) presented pairs of letters in pe- ripheral vision, and participants had to judge whether the two were the same. On the basis of saccade-latency data, a confus- ability matrix of letters was established. Eye movements have also been recorded as indices of mental workload (May, Ken- nedy, Williams, Dunlop, & Brannan, 1990; Moray & Rotenberg,
1989), concurrent memory load (Donk, 1994; Roberts, Hager,& Heron, 1994), and in conjunction with verbal protocols (J. P. Hansen, 1991) in problem-solving situations. Finally, eye move- ments have been examined in tasks in which viewers count dots or small lines (Kowler & Steinman, 1977,1979; VanOeffelen & Vos, 1984a, 1984b). Ikeda, Saida, and Sugiyama (1977) used the moving window paradigm in a line comparison task. They found that accurate performance deteriorated if the window was smaller than the comparison lines and concluded that our excel- lent ability for line comparison is only possible when the two lines can be observed at one time.
Face Perception
W hen exam ining faces, people tend to fixate on the eyes, nose, mouth, and ears (Luria & Strauss, 1978; Mertens, Siegmund, & Grusser, 1993; M. L. Phillips & David, 1997; Walker, Findlay, "ibung, & Lincoln, 1996; W alker-Smith et al., 1977). Fixations tend to be longer w hen com parisons have to be m ade betw een two faces rather than when a single face is examined (M. L. Phillips & David, 1997). V. Brown, Huey, and Findlay (1997) presented faces and scram bled face distractors in a circle around fixation and asked people to move their eyes to the face.Initially, viewers made short latency saccades to the distractors as fre- quently as to the faces. However, with practice, they quickly learned to delay the saccade and directed a much higher propor- tion of saccades to the face target.
Dynamic Situations
Eye m ovem ent behavior in a num ber of dynam ic situations, such as driving (A. S. Cohen, 1981; A. S. Cohen & Studach, 1977; Dishart & Land, 1998; Land, 1992; Land & Rirneaux, 1997; Land & Horwood, 1995; Land & Lee, 1994; Liu, 1998), basketball foul shooting (Vickers, 1995, 1996), golf putting (Vickers, 1992), table tennis (Land & Furneaux, 1997), baseball (Bahill & LaRitz, 1984; Shank & Haywood, 1987), gymnastics (Vickers, 1988), walking in uneven terrain (Patla & Vickers, 1997), mental rotation (Carpenter & Just, 1978; Just & Carpen- ter, 1985), and interacting w ith com puter screens (Deffner, 1995; Goldberg & Schryver, 1995; Goolkasian & Bunt, 1980; Jacob, 1991; Stampe & Reingold, 1995) have been examined. Studies in which eye-head coordination is important have re- vealed that although the eyes often m ove prior to the head, head and eye m ovem ents are generally highly coordinated in such tasks. Other studies of eye movements of drivers have shown
different looking patterns for experienced versus novice drivers (Chapman & Underwood, 1998; Crundall, Underwood, & Chap- man, 1998; Mourant & Rockwell, 1972), as well as effects of the difficulty of the driving situation (McDowell & Rockwell, 1978; Miura, 1990; Rahimi, Briggs, & Thorn, 1990; Shinar, McDowell, & Rockwell, 1977). Niemann, Lappe, and Hoffman (1996) asked viewers to look at three-dimensional objects on a rotating turntable and to memorize the objects. A strategy of thoroughly inspecting the object with a series of closely spaced fixations from a lim ited num ber of view points led to the best recognition rates. Studies in which eye-hand coordination is important, such as playing video games (Shapiro & Raymond, 1989), have revealed orderly sequences in w hich people coordi- nate looking and action (Ballard, Hayhoe, Li, & Whitehead, 1992; Ballard, Hayhoe, & Pelz, 1995; Epelboim, Steinman, Kowler, Edwards, Pizlo, Erkelens, & Collewijn, 1995). Epel- boim et al. found that eye movements in such situations were more efficient when participants had to perform an action than when they simply had to look at the sequence. Finally, Verfaillie (1997; Verfaillie, De Troy, & Van Rensbergen, 1994) examined transsaccadic memory for biological-motion for point-light walkers. He found that when the walker was the only visible object, displacem ents w ere quite difficult to detect.
Illusions and Imagery
Studies dealing with illusions, such as the Necker cube (El- lis & Stark, 1978; Kawabata, Yamagami, & Noaki, 1978; Scotto, Oliva, & Tuccio, 1990) or ambiguous figures (Gale & Findlay, 1983; Garcia-Per£z, 1989; Ruggieri & Fernandez, 1994; Stark & Ellis, 1981), appear to have declined since my earlier review (Rayner, 1978b). Brandt and Stark (1997) recently found that the pattern of eye movements was similar when viewers looked at checkered diagrams and when they formed images of them from memory. However, fixations tended to be 75-100 ms
EYE MOVEMENTS IN READING 403
longer during im agery than
Brain Damage
W ith the grow th of
neuroscience, it is not surprising that there have been many recent studies dealing with the eye movement characteristics of patients with some type of brain damage or clinical disability. Thus, for example, a number of studies have examined the eye movements of patients with scotomas and visual neglect (Behrmann, Watt, Black, & Barton, 1997; DeLuca, Spinelli, & Zoccolotti, 1996; Karnath, 1994; Karnath & Huber, 1992; Ken- tridge, Heywood, & Weiskrantz, 1997; Ladavas, Zeloni, Zac- cara, & G angem i, 1997; R izzo & H urtig, 1992; W alker & Find- lay, 1996; W alker et al., 1996; W alker & Young, 1996; Zihl,
1995) as they engage in reading, visual search, and scene percep- tion. Gilchrist,Brown,and Findlay (1997) reported an interest- ing case of a woman with congenital, extraocular muscular fi- brosis who cannot make eye movements. However, she reads by moving her head in saccadiclike movements. Studies of eye movements of individuals with schizophrenia (Katsanis, Korten- kamp, lacono, & Grove, 1997; M. L. Phillips & David, 1997; A. B. Sereno & Holzman, 1993) have also revealed qualitative
actual
looking.
cognitive neuropsychology and cognitive
 404 RAYNER
and quantitative individuals.
differences betw een them and
Sum m ary
norm al
Antes, J. R., & Penland, J. G. (1981). Picture context effects on eye movement patterns. In D. F. Fisher, R. Monty, & J. Senders (Eds.), Eye movements: Cognition and visual perception (pp. 157-170). Hillsdale, NJ: Erlbaum.
Aslin, R. N., & Shea, S. L. (1987). The amplitude and angle of saccades to double-step target displacements. Vision Research, 27, 1925-1942. Bahill, A. T., & LaRitz, T. (1984). Why can't batters keep then- eyes on
the ball? American Scientist, 72, 249-253.
Bains, R. A., Crawford, J. D., Cadera, W., & Vilis, T. (1992). The
conjugacy of human saccadic eye movements. Vision Research, 32, 1677-1684.
Bollard, D. H., Hayhoe, M. M., Li, E, & Whitehead, S. D. (1992). Head-eye coordination during sequential tasks. Philosophical Trans- actions of the Royal Society of London B, 337, 331-339.
Ballard, D. H., Hayhoe, M. M., & Pelz, J. B. (1995). Memory represen- tation in natural tasks. Journal of Cognitive Neuroscience, 7, 6 6 -8 0 . Balota, D. A., Pollatsek, A., & Rayner, K. (1985). The interaction of contextual constraints and parafoveal visual information in reading.
Cognitive Psychology, 17, 364-390.
Balota, D. A., & Rayner, K. (1983). Parafoveal visual information and
semantic contextual constraints. Journal of Experimental Psychology:
Human Perception and Performance, 9, 726-738.
Bassou, L., Pugh, A. K., Granie, M., & M oructi, J. P. (1993). Binocular
vision in reading: A study of die eye movements of ten-year-old children. In G. d'Ydewalle & J. Van Rensbergen (Eds.), Perception and cognition: Advances in eye movement research (pp. 295-308) Amsterdam: North Holland.
Beauvillain, C. (1996). The integration of morphological and whole- word form information during eye fixations on prefixed and suffixed words. Journal of Memory and Language, 35, 801-820.
Beauvillain, C., & Beauvillain, P. (1995). Calibration of an eye move- ment system for use in reading. Behavior Research Methods, Instru- ments, & Computers, 55, 1-7.
Beauvillain, C., & Dore, K. (1998). Orthographic codes are used in integrating information from the parafovea by the saccadic computa- tion system. Vision Research, 38, 115-123.
Beauvillain, C., Dore, K., & Baudouin, V. (1996). The "center of gravity" of words: Evidence for an effect of the word-initial letters. Vision Research, 36, 589-603.
Becker, W., & Jilrgens, R. (1979). Analysis of the saccadic system by means of double step stimuli. Vision Research, 19, 967-983.
Behrmann, M., Watt, S., Black, S. E., & Barton, J. J. S. (1997). Im- paired visual search in patients with unilateral neglect: An oculo- graphic analysis. Neuropsychologia, 35, 1445-1458.
Bertera, J. H., & Rayner, K. (1998). Eye movements and the span of the effective visual stimulus in visual search. Manuscript submitted for publication.
Bethell-Fox, C. E., Lohman, D. E, & Snow, R. E. (1984). Adaptive reasoning: Componential and eye movement analysis of geometric analogy performance. Intelligence, 8, 205-238.
Biederman, I., Mezzanotte, R. J., & Rabinowitz, J. C. (1982). Scene perception: Detecting and judging objects undergoing relational viola- tions. Cognitive Psychology, 14, 143-177.
Binder, K. S., & Morris, R. K. (1995). Eye movements and lexical ambiguity resolution: Effects of prior encounter and discourse topic. Journal of Experimental Psychology: Learning, Memory, and Cogni- tion, 21, 1186-1196.
Binder, K. S., Pollatsek, A., & Rayner, K, (in press). Extraction of information to the left of the fixated word in reading. Journal of Experimental Psychology: Human Perception and Performance.
Binder, K. S., & Rayner, K. (1998). Contextual strength does not modu- late the subordinate bias effect: Evidence from eye fixations and self- paced reading. Psychonomic Bulletin & Review, 5, 271-276.
Binello, A., Mannen, S. K., & Ruddock, K. H. (1995). The characteris-
As the present review documents, eye movement data have proved to be very valuable in studying reading and other infor- mation processing tasks. In general, it is probably not appro- priate to make generalizations from one task to another about what eye movements reveal about cognitive processes. However, much has been learned about the characteristics of eye move- ments in the various tasks investigated. More critically, eye m ovem ent data are very inform ative in revealing m om ent-to- moment processing activities in the tasks reviewed here. Thus, it w ill m ost likely be the case that the use of eye m ovem ent data w ill continue to be used to investigate and inform us concerning the types of processing activities discussed in the present review.
References
Abel, L. A., Troost, B.T., & Dell'Osso, L .F . (1983). The effects of age on normal saccade characteristics and their variability. Vision Research, 23, 33-37.
Abrams, R. A., & Jonides, J. (1988). Programming saccadic eye move- ments. Journal of Experimental Psychology: Human Perception and Performance, 14, 428-443.
Abrams, R. A., Meyer, D. E., & Kornblum, S. (1989). Speed and accu- racy of saccadic eye movements: Characteristics of impluse variability in the oculomotor system. Journal of Experimental Psychology: Hu- man Perception and Performance, IS, 529-543.
Abrams, S. G., & Zuber, B. L. (1972). Some temporal characteristics of information processing during reading. Reading Research Quar- terly, 12, 41-51.
Adams, B. C., Clifton, C., & M itchell, D. C. (1998). Lexical guidance in sentence processing? Psychonomic Bulletin & Review, 5, 265-270. Adler-Grinberg, D., & Stark, L. (1978). Eye movements, scanpaths and dyslexia. American Journal of Optometry and Physiological Optics,
55, 557-570.
Albrecht, J. E., & Clifton, C. (1998). Accessing singular antecedents in
conjoined phrases. Memory & Cognition, 26, 599-610.
Allopena, P. D., Magnuson, J. S., & Tanenhaus, M. K. (1998). Tracking the time course of spoken word recognition using eye movements: Evidence for continuous mapping models. Journal of Memory and
Language, 38, 419-439.
Altarriba, J., Kroll, J. E, Sholl, A., & Rayner, K. (1996). The influence
of lexical and conceptual constraints on reading mixed language sen- tences: Evidence from eye fixation and naming times. Memory & Cognition, 24, 477-492.
Altmann, O. T M. (1994). Regression-contingent analyses of eye move- ments during sentence processing:' A reply to Rayner and Sereno. Memory A Cognition, 22, 286-290.
Altmann, G. T.M., Garnham, A., & Dennis, Y. (1992). Avoiding the garden path: Eye movements in context. Journal of Memory and Language, 31, 685-712.
Altmann, O. T.M., Garnham, A., & Henstra, I. A. (1994). Effects of syntax in human sentence parsing: Evidence against a structure-based proposal mechanism. Journal of Experimental Psychology: Learning, Memory, and Cognition, 20, 209-216.
Altmann, G. T.M., van Nice, K., Garnham, A., & Henstra, J. A. (1998). Late closure in context. Journal of Memory and Language, 38, 459— 484.
Antes, J. R. (1974). The time course of picture viewing. Journal of Experimental Psychology, 103, 62-70.
 tics of eye movements made during visual search with multi-element
stimuli. Spatial Vision, 9, 343-362.
Birch, S., & Rayner, K. (1997). Linguistic focus affects eye movements
during reading. Memory & Cognition, 25, 653-660.
Biscaldi, M., Fischer, B., & Aiple, F. (1994). Saccadic eye movements
of dyslexic and normal reading children. Perception, 23, 45-64. Black, J. L., Collins, D. W. K., DeRoach, J. N., & Zubrick, S. (1984a). A detailed study of sequential saccadic eye movements for normal- and poor-reading children. Perceptual and Motor Skills, 59, 423 -434. Black, J. L., Collins, D. W . K., DeRoach, J. N., & Zubrick, S. (1984b). Dyslexia: Saccadic eye movements. Perceptual and Motor Skills, 58,
903-910.
Black, J. L., Collins, D. W . K., DeRoach, J. N., & Zubrick, S. (1984c).
Smooth pursuit eye movements in normal and dyslexic children. Per-
ceptual and Motor Skills, 59, 91-100.
Blackmore, S. J., Brelstaff, G., Nelson, K., & Troscianko, T. (1995). Is
the richness of our visual world an illusion? Iranssaccadic memory
for complex scenes. Perception, 24, 1075-1081.
Blanchard, H. E. (1985). A comparison of some processing time mea-
sures based on eye movements. Acta Psychologies, 58, 1-15. Blanchard, H. E. (1987). Pronoun processing during fixations: Effects on the time course of information utilization. Bulletin of the Psy-
chonomic Society, 25, 171-174.
Blanchard, H. E., & Iran-Nejad, A. (1987). Comprehension processes
and eye movement patterns in the reading of surprise-ending stories.
Discourse Processes, 10, 127-138.
Blanchard, H. E., McConfcie, G. W, Zola, D., & Wolverton, G. S.
(1984). The time course of visual information utilization during fixa- tions in reading. Journal of Experimental Psychology: Human Per- ception and Performance, 10, 7 5 -8 9 .
Blanchard, H. E., Pollatsek, A., & Raynet K. (1989). The acquisition of parafoveal word information in reading. Perception & Psychophys- ics, 46, 85-94.
Boer, L. C , & van der Weijgert, E. C. M. (1988). Eye movements and stages of processing. Acta Psychologica, 67, 3-17.
Boersma, X, Zwaga, H. J., & Adams, A. S. (1989). Conspicuity in realis- tic scenes: An eye-movement measure. Applied Ergonomics,20, 267-
273.
Bouma, H., & deVoogd, A. H. (1974). On the control of eye saccades
in reading. Vision Research, 14, 273-284.
Bowers, A. R., & Reid, V.M. (1997). Eye movements and reading with
simulated impaired vision. Oplhalmic and Physiological Optics, 17,
392-402.
Boyce, S. J., & Pollatsek, A. (1992). Identification of objects in scenes:
The role of scene background in object naming. Journal of Experi-
mental Psychology: Learning, Memory, and Cognition, 18, 531-543. Brandt, S. A., & Stark, L. W . (1997). Spontaneous eye movements dur- ing visual imagery reflect the content of the visual scene. Journal of
Cognitive Neuroscience, 9, 27-38.
Breitmeyer, B. G. (1983). Sensory masking, persistence, and enhance-
ment in visual exploration and reading. In K. Rayner (Ed.), Eye movements in reading: Perceptual and language processes (pp. 3-
30). New 'fcrk: Academic Press.
Breitmeyer, B. G., Kropfl, W., & Julesz, B. (1982). The existence and
role of retinotopic and spatiotopic forms of visual persistence. Acta
Psychologica, 52, 175-196.
Bridgeman, B., Hendry, D., & Stark, L. (1975). Failure to detect dis-
placement of the visual world during saccadic eye movements. Vision
Research, 15, 719-722.
Bridgeman, B., & Mayer, M. (1983). Failure to integrate visual informa-
tion from successive fixations. Bulletin of the Psychonomic Society,
21, 285-286.
Bridgeman, B., & Stark, L. (1979). Omnidirectional increase in thresh-
old for image shifts during saccadic eye movements. Perception &
Psychophysics, 25, 241-243.
Bridgeman, B., van der Heijden, L., & "Velichkovsky, B. (1994). A
theory of visual stability across saccadic eye movements. Behavioral
and Brain Sciences, 17, 247-292.
Briihl, D., & Inhoff, A. W. (1995). Integrating information across fixa-
tions during reading: The use of orthographic bodies and of exterior letters. Journal of Experimental Psychology: Learning, Memory, and Cognition, 21, 55-67.
Britt, M. A., Perfetti, C. A., Garrod, S., & Rayner, K. (1992). Parsing in discourse: Context effects and their limits. Journal of Memory and Language, 31, 293-314.
Brooks, B. A., Impelman, D. M. K., & Lum, J. T. (1981). Backward 'and forward masking associated with saccadic eye movements. Per-
ception & Psychophysics, 30, 6 2 -7 0 .
Brown, B., Haegersrrom-Portnoy, G., Adams, A. J., Yingling, C. D.,
Galin, D., Herron, J., & Marcus, M. (1983). Predictive eye movements do not discriminate between dyslexic and control children. Neuropsy- chologia, 21, 121-128.
Brown, B., Haegerstrom-Portnoy, G., Yingling, C. D., Herron, J., Galin, D., & Marcus, M. (1983). Tracking eye movements are normal in dyslexic children. American Journal of Optometry and Physiological Optics, 60, 376-383.
Brown, V., Huey, D., & Findlay, J. M. (1997). Pace detection in periph- eral vision: Do faces pop out? Perception, 26, 1555-1570.
Brutten, G.J., Bakker, K., Janssen, P., & VanderMeulen, S. (1984). Eye movements of stuttering and nonstuttering children during silent reading. Journal of Speech and Hearing Research, 27, 562—566.
Brysbaert, M. (1995). Arabic number reading: On the nature of the numerical scale and the origin of phonological receding. Journal of Experimental Psychology: General, 124, 434-452.
Brysbaert, M., & Mitchell, D. C. (1996). Modifier attachment in sen- tence parsing: Evidence from Dutch. Quarterly Journal of Experimen- tal Psychology, 49A, 664-695.
Brysbaert, M., & Vitu, F. (1998). Word skipping: Implications for theories of eye movement control in reading. In G. Underwood (Ed.), Eye guidance in reading and scene perception (pp. 125-148). Ox- ford, England: Elsevier.
Bullimore, M. A., & Bailey, I. L. (1995). Reading and eye movements in age-related maculopathy. Optometry and Vision Science, 72, 125-
138.
Buswell, G. T. (1922). Fundamental reading habits: A study of their
development. Chicago: University of Chicago Press. Buswell,G.T.(1935).Howpeople lookatpictures,Chicago:University
of Chicago Press.
Butsch, R. L. C. (1932). Eye movements and the eye-hand span in type-
writing. Journal of Educational Psychology, 23, 104-121. Campbell, F. W., & Wurtz, R. H. (1979). Saccadic omission: Why we do not see a gray-out during a saccadic eye movement. Vision Re-
search, IS, 1297-1303.
Carlson-Radvansky, L. A. (in press). Memory for relational information
across eye movements. Perception & Psychophysics. Carlson-Radvansky, L., & Irwin, D. E. (1995). Memory for structural information across eye movements. Journal of Experimental Psychol-
ogy: Learning, Memory, and Cognition, 21, 1441 —1458.
Carmody, D. P., Nodine, C. F., & Kundel, H. L. (1980). An analysis of perceptual and cognitive factors in radiographic interpretation. Per-
ception, 9, 339-344.
Carmody, D. P., Nodine, C. F., & Kundel, H. L. (1981). Finding lung
nodules with and without comparative visual scanning. Perception &.
Psychophysics, 29, 594-598.
Carpenter, P. A., &. Daneman, M. (1981). Lexical retrieval and error
recovery in reading: A model based on eye fixations. Journal of Verbal Learning and Verbal Behavior, 28, 138-160.
EYE MOVEMENTS
IN READING 4 0 5
 406 RA YNER
Carpenter, P. A., & Just, M. A. (1978). Eye fixations during mental rotation. In J. W. Senders, D. F. Fishes & R. A. Monty (Eds.), Eye movements and the higher psychological functions (pp. 115-133). Hillsdale, NJ: Erlbaum.
Carpenter, P. A., & Just, M. A. (1983). What your eyes do while your mind is reading. In K. Rayner (Ed.), Eye movements in reading: Perceptual and language processes (pp. 275-307). New Ybrk: Aca- demic Press.
Carpenter, P. A., Just, M. A., & Shell, P. (1990). What one intelligence test measures: A theoretical account of the processing in the Raven Progressive Matrices Test. Psychological Review, 97, 404-431.
Carrithers, C., & Bever, T. G. (1984). Eye-movement patterns confirm theories of language comprehension. Cognitive Science, 8, 157-172. Carroll, P.J., & Slowiaczek, M.L. (1986). Constraints on semantic priming in reading: A fixation time analysis. Memory & Cognition,
14, 509-522.
Carroll, P. J., & Slowiaczek, M. L. (1987). Models and modules: Multi-
ple pathways to the language processor. In J. L. Garfield (Ed.), Modu- larity in knowledge representation and natural-language understand- ing (pp. 221-248). Cambridge, MA: MIT Press.
Carroll, P.J., Voung, J. R., & Guertin, M. S. (1992). Visual analysis of cartoons: A view from the far side. In K. Rayner (Ed.), Eye movements and visual cognition: Scene perception and reading (pp. 4 4 4 -4 6 1 ). New York: Springer-Verlag.
Cavegn, D., & d'Ydewalle, G. (1996). Presaccadic attention allocation of express saccades. Psychological Research, 59, 157-175.
Chapman, P.R., & Underwood, G. (1998). Visual search of dynamic scenes: Event types and the role of experience in viewing driving situations. In G. Underwood (Ed.), Eye guidance in reading and scene perception (pp. 369-394). Oxford, England: Elsevier.
Chekaluk, E., & Llewellyn, K.R. (1990). Visual stimulus input, sac- cadic suppression, and detection of information from the postsaccadic scene. Perception & Psychophysics, 48, 135-142.
Chincotta, D., HySna, J., & Underwood, G. (1997). Eye fixations, speech rate and bilingual digit-span: Numeral reading indexes fluency not word length. Acta Psychologica, 97, 253-275.
Choi, Y.S., Mosley, A. D., & Stark, L. (1995). String editing analysis of hum an visual search. Optometry and Vision Science, 72, 4 3 9 -4 5 1 . Christiansen, S. A., Loftus, E. E, Hoffman, H., & Loftus, G. R. (1991). Eye fixations and memory for emotional events. Journal of Experi- mental Psychology: Learning, Memory, and Cognition, 17, 693-701. Ciuffreda, K. J. (1979). Jerk nystagmus: Some new findings. American
Journal of Optometry and Physiological Optics, S3, 389-395. Ciuffreda, K. J. (1994). Reading eye movements in patients with oculo- motor disturbances. In J. Ygge & G. Lennerstrand (Eds.), Eye move-
ments in reading (pp. 163-188). Oxford: Pergamon Press. Ciuffreda, K. J., Kenyon, R. W., & Stark, L. (1983). Saccadic intrusions contributing to reading disability: A case report. American Journal
of Optometry and Physiological Optics, 60, 242-249.
Ciuffreda, K .J., & Tannen, B. (1995). Eye movement basics for the
clinician. St. Louis, MO: C. V.Mosby.
Clifton, C. (1992). Tracing the course of sentence comprehension: How
lexical information is used. In K. Rayner (Ed.), Eye movements and visual cognition: Scene perception and reading (pp. 397-414). New York: Springer-Verlag.
Clifton, C. (1993). Thematic roles in sentence parsing. CanadianJour- nal of Experimental Psychology, 47, 222-246.
Clifton, C., Speer, S., & Abney, S. P. (1991). Parsing arguments: Phrase structure and argument structure as determinants of initial parsing decision. Journal of Memory and Language, 30, 251-271.
Coerfg, C. (1985). La visee du regard sur un mot isole [Where does the eye land in isolated words?] Annee Psychologique, 85, 169-184. Coeffe, C., & O'Regan, J. K. (1987). Reducing the influence of non- target stimuli on saccade accuracy: Predictability and latency effects.
Vision Research. 27, 227-240.
Cohen, A. S. (1981). Car drivers' pattern of eye fixations on the road and in the laboratory. Perceptual and Motor Skills, 52, 515-522. Cohen, A. S., & Studach, H. (1977). Eye movements while driving cars
around curves. Perceptual and Motor Skills, 44, 683-689.
Cohen, M. E., & Ross, L. E. (1977). Saccade latency in children and adults: Effects of warning signals and target eccentricity. Journal of
Experimental Child Psychology, 23, 5 3 9 -5 4 9 .
Collewijn, H., Erkelens, C. J., & Steinman, R. M. (1988a). Binocular
co-ordination of human horizontal saccadic eye movements. Journal
of Physiology, 404, 157-182.
Collewijn, H., Erkelens, C. J., & Steinman, R. M. (1988b). Binocular
co-ordination of human vertical saccadic eye movements. Journal of
Physiology, 404, 183-197.
Cooper, R. M. (1974). The control of eye fixation by the meaning of
spoken language: A new m ethodology for the real-tim e investigation of speech perception, memory, and language processing. Cognitive Psychology, 6, 8 4 -1 0 7 .
Crawford, T.J. (1990). Multi-stepping saccade sequences in humans. Acta Psychologica, 65, 371-394.
Crawford, T.J. (1996). Transient motion of visual texture delays sac- cadic eye movement, Acta Psychologica, 92, 251-262.
Crundall, D. W., Underwood, G., & Chapman, P.R. (1998). How much do novice drivers see? The effects of demand on visual search strate- gies in novice and experienced drivers. In G. Underwood (Ed.), Eye guidance in reading and scene perception (pp. 359-418). Oxford, England: Elsevier.
Daneman, M., & Reingold, E. (1993). What eye fixations tell us about phonological recoding during reading. Canadian Journal of Experi- mental Psychology, 47, 153-178.
Daneman, M., Reingold, E., & Davidson, M. (1995). Time course of phonological activation during reading: Evidence from eye fixations. Journal of Experimental Psychology: Learning, Memory, and Cogni- tion, 21, 884-898.
Davidson, M. L., Fox, M. J., & Dick, A. Q (1973). Effect of eye move- ments on backward masking and perceived location. Perception & Psychophysics, 14, 110-116.
De Corte, E., Verschaffel, L., & Pauwels, A. (1990). Influence of the semantic structure of word problems on second graders' eye move- ments. Journal of Educational Psychology, 82, 359-365.
Dee-Lucas, D., Just, M. A., Carpenter, P. A., & Daneman, M. (1982). What eye fixations tell us about the time course of text integration. In R. Groner & P. Fraisse (Eds.), Cognition and eye movements (pp. 155-168). Amsterdam: North Holland.
Deffner, G. (1995). Eye movement recordings to study determinants of image quality in new display technology. In J.M. Fmdlay, R. Walker, & R. W. Kentridge (Eds.), Eye movement research: Mecha- nisms, processes, and applications (pp. 479-490). Amsterdam: North Holland.
DeGraef, P. (1992). Scene-context effects and models of real-world perception. In K. Rayner (Ed.), Eye movements and visual cognition: Scene perception and reading (pp. 2 4 3 -2 5 9 ). New York: Springer- V erlag.
DeGraef, P. (1998). Prefixational object perception in scenes: Objects popping out of schemas. In G. Underwood (Ed.), Eye guidance in reading and scene perception (pp. 313-336). Oxford, England: Elsevier.
DeGraef, P., Christiaens, D., & d'Ydewalle, G. (1990). Perceptual ef- fects of scene context on object identification. Psychological Re- search, 52, 317-329.
DeGraef, P., Delroy, A., & d'Ydewalle, G. (1992). Local and global contextual constraints on the identification of objects in scenes. Cana- dian Journal of Experimental Psychology, 46, 4 8 9 -5 0 8 .
DeLuca, M., Spinelli, D., & Zoccolotti, P. (1996). Eye movement pat- terns in reading as a function of visual field defects and contrast sensitivity. Cortex, 32, 491-502.
 DenBuurman, R., Boersma, T, & Gerrissen, J. F. (1981). Eye move- ments and the perceptual span in reading. Reading Research Quarterly, 16, 227-235.
Deubel, H. (1995). Separate addaptive mechanisms for the control of reactive and volitional saccadic eye movements. Vision Research, 35, 3529-3540.
Deubel, H., & Bridgeman, B. (1995a). Fourth Purkinje image signals reveal eye-lens deviations and retinal image distortions during sac- cades. Vision Research, 35, 529-538.
Deubel, H., & Bridgeman, B. (1995b). Perceptual consequences of ocular lens overshoot, during saccadic eye movements. Vision Re- search, 35, 2897-2902.
Deubel,H.,Bridgeman,B.,&Schneider,W.X.(1998).Immediatepost- saccadic information mediates space constancy. Vision Research, 38, 3147-3159.
Deubel, H., & Schneider, W . X. (1996). Saccade target selection and object recognition: Evidence for a common attentional mechanism. Vision Research, 36, 1827-1837.
Deubel, H., Schneider, W . X., & Bridgeman, B. (1996). Postsaccadic target blanking prevents saccadic suppression of image displacement. Vision Research, 36, 985-996.
Deubel, H., Wolf, W., & Hauske, G. (1984). The evaluation of the oculomotor error signal. In A. G. Gale & R Johnson (Eds.), Theoreti- cal and applied aspects of eye movement research (pp. 55-62). Amsterdam: North Holland.
Deutsch, A. (1998). Subject-predicate agreement in Hebrew: Interrela- tions with semantic processes. Language and Cognitive Processes, 13, 575-597.
Dillon, R. F. (1985). Eye movement analysis of information processing under different testing conditions. Contemporary Educational Psy- chology, 10, 387-395.
Di Lollo, V, Bischof, W.F., Walther-Muller, P.U., Groner, M.T, & Groner, R. (1994). Phosphor persistence in oscilloscopic displays: Its luminance and visibility. Vision Research, 34, 1619-1620.
Dishart, D. C., & Land, M. F.(1998). The development of eye movement strategies of learner drivers. In G. Underwood (Ed.) Eye guidance in reading and scene perception (pp. 419-430). Oxford, England: Elsevier.
Donk, M. (1994). Human monitoring behavior in multiple-instrument setting: Independent sampling, sequential sampling, or arrangement dependent. Acta Psychologica, 86, 31-55.
Dopkins, S., Morris, R. K., & Rayner, K. (1992). Lexical ambiguity and eye fixations in reading: A test of competing models of lexical ambiguity resolution. Journal of Memory and Language, 31, 461- 477.
Dore, K., & Beauvillain, C. (1997). Latency dependence of word-initial letter integration by the saccadic system. Perception & Psychophysics, 59, 523-533.
Duffy, S. A. (1992). Eye movements and complex comprehension pro- cesses. In K. Rayner (Ed.), Eye movements and visual cognition: Scene perception and reading (pp. 4 6 2 -4 7 1 ). New York: Springer- V erlag.
Duffy, S. A., Morris, R. K., & Rayner, K. (1988). Lexical ambiguity and fixation times in reading. Journal of Memory and Language, 27, 429-446.
Duffy, S. A., & Rayner, K. (1990). Eye movements and anaphor resolu- tion: Effects of antecedent typicality and distance. Language and Speech, 33, 103-119.
Dunn-Rankin, P. (1978). The visual characteristics of words. Scientific American, 238, 122-130.
d'Ydewalle, G., & Gielen, I. (1992). Attention allocation with overlap- ping sound, image, and text. In K. Rayner (Ed.), Eye movements and visual cognition: Scene perception and reading (pp. 415-427). New York: Springer-V erlag.
M. K. (1995). Eye movements as a window into real-time spoken language comprehension in natural contexts. Journal of Psycholin- guistic Research, 24, 409-436.
Eden, G. P., Stein, J. E, Wood, H. M., & Wood, F.B. (1994). Differ- ences in eye movements and reading problems hi dyslexic and normal children. Vision Research, 34, 1345-1358.
Ehrlich, K. (1983). Eye movements in pronoun assignment: A study of sentence integration. In K. Rayner (Ed.), Eye movements in reading: Perceptual and language processes (pp. 253-268). New York: Aca- demic Press.
Ehrlich, K., & Rayner, K. (1983). Pronoun assignment and semantic integration during reading: Eye movements and immediacy of pro- cessing.JournalofVerbalLearningandVerbalBehavior,22,15- 87.
Ehrlich, S. E, & Rayner, K. (1981). Contextual effects on word percep- tion and eye movements during reading. Journal of Verbal Learning and Verbal Behavior, 20, 641-655.
Ellis, S. R., & Stark, L. (1978). Eye movements during the viewing of Necker cubes. Perception, 7, 575-581.
Ellis, S. R., & Stark, L. (1986). Statistical dependency in visual scan- ning. Human Factors, 28, 421-438.
Elterman, R.D., Abel, L.A., Daroff, R.B., Dell'Osso, L.E, & Bornstein, J. L. (1980). Eye movement patterns in dyslexic children. Journal of Learning Disabilities, 13, 312-317.
Engel, F.L. (1977). Visual conspicuity, visual search and fixation ten- dencies of the eye. Vision Research, 17, 95-108.
Epelboim, J., Booth, J. R., Ashkenazy, R., Taleghani, A., & Steinman, R. M. (1997). Fillers and spaces in text: The importance of word recognition during reading. Vision Research, 37, 2899-2914.
Epelboim, J., Booth, J. R., & Steinman, R. M. (1994). Reading un- spaced text: Implications for theories of reading eye movements. Vi- sion Research, 34, 1735-1766.
Epelboim, J., Booth, J. R., & Steinman, R. M. (1996). Much ado about nothing: The place of space in text. Vision Research, 36, 465-470.
Epelboim, J., Steinman, R. M., Kowler, E., Edwards, M., Pizlo, Z., Erkel- ens, C. J., & Collewijn, H. (1995). The function of visual search and memory in sequential looking tasks. Vision Research, 35, 3401-3422.
Eskenazi, B., & Diamond, S. P.(1983). Visualexploration of non-verbal material by dyslexic children. Cortex, 19, 353-370.
Everatt, J., Bradshaw, M. E, & Hibbard, P. B. (1998). Individual differ- ences in reading and eye movement control. In G. Underwood (Ed.), Eye guidance in reading and scene perception (pp. 223-242). Ox- ford, England: Elsevier.
Everatt, J., & Underwood, G. (1992). Parafoveal guidance and priming effects during reading: A special case of the mind being ahead of the eyes. Consciousness and Cognition, 1, 186-197.
Everatt, J., & Underwood, G. (1994). Individual differences in reading subprocesses: Relationships between reading ability, lexical access, and eye movement control. Language and Speech, 37, 283-297.
Farmer, M. E., & Klein, R. M. (1995). The evidence for a temporal processing deficit linked to dyslexia: A review. Psychonomic Bulle- tin & Review,2, 460-493.
Beldman, J. A. (1985). Four frames suffice: A provisional model of vision and space. Behavioral and Brain Sciences, 8, 265-289.
Ferreira, F., & Clifton, C. (1986). The independence of syntactic pro- cessing. Journal of Memory and Language, 25, 348-368.
Ferreira, R, & Henderson, J. M. (1990). Use of verb information in syntactic parsing: Evidence from eye movements and word-by-word self-paced reading. Journal of Experimental Psychology: Learning, Memory, and Cognition, 16, 555-568.
Ferreira, E, & Henderson, J. M. (1993). Reading processes during syn- tactic analysis and reanalysis. Canadian Journal of Experimental Psy- chology, 47, 247-276.
Eberhard, K. M., Spivey-Knowlton, M. J., Sedivy, J. C., & Tanenhaus,
Ferreira, E, & McClure, K. K. (1997). Parsing of garden-parh sentences
EYE MOVEMENTS
IN READING 4 0 7
 408 RA YNER
with reciprocal verbs. Language and Cognitive Processes, 12, 273-
306.
Fields, H., Wright, S., & Newman, S. (1993). Saccadic eye movement
while reading and tracking in dyslexics, reading-matched, and IQ- matched children. In G. d'Ydewalle & J. Van Rensbergen (Eds.), Perception and cognition: Advances in eye movement research (pp. 309-319). Amsterdam: North Holland.
Findlay, J. M. (1981). Local and global influences on saccadic eye movements. In D. F. Fisher, R. A. Monty, & J. W. Senders (Eds.), Eye movements: Cognition and visual perception (pp. 171 —179). Hillsdale, NJ: Erlbaum.
Findlay, J. M. (1982). Global processing for saccadic eye movements. Vision Research, 22, 1033-1045.
Findlay, J. M. (1992). Programming of stimulus-elicited saccadic eye movements. In K. Rayner (Ed.), Eye movements and visualcognition: Scene perception and reading (pp. 8 -3 0 ). New % rk: Springer-Verlag.
Findlay, J. M. (1995). Visual search: Eye movements and peripheral vision. Optometry and Vision Science, 72, 4 6 1 -4 6 6 .
Findlay, J. M . (1997). Saccade target selection during visual search. Vision Research, 37, 617-631.
Findlay, J. M., Brogan, D., & Wenban-Smith, M. G. (1993). The spatial signal for saccadic eye movements emphasizes visual boundaries. Perception & Psychophysics, 53, 633-641.
Findlay, J. M., & Gilchrist, I. D. (1997). Spatial scale and saccadic programming. Perception, 26, 1159-1167.
Findlay, J. M., & Gilchrist, I. D. (1998). Eye guidance and visual search. InG . Underwood (Ed.), Eye guidance in reading and scene perception (pp. 295-312). Oxford, England: Elsevier.
Findlay, J. M., & Kapoula, Z. (1992). Scrutinization, spatial attention, and the spatial programming of saccadic eye movements. Quarterly Journal of Experimental Psychology, 45A, 633-648.
Findlay, J. M., & Walker, R. (in press). A model of saccade generation based on parallel processing and competitive inhibition. Behavioral and Brain Sciences.
Fischer, B. (1992). Saccadic reaction time: Implications for reading, dyslexia, and visual cognition. In K. Rayner (Ed.), Eye movements and visual cognition: Scene perception and reading (pp. 3 1 -4 5 ). New %rk: Springer-Verlag.
Fischer, B., Biscaldi, M., & Otto, P. (1993). Saccadic eye movements of dyslexic adult subjects. Neuropsychologia, 31, 887-906.
Fischer, B., & Boch, R. (1983). Saccadic eye movements after extremely short reaction times in the monkey. Brain Research, 260, 21-26. Fischer, B., & Rampsberger, E. (1984). Human express saccades: Ex-
tremely short reaction times of goal directed eye movements. Experi-
mental Brain Research, 57, 191-195.
Fischer, B., & Weber, H. (1990). Saccadic reaction times of dyslexic
and age-matched normal children. Perception, 19, 805-818.
Fischer, B., & W eber, H. (1993). Express saccades and visual attention.
Behavioral and Brain Sciences, 16, 553—567.
Fischer, M. H. (in press). An investigation of attention allocation during
sequential eye movement tasks. Quarterly Journal of Experimental
Psychology.
Fischer, M. H., & Rayner, K. (1993). On the functional significance of express saccades. Behavioral and Brain Sciences, 16, 577.
Fisher, D. F., & Shebilske, W . L. (1985). There is more that meets the eye than the eyemind assumption. In R. Groner, G. W. McConkie, & C. Menz (Eds.), Eye movements and human information processing (pp. 149-158). Amsterdam: North Holland.
Fletcher, J.M. (1990). Component skill comparisons across reading level and processing demand. Reading Psychology, 11, 193-239. Fletcher,!. M. (1991). Qualitative descriptions of error recovery patterns
across reading level and sentence topic: A n eye-m ovem ent analysis.
Journal of Learning Disabilities, 24, 568-575.
Fletcher, J. M. (1993). Eye-movement rhythmicity and reading compre-
hension. Journal of Learning Disabilities, 26, 683-688.
Folk, J. R. (1998). Phonological codes are used to access the lexicon during silent reading. Manuscript submitted for publication.
Folk, J. R., & Morris, R. K. (1995). Multiple lexical codes in reading: Evidence from eye movements, naming time, and oral reading. Jour- nal of Experimental Psychology: Learning, Memory, and Cognition, 21, 1412-1429.
Frazier, L., & Clifton, C. (1998). Comprehension of sluiced sentences. Language and Cognitive Processes, 13, 499-520.
Frazier, L., & Rayner, K. (1982). Making and correcting errors during sentence comprehension: Eye movements in the analysis of structur- ally ambiguous sentences. Cognitive Psychology, 14, 178-210.
Frazier, L., & Rayner, K. (1987). Resolution of syntactic category ambi- guities: Eye movements in parsing lexically ambiguous sentences. Journal of Memory and Language, 26, 505-526.
Frazier, L., & Rayner,K. (1988). Parameterizing the language processing system: Left- vs. right-branching within and across languages. In J. A. Hawkins (Ed.), Explaining language universal* (pp. 247-279). Oxford, England: Blackwell.
Frazier, L., & Rayner, K. (1990). Taking on semantic commitments:' Processing multiple meanings vs. multiple senses. Journal of Memory and Language, 29, 181-200.
Frenck-M estre, C., & Pynte, J. (1997). Syntactic ambiguity resolution while reading in second and native languages. Quarterly Journal of Experimental Psychology, SOA, 119-148.
Friedman, A. (1979). Framing pictures: The role of knowledge in au- tomatized encoding and memory for gist. Journal of Experimental Psychology: General, 108, 316-355.
Friedman, A., & Liebelt, L. S. (1981). On the time course of viewing pictures with a view towards remembering. In D. F. Fisher, R. A. Monty,&1.W.Senders(Eds.),Eyemovements:Cognitionand visual perception (pp. 137-156). Hillsdale, NJ: Erlbaum.
Frisson, S., & Pickering, M. (1998). The processing of metonymy: Evi- dencefromeyemovements.Manuscriptsubmittedforpublication. Gale, A. G., & Findlay, J. M. (1983). Eye movement patterns inviewing
ambiguous figures. In R. Grbner, C.Menz,D. F.Fisher, & R. A.Monty (Eds.), Eye movements and psychological functions: International views (pp. 145-168). Hillsdale, NJ: Erlbaum.
Garcia-Perez, M. A. (1989). Visual homogeneity and eye movements in multistable perception. Perception & Psychophysics, 46, 397-400. Gamsey, S. M., Pearlmutter, N. J., Myers, E., & Lotocky, M. A. (1997). The contributions of verb bias and plausibility to the comprehension of temporarily ambiguous sentences. Journal of Memory and Lan-
guage, 37, 58-93.
Garrod, S., Freudenthal, S., & Boyle, E. (1994). The role of different
types of anaphor in the on-line resolution of sentences in a discourse.
Journal of Memory and Language, 33, 3 9 -6 8 .
Garrod, S., O'Brien, E. J., Morris, R. K., & Rayner,K. (1990). Elabora-
tive inferencing as an active or passive process. Journal of Experimen-
tal Psychology: Learning, Memory, and Cognition, 16, 250-257. Geiger, G., & Lettvin, J. Y . (1987). Peripheral vision in persons with
dyslexia.NewEnglandJournalofMedicine,316,1238-1243. Geiger, G., Lettvin, J. Y., & Fahle, M. (1994). Dyslexic children learn a new visual strategy for reading: A controlled experiment. Vision
Research, 34, 1223-1233.
Gielen, L, Brysbaert, M., & Dhondt. A. (1991). The syllable-length
effect in number processing is task-dependent. Perception & Psycho-
physics, 50, 449-458.
Gilchrist, I. D., Brown, V, & Findlay, J. M. (1997). Saccades without
eye movements. Nature, 390, 130-131.
Gilchrist, I. D., Findlay, J. M:, & Heywood, C. A. (in press). Surface
and edge inform ation for spatial integration: A saccade-selection task.
Visual Cognition.
Goldberg, J. H., & Schryver, J. C. (1995). Eye-gaze determination of user intent at the computer interface. In J. M. Findlay, R. Walker, &
 R. W. Kentridge (Eds.), Eye movement research: Mechanisms, pro-
cesses and applications (pp. 491-502). Amsterdam: North Holland. Goolkasian, P., & Bunt, A. A. (1980). Eye movements and a dynamic
stimulus situation. American Journal of Psychology, 91, 251-265. Goolkasian, P., & King, J. (1990). Letter identification and lateral mask- ing in dyslexic and average readers. American Journal of Psychology,
103, 519-538.
Goolsby,T.W.(1989).Computer applications toeyemovement research
in music reading. Psychomusicology, 8, 111-126.
Goolsby, T.W . (1994a). Eye movement in music reading: Effects of
reading ability, notational complexity, and encounters. Music Percep-
tion, 72, 77-96.
Goolsby, T. W . (1994b). Profiles of processing: Eye movements during
sightreading.MusicPerception,12,97-123.
Gould, J. D. (1973). Eye movements during visual search and memory
search. Journal of Experimental Psychology, 98, 184-195.
Grabe, M., Antes, J., Kahn, H., & Kristjanson, A. (1991). Adult and adolescent readers' comprehension monitoring performance: An in- vestigation of monitoring accuracy and related eye movements. Con-
temporary Educational Psychology, 16, 4 5 -6 0 .
Grabe, M., Antes, J., Thorson, I., & Kahn, H. (1987). Eye fixation
patterns during informed and uninformed comprehension monitoring. Journal of Reading Behavior, 19, 123-140.
Graefe, T.M., & Vaughan, J. (1978). Saccadic and manual reaction times to stimuli initiated by eye or finger movements. Bulletin of the Psychonomic Society, 11, 97-99.
Grainger, J., O'Regan, J. K., Jacobs, A. M., & Sequi, J. (1992). Neigh- borhood frequency affects and letter visibility in visual w ord recogni- tion. Perception & Psychophysics, 51, 4 9 -5 6 .
Groner, R., Groner, M.T., Muller, P.U., Bischof, W.E, & Di Lollo, V. (1993). On the confounding effects of phosphor persistence in oscilloscopic displays. Vision Research, 33, 913-917.
Mainline, L., Tiirkel, J., Abramov, I., Lemerise, E., & Harris, C. M. (1984). Characteristics of saccades in human infants. Vision Re- search, 24, 1771-1780.
Hallett, P.E. (1978). Primary and secondary saccades to goals denned by instructions. Vision Research, 18, 1279-1296.
Hansen, J. P. (1991). The use of eye mark recordings to supplement verbal retrospective in software testing. Acta Psychologica, 76, 3 1 - 49.
Hansen, W., & Sanders, A. F. (1988). On the output of encoding during stimulus fixation. Acta Psychologica, 69, 95-107.
Harris, C. M., Abramov, I., & Hainline, L. (1984). Instrument consider- ations in measuring fast eye movements. Behavior Research Methods, Instruments, & Computers, 16, 341-350.
Harris, C. M., Hainline, L., Abramov, I., Lemerise, E., & Camenzuli, C. (1988). The distribution of fixation durations in infants and naive adults. Vision Research, 28, 419-432.
Hayhoe, M. M., Bensinger, D. G., & Ballard, D. H. (1998). Task con- straints in visual working memory. Vision Research, 38, 125-137. Hayhoe, M. M., Lachter, J., & Feldman, J. (1991). Integration of form
across saccadic eye movements. Perception, 20, 393-402.
Hayhoe, M. M., Lachter, J., & Moeller, P. (1992). Spatial memory and integration across saccadic eye movements. In K. Rayner (Ed.), Eye
movements and visual cognition: Scene perception and reading (pp.
130-145). New York: Springer-Verlag.
He, P., & Kowler, E. (1989). The role of location probability in the
programming of saccades: Implications for "center-of-gravity" ten-
dencies. Vision Research, 29, 1165-1181.
He, P., & Kowler, E. (1992). The role of saccades in the perception of
texture patterns. Vision Research, 32, 2151-2163.
Hegarty, M. (1992a). The mechanics of comprehension and comprehen-
sion of mechanics. In K. Rayner (Ed.), Eye movements and visual cognition: Scene perception and reading (pp. 428-443). New "Vbrk: Springer.
Hegarty, M. (1992b). Mental animation: Inferring motion from static displays of mechanical systems. Journal of Experimental Psychology: Learning, Memory and Cognition, 18, 1084-1102,
Hegarty, M., & Just, M. A. (1993). Constructing mental models of machines from text and diagrams. Journal of Memory and Language, 32, 717-742.
Hegarty, M., Mayer, R. E., & Green, C. E. (1992). Comprehension of arithmetic word problems: Evidence from students' eye fixations. Journal of Educational Psychology, 84, 7 6 -8 4 .
Heller, D. (1982). Eye movements in reading. In R. Groner & P.Fraisse (Eds.), Cognition and eye movements (pp. 139-154). Amsterdam: North Holland.
Heller, D. (1983). Problems of on-line processing of EOG-data in read- ing. In R. Groner, C. Menz, D. F. Fisher, & R. A. Monty (Eds.), Eye movements and psychologicalfunctions: International views (pp. 43- 52). Hillsdale, NJ: Erlbaum.
Heller, D., & Radach. R. (1995). Binocular coordination in complex visual tasks. Perception, 24, 72 (Supplem ent).
Henderson, J. M . (1992a). Identifying objects across saccades: Effects of extrafoveal preview and flanker object context. Journal of Experi- mental Psychology: Learning, Memory, and Cognition, 18, 521-530.
Henderson, J. M. (1992b). Visual attention and eye movement control during reading and picture viewing. In K. Rayner (Ed.), Eye move- ments and visual cognition: Scene perception and reading (pp. 260- 283). New Tfork: Springer-V erlag.
Henderson, J. M. (1993). Eye movement control during visual object processing: Effects of initial fixation position and semantic constraint. Canadian Journal of Experimental Psychology, 47, 7 9 -9 8 .
Henderson, J. M. (1994). Two representational systems in dynamic vi- sual identification. Journal of Experimental Psychology: General, 123, 410-426.
Henderson, J. M. (1997). Transsaccadic memory and integration during real-world object perception. Psychological Science, 8, 51-55.
Henderson, J. M., & Anes, M. D. (1994). Roles of object-file review and type prim ing in visual identification w ithin and across eye fixations. Journal of Experimental Psychology: Human Perception and Perfor- mance, 20, 826-839.
Henderson, J. M., Dixon, P., Petersen, A., Twilley, L. C., & Ferreira, F. (1995). Evidence for the use of phonological representations during transsaccadic word recognition. Journal of Experimental Psychology: Human Perception and Performance, 21, 82-97.
Henderson, J. M., & Ferreira, F. (1990). Effects of foveal processing difficulty on the perceptual span in reading: Implications for attention and eye movement control. Journal of Experimental Psychology: Learning, Memory, and Cognition, 16, 417-429.
Henderson, J. M., & Ferreira, F. (1993). Eye movement control during reading: Fixation measures reflect foveal but not parafoveal pro- cessing difficulty. Canadian Journal of Experimental Psychology, 47, 201-221.
Henderson, J. M., & Hollingworth, A. (1998). Eye movements during scene viewing: An overview. In G. Underwood (Ed.), Eye guidance in reading and scene perception (pp. 269-294). Oxford, England: Elsevier.
Henderson, J. M., M cClure, K. K., Pierce, S., & Schrock, G. (1997). Object identification without foveal vision: Evidence from an artificial scotoma paradigm. Perception & Psychophysics, 59, 323-346.
Henderson, J. M., Pollatsek, A., & Rayner, K. (1987). The effects of foveal priming and extrafoveal preview on object identification. Jour- nal of Experimental Psychology: Human Perception and Perfor- mance, 13, 449-463.
Henderson, J. M., Pollatsek, A., & Rayner, K. (1989). Covert visual attention and extrafoveal information use during object identification. Perception & Psychophysics, 45, 196-208.
Henderson, J. M., & Siefert, A. B. C. (in press-a). The influence of enantiom orpnic transform ation on transsaccadic object integration.
EYE MOVEMENTS IN READING 409
 410 RA YNER
Journal of Experimental Psychology: Human Perception and
Performance.
Henderson, J. M., & Siefert, A. B. D. (in press-b). Types and tokens in transsaccadic object identification: Effects of spatial position and left- right orientation. Psychonomic Bulletin & Review.
Henderson, I. M., Weeks, P. A., & Hollingworth, A. (in press). The effects of semantic consistency on eye movements during complex scene viewing. Journal of Experimental Psychology: Human Percep- tion and Performance.
Henriks, A. W . (1996). Vergence eye movements during fixations in reading. Ada Psychologica, 92, 131-151.
Heywood, S., & Churcher, J. (1980). Structure of the visual array and saccadic latency: Implications for oculomotor control. Quarterly Jour- nal of Experimental Psychology, 32, 335-341.
Hoffman, J. E., & Subramaniam, B. (1995). The role of visual attention in saccadic eye movements. Perception & Psychophysics, 57, 787- 795.
Hogaboam, T. W . (1983). Reading patterns in eye movements. In K. Rayner (Ed.), Eye movements in reading (pp. 309-332). New Tfork: Academic Press.
Hogaboam, T.W., & McConkie, G.W. (1981). The rocky road from eye fixations to comprehension (Tech. Rep. No. 207). University of Illinois at Urbana-Champaign, Center for the Study of Reading.
Holmes, V.M., & O'Regan, J. K. (1981). Eye fixation patterns during the reading of relative-clause sentences. Journal of Verbal Learning and Verbal Behavior, 20, 417-430.
Holmes, V.M., & O'Regan, J. K. (1987). Decomposing French words. In J. K. O'Regan & A. Levy-Schoen (Eds.), Eye movements: From physiology to cognition (pp. 459-466). Amsterdam: North Holland.
Holmes, V.M., & O'Regan,}. K. (1992). Reading derivationally affixed French words. Language and Cognitive Processes, 7, 163-192.
Hooge, I. T. C , & Erkelens, C. J. (1996). Control of fixation duration during a simple search task. Perception & Psychophysics, 58, 969- 976.
Hooge, I. T.C., & Erkelens, C. 1.(1998).Adjustment of fixation duration during visual search. Vision Research, 38, 1295-1302.
Huey, E. B. (1908). The psychology and pedagogy of reading. New York: Macmillan.
HyOna, J. (1993). Effects of thematic and lexical priming on readers' eye movements. Scandinavian Journal of Psychology, 34, 293-304. Hyona, J. (1995a). Do irregular letter combinations attract readers'
attention? Evidence from fixation locations in words. Journal of Ex- perimental Psychology: Human Perception and Performance, 21, 68-
81.
Hyona, I. (1995b). A n eye movement analysis of topic-shift effect dur-
ing repeated reading. Journal of Experimental Psychology: Learning,
Memory, and Cognition, 21, 1365-1373.
Hyona, J., & Hujanen, H. (1997). Effects of case marking and word
order on sentence parsing in Finnish: An eye fixation analysis. Quar-
terly Journal of Experimental Psychology, 50A, 841-858.
Hyona, J., & Jarvella, R.J. (1993). Time course of context effects during reading: An eye fixation analysis. In G. d'Ydewalle & J. Van Rensbergen (Eds.), Perception and cognition: Advances in eye move-
ment research (pp. 239-249). Amsterdam: North Holland.
Hyona, J., & Niemi, P. (1990). Eye movements in repeated reading of
a text. Acta Psychologica, 73, 259-280.
HyonS, J., Niemi, P., & Underwood, G. (1989). Reading long words
embedded in sentences: Informativeness of word halves affects eye movements. Journal of Experimental Psychology: Human Perception and Performance, 15, 142-152.
Hyona, J., & Olson, R. K. (1995). Eye movement patterns among dys- lexic and normal readers: Effects of word length and word frequency. Journal of Experimental Psychology: Learning, Memory, and Cogni- tion, 21, 1430-1440.
Hyona, J., & Pollarsek, A. (in press). Reading Finnish compound words:
Eye fixations are affected by component morphemes. Journal of Ex-
perimental Psychology: Human Perception and Performance.
Ikeda, M., & Saida, S. (1978). Span of recognition in reading. Vision
Research, IS, 83-88.
Ikeda, M ., Saida, S., & Sugiyam a, T . (1 9 7 7 ). V isual field size necessary
for length comparison. Perception & Psychophysics, 22, 165-170. Inhoff,A.W.(1984).Twostagesofwordprocessing duringeyefixations in the reading of prose. Journal of Verbal Learning and Verbal Behav-
ior, 23, 612-624.
Inhoff, A .W . (1 9 8 5 ). The effect of factivity on lexical retrieval and
postlexical processing during eye fixations in reading. Journal of
Psycholinguistic Research, 14, 4 5 -5 6 .
Inhoff, A. W . (1986). Preparing sequences of saccades. Acta Psycholog-
ica, 61, 211-228.
Inhoff, A. W .(1987). Parafoveal word perception during eye fixations in
reading: Effects of visual salience and word structure. In M. Coltheart (Ed.), Attention and performance (V ol. 12, pp. 403-420). London: Erlbaum.
Inhoff, A. W . (1989a). Lexical access during eye fixations in reading: A re w ord access codes used to integrate lexical inform ation across interword fixations? Journal of Memory and Language, 28, 444-461.
Inhoff, A .W . (1989b). Parafoveal processing of words and saccade computation during eye fixations in reading. Journal of Experimental Psychology: Human Perception and Performance, 15, 544-555.
Inhoff, A . W . (1 9 9 0 ). Integrating inform ation across eye fixations in reading: The role of letter and word units. Acta Psychologica, 73, 281-297.
Inhoff, A .W . (1991). Effects of word-frequency during copytyping.
Journal of Experimental Psychology: Human Perception and Perfor- mance, 17, 478-488,
Inhoff, A. W , Bohemier, G., & Briihl, D. (1992). Integrating text across fixations in reading and copytyping. In K. Rayner (Ed.), Eye move- ments and visual cognition: Scene perception and reading (pp. 355-
368). New "fork: Springer-Verlag.
Inhoff, A. W , & Briihl, D. (1991). Semantic processing of unattended
text during selective reading: How the eyes see it. Perception &
Psychophysics, 49, 289-294.
Inhoff, A.W, Briihl, D., Bohemier, G., & Wang, J. (1992). Eye-hand
span and coding of text during copytyping. Journal of Experimental
Psychology: Learning, Memory, and Cognition, 18, 298-306. Inhoff, A. W , Briihl, D., & Schwartz, J. (1996). Compound word effects differ in reading, on-line naming, and delayed naming tasks. Mem-
ory & Cognition, 24, 466-476.
Inhoff, A. W , Briihl, D., & Starr, M. (1998). Parafovea-to-fovea prim-
ing during eye fixations in reading. M anuscript subm itted for
publication.
Inhoff, A. W , Calabrese, I., & Morris, R. K. (1986). Eye movements
in skilled typing. Bulletin of the Psychonomic Society, 24, 113-114. Inhoff, A .W , Chiu, T., & W ang, J. (1990). Coordination of eye and finger movements in copytyping. Bulletin of the Psychonomic Society,
28, 302-304.
Inhoff, A. W., & Gordon, A. M. (1997). Eye movements and eye-hand
coordination during typing. Current Directions in Psychological Sci-
ence, 6, 153-157.
Inhoff, A. W., Lima, S. D., & Carroll, P.J. (1984). Contextual effects
on metaphor comprehension in reading. Memory & Cognition, 12,
558-567.
Inhoff, A. W , & Liu, W . (1998). The perceptual span and oculomotor
activity during the reading of Chinese sentences. Journal of Experi-
mental Psychology: Human Perception and Performance, 24, 20-34. Inhoff, A. W., Pollatsek, A., Posner, M. I., & Rayner, K. (1989). Covert attention and eye movements during reading. Quarterly Journal of
Experimental Psychology, 41A, 6 3 -8 9 .
Inhoff, A. W., & Radach, R. (1998). Definition and computation of
 oculomotor measures in the study of cognitive processes. In G. Un- derwood (Ed.), Eye guidance in reading and scene perception (pp. 2 9 -5 4 ). Oxford, England: Elsevier.
Inhoff, A. W., Radach, R., & Heller, D. (1996). Morphological processes during word recognition. Abstracts of the Psychonomic Society, 1, 54.
Inhoff, A. W., & Rayner, K. (1986). Parafoveal word processing during eye fixations in reading: Effects of word frequency. Perception & Psychophysics, 40, 431-439.
Inhoff, A. W., Starr, M., Liu, W., & Wang, J. (1998). Eye-movement- contingent display changes are not compromised by flicker and phos- phor persistence. Psychonomic Bulletin & Review, 5, 101-106.
Inhoff, A. W., & Tbpolski, R. (1992). Lack of semantic activation from unattended text during passage reading. Bulletin of the Psychonomic Society, 30, 365-366.
Inhoff, A. W., & Topolski, R. (1994). Use of phonological codes during eye fixations in reading and in on-line and delayed naming tasks. Journal of Memory and Language, 33, 689-713.
Inhoff, A. W., Topolski, R., Vitu, E, & O'Regan, J. K. (1993). Attention demands during reading and the occurrence of brief (express) fixa- tions. Perception & Psychophysics, 54, 814-823.
Inhoff, A. W., Topolski, R., & W ang, J. (1992). Saccade programming during short duration fixations: An examination of copytyping, letter detection, and reading. Acta Psychologica, 81, 1-21.
Inhoff, A.W., & Tbusman, S. (1991). Lexical integration across sac- cades in reading. Psychological Research, 51, 330-337.
Inhoff, A. W, & Wang, J. (1992). Encoding of text, manualmovement planning, and eye-hand coordination during copytyping. Journal of Experimental Psychology: Hitman Perception and Performance, 18, 437-448.
Irwin, D. E. (1991). Information integration across saccadic eye move- ments. Cognitive Psychology, 23, 420-456.
Irwin, D. E. (1992a). Memory for position and identity across eye move- ments. Journal of Experimental Psychology: Learning, Memory, and Cognition, 18, 307-317.
Irwin, D. E. (1992b). Visual memory within and across fixations. In K. Rayner (Ed.), Eye movements and visual cognition: Scene perception and reading (pp. 146-165). New \brk: Springer-Verlag.
Irwin, D. E. (1993). Perceiving an integrated world. In D. E. Meyer & S. Kornblum (Eds.), Attention and performance (V ol. 14, pp. 121-
142). Cambridge, MA: MIT Press.
Irwin, D.E. (1994). On the measurement of phosphor persistence in
oscilloscopic displays. Vision Research, 34, 1623.
Irwin, D. E. (1996). Integrating information across saccadic eye move-
ments. Current Directions In Psychological Science, 5, 94-100. Irwin, D. E. (1998). Lexical processing during saccadic eye movements.
Cognitive Psychology, 36, 1-27.
Irwin, D. E., & Andrews, R. (1996). Integration and accumulation of
information across saccadic eye movements. In T. Inui & J. McClel- land (Eds.), Attention and performance (V ol. 16, pp. 125-155). Cam- bridge, MA: MTT Press.
Irwin, D. E., Brown, J. S., & Sun, J. (1988). Visual masking and visual integration across saccadic eye movements. Journal of Experimental Psychology: General, 117, 276-287.
Irwin, D. E., & Carlson-Radvansky, L. A. (1996). Cognitive suppres- sion during saccadic eye movements. Psychological Science, 7, 83-
88. Irwin,D.E.,Carlson-Radvansky,L.A.,&Andrews,R.V.(1995).Infor-
mation processing during saccadic eye movements. Acta Psycholog-
ica, 90, 261-273.
Irwin, D. E., & Gordon, R. D. (1998). Eye movements, attention, and
transsaccadic memory. Visual Cognition, 5, 127-156.
Irwin, D. E., McConkie, G. W., Carlson-Radvansky, L., & Currie, C.
(1994). A localist solution for visual stability across saccades. Behav-
ioral and Brain Sciences, 17, 265-266.
Irwin, D. E.,'Y antis, S., & Jonides, J. (1983). Evidence against visual
integration across saccadic eye movements. Perception & Psycho-
physics, 34, 49-57.
Irwin, D. E., Zacks, J. L., & Brown, J. S. (1990). Visual memory and
the perception of a stable visual environment. Perception & Psycho-
physics, 47, 35-46.
Ishida, T., & Ikeda, M. (1989). Temporal properties of information
extraction in reading studied by a text-mask replacement technique. JournaloftheOpticalSocietyA:OpticsandlmageScience,6,1624- 1632.
Jacob, R. J. K. (1991). The use of eye movements in human-computer interaction techniques: What you look at is what you get. ACM Trans- actions on Information Systems, 9, 152-169.
Jacobs, A .M . (1986). Eye movement control in visual search: How direct is visual span control? Perception & Psychophysics, 39, 47- 58.
Jacobs, A .M . (1987a). On localization and saccade programming. Vi- sion Research, 27, 1953-1966.
Jacobs, A. M. (1987b). On the role of blank spaces for eye-movement control in visual search. Perception & Psychophysics, 41, 473-479. Jacobs, A. M. (1987c). Tbward a model of eye movement control in visual search. In J. K. O'Regan & A. Levy-Schoen (Eds.), Eye move- ments: From physiology to cognition (pp. 275-284). Amsterdam:
North Holland.
Jacobs, A. M. (1991). The search operating characteristic as a tool for
analyzing performance in dynamic visual search tasks. Spatial Vision,
5, 269-277.
Jacobs, A. M., Nazir, T. A., & Heller, O. (1989). Perception of lower
case letters in peripheral vision: A discrim ination m atrix based on
saccade latencies. Perception & Psychophysics, 46, 95-102.
Jacobs, A. M., & O'Regan, J. K. (1987). Spatial and/or temporal adjust- ments of scanning behavior to visibility changes. Acta Psychologica,
EYE MOVEMENTS IN READING 4 1 1
65, 133-146.
Jacobson, J. Z., & Dodwell, P . C. (1979). Saccadic
eye movements
during reading. Brain and Language, 8, 303-314.
Jared, D., Levy, B. A., & Rayner, K. (in press). The role of phonology
in the activation of word meanings during reading: Evidence from proofreading and eye movements. Journal of Experimental Psychol- ogy: General.
Jonides, J., Irwin, D. E., & Yantis, S. (1982). Integrating visual informa- tion from successive fixations. Science, 215, 192-194.
Jonides, J., Irwin, D. E., & Yantis, S. (1983). Failure to integrate infor- mation from successive fixations. Science, 222, 188.
Jordan, T. R., Patching, G. R., & Milner, A. D. (1998). Ensuring central fixations in divided visual field studies of cerebral asymmetry: Instruc- tions are not enough. Quarterly Journal of Experimental Psychology, 51A, 371-392.
Juola, J. E, Ward, N. J., & McNamara, T. (1982). Visual search and reading of rapid serial visual presentations of letter strings, words, and text. Journal of Experimental Psychology: General, 111, 208-
227.
Just, M. A., & Carpenter, P. A. (1978). Inference processing during
reading: Reflections from eye fixations. In J. W . Senders, D. F. Fisher, & R. A. Monty (Eds.), Eye movements and the higher psycho- logical functions (pp. 157-174). Hillsdale, NJ: Erlbaum.
Just, M. A., & Carpenter, P. A. (1980). A theory of reading: From eye fixationstocomprehension. PsychologicalReview,87,329-354.
Just, M. A, & Carpenter, P. A. (1985). Cognitive coordinate systems: Accounts of mental rotation and individual differences in spatial abil- ity. Psychological Review, 92, 137-172.
Just, M. A., & Carpenter, P. A. (1987). The psychology of reading and language comprehension. Boston: Allyn & Bacon.
 412 RA YNER
Just, M. A., Carpenter P. A., & Masson, M. E. 1. (1982). What eye fixations tell us about speed reading and skimming. (Eye-lab Technical
Report). Carnegie-Mellon University, Pittsburgh, P A.
Just, M. A., Carpenter, P. A., & Woolley, J. D. (1982). Paradigms and processes in reading comprehension. Journal of Experimental Psy-
chology: General, 111, 228-238.
Juttner, M. (1997). Effects of perceptual context on transsaccadic visual
matching. Perception & Psychophysics, 59, 762-773.
Juttner, M., & Rohler, R. (1993). Lateral information transfer across saccadic eye movements. Perception & Psychophysics, 53, 210-220. Kalesnykas, R. P., & Hallett, P.E. (1995). Retinal eccentricity and the
latency of eye saccades. Vision Research, 34, 517-531.
Kapoula, Z. A. (1983). The influence of peripheral preprocessing on oculomotor programming in a scanning task. In R. Groner, C. Menz, D. F.Fisher, & R. A. Monty (Eds.), Eye movements and psychological functions: International views (pp. 101-114). Hillsdale, NJ:
Erlbaum.
Kapoula, Z. A. (1984). Comment les yeux explorentails des alignments
graphiques? [How do the eyes explore lines of symbols?] Annee
Psychologique, 84, 207-226.
Kapoula, Z. A. (1985). Evidence for a range effect in the visual system.
Vision Research, 25, 1155-1157.
Kapoula, Z. A., & Robinson, D. A. (1986). Saccadic undershoot is not
inevitable: Saccades can be accurate. Vision Research, 26, 735-743. Karnath, H. O. (1994). Spatial limitations of eye movements during ocular exploration of simple line drawings in neglect syndrome. Cor-
tex, 30, 319-330.
Kamath, H. Q , & Huber, W. (1992). Abnormal eye movement behavior
during text reading in neglect syndrome: A case study. Neuropsycho-
togia, 30, 593-598.
Katsanis, J., Kortenkamp, S., lacono, W.O., & Grove, W.M. (1997).
Antisaccade performance in patients with schizophrenia and affective
disorder. Journal of Abnormal Psychology, 106, 468-472. Kawabata, N., Yamagami, K., & Noaki, M. (1978). Visual fixation
points and depth perception. Vision Research, 18, 853-854. Kennedy, A. (1983). On looking into space. In K. Rayner (Ed.), Eye movements in reading: Perceptual and language processes (pp. 237—
251). New Tfcrk: Academic Press.
Kennedy, A. (1987). Eye movements, reading skill and the spatial code.
In J. Beech & A. Colley (Eds.), Cognitive approaches to reading
(pp. 169-186). Chichester, England: Wiley.
Kennedy, A. (1992). The spatial coding hypothesis. In K. Rayner (Ed.),
Eye movements and visual cognition: Scene perception and reading
(pp. 379-396). New York: Springer-Verlag.
Kennedy, A. (1998). The influence of parafoveal words on foveal in-
spection time: Evidence for a processing tradeoff. In G. Underwood (Ed.), Eye guidance in reading and scene perception (pp. 149-180). Oxford, England: Elsevier.
Kennedy, A., & Baccino, T. (1995). The effects of screen refresh rate on editing operations using a computer mouse pointing device. Quarterly Journal of Experimental Psychology, 48A, 5 5 -7 1 .
Kennedy, A., Brysbaert, M., & Murray, W.S. (1998). The effects of intermittent illumination of a visual inspection task. Quarterly Journal of Experimental Psychology, 51A, 135-152.
Kennedy, A., & Murray, W . S. (1984). Inspection times for words in syntactically ambiguous sentences under three presentation conditions. Journal of Experimental Psychology: Human Perception and Perfor- mance, 10, 833-849.
Kennedy, A., & Murray, W.S. (1987a). The components of reading time: Eye movement patterns of good and poor readers. In J. K. O'Re- gan & A. Levy-Schoen (Eds.), Eye movements: From physiology to cognition (pp. 509-520). Amsterdam: North Holland.
Comments on Monk. Quarterly Journal of Experimental Psychology,
39A, 649-656.
Kennedy, A., & Murray, W. S. (1991). The effects of flicker on eye
movement control. Quarterly Journal of Experimental Psychology,
43A, 79-99.
Kennedy, A., & Murray, W . S. (1993). 'Flicker' on VDU screens. Na-
ture, 365, 213.
Kennedy, A., & Murray, W . S. (1996). Eye movement control during
the inspection of words under conditions of pulsating illumination.
European Journal of Cognitive Psychology, 8, 381—404.
Kennedy, A., Murray, W . S., Jennings, F., & Reid, C. (1989). Parsing complements: Comments on the generality of the principle of minimal attachment [Special issue]. Language and Cognitive Processes, 4,
51-76.
Kennedy, A., & Pidcock, B. (1981). Eye movements and variations in
reading time. Psychological Research, 43, 69-79.
Kennison, S.M., & Clifton, C. (1995). Determinants of parafoveal
preview benefit in high and low working memory capacity readers: Implications for eye movement control. Journal of Experimental Psy- chology: Learning, Memory, and Cognition, 2], 6 8 -8 1 .
Kennison, S. M., & Gordon, P. C. (1998). Comprehending referential expressions during reading: Evidence from eye tracking. Discourse Processes, 24, 229-252.
Kentridge, R. W., Heywood, C. A., & W eiskrantz, L. (1997). Residual vision in multiple retinal locations within a scotoma: Implications for blindsight. Journal of Cognitive Neuroscience, 9, 191-202.
Kerr, J. S., & Underwood, G. (1984). Fixation time on anaphoric pro- nouns decreases with contiguity of reference. In A. G. Gale & F. Johnson (Eds.), Theoretical and applied aspects of eye movement research (pp. 195-202). Amsterdam: North Holland.
Kingstone, A., & Klein, R. M. (1993a). Visual offset facilitates saccade latency: Does pre-disengagement of attention mediate the gap effect? Journal of Experimental Psychology: Human Perception and Perfor- mance, 19, 251-265.
Kingstone, A., & Klein, R. M. (1993b). What are human express sac- cades? Perception & Psychophysics, 54, 260-273.
Kinsler, V., & Carpenter, R. H. S. (1995). Saccadic eye movements while reading music. Vision Research, 35, 1447-1458.
Klein, R. M. (1980). Does oculomotor readiness mediate cognitive con- trol of visual attention? In R. Nickerson (Ed.), Attention and perfor- mance. (V ol. 8, pp. 259-276). Hillsdale, NJ: Erlbaum.
Klein, R., Berry, G., Briand, K., D'Entremont, B., & Farmer, M. (1990). Letter identification declines with increasing retinal eccentricity at the same rate for normal and dyslexic readers. Perception & Psychophys- ics, 47, 601-606.
Klein, R., & Farrell, M. (1989). Search performance without eye move- ments. Perception & Psychophysics, 46, 476-482.
Klein, R. M., Kingstone, A., & Pontefract, A. (1992). Orienting of visual attention. In K. Rayner (Ed.), Eye movements and visual cognition: Scene perception and reading (pp. 4 6 -6 5 ). New \brk: Springer- V erlag.
Kliegl, R., & Olson, R. K. (1981). Reduction and calibration of eye movement data. Behavior Research Methods and Instrumentation, 13, 107-111.
Kliegl, R., Olson, R. K., & Davidson, B. J. (1982). Regression analyses as a tool for studying reading processes: Comments on Just and Car- penter's eye fixation theory. Memory & Cognition, 10, 287-296.
Kohsom, C., & Gobet, F. (1997). Adding spaces to Thai and English: Effects on reading. Proceedings of the Cognitive Science Society, 19, 388-393.
Kolers, P. A. (1976). Buswell's discoveries. In R. A. Monty & J. W. Senders (Eds.), Eye movements and psychological processes (pp. 371-395). Hillsdale, NJ: Erlbaum.
Kennedy, A., & Murray, W . S. (1987b). Spatial coordinates and reading:
Kolers, P. A., Duchnicky, R. L., & Ferguson, D. C. (1981). Eye move-
 ment measurement of readability of CRT displays. Human Factors,
23, 517-527.
Konieczny, L., Hemforth, B., Scheepers, C., & Strube, G. (1997). The
role of lexical heads in parsing: Evidence from German. Language
and Cognitive Processes, 12, 307-348.
Kowler, E., Anderson, E., Dosher, B., & Blaser, E. (1995). The role of
attention in the programming of saccades. Vision Research, 35, 1897- 1916.
Kowler, E., & Anton, S. (1987). Reading twisted text: Implications for the role of saccades. Vision Research, 27, 4 5 -6 0 .
Kowler, E., & Blaser, E. (1995). The accuracy and precision of saccades to small and larger targets. Vision Research, 35, 1741-1754.
Kowler, E., & Martins, A. J. (1985). Eye movements of preschool chil- dren. Science, 215, 997-999.
Kowler, E., & Steinman, R. M. (1977). The role of small saccades in counting. Vision Research, 17, 141-146.
Kowler, E., & Steinman, R. M. (1979). Miniature saccades: Eye move- ments that do not count. Vision Research, 19, 105-108.
Kristjanson, A. E, & Antes, J. R. (1989). Eye movement analysis of artistsandnonartistsviewingpaintings.VisualAm Research,15,21- 30.
Ladavas, E., Zeloni, G., Zaccara, G., & Gangemi, P.(1997). Eye move- ments and orienting of attention in patients with visual neglect. Jour- nal of Cognitive Neumscience, 9, 67-74.
Land, M. F. (1992). Predictable eye-head coordination during driving. Nature, 359, 318-320.
Land, M. F., & Furneaux, S. (1997). The knowledge base of the oculo- motor system. Philosophical Transactions of the Royal Society of London B. 352, 1231-1239.
Land, M.F., & Horwood, J. (1995). Which part of the road guides steering? Nature, 377, 339-340.
Land, M. F, & Lee, D. N. (1994). Where we look when we steer. Nature, 369, 742-744.
Lansing, C. R., & McConkie, G. W . (1994). A new method for speech reading research: Tracking observers' eye movements. Journal of the Academy of Rehabilitation Audiology, 27, 25-43.
Larsen, S., & Parlenvi, P. (1984). Patterns of inverted reading and subgroups in dyslexia. Annals of Dyslexia, 34, 195-203.
Lee, Y., Binder, K. S., Kim, J., Pollatsek, A., & Rayner, K. (in press). Activation of phonological codes during eye fixations in reading. Journal of Experimental Psychology: Human Perception and Performance.
Lefton, L. A., Nagle, R.J., Johnson, G., & Fisher, D.F. (1979). Eye movement dynamics of good and poor readers: Then and now. Journal of Reading Behavior, 11, 319-328.
Legge, G. E., Klitz, T.S., & Tjan, B. S. (1997). Mr. Chips: An ideal- observer model of reading. Psychological Review, 104, 524-553. Legge, G.E., Rubin, G. S., Pelli, D. G., & Schleske, M. M. (1985).
Psychophysics of reading: II. Low vision. Vision Research, 25, 253-
266.
Leigh, R. J., & Zee, D. S. (1991). The neurology of eye movements.
Philadelphia: F. A. Davis.
Lennerstrand, G., Ygge, J., & Rydberg, A. (1994). Binocular control in
normally reading children and dyslexics. In J. Ygge & G. Lennerstrand (Eds.), Eye movements in reading (pp. 291-300). Oxford, England: Pergamon Press.
Lesch, M. F., & Pollatsek, A. (1993). Automatic activation of semantic information by phonological codes in visual word recognition. Jour- nal of Experimental Psychology: Learning, Memory, and Cognition, 19, 285-294.
Lesch, M. F., & Pollatsek, A. (1998). Evidence for the use of assembled phonology in accessing the meaning of printed words. Journal of Experimental Psychology: Learning, Memory, and Cognition, 24, 573-592.
LeVy-Schoen, A. (1981). Flexible and/or rigid control of oculomotor scanning behavior. In D. F. Fisher, R. A. Monty, & J. W . Senders (Eds.), Eye movements: Cognition and visual perception (pp. 299- 316). Hillsdale, NJ: Erlbaum.
Levy-Schoen, A., & O'Regan, K. (1979). The control of eye movements in reading. In P.A. Kolers, M. E. Wrolstad, & H. Bouma (Eds.), Processing of visible language (pp. 7 -3 6 ). New "fork: Plenum.
Lima, S. D. (1987). Morphological analysis in sentence reading. Journal of Memory and Language, 26, 84-99.
Lima, S. D., & Inhoff, A. W.(1985). Lexical access during eye fixations in reading: Effects of word-initial letter sequences. Journal of Experi- mental Psychology: Human Perception and Performance, H, 272- 285.
Liu, A. (1998). What the driver's eye tells the car's brain. In G. Un- derwood (Ed.), Eye guidance in reading and scene perception (pp. 431-452). Oxford, England: Elsevier.
Liversedge, S. P., Paterson, K. B., & Pickering, M. J. (1998). Eye move- ments and measures of reading time. In G. Underwood (Ed.), Eye movements in reading and scene perception (pp. 5 5 -7 6 ). Oxford, England: Elsevier.
Liversedge, S. P., Pickering, M. J., Branigan, H. P., & van Gompel, R. P.G. (1998). Processing arguments and adjuncts in isolation and context: The case of by-phrase ambiguities in passives. Journal of Experimental Psychology: Learning, Memory, and Cognition, 24, 461-475.
Liversedge, S. P., & Underwood, G. (1998). Foveal processing load and landing position effects in reading. In G. Underwood (Ed.), Eye guidance in reading and scene perception (pp. 201-222). Oxford, England: Elsevier.
Locher, P.J., & Nodine, C.G. (1974). The role of scanpaths in the recognition of random shapes. Perception & Psychophysics, 15, 308- 314.
Loftus, E. F., Loftus, G. R., & Messo, J. (1987). Some facts about "W eapon Focus." Law and Human Behavior, 11, 55-62.
Loftus, G. R. (1972). Eye fixations and recognition memory for pic- tures. Cognitive Psychology, 3, 525-551.
Loftus, G. R. (1981). Tachistoscopic simulations of eye fixations on pictures. Journal of Experimental Psychology: Human Learning and Memory, 7, 369-376.
Loftus, G. R. (1983). Eye fixations on text and scenes. In K. Rayner (Ed.), Eye movements in reading: Perceptual and language processes (pp. 359-376). New "tort: Academic Press.
Loftus, G. R. (1985). Picture perception: Effects of luminance on avail- able information and information-extraction rate. Journal of Experi- mental Psychology: General, 114, 342-356.
Loftus, G. R., Kaufman, L., Nishimoto, X, & Ruthruff, E. (1992). Ef- fects of visual degradation on eye-fixation durations, perceptual pro- cessing, and long-term visual memory. In K. Rayner (Ed.), Eye move- ments and visual cognition: Scene perception and reading (pp. 2 0 3 - 226). New York: Springer.
Loftus, G. R., & Mackworth, N. H. (1978). Cognitive determinants of fixation location during picture viewing. Journal of Experimental Psychology: Human Perception and Performance, 4, 565-572.
Luria, S. M., & Strauss, M. S. (1978). Comparison of eye movements over faces in photographic positives and negatives. Perception, 7, 349-358.
Mackworth, N. H., & Morandi, A. J. (1967). The gaze selects informa- tive details within pictures. Perception and Psychophysics, 2, 547-
552.
Magliano, J. P., Graesser, A. C., Eymard, L. A., Haberlandt, K., & Ghol-
son, B. (1993). Locus of interpretive and inference processes during text comprehension: A comparison of gaze durations and word reading times. Journal of Experimental Psychology: Learning, Memory, and Cognition, 19, 704-709.
EYE MOVEMENTS
IN READING 4 1 3
 414 RA YNER
Mannen, S. K., Ruddock, K. H., & Wooding, D. S. (1995). Automatic control of saccadic eye movements made in visual inspection of briefly presented 2-D images. Spatial Vision, 9, 363-386.
Mannen, S. K., Ruddock, K. H., & Wooding. D. S. (1996). The relation- ship between the locations of spatial features and those of fixation made during visual examination of briefly presented images. Spatial Vision, 10, 165-188.
Mannen, S. K., Ruddock, K. H., & Wooding, D. S. (1997). Fixation patterns made during brief examination of two-dimensional images. Perception, 26, 1059-1072.
Marios, F.J., & Vila, J. (1990). Differences in eye movement control among dyslexic, retarded and normal readers in the Spanish popula- tion. Reading and Writing, 2, 175-188.
Masson, M. E. J. (1983). Conceptual processing of text during skim- ming and rapid sequential reading. Memory & Cognition, 11, 262- 274.
Matin, E. (1974). Saccadic suppression: A review. Psychological Bulle- tin, 81, 899-917.
Matin, E., Shao, K., & Boff, K. (1993). Saccadic overhead: Information processing time with and without saccades. Perception & Psychophys- ics. 53, 372-380.
May, J. G., Kennedy, R. S., W illiams, M. C., Dunlop, W . P., & Brannan, J. R. (1990). Eye movement indices of mental workload. Acta Psy- chologica, 75,75-89.
Mazuka, R., Itoh, K., & Kondo, T. (1997). Processing down the garden path in Japanese: Processing of sentences with lexical homonyms. Journal of Psycholinguistic Research, 26, 207-228.
McClelland, J. L., & O'Regan, J. K. (1981). Expectations increase the benefit derived from parafoveal visual information in reading words aloud. Journal of Experimental Psychology: Human Perception and Performance, 7, 634-644.
McConkie, G. W. (1979). On the role and control of eye movements in reading. In P.A. Kolers, M.E. Wrolstad, & H. Bouma (Eds.), Processing of visible language (pp. 3 7 -4 8 ). New York: Plenum.
McConkie, G. W . (1981). Evaluating and reporting data quality in eye movement research. Behavior Research Methods & Instrumentation, 13, 97-106.
McConkie, G. W . (1997). Eye movement contingent display control: Personal reflections and comments. Scientific Studies of Reading, 1, 303-316.
McConkie, G. W., & Currie, C. B. (1996). Visual stability across sac- cades while viewing complex pictures. Journal of Experimental Psy- chology: Human Perception and Performance, 22, 563-581.
McConkie, G. W, & Hogaboam, T.W. (1985). Eye position and word identificationinreading.InR.Groner,G.W.McConkie,&C.Menz (Eds.), Eye movements and human information processing (pp. 159-
172). Amsterdam: North Holland.
McConkie, G. W, Hogaboam, T.W., Wolverton, G. S., Zola, D. W., &
Lucas, P. A. (1979). Toward the use of eye movements in the study
of language processing. Discourse Processes, 2, 157-177. McConkie, G. W , Ken, P. W , & Dyre, B. P. (1994). What are "nor- mal'' eye movements during reading: Toward a mathematical descrip- tion. In J. Ygge & G. Lennerstrand (Eds.), Eye movements in reading
(pp. 315-328). Oxford, England: Pergamon Press.
McConkie, G. W, Kerr, P.W., Reddix, M.D., & Zola, D. (1988). Eye movement control during reading: I. The location of initial eye fixa-
tions in words. Vision Research, 28, 1107-1118.
McConkie, G. W., Kerr, P.W., Reddix, M. D., Zola, D., & Jacobs,A. M.
(1989). Eye movement control during reading: n. Frequency of re-
fixating a word. Perception & Psychophysics, 46, 245-253. McConkie, G. W., & Rayner, K. (1975). The span of the effective stimu- lus during a fixation in reading. Perception & Psychophysics, 17,
578-586.
McConkie, G. W , & Rayner, K. (1976a). Asymmetry of the perceptual span in reading. Bulletin of the Psychonomic Society, 8, 365-368. McConkie, G.W., & Rayner, K. (1976b). Identifying the span of the
effective stimulus of reading: Literature review and theories of read- ing. In H. Singer & R. B. Ruddell (Eds.), Theoretical models and processes in reading (pp. 137-162). Newark, DE: International Read- ing Association.
McConkie, G. W , Reddix, M. D., & Zola, D. (1992). Perception and cognition in reading: Where is the meeting point? In K. Rayner (Ed.), Eye movements and visual cognition: Scene perception and reading (pp. 293-303). New York: Springer-Verlag.
McConkie, G. W., Underwood, N. R., Wolverton, G. S., & Zola, D. (1988). Some properties of eye movement control during reading. In G. Luer, U. Lass, & J. S hallo-Hoffman (Eds.), Eye movement re- search: Physiological and psychological aspects (pp. 2 2 6 -2 4 5 ). To- ronto: C. J. Hogrefe.
McConkie, G. W., Wolverton, G. S., & Zola, D. (1984). Instrumentation considerations in research involving eye-movement contingent stimu- lus control. In A. G. Gale & F. Johnson (Eds.), Theoretical and ap- plied aspects of eye movement research (pp. 3 9 -4 7 ). Amsterdam: North Holland.
McConkie, G. W., Wolverton, G. S., Zola, D., & Burns, D. B. (1978). Eye movement contingent display control in studying reading. Behav- ior Research Methods & Instruments, 10, 154-166.
McConkie, G. W , & Zola, D. (1979). Is visual information integrated across successive fixations in reading? Perception & Psychophysics, 25, 221-224.
McConkie, G. W , & Zola, D. (1984). Eye movement control during reading: The effect of words units. In W . Prinz & A. F. Sanders (Eds.), Cognition and motorprocesses (pp. 63-74). Berlin: Springer-Verlag.
McConkie, G. W., & Zola, D., (1987). Visual attention during eye fixa- tions while reading. In M. Coltheart (Ed.), Attention and performance (Vol. 12, pp. 385-401). London: Erlbaum.
McConkie, G. W., Zola, D., & Blanchard, H. E. (1984). What is the basis for making an eye movement during reading? In A. G. Gale & F. Johnson (Eds.), Theoretical and applied aspects of eye movement research (pp. 169-178). Amsterdam: North Holland.
McConkie, G. W , Zola, D., Blanchard, H. E., & W olverton, G. S. (1982). Perceiving words during reading: Lack of facilitation from prior peripheral exposure. Perception & Psychophysics, 32, 271-282.
McConkie, G. W., Zola, D., Grimes, J., Kerr, P.W, Bryant, N. R., & Wolff, P .M . (1991). Children's eye movements during reading. In J. F. Stein (Ed.), Vision and visual dyslexia (pp. 251-262). London: Macmillan Press.
McConkie,G.W,Zola,D.,&Wolverton,G.S.(1980,April).How precise is eye guidance in reading? Paper presented at the meeting of the American Educational Research Association, Boston.
McConkie, G. W., Zola, D., & Wolverton, G. S. (1985). Estimating frequency and size of effects due to experimental manipulations in eye movement research. In R. Groner, G. W. McConkie, & C. Menz (Eds.), Eye movements andhuman information processing (pp. 137-
148). Amsterdam: North Holland.
McDowell, E. D., & Rockwell, T.H. (1978). An exploratory investiga-
tion of the stochastic nature of the drivers' eye movements and their relationship to the roadway geometry. In J. W . Senders, D. F. Fisher, & R. A. Monty (Eds.), Eye movements and the higher psychological
functions (pp. 329-345). Hillsdale, NJ: Erlbaum.
McGowan, J. W., Kowler, E., Sharma, A., & Chubb, C. (1998). Saccadic
localization of random dot targets. Vision Research, 38, 895-910. McRae, K., Butler, B. E., & Popiel, S. J. (1987). Spatiotopic and retino- topic components of iconic memory. Psychological Research, 49,
221-227.
Mertens, I., Siegmund, H., & Grosser, O. (1993). Gaze motor asymmet-
 ties in the perception of faces during a memory task. Neuropsycho-
logia, 31, 989-998.
Meyer, A. S., Sleiderink, A., & Levelt, W. J. M. (1998). Viewing and
naming objects: Eye movements during noun phrase production. Cog-
nition, 66, B 25-B 33.
Millet; L. K. (1978). Development of selective attention during visual
search. Developmental Psychology, 14, 439-440.
Miura, T. (1990). Active function of eye movement and useful field of
view in a realistic setting. In R. Groner, G. d'Ydewalle, & R. Parnham (Eds.), From eye to mind: Information acquisition in perception, search, and reading (pp. 119-127). Amsterdam: North Holland.
Moffitt, K. (1980). Evaluation of the fixation duration in visual search. Perception & Psychophysics, 27, 370-372.
Moray, N., & Rotenberg, I. (1989). Fault management in process control: Eye movements and action. Ergonomics, 32, 1319-1342.
Morris, R. K. (1994). Lexical and message-level sentence context ef- fects on fixation times in reading. Journal of Experimental Psychol- ogy: Learning, Memory, and Cognition, 20, 92-103.
Morris, R. K., & Folk, J. R. (1998). Focus as a contextual priming mechanism in reading. Memory & Cognition 26, 1313-1322.
Morris, R. K., & Rayner, K. (1991). Eye movements in skilled reading: Implications for developmental dyslexia. In J. F. Stein (Ed.), Vision and visual dyslexia (pp. 233-242). London: Macmillan.
Morris, R. K., Rayner, K., & Pollatsek, A. (1990). Eye movementguid- ance in reading: The role of parafoveal letter and space information. Journal of Experimental Psychology: Human Perception and Perfor- mance, 16, 268-281.
Morrison, R. E. (1983). Retinal image size and the perceptual span in reading. In K. Rayner (Ed.), Eye movements in reading: Perceptual and language processes (pp. 3 1 -4 0 ). New York: Academic Press.
Morrison, R. E. (1984). Manipulation of stimulus onset delay in read- ing: Evidence for parallel programming of saccades. Journal of Ex- perimental Psychology: Human Perception and Performance, 10, 667-682.
M orrison, R. E., & Inhoff, A. W . (1981). Visual factors and eye move- ments in reading. Visible Language, 15, 129-146.
Morrison, R. E., & Rayner, K. (1981). Saccade size in reading depends upon character spaces and not visual angle. Perception & Psychophys- ics, 30, 395-396.
Motter, B. C., & Belky, E. J. (1998a). The guidance of eye movements during active visual search. Vision Research, 38, 1805-1815.
Motter, B. C., & Belky, E. J. (1998b). The zone of focal attention during active visual search. Vision Research, 38, 1007-1022.
Mourant, R., & Rockwell, T.H. (1972). Strategies of visual search by novice and experienced drivers. Human Factors, 14, 325-335.
Muller, P.,Cavegn, D., d'Ydewalle, G., & Groner, R. (1993).A compari- son of a new limbus tracker, cornea! reflection technique, Purkinje eye tracking and electro-oculography. In G. d'Ydewalle & J. Van Rensbergen (Eds.), Perception and cognition: Advances in eye move- ment research (pp. 393-401). Amsterdam: North Holland.
Murray, W.S. (1998). Parafoveal pragmatics. In G. Underwood (Ed.), Eye guidance in reading and scene perception (pp. 181-200). Ox- ford, England: Elsevier.
in visual search. In A. G. Gale & F. Johnson (Eds.), Theoretical and applied aspects of eye movement research (pp. 343-351). Amster- dam: North Holland.
Nattkemper, D., & Prinz, W . (1986). Saccade amplitude determines fixation duration: Evidence from continuous search. In I. K. O'Rc- gan & A. Levy-Schoen (Eds.), Eye movements: From physiology to cognition (pp. 285-292). Amsterdam: North Holland.
Nazir, T. A. (1991). On the role of refixations in letter strings: The influence of oculomotor factors. Perception & Psychophysics, 49, 373-389.
Nazir, T.A. (1993). On the relation between the optimal and the pre- ferred viewing position in words during reading. In G. d'Ydewalle & J. Van Rensbergen (Eds.), Perception and cognition: Advances in eye movement research (pp. 171-180). Amsterdam: North Holland.
Nazir, T.A., Heller, D., & Sussmann, C. (1992). Letter visibility and word recognition: The optimal viewing position in printed words. Perception & Psychophysics, 52, 315-328.
Nazir, T. A., & Jacobs, A. M. (1991). The effects of target discriminabil- ity and retinal eccentricity on saccade latencies: An analysis in terms of variable-criterion theory. Psychological Research, 53, 281-289.
Nazir. T. A., O'Regan, J. K., & Jacobs, A. M. (1991). On words and their letters. Bulletin of the Psychonomic Society, 29, 171-174.
Neary, C., & W ilkins, A. J. (1989). Effects of phosphor persistence on perception and the control of eye movements. Perception, IS, 257- 264.
Nelson, W.W., & Loftus, G.R. (1980). The functional visual field during picture viewing. Journal of Experimental Psychology: Human Learning and Memory, 6, 391-399.
Ni, W., Crain, S., & Shankweiler, D. (1996). Sidestepping garden paths: Assessing the contributions of syntax, semantics, and plausibility in resolving ambiguities. Language and Cognitive Processes, 11, 283- 334.
Ni, W., Fodor, J. D., Crain, S., & Shankweiler, D. (1998). Anomaly detection: Eye movement patterns. Journal of Psycholinguistic Re- search, 27, 515-540.
Niemann, T., Lappe, M., & Hoffman, K. (1996). Visual inspection of three-dimensional objects by human observers. Perception, 25, 1027- 1042.
Nodine, C. R, Carmody, D. P., & Herman, E. (1979). Eye movements during visual search for artistically embedded targets. Bulletin of the Psychonomic Society, 13, 371-374.
Nodine, C. E, Carmody, D. P., & Kundel, H. L. (1978). Searching for Nina. In J. Senders, D. F. Fisher, & R. Monty (Eds.), Eye movements and the higherpsychologicalfunctions (pp. 241-258). Hillsdale, NJ: Erlbaum.
Nodine, C. F., Kundel, H. L., Toto, L. C., & Krupinski, E. A. (1992). Recording and analyzing eye-position data using a microcomputer workstation. Behavior Research Methods, Instruments & Computers, 24, 475-485.
Noton, D., & Stark, L. (1971a). Scan paths in eye movements during pattern perception. Science, 171, 308-311.
Noton, D., & Stark, L. (197lb). Scan paths in.saccadic eye movements while viewing and recognizing patterns. Vision Research, 11, 929- 942.
Noyes, L. (1980). The position of type on maps: The effect of sur- rounding material on word recognition times. Human Factors, 22,
Murray, W . S., & Kennedy, A. (1988). Spatial coding in the processing
of anaphor by good and poor readers: Evidence from eye movement
analyses. Quarterly Journal of Experimental Psychology, 40A, 693-
718. 353-360.
Murray, W . S., & Liversedge, S. P. (1994). Referential context effects on syntactic processing. In C. Clifton, L. Frazier, & K. Rayner(Eds.), Perspectives on sentence processing (pp. 359-388). Hillsdale, NJ: Erlbaum.
Murray, W., & Rowan, M. (1998). Early, mandatory, pragmatic pro- cessing. Journal of Psycholinguistic Research, 27, 1-22.
Nattkemper, D., & Prinz, W . (1984). Costs and benefits of redundancy
O'Brien, E. J., Raney, G. E., Albrecht, J. E., & Rayner, K. (1997). Pro- cesses involved in the resolution of explicit anaphors. Discourse Pro- cesses, 23, 1 -24.
O'Brien, E. J., Shank, D. M., Myers, J. L., & Rayner, K. (1988). Elabo- rative inferences during reading: Do they occur on-line? Journal of Experimental Psychology: Learning, Memory, and Cognition, 14, 410-420.
EYE MOVEMENTS IN READING 4 1 5
 416 RA YNER
Olson, R. K., Conneis, F. A., & Rack, J. P. (1991). Eye movements in dyslexia and normal readers. In }. F. Stein (Ed.), Vision and visual dyslexia (pp. 243-250). London: Macmillan.
Olson, R. K., Kliegl, R., & Davidson, B. J. (1983a). Dyslexic and nor- mal children's tracking eye movements. Journal of Experimental Psy- chology: Human Perception and Performance, 9, 816-825.
Olson, R. K., Kliegl, R., & Davidson, B. J. (1983b). Eye movements in reading disability. In K. Rayner (Ed.), Eye movements in reading: Perceptual and language processes (pp. 467-480). New "fork: Aca- demic Press.
Olson, R. K., Kliegl, R., Davidson. B. J., & Foltz, O. (1985). Individual differences and developmental differences in reading disability. In G. MacKinnon & T.G. Waller (Eds.), Reading research: Advances in theory and practice (pp. 1-64). New "fork: Academic Press.
O'Regan, J. K. (1979). Eye guidance in reading: Evidence for the lin- guistic control hypothesis. Perception & Psychophysics^ 25,501-509. O 'R egan,!. K. (1980). The control of saccade size and fixation duration in reading: The limits of linguistic control. Perception & Psychophys-
ics, 28, 112-117.
O'Regan, J. K. (1981). The convenient viewing position hypothesis. In
D. F. Fisher, R. A. Monty, & J. W . Senders (Eds.), Eye movements: Cognition and visual perception (pp. 289-298). Hillsdale, NJ: Erlbaum.
O'Regan, J. K. (1983). Elementary perceptual and eye movement con- trol processes in reading. In K. Rayner (Ed.), Eye movements in reading: Perceptual and language processes (pp. 121-140). New York: Academic Press.
O'Regan, J. K. (1990). Eye movements and reading. In E. Kowler (Ed.), Eye movements and their role in visual and cognitive processes (pp. 395-453). Amsterdam: Elsevier.
O'Regan, J. K. (1992). Optimal viewing position in words and the strategy-tactics theory of eye movements in reading. In K. Rayner (Ed.), Eye movements and visual cognition: Scene perception and reading (pp. 333-354). New Y>rk: Springer-Verlag.
O'Regan, J. K., & Jacobs, A. M. (1992). The optimal viewing position effect in word recognition: A challenge to current theory. Journal of Experimental Psychology: Human Perception and Performance, 18, 185-197.
O'Regan, J. K., & Levy-Schoen, A. (1983). Integrating visual informa- tion from successive fixations: Does trans-saccadic fusion exist? Vi- sion Research, 23, 765-768.
O'Regan, J. K., & Levy-Schoen, A. (1987). Eye movement strategy and tactics in word recognition and reading. In M. Coltheart (Ed.), Atten- tionandperformance: Vol.12.Thepsychologyofreading(pp. 363- 383). Hillsdale, NJ: Erlbaum.
O'Regan, J. K., Levy-Schoen, A., & Jacobs, A. M. (1983). The effect of visibility on eye movement parameters in reading. Perception & Psychophysics, 34, 457-464.
O'Regan, J. K., Levy-Schoen, A., Pynte, J., & Brugaillere, B. (1984). Convenient fixation location within isolated words of different length and structure. Journal of Experimental Psychology: Human Percep- tion and Performance, 10, 2 5 0 -2 5 7 .
Osaka, N. (1987). Effect of peripheral visual field size upon eye move- ments during Japanese text processing. In J. K. O'Regan & A. Levy- Schoen (Eds.), Eye movements: From physiology to cognition (pp. 421-429). Amsterdam: North Holland.
Osaka, N. (1989). Eye fixation and saccade during kana and kanji text reading: Comparison of English and Japanese text processing. Bulletin of the Psychonomic Society, 27, 548-550.
Osaka, N. (1992). Size of saccade and fixation duration of eye move- ments during reading: Psychophysics of Japanese text processing. Journal of the Optical Society of America A, 9, 5-13.
Osaka, N. (1993a). Asymmetry of the effective visual field in vertical reading as measured with a moving window. In G. d'Ydewalle & J.
Van Rensbergen (Eds.), Perception and cognition: Advances in eye
movement research (pp. 275-283). Amsterdam: North Holland. Osaka, N. (1993b). Reading vertically withouta fovea. In S. F.Wright& R. Groner (Eds.), Facets of dyslexia and its remediation (pp. 257-
265). Amsterdam: North Holland.
Osaka, N., & Oda, K. (1991). Effective visual field size necessary
for vertical reading during Japanese text processing. Bulletin of the
Psychonomic Society, 29, 345-347.
Ottes, F. P., VanGisbergen, J. A. M., & Eggermont, J. J. (1984). Metrics
of saccade responses to double-stimuli: Two different modes. Vision
Research, 24, 1169-1179.
Ottes, F. P.,VanGisbergen, J. A. M., & Eggermont, J. J. (1985). Latency
dependence of color-based target vs nontarget information by the sac-
cadic system. Vision Research, 25, 849-862.
Paap, K. R., & Newsome, S. L. (1981). Parafoveal information is not
sufficient to produce semantic or visual priming. Perception & Psy-
chophysics, 29, 457-466.
Pacht, J. M., & Rayner, K. (1993). The processing of homophonic homo-
graphs during reading: Evidence from eye movement studies. Journal
of Psycholinguistic Research, 22, 252-271.
Palmer, J., & Ames, C. T. (1992). Measuring the effect of multiple eye
fixations on memory for visual attributes. Perception & Psychophys-
ics, 52, 295-306.
Parker, R. E. (1978). Picture processing during recognition. Journal of
Experimental Psychology: Human Perception and Performance, 4,
284-293.
Pashler,H., Carrier, M .,& Hoffman, J. (1993). Saccadic eye movements
and dual-task interference. Quarterly Journal of Experimental Psy-
chology, 46A, 51-82.
Patching, G. R., & Jordan, T. R. (in press). Increasing the benefits of
eye-tracking devices in divided visual field studies of cerebral asym-
metry. Behavior Research Methods, Instruments, & Computers. Paterson, K. B., Liversedge, S.P ., & Underwood, G. (in press). The influence of focus operators on syntactic processing of short relative
clause sentences. Quarterly Journal of Experimental Psychology. Paterson, K. B., Sanford, A. J., Moxey, L. M., & Dawydiak, E. (1998). Quantifier polarity and referential focus during reading. Journal of
Memory and Language, 39, 290-306.
Patla, A. E., & Vickers, J. N. (1997). Where and when do we look as
we approach and step over an obstacle in the travel path? NeurvReport,
S, 3661-3665.
Pavlidis, G. T. (1978). The dyslexics erratic eye movements: Case stud-
ies. Dyslexia Review, I, 22-28.
Pavlidis, G. T. (1981). Do eye movements hold the key to dyslexia?
Neuropsychologia, 19, 5 7 -6 4 .
Pavlidis, G. T.(1983). The ''dyslexia syndrome'' and its objective diag-
nosis by erratic eye movements. In K. Rayner (Ed.), Eye movements in reading: Perceptual and language processes (pp. 441-466). New York: Academic Press.
Pavlidis, G. T. (1985). Eye movement differences between dyslexics, normal and slow readers while sequentially fixating digits. American Journal of Optometry and Physiological Optics, 62, 820-822.
Pavlidis, G. T. (1991). Diagnostic significance and relationship between dyslexia and erratic eye movements. In J. F. Stein (Ed.), Vision and visual dyslexia (pp. 263-270). London: Macmillan.
Peng, D. L., Orchard, L. N., & Stern, J. A. (1983). Evaluation of eye movement variables of Chinese and American readers. Pavlovian Journal of Biological Sciences, 18, 94-102.
Perea, M., & Pollatsek, A. (1998). The effects of neighborhood fre- quency in reading and lexical decision. Journal of Experimental Psy- chology: Human Perception and Performance, 24, 767-779.
Perry, A. R., Dember, W . N., W arm, J. S., & Sacks, J. G. (1989). Letter identification in normal and dyslexic readers: A verification.Bulletin of the Psychonomic Society, 27, 445-448.
 Phillips, M. L., & David, A. S. (1997). Visual scan paths are abnormal in deluded schizophrenia. Neuropsychologia, 35, 9 9 -1 0 5 .
Phillips, R. J. (1981). Searching for a target in a random arrangement of names: An eye fixation analysis. Canadian Journal of Psychology, 35, 330-346.
Pickering. M. J., & Traxler, M. J. (1998). Plausibility and recovery from garden paths: An eye-tracking study. Journal of Experimental Psy- chology: Learning, Memory, and Cognition, 24, 940-961.
Pillalamarri, R. S., Barnette, B. D., Birkmire, D., & Karsh, R. (1993). Cluster: A program for the identification of eye-fixation-cluster char- acteristics. Behavior Research Methods, Instruments, & Computers, 25, 9-15.
Pirozzolo, F. J., & Hansch, E. C. (1981). Oculomotor reaction time in dementia reflects degree of cerebral dysfunction. Science, 214, 349- 351.
Pirozzolo, F.J., & Rayner, K. (1978). Disorders of oculomotor scanning and graphic orientation in developmental Gerstmann syndrome. Brain and Language, 5, 119-126.
Pirozzolo, F. I., & Rayner, K. (1979). The neural control of eye move- ments in acquired and developmental reading disorders. In H. A. W hi- taker & H. A. Whitaker (Eds.), Studies in neurolinguistics (pp. 97-
123). New "fork: Academic Press.
Polanka, M. (1995). Factors affecting eye movements during the reading
of short melodies. Psychology of Music, 23, 177-183.
Pollatsek, A. (1983). What can eye movements tell us about dyslexia? In K. Rayner (Ed.), Eye movements in reading: Perceptual and lan-
guage processes (pp. 511-522). New York: Academic Press. Pollatsek, A. (1993). Eye movements in reading: In D. M. Willows, R. S. Kruk, & E. Corcos (Eds.), Visual processes in reading and
reading disabilities (pp. 191-214). Hillsdale, NJ: Erlbaum. Pollatsek, A., Bolozky, S., Well, A. D., & Rayner K. (1981). Asymmet- ries in the perceptual span for Israeli readers. Brain and Language,
14, 174-180.
Pollatsek, A., Lesch, M., Morris, R. K., & Rayner, K. (1992). Phonolog-
ical codes are used in integrating information across saccades in word identification and reading. Journal of Experimental Psychology: Hu- man Perception and Performance, IS, 148-162.
Pollatsek, A., Perea, M., & Binder, K. S. (in press). The effects of neighborhood size in reading and lexical decision. Journal of Experi- mental Psychology: Human Perception and Performance.
Pollatsek, A., Raney, G. E., LaGasse, L., &. Rayner, K. (1993). The use of information below fixation in reading and in visual search. Cana- dian Journal of Experimental Psychology, 47, 179-200.
Pollatsek, A., & Raynei; K. (1982). Eye movement control in reading: The role of word boundaries. Journal of Experimental Psychology: Human Perception and Performance, 8, 817-833.
Pollatsek, A., & Rayner, K. (1990). Eye movements and lexical access in reading. In D. A. Balota, G. B. Flores d'Arcais, & K. Rayner (Eds.), Comprehension processes in reading (pp. 143-164). Hillsdale, NJ: Erlbaum.
Pollatsek, A., & Rayner, K. (1992). What is integrated across fixations? In K. Rayner (Ed.), Eye movements and visual cognition: Scene perception and reading (pp. 166-191). New York: Springer-Verlag.
Pollatsek, A., Rayner, K., & Balota, D. A. (1986). Inferences about eye movement control from the perceptual span in reading. Perception & Psychopkysics, 40, 123-130.
Pollatsek, A., Rayner, K., & Collins, W . E. (1984). Integrating pictorial information across eye movements. Journal of Experimental Psychol- ogy: General, 113, 426-442.
Pollatsek, A., Rayner, K., & Henderson, J.M. (1990). Role of spatial location in integration of pictorial information across saccades. Jour- nal of Experimental Psychology: Human Perception and Perfor- mance, 16, 199-210.
Ponsoda, V., Scott, D., & Findlay, J. M. (1995). Probability vector and
transition matrix analysis of eye movements during visual search.
Acta Psychologica, 88, 167-185.
Posner, M. I. (1980). Orienting of attention. Quarterly Journal of Exper-
imental Psychology, 32, 3-25.
Previc, F. (1996). Attentional and oculomotor influences on visual field
anisotropies in visual search performance. Visual Cognition, 3, 277-
301.
Prinz, W . (1984). Attention and sensitivity in visual search. Psychologi-
cal Research, 45, 355-366.
Prinz, W , Nattkemper, D., & Ullmann, T. (1992). Moment-to-moment
control of saccadic eye movements: Evidence from continuous search. In K. Rayner (Ed.), Eye movements and visual cognition: Scene perception and reading (pp. 108-129). New \fork: Springer-Verlag.
Pynte, J. (1974). Readiness for pronunciation during the reading pro- cess. Perception & Psychophysics, 16, 110-112.
Pynte, J. (1996). Lexical control of within-word eye movements. Jour- nal of Experimental Psychology: Human Perception and Perfor- mance, 22, 958-969.
Pynte, J., Kennedy,A., & Murray,W. S. (1991). Within-word inspection strategies in continuous reading: Time course of perceptual, lexical, and contextual processes. Journal of Experimental Psychology: Hu- man Perception and Performance, 17, 458-470.
Radach, R., & Kempe, V . ,(1993). An individual analysis of initial fixation positions in reading. In G. d'Ydewalle & J. Van Rensbergen (Eds.), Perception and cognition: Advances in eye movement research (pp. 213-226). Amsterdam: North Holland.
Radach, R., Krummenachei; J., Heller, D., & Hofmesiter, J. (1995). Individual eye movement patterns in word recognition: Perceptual and linguistic factors. In J. M. Findlay, R. Walker, & R. W. Kentridge (Eds.), Eye movement research:Mechanisms,processes and applica- tions (pp. 421-432). Amsterdam: North Holland.
Radach, R., & McConkie, G. W.(1998). Determinants of fixation posi- tions in words during reading. In G. Underwood (Ed.), Eye guidance in reading and scene perception (pp. 7 7 -1 0 1 ). Oxford, England: Elsevier.
Rafal, R. D., Calabresi, P. A., Brennan, C. W., & Sciolto, T.K. (1989). Saccade preparation inhibits reorienting to recently attended locations. Journal of Experimental Psychology: Human Perception and Perfor- mance, 15, 673-685.
Rahimi, M., Briggs, R. P., & Thorn, D. R. (1990). A field evaluation of driver eye and head movement strategies toward environmental targets and distractors. Applied Ergonomics, 21, 267-274.
Raney, G. E., & Rayner, K. (1993). Event-related brain potentials, eye movements, and reading. Psychological Science, 4, 283-286.
Raney, G. E., & Rayner, K. (1995). W ord frequency effects and eye movements during two readings of a text. Canadian Journal of Experi- mental Psychology, 49, 151-172.
Raymond, J. E., Ogden, N. A., Pagan, J. E., & Kaplan, B. J. (1988). Fixational instability in dyslexic children. American Journal of Op- tometry and Physiological Optics, 65, 174-181.
Rayner, K. (1975a). Parafoveal identification during a fixation in read- ing. Acta Psychologica, 39, 272-282.
Rayner, K. (1975b). The perceptual span and peripheral cues in reading. Cognitive Psychology, 7, 6 5 -8 1 .
Rayner, K. (1977). Visual attention in reading: Eye movements reflect cognitive processes. Memory & Cognition, 4, 443-448.
Rayner, K. (1978a). Eye movement latencies for parafoveally presented words. Bulletin of the Psychonomic Society, 11, 13-16.
Rayner, K. (1978b). Eye movements in reading and information pro- cessing. Psychological Bulletin, 85, 618-660.
Rayner, K. (1978e). Foveal and parafoveal cues in reading. In J. Requin (Ed.), Attention and performance (Vol. 7, pp. 149-162). Hillsdale, NJ: Erlbaum.
EYE MOVEMENTS
IN READING 4 1 7
 418 RA YNER Rayner, K, (1979a). Eye guidance in reading: Fixation locations within
fixations in reading. Journal of Experimental Psychology: Human
words. Perception, 8, 2 1 -3 0 .
Rayner, K. (1979b). Eye movements and cognitive psychology: On-
line computer approaches to studying visual information processing.
Behavior Research Methods & Instruments, 11, 164-171.
Rayner, K. (1983). Eye movements, perceptual span, and reading dis-
ability. Annals of Dyslexia, 33, 163-173.
Rayner, K. (1984). Visual selection in reading, picture perception, and
visual search: A tutorial review. In H. Bouma & D. Bouwhuis (Eds.),
Attention and performance (V ol.10). Hillsdale, NJ: Erlbaum. Raynei; K. (1985a). Do faulty eye movements cause dyslexia? Develop-
mental Neuropsychology, 1, 3-15.
Rayner, K. (1985b). The role of eye movements in learning to read and
reading disability. Remedial and Special Education, 6, 53-60. Raynei; K. (1986). Eye movements and the perceptual span in beginning and skilled readers. Journal of Experimental Child Psychology, 41,
211-236.
Rayner, K. (1993). Eye movements in reading: Recent developments.
Current Directions in Psychological Science, 2, 81-85.
Rayner, K. (1995). Eye movements and cognitive processes in reading, visual search, and scene perception. In J. M. Findlay, R. Walker, & R. W Kentridge (Eds.), Eye movement research: Mechanisms,pro-
cesses and applications (pp. 3-22). Amsterdam: North Holland. Rayner, K. (1997). Understanding eye movements in reading. Scientific
Studies of Reading, 1, 301-323.
Rayner, K., flalota, D. A., & Pollatsek, A. (1986). Against parafoveal
semantic preprocessing during eye fixations in reading. Canadian
Journal of Psychology, 40, 473-483.
Rayner, K., & Bertera, J. H. (1979). Reading without a fovea. Science,
206, 468-469.
Rayner, K., Carlson, A., & Frazier, L. (1983). The interaction of syntax
and semantics during sentence processing: Eye movements in the analysis of semantically biased sentences. Journal of Verbal Learning and Verbal Behavior, 22, 358-374.
Rayner, K., & Duffy, S. A. (1986). Lexical complexity and fixation times in reading: Effects of word frequency, verb complexity, and lexical ambiguity. Memory & Cognition, 14, 191-201.
Rayner, K., & Duffy, S. A. (1988). On-line comprehension processes and eye movements in reading. In M. Daneman, G. E. MacKinnon, & T.G.Waller(Ms.),Readingresearch:Advancesintheoryandprac- tice (p p . 1 3 -6 6 ). New York: Academic Press.
Rayner, K., & Fischer, M. H. (1996). Mindless reading revisited: Eye movements during reading and scanning are different. Perception & Psychophysks, 58, 734-747.
Rayner, K., Fischer, M. H., & Pollatsek, A. (1998). Unspaced text inter- feres with both word identification and eye movement control. Vision Research, 38, 1129-1144.
Rayner, K., & Fisher, D. L. (1987a). Eye movements and the perceptual span during visual search. In J. K. O'Regan & A. Levy-Schoen (Eds.), Eye movements: From physiology to cognition (pp.293-302). Am- sterdam: North Holland.
Rayner, K,, & Fisher, D. L. (1987b). Letter processing during eye fixa- tions in visual search. Perception & Psychophysics, 42, 87-100. Rayner, K., & Frazier, L. (1987). Parsing temporarily ambiguous com -
plements. Quarterly Journal of Experimental Psychology, 39A, 657-
673.
Rayner, K., & Frazier, L. (1989). Selection mechanisms in reading
lexically ambiguous words. Journal of Experimental Psychology:
Learning, Memory, and Cognition, 15, 779-790.
Rayner, K., Garrod, S. C., & Perfetti, C. A. (1992). Discourse influences
during parsing are delayed. Cognition, 45, 109-139.
Rayner, K., Inhoff, A. W., Morrison, R., Slowiaczek, M. L., & Bertera, J. H. (1981). Masking of foveal and parafoveal vision during eye
Perception and Performance, 7, 167-179.
Rayner, K., & McConkie, G. W . (1976). What guides a reader's eye
movements. Vision Research, 16, 829-837.
Rayner, K., McConkie, G. W., & Ehrlich, S. F. (1978). Eye movements
and integrating information across fixations. Journal of Experimental
Psychology: Human Perception and Performance, 4, 529-544. Rayner, K., McConkie, G. W , & Zola, D. (1980). Integrating informa-
tion across eye movements. Cognitive Psychology, 12, 206-226. Rayner, K., & Morris, R. K. (1990). Do eye movements reflect higher order processes in reading?'In R. Groner, G. d'Ydewalle, & R. Parn- ham (Eds.), From eye to mind: Information acquisition in perception,
search, and reading (pp. 170-190). Amsterdam: North Holland. Rayner, K., & Morris, R. K. (1992). Eye movement control in reading: Evidence against semantic preprocessing. Journal of Experimental
Psychology: Human Perception and Performance, 18, 163-172. Rayner, K., & Morrison, R. M. (1981). Eye movements and identifying words in parafoveal vision. Bulletin of the Psychonomic Society, 17,
135-138.
Rayner, K., Murphy, L. A., Henderson, J. M., & Pollatsek, A. (1989).
Selective attenQ'onal dyslexia. Cognitive Neuropsychology, 6, 357-
378.
Rayner, K., Pacht, J. M., & Duffy, S. A. (1994). Effects of prior encoun-
ter and global discourse bias on the processing of lexically ambiguous words: Evidence from eye fixations. Journal of Memory andLan- guage, 33, 527-544.
Rayner, K., & Pollatsek, A. (1981). Eye movement control during read- ing: Evidence for direct control. Quarterly Journal of Experimental Psychology, 33A, 351-373.
Rayner, K., & Pollatsek, A. (1983). Is visual information integrated across saccades? Perception & Psychophysics, 34, 3 9 -4 8 .
Rayner, K., & Pollatsek, A. (1987). Eye movements in reading: A tutorial review. In M. Coltheart (Ed.), Attention and performance (Vol. 12, pp. 327-362). London: Erlbaum.
Rayner, K., & Pollatsek, A. (1989). The psychology of reading. Engle- wood Cliffs, NJ: Prentice Hall.
Rayner, K., & Pollatsek, A. (1992). Eye movements and scene percep- tion. Canadian Journal of Psychology, 46, 342-376.
Rayner, K., & Pollatsek, A. (1996). Reading unspaced text is not easy: Comments on the implications of Epelboim et al.'s study for models of eye movement control in reading. Vision Research, 36, 461-470.
Rayner, K., & Pollatsek, A. (1997). Eye movements, the eye-hand span, and the perceptual span during sight-reading of music. CurrentDirec- tions in Psychological Science, 6, 49-53.
Rayner, K.., Pollatsek, A., & Bilsky, A. B. (1995). Can a temporalpro- cessing deficit account for dyslexia? Psychonomic Bulletin & Review, 2, 501-507.
Rayner, K., Pollatsek, A., & Binder, K. S. (1998). Phonological codes and eye movements in reading. Journal of Experimental Psychology: Learning, Memory, and Cognition, 24, 476-497.
Rayner, K., & Raney, G. E. (1996). Eye movement control in reading and visual search: Effects of word frequency. Psychonomic Bulletin & Review, 3, 238-244.
Rayner, K., Raney, G. E., & Pollatsek, A. (1995). Eye movements and discourse processing. In R. F. Lorch & E. J. O'Brien (Eds.), Sources of coherence in reading (pp.9-36). Hillsdale, NJ: Erlbaum.
Rayner, K., Reichle, E. D., & Pollatsek, A. (1998). Eye movementcon- trol in reading: An overview and model. In G. Underwood (Ed.), Eye guidance in reading and scene perception (pp. 243-268). Oxford, England: Elsevier.
Rayner, K., & Serene, S. C. (1994a). Eye movements in reading: Psy- cholinguistic studies. In M. Gernsbacher (Ed.), Handbook of psycho- linguistics (p p . 5 7 -8 2 ). New "fork: Academic Press.
 Rayner, K., & Sereno, S. C. (1994b). Regression-contingent analyses: A reply to Altmann. Memory & Cognition, 22, 291-292.
Rayner, K., & Sereno, S.C. (1994c). Regressive eye movements and sentence parsing: On the use of regression-contingent analyses. Mem- ory & Cognition, 22, 281-285.
Rayner, K., Sereno, S. C., Lesch, M. E, & Pollatsek, A. (1995). Phono- logical codes are automatically activated during reading: Evidence from an eye movement priming paradigm. Psychological Science, 6, 26-32.
Rayner, K., Sereno, S. C., Morris, R. K., Schmauder, A. R., & Clifton, C. ( 1989). Eye movements and on-line language comprehension pro- cesses [Special issue]. Language and Cognition Processes, 4, 21- 49.
Rayner, K., Sereno, S. C., & Raney, G. E. (1996). Eye movement control in reading: A comparison of two types of models. Journal of Experi- mental Psychology: Human Perception and Performance, 22, 1188-
1200.
Rayner, K., Slowiaczek, M. L., Clifton, C., & Bertera, J. H. (1983).
Latency of sequential eye movements: Implications for reading. Jour- nal of Experimental Psychology: Human Perception and Perfor- mance, 9, 912-922.
Rayner, K., & Well, A. D. (1996). Effects of contextual constraint on eye movements in reading: A further examination. Psychonomic Bul- letin & Review, 3, 504-509.
Rayner, K., Well, A. D., & Pollatsek, A. (1980). Asymmetry of the effective visual field in reading. Perception & Psychophysics, 27, 537-544.
Rayner, K., Well, A. D., Pollatsek, A., & Bertera, J.H . (1982). The availability of useful information to the right of fixation in reading. Perception & Psychophysics, 31, 537-550.
Reder, S.M. (1973). On-line monitoring of eye position signals in contingent and noncontingent paradigms. Behavior Research Methods and Instrumentation, 5, 218-228.
Reichle, E. D., Pollatsek, A., Fisher, D. L., & Rayner, K. (1998). Tbward a model of eye movement control in reading. Psychological Review, 105, 125-157.
Reilly, R. (1993). A connectionist framework for modeling eye-move- ment control in reading. InG. d'Ydewalle& J. VanRensbergen(Eds.), Perception and cognition: Advances in eye movement research (pp. 191-212). Amsterdam: North Holland.
Reilly, R., & O'Regan, J. K. (1998). Eye movement control in reading: A simulation of some word-targeting strategies. Vision Research, 38, 303-317.
Remington, R. W .(1980). Attention and saccadic eye movements. Jour- nal of Experimental Psychology: Human Perception and Perfor- mance, 6, 726-744.
Reuter-Lorenz, P. A., & Fendrich, R. (1992). Oculomotor readiness and covert orienting: Differences between central and peripheral cues. Perception & Psychophysics, 52, 336-344.
Riggs, L. A., Merton, P. A., & Morton, H. B. (1974). Suppression of visual phosphenene during saccadic eye movements. Vision Research, 14, 997-1010.
Ritter, M. (1976). Evidence for visual persistence during saccadic eye movements. Psychological Research, 39, 67-85.
Rizzo, M., & Hurtig, R. (1992). Visual search in hemineglect: What stirs idle eyes? Clinical Visual Science, 7, 39-52.
Roberts, R. J., Hager, L. D., & Heron, C. (1994). Prefrontal cognitive processes: Working memory and inhibition in the antisaccade task. Journal of Experimental Psychology: General, 123, 374-393.
Ross, L. E., & Ross, S. M. (1980). Saccade latency and warning signals: Stimulus onset, offset and change as warning events. Perception & Psychophysics, 27, 2 5 1 -2 5 7 .
Effects of auditory and visual offset and onset. Perception & Psycho-
physics, 29, 429-437.
Rothkopf, E. Z. (1978). Analyzing eye movements to infer processing
styles during learning from text. In J. W . Senders, D. F. Fisher, & R. A. Monty (Eds.), Eye movements and the higher psychological
functions (pp. 209-224). Hillsdale, NJ: Erlbaum.
Rothkopf, E. Z., & Billington, M. J. (1979). Goal-guided learning from
text: Inferring a descriptive processing model from inspection times
and eye movements. Journal of Educational Psychology, 3, 310-327. Rubin, G. S., & Turano, K. (1992). Reading without saccadic eye move-
ments. Vision Research, 32, 895-902.
Ruggieri, V, & Fernandez, M. F.(1994). Gaze orientation in perception
of reversible figures. Perceptual and Motor Skills, 78, 299-303. Saida, S., & Fkeda, M. (1979). Useful field size for pattern perception.
Perception & Psychophysics, 25, 119-125.
Salthouse, T. A. (1984). Effects of age and skill in typing. Journal of
Experimental Psychology: General, 113, 345-371.
Salthouse, T.A., & Ellis, C. L. (1980). Determinants of eye-fixation
duration. American Journal of Psychology, 93, 207-234.
Salthouse, T. A., Ellis, C. L., Diener, D. C., & Somberg, B. L. (1981). Stimulus processing during eye fixations. Journal of Experimental
Psychology: Human Perception and Performance, 7, 611-623. Sanders, A. F. (1993). Processing information in the functional visual field. In G. d'Ydewalle & J. Van Rensbergen (Eds.), Perception and cognition: Advances in eye movement research (pp. 3 -2 2 ). Amster-
dam: North Holland.
Sanders, A. P., & Houtmans, M. J. M. (1985).There is no central stimu-
lus encoding during saccadic eye shifts: A case against general parallel
processing notions. Acta Psychologica, 60, 323-338.
Sanders, A. F., & Rath, A. M. (1991). Perceptual processing and speed-
accuracy trade-off. Acta Psychologica, 77, 2T5-291.
Sanders, A. E, & van Duren, L. L. (1998). Stimulus control of visual fixation duration in a single saccade paradigm. Acta Psychologica,
99, 163-176.
Schilling, H. E. H., Rayner, K., & Chumbley, J. I. (1998). Comparing
naming, lexical decision, and eye fixation times: Word frequency ef- fects and individual differences. Memory & Cognition, 26, 1270-
1281.
Schmauder, A. R. (1991). Argument structure frames: A lexical com-
plexity measure. Journal of Experimental Psychology: Learning,
Memory, and Cognition, 17, 4 9 -6 5 .
Schmauder, A. R. (1992). Eye movements and reading processes. In K,
Rayner (Ed.), Eye movements and visual cognition: Scene perception
and reading (pp. 3 6 9 -3 7 8 ). New York: Springer-V erlag.
Schmauder, A. R., & Egan, M. C. (1998). The influence of semantic fit on on-line sentence processing. Memory & Cognition, 26, 1304-1312. Schroyens, W, Vitu, E, Brysbaert, M., & d'Ydewalle, G. (in press). Visual attention and eye movement control during reading: The case of parafoveal processing. Quarterly Journal of Experimental
Psychology.
Schustack, M. W , Ehrlich, S. E, & Rayner, K. (1987). The complexity of contextual facilitation in reading: Local and global influences. Journal of Memory and Language, 26, 322—340.
Scialfa, C. T, & Joffe, K. M. (1998). Response times and eye move- ments in feature search and conjunction search as a function of target eccentricity. Perception & Psychophysics, 60, 1067-1082.
Scialfa, C. T., Thomas, D. M., & Joffe, K. M. (1994). Age differences in the useful field of view: An eye movement analysis. Optometry and Vision Science, 71, 1-7.
Scinto, L. F. (1978). Relation of eye fixations to old-new information in texts. In J. W. Senders, D. F. Fisher, & R. A. Monty (Eds.), Eye movements and the higher psychological functions (pp. 175-194). Hillsdale, NJ: Erlbaum.
Ross, S. M., & Ross, L. E. (1981). Saccade latency and warning signals:
Scinto, L. E, & Barnette, B. D. (1986). An algorithm for determining
EYE MOVEMENTS
IN READING 4 1 9
 420 RA YNER
clusters, pairs or singletons in eye-movement scan-path records. Be-
havior Research Methods, Instruments, & Computers, 18, 4 1 -4 4 . Scinto, L. E, Pillalamarri, R., & Karsh, R. (1986). Cognitive strategies
for visual search. Ada Psychologies, 62, 263-294.
Sclingensiepen, K. H., Campbell, F.W., Legge, G. E., & Walker, T.D. (1986). The importance of eye movements in the analysis of simple
patterns. Vision Research, 26, 1111-1117.
Scotto, M. A., Oliva, G. A., & Tuccio, M. T. (1990). Eye movements
and reversal rates of ambiguous figures. Perceptual and Motor Skills,
70, 1059-1073.
Sereno, A. B. (1992). Programming saccades: The role of attention. In
K. Rayner (Ed.), Eye movements and visual cognition: Scene percep-
tion and reading (pp. 89-107). New "Stork: Springer-Verlag.
Sereno, A. B., & Holzman, P. S. (1993). Express saccades and smooth pursuit eye movement functions in schizophrenia, affective disorders, and normal subjects. Journal of Cognitive Neuroscience, 5, 303-316. Sereno, S.C. (1992). Early lexical effects when fixating a word in reading. In K. Rayner (Ed.), Eye movements and visual cognition: Scene perception and reading (pp. 3 0 4 -3 1 6 ). New York: Springer-
V erlag.
Sereno, S. C. (1995). Resolution of lexical ambiguity: Evidence from
an eye movement priming paradigm. Journal of Experimental Psy-
chology: Learning, Memory, and Cognition, 21, 582-595.
Sereno, S. C., Pacht, J. M., & Rayner, K. (1992). The effect of meaning frequency on processing lexically ambiguous words: Evidence from
eye fixations. Psychological Science, 3, 296-300.
Sereno, S. C., & Rayner, K. (1992). Fast priming during eye fixations
in reading. Journal of Experimental Psychology: Human Perception
and Performance, 18, 173-184.
Sereno, S. C., Rayner, K., & Posner, M. I. (1998). Establishing a time-
line of processing during reading: Evidence from eye movementsand
event-related potentials. NeuroReport, 9, 2195-2200.
Shaffer, L. H. (1973). Latency mechanisms in transcription. In S. Kom- blum (Ed.), Attention and performance (Vol. 4, pp. 435-446). New
% rk: Academic Press.
Shank, M. D., & Haywood, K. M. (1987). Eye movements while view-
ing a baseball pitch. Perceptual and Motor Skills, 64, 1191 -1197. Shapiro, K. L., & Raymond, J. E. (1989). Training efficient oculomotor strategies enhances skill acquisition. Ada Psychologica, 71, 217-
242.
Shebilske, W . L., & Fisher, D, F. (1983). Eye movements and context
effects during reading of extended discourse. In K. Rayner (Ed.), Eye movements in reading: Perceptual and language processes (pp. 153-
179). New York: Academic Press.
Shepherd, M., Findlay, J. M., & Hockey, R. J. (1986). The relationship
between eye movements and spatial attention. Quarterly Journal of
Experimental Psychology, 38A, 475-491.
Shinar, D., McDowell, E. D., & Rockwell, T.H. (1977). Eye movements
in curve negotiation. Human Factors, 19, 63-71.
Shiori, S. (1993). Postsaccadic processing of the retinal image during
picture scanning. Perception & Psychophysics, 53, 305-314. Shioiri, S., & Ikeda, M. (1989). Useful resolution for picture perception
as a function of eccentricity. Perception, 18, 347-361.
Slaghuis, W .L., Lovegrove, W .J., & Freestun, J. (1992). Letter recogni- tion in peripheral vision and metacontrast masking in dyslexic and
normal readers. Clinical Vision Sciences, 7, 5 3 -6 5 .
Slowiaczek, M.L. (1983). What does the mind do while the eyes are gazing? In K. Rayner (Ed.), Eye movements in reading: Perceptual and language processes (pp. 345-358). New York: Academic Press. Slowiaczek, M. L., & Rayner, K. (1987). Sequential masking during eye fixations in reading. Bulletin of the Psychonomic Society, 25,
175-178.
Solan, H. A. (1985 ). Deficient eye movement patterns in achieving high
school students: Three case histories. Journal of Learning Disabili-
ties, 18, 66-70.
Solan, H. A., Feldman, J., & Tujak, L. (1995). Developing visual and
reading efficiency in older adults. Optometry and Vision Science, 72,
139-145.
Speer, S., & Clifton, C. (1998). Plausibility and argument structure in
sentence comprehension. Memory & Cognition, 26, 965-978. Spragins, A. B., Lefton, L. A., & Fisher, D. F. (1976). Eye movements while reading spatially transformed text: A developmental study. Mem-
ory & Cognition, 4, 36-42.
Staller, J., & Sekuler, R. (1980). Eye movements and choice times in
mirror image pattern discrimination. American Journal of Psychology,
93, 665-681.
Stampe, D. M., & Reingold, E. M. (1995). Selection by looking: A
novel com puter interface and its application to psychological research. In J. M. Findlay, R. W alker, & R. W . Kentridge (Eds.), Eye movement research: Mechanisms, processes and applications (pp. 467-478). Amsterdam: North Holland.
Stanley, G. (1994). Eye movements in dyslexic and normal children. In J. Ygge & G. Lennerstrand (Eds.), Eye movements In reading (pp. 261-271). Oxford, England: Pergamon Press.
Stanley, G., Smith, G. A., & Howell, E. A. (1983a). Eye movements and sequential tracking in dyslexic and control children. British Journal of Psychology; 74, 181-187.
Stanley, G., Smith, G. A., & Howell, E. A. (1983b). Eye movements in 1
dyslexic children: Comments on Pavlidis reply. British Journal of
Psychology, 74, 195-197.
Stark, L., & Ellis, S. R. (1981). Scanpaths revisited: Cognitive models
direct active looking. In D. F. Fisher, R. A. Monty, & J. W . Senders (Eds.), Eye movements: Cognition and visual perception (pp. 193- 226). Hillsdale, NJ: Erlbaum. •
Stein, J. F. (1989). Unstable vergence control and specific reading im- pairment. British Journal of Ophthalmology, 73, 49.
Stein, J. F. (1994). Binocular control in dyslexics. In J. Ygge & G. Lennerstrand (Eds.), Eye movements in reading (pp. 273-290). Ox- ford, England: Pergamon Press.
Stein, J.F., Riddell, P., & Fowler, M. S. (1988). Disordered vergence eye movement control in dyslexic children. British Journal of Oph- thalmology, 72, 162-166.
Stelmach, L. B., Campsall, J. M., & Herdman, C. M. (1997). Attention and ocular movements. Journal of Experimental Psychology: Human Perception and Performance, 23, 823—844.
Sun, F., Morita, M., & Stark, L. W. (1985). Comparative patterns of reading eye movement in Chinese and English. Perception & Psycho- physics, 37, 502-506.
Suppes, P. (1990). Eye-movement models for arithmetic and reading performance. In E. Kowler (Ed.), Eye movements and their role in visual and cognitive processes (pp. 455-477). Amsterdam: Elsevier.
Suppes, P. (1994). Stochastic models of reading. In J. Ygge & G. Lennerstrand (Eds.), Eye movements in reading (pp. 349-364). Ox- ford, England: Pergamon Press.
Suppes, P., Cohen, M., Laddaga, R., Anliker, J., & Floyd, R. (1983). A procedural theory of eye movements in doing arithmetic. Journal of Mathematical Psychology, 27, 341 -369.
Tanenhaus, M. K., & Spivey-Knowlton, M. J. (1996). Eye-tracking. Language and Cognitive Processes, 11, 583—588.
Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard, K. M., & Sedivy, J. C. (1995). Integration of visual and linguistic information during spoken language comprehension. Science, 268, 1632-1634.
Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard, K. M., & Sedivy, J. C. (1996). Using eye movements to study spoken language compre- hension. In T.Inui & J. McClelland (Eds.),Attention and performance ("Vol. 16, pp. 457-478). Cambridge, MA: MIT Press.
 Taylor, E. A. (1966). The fundamental reading skill. Springfield, IL: Charles C Thomas.
Taylor, S. E. (1965). Eye movements while reading: Facts and fallacies. American Educational Research Journal, 2, 187-202.
Theeuswes, J., Kramer,A. E, Hahn, S., & Irwin, D. E. (1998). Our eyes do not always go where we want them to go: Capture of the eyes by new objects. Psychological Science, 9, 379-385.
Thibadeau, R., Just, M. A., & Carpenter, P. A. (1982). A model of the time course and content of human reading. Cognitive Science, 6, 101-155. Tinker, M. A. (1939). Reliability and validity of eye-movement mea-
sures of reading. Journal of Experimental Psychology, 19, 732-746. Tinker, M. A. (1946). The study of eye movements in reading. Psycho-
logical Bulletin, 43, 93-120.
Tinker, M. A. (1958). Recent studies of eye movements in reading.
Psychological Bulletin, 55, 215-231.
Togami, H. (1984). Affects of visual search performance on individual differ-
ences in fixation time and number of fixations. Ergonomics, 27, 789-799. Traxler, M. J., Bybee, M., & Pickering, M. J. (1997). Influence of connectives on language comprehension: Eye-tracking evidence for incremental inter- pretation. Quarterly Journal of Experimental Psychology, 50A,481-497. Traxler, M. J., & Pickering, M. J. (1996a). Case marking in the parsing of complement sentences: Evidence from eye movements. Quarterly
Journal of Experimental Psychology, 49A, 991-1004.
Traxler, M. J., & Pickering, M. J. (1996b). Plausibility and the pro- cessing of unbounded dependencies: An eye-tracking study. Journal
of Memory and Language, 35, 454-475.
Trueswell, J. C., Tanenhaus, M. K., & Garnsey, S. M. (1994). Semantic
influences on parsing: Use of thematic role information in syntactic
ambiguity resolution. Journal of Memory and Language, 33, 285-318. Trueswell, J. C., Tanenhaus, M. K., & Kello, C. (1993). Verb-specific constraints in sentence processing: Separating effects of lexical pref- erence from garden-paths. Journal of Experimental Psychology:
Learning, Memory, and Cognition, 19, 528-553.
Truitt, F.E., Clifton, C., Pollatsek, A., & Rayner,K. (1997). The percep-
tual span and the eye-hand span in sight reading music. Visual Cogni-
tion, 4, 143-162.
Tversky, B. (1974). Eye fixations in prediction of recognition and recall.
Memory & Cognition, 2, 275-278.
Underwood, G. (1985). Eye movements during the comprehension of
written language. In A. W.Ellis (Vol.Ed.), Progressin the psychology
of language (V ol. 2, pp. 4 5 -7 1 ). London: Erlbaum.
Underwood, G., Bloomfield, R., & Clews, S. (1988). Information influ- ences the pattern of eye fixations during sentence comprehension.
Perception, 17, 267-278.
Underwood, G., Clews, S., & Everatt, J. (1990). How do readers know
where to look next? Local information distributions influence eye fixa-
tions. Quarterly Journal of Experimental Psychology, 42A, 39—65. Underwood, G., Clews, S., & Wilkinson, H. (1989). Eye fixations are influenced by the distribution of information within words. Acta Psy-
cholagica, 72, 263-280.
Underwood, G., Hubbard, A., & Wilkinson, H. (1990). Eye fixations
predict reading comprehension: The relationship between reading skill,
reading speed and visual inspection. Language and Speech, 33, 6 9 -8 1 . Underwood, G., Hyona, J., & Niemi, P. (1987). Scanning patterns on individual words during the comprehension of sentences. In J. K. O'Regan & A. Levy-Schoen (Eds.), Eye movements:From physiology
to cognition (pp. 467-477). Amsterdam: North Holland. Underwood, N. R., & McConkie, G. W. (1985). Perceptual span for letter distinctions during reading. Reading Research Quarterly, 20, 153-162. Underwood, N. R., & Zola, D. (1986). The span of letter recognition
of good and poor readers. Reading Research Quarterly, 21, 6 -1 9 . Uttal, W . R., & Smith, P. (1968). Recognition of alphabetic characters during voluntary eye movements. Perception & Psychophysics, 3,
Van der Heijden, A. H. C., Bridgeman, B., & Mewhort, D. J. K. (1986). Is stimulus persistence affected by eye movements? A critique of David- son, Fox, and Dick (1973). Psychological Research, 48, 179-181.
van Diepen, P. M. J. (1997). A pixel-resolution video switcher for eye contingent display changes. Spatial Vision, 10, 335-344.
van Diepen, P. M. J. (1998a). Foveal stimulus degradation in scene perception. Manuscript submitted for publication.
van Diepen, P. M. J. (1998b). Peripheral versusfoveal information pro- cessing in scene perception. Manuscript submitted for publication. van Diepen, P. M. J., DeGraef, P., & d'Ydewalle, G. (1995). Chronome-
try of foveal information extraction during scene perception. In J. M. Findlay, R. Walker, & R. W. Kentridge (Eds.), Eye movement re- search:Mechanisms,processesandapplications (pp.349-362).Am- sterdam: North Holland.
van Diepen, P. M. J., DeGraef, P., & Van Rensbergen, J. (1994). On- line control of moving masks and windows on a complex background using the ATVista Videographics Adapter. Behavior Research Meth- ods, Instruments, and Computers, 24, 454-460.
van Diepen, P. M. J., Wampers, M., & d'Ydewalle, G. (1998). Func- tional division of the visual field: Moving masks and moving windows. In G. Underwood (Ed.), Eye guidance in reading and scene perception (pp. 337-356). Oxford, England: Elsevier.
Van Duren, L. (1993). Central stimulus processing during saccadic eye movements. In G. d'Ydewalle & J. VanRensbergen (Eds.), Perception and cognition: Advances in eye movement research (pp. 23-36). Amsterdam: North Holland.
Van Duren, L., & Sanders, A. F. (1992). The output code of a visual fixation. Bulletin of the Psychonomic Society, 30, 305-308.
Van Duren, L., & Sanders, A. F. (1995). Signal processing during and across saccades. Acta Psychologica, 89, 121-147.
Van Oeffelen, M. P., & V os, P. G. (1984a). Enumeration of dots: An eye movement analysis. Memory & Cognition, 12, 607-612.
Van Oeffelen, M. P.,& Vos,P.G. (1984b).The young child's processing of dot patterns: A chronometric and eye movement analysis. Interna- tional Journal of Behavioral Development, 7, 53-66.
Vatikiotis-Bateson, E., Eigsti, L, Yano, S., & Munhall, K. G. (1998). Eye m ovem ent of perceivers during audiovisual speech perception. Perception & Psychophysics, 60, 926-940.
Vaughan, J. (1982). Control of fixation duration in visual search and memory search: Another look. Journal of Experimental Psychology: Human Perception and Performance, S, 7 0 9 -7 2 3 .
Vaughan, J. (1983). Saccadic reaction time in visual search. In K. Rayner (Ed.), Eye movements in reading: Perceptual and language processes (pp. 397-411). New Tfork: Academic Press.
Vaughan, I, & Graefe, T.M. (1977). Delay of stimulus presentation after the saccade in visual search. Perception & Psychophysics, 22, 201-205. Vauras, M., Hyona, J., & Niemi, P. (1992). Comprehending coherent and incoherent texts: Evidence from eye movement patterns and recall
performance. Journal of Research in Reading, 15, 39-54.
Verfaillie, K. (1997). Transsaccadic memory for the egocentric and allocentric position of a biological-motion walker. Journal of Experi- mental Psychology: Learning, Memory, and Cognition, 23, 739-760. Verfaillie, K., De Troy, A., & Van Rensbergen, J. (1994). Transsaccadic integration of biological motion. Journal of Experimental Psychol-
ogy: Learning, Memory, and Cognition, 20, 649-670.
Verschaffel, L., De Cone, E., & Pauwels, A. (1992). Solving compare problem s: A n eye m ovem ent test of L ew is and M ayer's consistency
hypothesis. Journal of Educational Psychology, 84, 8 5 -9 4 .
Vickers, J. N. (1988). Knowledge structures of expert-novice gym-
nasts. Human Movement Science, 7, 4 7 -7 2 .
Vickers, J. N. (1992). Gaze control in putting. Perception, 21, 117-
132.
Vickers, J. N. (1995). Gaze control in basketball foul shooting. In J. M.
257-264.
Findlay, R. Walker, & R. W. Kentridge (Eds.), Eye movement re-
EYE MOVEMENTS IN READING 4 2 1
 422 RA YNER
search: Mechanisms, processes and applications (pp. 527-541). Am-
sterdam: North Holland.
Vickers, J. N. (1996). Visual control when aiming at a far target. Jour-
nal of Experimental Psychology: Human Perception and Perfor-
mance, 22, 342-354.
Vitu, F. (1991a). Against the existence of arange effect during reading.
Vision Research, 31, 2009-2015.
Vitu, F. (1991b). The existence of a center of gravity effect in reading.
Vision Research, 31, 1289-1313.
Vitu, F. (1991c). The influence of parafoveal processing and linguistic
context on the optimal landing position effect. Perception & Psycho-
physics, 50, 58-75.
Vitu, F. (1993). The influence of the reading rhythm on the optimal
landing position effect. In G. d'Ydewalle & J. VanRensbergen (Eds.), Perception and cognition: Advances in eye movement research (p p . 181-192). Amsterdam: North Holland.
Vitu, F., McConkie, G. W., & Zola, D. (1998). About regressive sac- cades in reading and their relation to word identification. In G. Un- derwood (Ed.), Eye guidance in reading and scene perception (pp. 101-124). Oxford, England: Elsevier.
Vitu, F., & O'Regan, J. K. (1988). Effects of parafoveal processing and reading rhythm on optimal landing position in words of different length and frequency. In G. Luer, U. Lass, & J. Shallo-Hoffmann (Eds.), Eye movement research: Physiological and psychological as- pects (pp. 286-292). Toronto: Hofgrefe.
Vitu, F., & O'Regan, J. K. (1991). Is there an optimal landing position in words during reading of texts? In R. Schmid & D. Zambarbieri (Eds.), Oculomotor control and cognitive processes: Normal and pathological aspects (pp. 341-352). Amsterdam: North Holland.
Vitu, E, & O'Regan, J. K. (1995). A challenge to current theories of eye movements in reading. In J. M. Findlay, R. Walker,& R. W. Kentridge (Eds.), Eye movement research: Mechanisms,processes, and applica- tions (pp. 381-393). Amsterdam: North Holland.
Vitu, F., O'Regan, J. K., Inhoff, A. W., & Topolski, R. (1995). Mindless reading: Eye movement characteristics are similar in scanning letter strings and reading text. Perception & Psychophysics, 57, 352-364.
Vitu, F., O'Regan, J. K, & Mittau, M. (1990). Optimal landing position in reading isolated words and continuous text. Perception & Psycho- physics, 47, 583-600.
Viviani, P. (1990). Eye movements in visual search: Cognitive, percep- tual, and motor control aspects. In E. Kowler (Ed.), Eye movements and their role in visual and cognitive processes (pp. 353-393). Am- sterdam: Elsevier.
Viviani, P., & Swensson, R. G. (1982). Saccadic eye movements to peripherally discriminated visual targets. Journal of Experimental Psychology: Human Perception and Performance, 8, 113-126.
Vonk, W. (1984). Eye movements during comprehension of pronouns. In A. G. Gale &. F. Johnson (Eds.), Theoretical and applied aspects of eye movement research (pp. 203-212). Amsterdam: North Holland.
Vonk, V. (1985). On the purpose of reading and the immediacy of processing pronouns. In R. Groner, G. W . McConkie, & C. Menz (E<ls.), Eye movements and human information processing (pp. 207- 218). Amsterdam: North Holland.
Walker, R., & Findlay, J. M. (1996). Saccadic eye movement program- ming in unilateral neglect. Neuropsychologia, 34, 493-508.
Walker, R., Findlay, J. M., Young, A. W., & Lincoln, N. B. (1996). Sac- cadic eye movements in object-based neglect. Cognitive Neuropsy- chology, 13, 569-615.
Walker, R., & Young, A. W . (1996). Object-based neglect: An investiga- tion of the contributions of eye movements and perceptual completion. Cortex, 32, 279-295.
Walker-Smith, G. J., Gale, A. G., & Findlay, J. M. (1977). Eye move- ment strategies in face perception. Perception, 6, 313-326.
Wampers, M., van Diepen, P. M. J., & d'Ydewalle, G. (1998). The use of coarse and fine peripheral information during scene perception. Manuscript submitted for publication.
Waters, A. J., Underwood, G., & Findlay, J. M. (1997). Studying exper- tise in music reading: Use of a pattern-matching paradigm. Percep- tion & Psychophysics, 59, 477-488.
Weaver, H. E. (1943). A survey of visual processes in reading differently constructed musical selections. Psychological Monographs, 55, 1-30. Well, A. D. (1983). Perceptual factors in reading. In K. Rayner (Ed.),
Eye movements in reading: Perceptual and language processes (pp. 141-150). New York: Academic Press.
Westheimer, G. (1993). Phosphor persistence in oscilloscopic displays. Vision Research, 33, 2337-2338.
White, C. W. (1976). Visual masking during pursuit eye movements.
Journal of Experimental Psychology: Human Perception and Perfor- mance, 2, 469-478.
Whittaker, S. G., Cummings, R. W., & Swieson, L. R. (1991). Saccade control without a fovea. Vision Research, 31, 2209-2218.
Wilkins, A. J. (1986). Intermittent illumination from fluorescent lighting and visual display units affects movements of the eyes across text. Human Factors, 28, 75-81.
Williams, D. E., Reingold, E., Mbscovitch, M., & Behrmann, M. (1997). Patterns of eye movements during parallel and serial visual search tasks. Canadian Journal of Experimental Psychology, 51, 151-164.
Wolf, W., Hauske, G., & Lupp, U. (1978). How presaccadic gratings modify post-saccadic modulation transfer function. Vision Research, 18, 1173-1179.
Wolf, W , Hauske, G., & Lupp, U. (1980). Interaction of pre- and post-saccadic patterns having the same coordinates in space. Vision Research, 20, 117-125.
W olverton, G. S., & Zola, D. (1983). The temporal characteristics of visual information extraction during reading. In K. Rayner (Ed.), Eye movements in reading: Perceptual and language processes (p p . 4 1 - 52). New "fork: Academic Press.
Yarbus, A. (1967). Eye movements and vision. New York: Plenum Press. Ygge, J., & Jacobson, C. (1994). Asymmetrical saccades in reading. In J. Ygge & G. Lennerstrand (Eds.), Eye movements in reading (pp.
301-313). Oxford, England: Pergamon Press.
Zagar, D., Pynte, J., & Rativeau, S. (1997). Evidence for early-closure
attachment on first-pass reading times in French. Quarterly Journal
of Experimental Psychology, 50A, 421-438.
Zangemeister, W.H., Sherman, K., & Stark, L. (1995). Evidence for a
global scanpath strategy in viewing abstract compared with realistic
images. Neuropsychologia, 33, 1009-1025.
Zangwill, O.L., & Blakemore, C. (1972). Dyslexia: Reversal of eye
movements during reading. Neuropsychologia, 10, 371-373. Zelinsky, G.J. (1996). Using eye saccades to assess the selectivity of
search movements. Vision Research, 36, 2177-2187.
Zelinsky, G. J., Rao, R. P. N., Hayhoe, M. M., & Ballard, D. H. (1997). Eye movements reveal the spatiotemporal dynamics of visual search.
Psychological Science, 8, 448-453.
Zelinsky, G. J., & Sheinberg, D. L. (1997). Eye movements during par-
allel-serial visual search. Journal of Experimental Psychology: Hu-
man Perception and Performance, 23, 244-262.
Zihl, J. (1995). Eye movement patterns in hemianopic dyslexia. Brain,
118, 891-912.
Zingale, C. M., & Kowler, E. (1987). Planning sequences of saccades.
Vision Research, 27, 1327-1341.
Zola, D. (1984). Redundancy and word perception during reading. Per-
ception & Psychophysics, 36, 277-284.
Received August 18, 1997 R evision received M arch 6, 1998
A ccepted M arch 2 0 , 1998 •
Eye Tracking to Support eLearning
Leana Copeland
August 2016
A thesis submitted for the degree of Doctor of Philosophy
of The Australian National University
Research School of Computer Science College of Engineering and Computer Science The Australian National University
 © Copyright by Leana Diane Copeland 2016 All Rights Reserved
To Mum, Dad and Michael, My favourite people in the world!
Except where otherwise indicated, this thesis is my own original work.
Leana Copeland August 2016

Acknowledgements
First I would like to thank my supervisor Professor Tom Gedeon. I would not have got through this without you. The support and encouragement you gave me during my honours year and then throughout my candidature will never been forgotten. You fostered within me a curiosity and drive to succeed that I never knew I had. You are a one million supervisor and I am very lucky to have had you as mine. Not every supervisor would put up with their students following them down hallways or pounce on them when they get back their office to ask non-stop questions!
Mum, Dad, and Michael, thank you does not cover my gratitude to you for not only supporting me financially, emotionally, and mentally throughout my PhD, but also for maintaining an open fridge-door policy! Thank you for not only reading all of my conference and journal papers but also for reading every single chapter of this thesis (twice)! This thesis would not be in the state that it is without your help picking up the many spelling and grammar mistakes. Thank you for everything you have done for me. It is because of you I have made it this far, and once again you have helped get me over the line. I love you all and will forever appreciate your never-ending love and support in everything I do.
Many thanks go to my panel, Dr Sumudu Mendis and Dr Kristen Palmer, for your support and help. Thank you also to the staff from CECS for your support and help, notably Dr Richard Jones, Dr Duncan Stevenson, and Lynette Johns-Boast.
Thank you to Dr Sabrina Caldwell, your chats and insightful wisdom will be missed and were much needed at many times during my candidature. Thank you also for being a fantastic co-experimenter, my thesis really would not be what it is without you.
A very big thank you goes to everyone who proofread for me and provided invaluable feedback, especially in correcting my grammar. In particular, Tom Gedeon, Wendy and Chris Copeland, Richard Jones, Jaimi Pigram, and Chris Chow.
Thank you to my wonderful friends Nandita Sharma, Maria Memmolo, Brigette Boast, Lara Connolly, Jessica Tsimeris, Jennyfer Lawrence Taylor, Oliver Thearle, Jonathan and Melanie Greenshaw and Richa Awasthy for your love and support throughout my candidature. The time would not have been as enjoyable without you all by my side.
vii
Acknowledgements
 Thank you to my amazing friends Jaimi Pigram and Katie Hotchkis who have seen me through high school, my undergraduate studies, and now my postgraduate studies. Thank you for your love and support. I know that there were many times that you had to deal with me being tired and stressed, and you were always there for me to keep me going and put a smile back on my face. I never could have got to this point without you both and I am and always will be grateful for our everlasting friendship.
Last, but by no means least, a big thank you to Chris Chow for your love and support. You have always been there to help out when I needed it; especially in expanding my vocabulary to include words such as woodpile and trunk, but also for always being available to grab a coffee and having a chat when I needed it most! Thank you for staying up until after midnight to help me print copies of this thesis, for being there while I read the examiners comments, and for making me the best cheesecake of my life! I am forever grateful for your support, advice on TV shows, modelling in my photo shoots, and for always making me smile.
viii
Abstract
Online eLearning environments to support student learning are of growing importance. Students are increasingly turning to online resources for education; sometimes in place of face-to-face tuition. Online eLearning extends teaching and learning from the classroom to a wider audience with different needs, backgrounds, and motivations. The one-size-fits-all approach predominately used is not effective for catering to the needs of all students. An area of the increasing diversity is the linguistic background of readers. More students are reading in their non-native language. It has previously been established that first English language (L1) students read differently to second English language (L2) students. One way of analysing this difference is by tracking the eyes of readers, which is an effective way of investigating the reading process.
In this thesis we investigate the question of whether eye tracking can be used to make learning via reading more effective in eLearning environments. This question is approached from two directions; first by investigating how eye tracking can be used to adapt to individual student’s understanding and perceptions of text. The second approach is analysing a cohort’s reading behaviour to provide information to the author of the text and any related comprehension questions regarding their suitability and difficulty.
To investigate these questions, two user studies were carried out to collect eye gaze data from both L1 and L2 readers. The first user study focussed on how different presentation methods of text and related questions affected not only comprehension performance but also reading behaviour and student perceptions of performance. The data from this study was used to make predictions of reading comprehension that can be used to make eLearning environments adaptive, in addition to providing implicit feedback about the difficulty of text and questions.
In the second study we investigate the effects of text readability and conceptual difficulty on eye gaze, prediction of reading comprehension, and perceptions. This study showed that readability affected the eye gaze of L1 readers and conceptual difficulty affected the eye gaze of L2 readers. The prediction accuracy of comprehension was consequently increased for the L1 group by increased difficulty in readability, whereas increased difficulty in conceptual level corresponded to increased accuracy for the L2 group. Analysis of participants’ perceptions of complexity revealed that readability and conceptual difficulty interact making the
ix
Abstract
 two variables hard for the reader to disentangle. Further analysis of participants’ eye gaze revealed that both the predefined and perceived text complexity affected eye gaze. We therefore propose using eye gaze measures to provide feedback about the implicit reading difficulty of texts read.
The results from both studies indicate that there is enormous potential in using eye tracking to make learning via reading more effective in eLearning environments. We conclude with a discussion of how these findings can be applied to improve reading within eLearning environments. We propose an adaptive eLearning architecture that dynamically presents text to students and provides information to authors to improve the quality of texts and questions.
x
List of Publications
The work in this thesis is the original work of the author except where specific reference or acknowledgement is made to the work or contribution of others. Some of the material in this work has appeared in publications and presentations by the author. Only the contribution made by the author has been included in this work unless specific reference to the contrary has been made.
The publications produced during the thesis are:
1. Copeland, L., & Gedeon, T. D. (2015a). Tutorials in eLearning; How Presentation Affects Outcomes. Emerging Topics in Computing, IEEE Transactions on, PP(99), 1-1.
2. Copeland, L., Gedeon, T., & Caldwell, S. (2015). Effects of Text Difficulty and Readers on Predicting Reading Comprehension from Eye Movements. Paper presented at the IEEE 6th International Conference on Cognitive Infocommunications (CogInfoCom) 2015, Győr, Hungary.
3. Copeland, L., & Gedeon, T. (2015). Visual Distractions Effects on Reading in Digital Environments: A Comparison of First and Second English Language Readers. Paper presented at the Proceedings of the Annual Meeting of the Australian Special Interest Group for Computer Human Interaction, 506-516.
4. Copeland, L., & Gedeon, T. (2014a). Effect of presentation on reading behaviour. InProceedings of the 26th Australian Computer-Human Interaction Conference on Designing Futures: the Future of Design (pp. 230- 239). ACM.
5. Copeland, L., & Gedeon, T. (2014b). What are You Reading Most: Attention in eLearning. Procedia Computer Science, 39, 67-74.
6. Copeland, L., Gedeon, T., & Mendis, S. (2014a). Fuzzy Output Error as the Performance Function for Training Artificial Neural Networks to Predict Reading Comprehension from Eye Gaze. Paper presented at The 21st International Conference on Neural Information Processing 2014.
7. Copeland, L., Gedeon, T., & Mendis, S. (2014b). Predicting reading comprehension scores from eye movements using artificial neural networks and fuzzy output error. Artificial Intelligence Research, 3(3), p35.
xi
List of Publications
 8. Copeland, L., Gedeon, T., & Mendis, B. S. (2014). An Investigation of Fuzzy Output Error as an Error Function for Optimisation of Fuzzy Signature Parameters. RCSC TR-1 2014.
9. Copeland, L., Gedeon, T., & Caldwell, S. (2014). Framework for Dynamic Text Presentation in eLearning. Procedia Computer Science, 39, 150-153.
10. Copeland, L., & Gedeon, T. (2013a). The effect of subject familiarity on comprehension and eye movements during reading. Paper presented at the Proceedings of the 25th Australian Computer-Human Interaction Conference: Augmentation, Application, Innovation, Collaboration.
11. Copeland, L., & Gedeon, T. (2013b). Measuring reading comprehension using eye movements. Paper presented at the Cognitive Infocommunications (CogInfoCom), 2013 IEEE 4th International Conference on.
12.
13.
14.
challenge)
15. Gedeon, T., Zhu, X., He, K., & Copeland, L. (2014, January). Fuzzy Signature Neural Networks for Classification: Optimising the Structure. In Neural Information Processing (pp. 335-341). Springer International Publishing.
16. Gedeon, T., Copeland, L., & Mendis, B. S. (2012). Fuzzy Output Error. Australian Journal of Intelligent Information Processing Systems, 13(2), 37- 43.
 Caldwell, S., Gedeon, T., Jones, R., & Copeland, L. (2015) Imperfect Understandings: A Grounded Theory and Eye Gaze Investigation of Human Perceptions Of Manipulated And Unmanipulated Digital Images. In
  Proceedings of
3rd International Conference on Multimedia and Human-
 Computer Interaction (Winner of the Best Paper award)
 Naqshbandi, K., Gedeon, T., Abdulla, U. A., & Copeland, L. (2015). Factors affecting identification of tasks using eye gaze. Paper presented at the Cognitive Infocommunications (CogInfoCom), 2015 6th IEEE International
 Conference on.
 Taylor, J. L., Copeland, L., Chow, C., & Nitschke, K. (2015) VIRK: Virtual
 Paper presented at the Proceedings of the Annual Meeting of the Australian Special Interest Group for Computer Human Interaction as part of the OzCHI 24 hour student design challenge. (Winner of the student design
work environment to facilitate interaction between the unemployed.
  xii
Contents
ACKNOWLEDGEMENTS ______________________________________________ VII
ABSTRACT ___________________________________________________________ IX
LIST OF PUBLICATIONS _______________________________________________ XI
LIST OF FIGURES ____________________________________________________XVII
LIST OF TABLES ______________________________________________________XXI
CHAPTER 1. INTRODUCTION___________________________________________ 1
1.1 Motivation .......................................................................................................... 3
1.2 Primary Research Questions............................................................................4
1.3 Hypotheses ......................................................................................................... 5
1.4 Methodology ...................................................................................................... 5
1.5 Thesis Outline .................................................................................................... 6
1.6 Acronyms ........................................................................................................... 9
1.7 Glossary ............................................................................................................ 10
CHAPTER 2. LITERATURE SURVEY _____________________________________ 11
2.1 Attention and Effort ........................................................................................ 12
2.2 The Human Eye ............................................................................................... 13
2.3 Reading ............................................................................................................. 19
2.4 Eye Tracking .................................................................................................... 32
2.5 The Use of Eye Tracking in HCI....................................................................37
2.6 Digital Text and eLearning ............................................................................41
2.7 Summary .......................................................................................................... 48
CHAPTER 3. EFFECT OF PRESENTATION ON READING BEHAVIOUR____ 49
3.1 Introduction ..................................................................................................... 50
3.2 Method .............................................................................................................. 51
3.3 Result & Analysis ............................................................................................ 56
3.4 Discussion and Implications .......................................................................... 66
3.5 Conclusion and Further Work.......................................................................68
xiii
CHAPTER
7. PERCEPTION AND PREDICTION OF TEXT DIFFICULTY____ 123
CHAPTER
8. DERIVING TEXT DIFFICULTY FROM EYE GAZE ___________ 143
CHAPTER
9. DISCUSSION AND IMPLICATIONS _______________________ 161
xiv
Contents
 CHAPTER 4. ANSWERING QUESTIONS IN ELEARNING TUTORIALS ____ 71
4.1 Introduction ..................................................................................................... 71 4.2 What happens when text is presented with questions? ............................72 4.3 Using Answer-Seeking Behaviour for Feedback ........................................ 79 4.4 Conclusion and Further Work.......................................................................80
CHAPTER 5. EFFECTS OF PRESENTATION ON PREDICTION OF COMPREHENSION_____________________________________________________ 83
5.1 Introduction ..................................................................................................... 84
5.2 Making Predictions ......................................................................................... 85
5.3 Fuzzy Output Error (FOE) ............................................................................. 87
5.4 Description of data sets .................................................................................. 90
5.5 Results and Analysis ....................................................................................... 92
5.6 Discussion.........................................................................................................99
5.7 Conclusion......................................................................................................102
CHAPTER COMPREHENSION____________________________________________________ 103
6. EFFECT OF TEXT DIFFICULTY ON PREDICTION OF
6.1 Introduction ................................................................................................... 104
6.2 Feature selection using genetic algorithms ............................................... 105
6.3 Method ............................................................................................................ 106
6.4 Results ............................................................................................................. 113
6.5 Discussion and Implications ........................................................................ 119
6.6 Conclusion and Further Work.....................................................................121
7.1 Introduction ................................................................................................... 124
7.2 Background .................................................................................................... 125
7.3 Method ............................................................................................................ 127
7.4 Predicting text difficulty...............................................................................128
7.5 Effects of text properties on understanding and confidence ..................137
7.6 Discussion and Implications ........................................................................ 140
7.7 Conclusion and Further Work.....................................................................142
8.1 Introduction ................................................................................................... 144
8.2 Method ............................................................................................................ 145
8.3 Differentiating L1 and L2 readers ............................................................... 146
8.4 Deriving text difficulty from eye gaze .......................................................151
8.5 Discussion and Implications ........................................................................ 158
8.6 Conclusion and Further Work.....................................................................160
9.1 Eye tracking in eLearning ............................................................................ 162
Contents
 9.2 Framework for dynamic text selection and presentation based on eye gaze .......................................................................................................................... 165 9.3 Summary ........................................................................................................171
CHAPTER 10.CONCLUSION ___________________________________________ 173
10.1 Limitations.................................................................................................... 174
10.2 Future work..................................................................................................177
REFERENCES _________________________________________________________ 181
APPENDIX A. MATERIALS FOR EXPERIMENT 1 - EYE GAZE IN ELEARNING ENVIRONMENTS ________________________________________ 199
A.1 Participant Information Sheet .................................................................... 200
A.2 Participant Consent Form ........................................................................... 201
A.3 Experiment texts...........................................................................................202
A.4 Web Search Tutorial Quiz ........................................................................... 207
A.5 Questionnaires .............................................................................................. 209
APPENDIX B. MATERIALS FOR EXPERIMENT 2 - ADAPTIVE ELEARNING AND DIGITAL IMAGES _______________________________________________ 211
B.1 Participant Information Sheet ..................................................................... 212
B.2 Participant Consent Form ............................................................................ 214
B.3 Run sheet for user study .............................................................................. 215
B.4 Pre-experiment Questionnaire .................................................................... 218
B.5 Experimental Content .................................................................................. 218
APPENDIX C. DEALING WITH IMPERFECT EYE GAZE DATA __________ 229 APPENDIX D. READING IN DISTRACTING ENVIRONMENTS _________ 233
D.1 Introduction .................................................................................................. 233
D.2 Background ................................................................................................... 234
D.3 Method...........................................................................................................236
D.4 Results ............................................................................................................ 241
D.5 Discussion ..................................................................................................... 247
D.6 Future work................................................................................................... 249
xv
Contents
 xvi
List of Figures
Figure 2.1. The layers of the retina. Image taken from (Dyer & Cepko, 2001)............14 Figure 2.2. The optic tract in the human brain. Image taken from https://senseofvision.wikispaces.com/ (Last accessed: 8th November 2015)........15 Figure 2.3. Diagram of the anatomy of the eye. Image taken from: https://nei.nih.gov/sites/default/files/nehep-images/eyediagram.gif (Last accessed: 29th January 2016) ........................................................................................16 Figure 2.4. Figure 2 from (Rayner, 1998) examples of the moving window paradigm .......................................................................................................................................... 23 Figure 2.5. Pupil and corneal reflection are tracked with camera-based eye tracking to estimate eye gaze. Image take from (Poole & Ball, 2005). ...................................32 Figure 2.6. Example of calibration screen for “The Eye Tribe” eye tracker. Image taken from The Eye Tribe website: http://dev.theeyetribe.com/start/....................33 Figure 2.7. The Eye Tribe eye tracker. Image taken from https://theeyetribe.com/order/ Last accessed: 27th January 2016 ............................33 Figure 2.8. Eye movement trajectories of one participant; to the left is the eye movement whilst reading a paragraph and the right is the eye movement pattern whilst reading a question. Images taken from (Fahey, 2009). ...................36 Figure 2.9. The scoring system for fixation transitions for the reading algorithm outlined in (Buscher et al., 2008). Taken from (Buscher et al., 2008)......................38 Figure 3.1. Example of text only tutorial page (T)...........................................................52 Figure 3.2. Example of text and comprehension question tutorial page (T/Q)...........53 Figure 3.3. Example of comprehension questions only tutorial page (Q). ..................53 Figure 3.4. Experiment set up; Participant to the left with the experimenter’s laptop and view to the right. ....................................................................................................54 Figure 3.5. Means and standard deviations of reading ratios (% of eye movements detected as reading) for text only page which are in formats A, C and D (A: ! → !/$; C: ! → $; D: $ → ! → $)......................................................................................61 Figure 3.6. Example of fixations recorded from reading text only page in format A (! → !/$) ........................................................................................................................61 Figure 3.7. Example of fixations from reading text only page in format C (! → $)...62 Figure 3.8. Example of fixations recorded from reading text only page for format D ($ → ! → $) ....................................................................................................................63 Figure 3.9. Means and standard deviations of reading ratios (% of eye movements detected as reading for text and questions pages) for Formats A and B. (A: ! → !/$; B: !/$) ....................................................................................................................64
xvii
List of Figures
 Figure 3.10. Example of eye movements from reading and answering questions on questions and text tutorial page for Format A (! → !/$) .......................................65 Figure 3.11. Example of eye movements from reading and answering questions on questions and text tutorial page for Format B (B: !/$)............................................66 Figure 4.1. Example of answer-seeking behaviour .........................................................74 Figure 5.1. Plots of : (a) FMF1; (b) FMF2; and (c) FMF3 .................................................90 Figure 5.2. Plots of (a) FMF4; (b) FMF5; (c) FMF6; and (d) FMF7.................................90 Figure 5.3. Hierarchical clustering of eye movement measures for Format C (! → $) .......................................................................................................................................... 96 Figure 5.4. Hierarchical clustering for eye movement measures from format D ($ → ! → $) ..............................................................................................................................97 Figure 6.1. Description of the text property breakdown..............................................107 Figure 6.2. The Flesch-Kincaid readability grade level and COH-Metrix L2 readability for each level of readability....................................................................108 Figure 6.3 Process used to generate the paths ...............................................................109 Figure 6.4. Example of text presented in the Wattle online eLearning environment ........................................................................................................................................ 111 Figure 6.5. Normalised number of fixations (NNF) for each text...............................117 Figure 6.6. Regression ratios for each text......................................................................118 Figure 7.1. Description of the text difficulty ..................................................................127 Figure 7.2. Participant versus GA-kNN predictions of conceptual level (for each level of readability)......................................................................................................129 Figure 7.3. Participant versus GA-kNN prediction of readability level (for each level of conceptual difficult) ................................................................................................130 Figure 7.4. Classification of perceived conceptual difficulty versus predefined conceptual difficulty from eye tracking data...........................................................131 Figure 7.5. Classification of perceived readability level versus predefined readability level from eye tracking data.......................................................................................132 Figure 7.6. L1 readers’ subjective understanding on the text......................................137 Figure 7.7. L2 readers subjective understanding ratings .............................................138 Figure 7.8. Average comprehension score per question ..............................................138 Figure 7.9. L1 participants’ confidence ratings..............................................................139 Figure 7.10. L2 participants’ confidence ratings............................................................140 Figure 9.1. Framework for Dynamic presentation of reading material in an online learning environment (Copeland, Gedeon, & Caldwell, 2014).............................166 Figure C.1. Example of misaligned fixation data. .........................................................230 Figure C.2. Example of re-aligned fixation data............................................................231 Figure D.1. Example of distracting environment..........................................................238 Figure D.2. Example of signal A; highlighting and bolding of the last word read before a distraction. .....................................................................................................238 Figure D.3. Example of signal B; greying out and italicizing the last word read before a distraction. .....................................................................................................239 Figure D.4. Example of the 9-point calibration screen used in the experiment showing that perfect calibration was accomplished...............................................239 Figure D.5. Experiment setup ..........................................................................................240 Figure D.6. Pre-experiment questionnaire data on self-rated distraction levels from communication technologies, grouped by frequency use of technologies .........242
xviii
List of Figures
 Figure D.7. Time taken to complete for each condition ...............................................243 Figure D.8. Comprehension for each condition ............................................................243
xix
List of Figures
 xx
List of Tables
Table 2.1. Eye movement measures ..................................................................................26 Table 3.1. Comparison of eye movement measures for text only (T) pages (Mean ± Standard Deviation) (A: ! → !/$; C: ! → $; D: $ → ! → $). .................................60 Table 3.2. Comparison of eye movement measures for Questions and Text pages (Mean ± Standard Deviation) (A: ! → !/$; B: !/$) .................................................64 Table 4.1. Mean ± standard deviation answer-seeking behaviour for formats A and B (A: ! → !/$; B: !/$)......................................................................................................75 Table 4.2. Answer-seeking behaviour averages per question for format A (A: ! → !/$)..................................................................................................................................77 Table 4.3. Average answer-seeking behaviour per participant for format A (A: ! → !/$)..................................................................................................................................79 Table 5.1. Properties of each data set (A: ! → !/$; B: !/$; C: ! → $; D: $ → ! → $) .......................................................................................................................................... 91 Table 5.2. Misclassification rate (MCR) comparison: FOE versus MSE as the performance function for ANN training (A: ! → !/$; B: !/$; C: ! → $; D: $ → ! → $) ..............................................................................................................................93 Table 5.3. Comparison of Misclassification (MCR) results for predicting total comprehension scores for all eye movement measures (A: ! → !/$; B: !/$; C: ! → $; D: $ → ! → $)....................................................................................................95 Table 5.4. Comparison of average eye movement measures for clusters obtained from hierarchical clustering of format C data (! → $).............................................96 Table 5.5. Comparison of Misclassification (MCR) results for predicting questions scores for text only pages eye movement measures from Format C using Random Forest Ensemble Classification ....................................................................97 Table 5.6. Cluster details for Format D ($ → ! → $) ......................................................98 Table 5.7. Comparison of Misclassification (MCR) results for predicting questions scores for text only pages eye movement measures from Format C using Random Forest Ensemble Classification ....................................................................98 Table 6.1 Example of chunking concepts to derive the levels of concept difficulty for Topic 3 - Photo Credibility .........................................................................................108 Table 6.2. Participants’ ratings of familiarity to each topic..........................................110 Table 6.3. Distribution (%) of comprehension scores for each text and for the L1 and L2 data sets....................................................................................................................113 Table 6.4. GA parameter settings for feature selection.................................................113 Table 6.5. Classification rates (%) from no windowing or feature selection .............114 Table 6.6. Classification rates (%) using feature selection and no windowing.........115
xxi
List of Tables
 Table 6.7. Correct classification (%) of reading comprehension for different windows ........................................................................................................................................ 116 Table 7.1. Expected versus reported text difficulty for L1 readers.............................133 Table 7.2. Expected versus reported text difficulty for L2 readers.............................134 Table 7.3. Average correct classification rates (%) of text difficulty for the L1 group from GA-kNN classification from eye tracking data..............................................136 Table 7.4. Average correct classification rates (%) for the L2 group of text difficulty group from GA-kNN classification from eye tracking data..................................136 Table 8.1. Average silhouette widths for clustering of A.............................................146 Table 8.2. Eye movement averages from clusters for text A........................................147 Table 8.3. Average silhouette widths for clustering of E .............................................148 Table 8.4. Eye movement averages from clusters for text E ........................................149 Table 8.5. Average silhouette widths for clustering of J...............................................150 Table 8.6. Eye movement averages from clusters for text J .........................................150 Table 8.7. Average silhouette widths for clustering of average eye movement measures for each text.................................................................................................152 Table 8.8. Averages of measures for each clusters for L1 readers, based on text averages.........................................................................................................................152 Table 8.9. Texts within each cluster, for L1 averages for text......................................154 Table 8.10. Average silhouette widths for clustering of average eye movement measures for each text.................................................................................................155 Table 8.11. Averages of measures for each clusters for L1 readers, based on text averages.........................................................................................................................155 Table 8.12. Texts within each cluster, for L1 averages for text....................................156 Table D.1. Readability scores for each text type............................................................237 Table D.2. Text statistics for each text type ....................................................................237 Table D.3. Distraction rates for each experimental condition .....................................245
xxii
Chapter 1
Chapter 1. Introduction
 “The more that you read, the more things you will know.
 The more that you learn, the more places you'll go.”
 Online learning could extend teaching and learning from the classroom to a wide and varied audience with different needs, backgrounds, and motivations. Particularly in tertiary education, online learning technologies are becoming ubiquitous. This is due in part to increased accessibility and availability of computer hardware but also due to an influx of eLearning software and services. Universities now frequently offer online or off-campus degrees where students may have little or no face-to-face interaction with their instructors or other students. Even for university courses that deliver traditionally using face-to-face tuition, absenteeism from lectures is more prevalent and has been shown to negatively affect learning (Romer, 1993; Woodfield et al., 2006).
However, the use of online learning can be beneficial in dealing with not only this problem but also the problems encountered by large class sizes as well as dispersed students, by providing consistency and accessibility in delivered materials (Welsh, Wanberg, Brown, & Simmering, 2003). The advent of massive open online courses (MOOCs) has not only increased the number of online learning users but also increased diversity (especially in native language) as students from around the world are able to access the content (Breslow et al., 2013; DeBoer et al., 2013). The low completion rates of MOOCs highlight the significant need to improve online learning technologies (Breslow et al., 2013). As a result of these factors there is growing importance in designing effective eLearning environments.
Most eLearning environments are one-size-fits-all, yet this does not account for differences in students’ needs, backgrounds, or native language. One of the increasing diversities is the linguistic background of readers. There are an increasing number of students who are reading in their non-native language. A
― Dr. Seuss, I Can Read With My Eyes Shut!
1
Introduction
 study of edX’s1 first MOOC showed students came from 194 different countries and two-thirds spoke English where the other third spoke other languages (Breslow et al., 2013). It is known that first English language (L1) and second English language (L2) readers differently (Rayner, 1998). Students also vary in respect to their prior knowledge, expertise, and reading abilities. These differences can impact the processing needed to properly comprehend text. Text characteristics have been shown to affect comprehension by which, in the context of legal documents, making text simpler would benefit vulnerable populations (Scherr, Agauas, & Ashby, 2015). This can be extended to considering the differences of students in eLearning, where some students may be supported by simpler texts.
Whilst some eLearning environments provide personalisation, the learner often does this explicitly. Adaption can be based on different qualities about the learner such as the current understanding, emotional state such as stress (Calvi et al., 2008; Porta, 2008) boredom (Jaques, Conati, Harley, & Azevedo, 2014), motivation (Kareal & Klema, 2006), learner style (Mehigan et al., 2011; Spada et al., 2008; Surjono, 2011, 2014), cognitive load (Coyne et al., 2009), learner style (Bondareva et al., 2013),and skill level (Chen, 2008). Adaption achieved in real time, without disruption to the learner, is the optimal solution rather than explicitly asking the learner. Progress in technology and understanding of psychophysiological responses provide the unique opportunity of adapting eLearning environments in real time and doing so based upon implicit behaviour. These methods include the use of biometric technology (Mehigan et al., 2011; Spada et al., 2008) and psychophysiological response data (Rosch & Vogel-Walcutt, 2013), especially eye tracking (Alsobhi et al., 2015; Barrios et al., 2004; Bondareva et al., 2013; Calvi et al., 2008; Conati, Jaques, & Muir, 2013; Conati & Merten, 2007; Kardan & Conati, 2013; Merten & Conati, 2006; D'Mello et al., 2012; Gütl et al., 2005; Mehigan, 2014; Mehigan & Pitt, 2013; Mehigan, 2013; Mehigan et al., 2011; Porta, 2008).
There is a broad range of scenarios that these adaptive technologies are directed at helping students, such as plugging into traditional online learning environments (Barrios et al., 2004; De Bra et al., 2013), or providing adaption in mobile environments (Mehigan & Pitt, 2013), or accounting for dyslexia (Alsobhi et al., 2015), or foreign language reading (Hyrskykari et al., 2000). With this past research we are able to take the results from the studies presented in this thesis and add to the current knowledge base of adaptive eLearning. The contribution lies solely in the domain of text-based learning materials that have not been focused upon in the past. Eye tracking can certainly be used to make learning via reading more effective in the context of eLearning.
Using eye gaze to control adaption of eye learning environments provides the ability to go beyond the student’s surface answering behaviour or preferences and adapt to the student’s implicit behaviour. Eye tracking has been shown to be a powerful tool for investigating how humans interact with computer interfaces. It has also been shown to provide information about the differences between L1 and L2 readers (Dednam et al., 2014; Kang, 2014). Eye movements can reveal abundant
1 edX is a MOOC provider - https://www.edx.org/ (Last accessed: 24th January 2016) 2

Introduction
 information about the cognitive processes behind human behaviours. Louis Emile Javal noted in the late 1800s that the eyes move in a particular way when someone is reading. Since then eye tracking technologies have vastly improved and together with new brain scanning techniques such as functional magnetic resonance imaging (fMRI) we have a greater knowledge of how humans read (Bowman et al., 2010).
While brain scans provide a good way of seeing how the brain reacts during reading, eye tracking affords the unique ability to observe the underlying cognitive processes of reading in an unobtrusive manner. There is now a plethora of research that investigates how the eye moves during the reading process which go down to the level of predicting where the eye will land on a word and for how long it will fixate (e.g. the E-Z Reader model (Reichle et al., 2006; Reichle et al., 1999, 2003, 2012; Reichle et al., 2009) and the SWIFT model (Engbert et al., 2005). Additionally, research on eye movements during reading has shown that eye movements reveal difficulties in reading (Frazier & Rayner, 1982), text difficulty and comprehension (Rayner et al., 2006), as well as differentiating between L1 and L2 readers (Dednam et al., 2014; Kang, 2014). While we now know a lot about the reading process, the application of it into the design eLearning environments is still in early days. Additionally, there is still much to be learnt about the differences between L1 and L2 readers in the context of eLearning. With eye tracking becoming increasingly more precise whilst decreasing dramatically in cost, the use of such technology in adaptive eLearning is becoming plausible.
The problem of how to make eLearning environments effective to a wide and varied audience is significant; especially when learning materials come in many types and forms, and quite often depend on the subject being taught. For example a mathematics course would have exercises including many mathematical symbols as opposed to a history course, which would be more likely to have text-based materials. The focus of this thesis is on text-based materials and the use of eye tracking technology to analyse reading and learning behaviour. This thesis investigates ways of using eye tracking to make eLearning environments adaptive to the reader based upon their reading behaviours. This can mean real-time alteration of the learning environment to reflect the student’s current comprehension and state. It can also mean the use of eye tracking to monitor the cohort’s reading and learning behaviours and using this information as a means of improving the quality of the learning materials. Both are investigated in this thesis as a means of exploring the potential for using eye tracking to make eLearning environments better for learning.
The remainder of the introduction chapter outlines the motivations for this thesis; the primary research questions and hypotheses of the investigation; and finally the thesis structure is outlined.
1.1 Motivation
In many countries reading is part of everyday life. Such as reading signs in a building to direct you to the room you want to go to or reading the labels on food packaging. If you have an Internet connection and device capable of connecting to it
3
Introduction
 then chances are, you are reading something, like Facebook posts, tweets, online news, instant messaging, or your email, since communication is now often carried out via textual means. As Dr Seuss points out so eloquently, reading is a very good way for gaining information and the Internet and computer devices make it easier to access vast amount of information. Reading moulds what we know and what we know is used to develop opinions and base actions upon. This suggests that what a person reads can have a large bearing on their current knowledge, their beliefs, and what they are likely to be interested in. Learning itself is an ability that is shaped by what we know. Reading and learning can therefore be seen as having a somewhat reciprocal relationship.
There has been an increase is the use of eLearning systems. This can be seen both in the educational sector where tertiary institutions quite often use online learning environments in addition to the traditional face-to-face teaching, as well as in industry for employee training. This provides us with a unique opportunity to enhance both the reading and the learning processes due to the capabilities of electronic systems to provide feedback to their users. Already there are systems that record eye movements whilst reading documents to provide implicit feedback about the perceived relevance of parts of a document (Buscher et al., 2012). Furthermore, the ability to record the parts of a document that have not been comprehended properly or read thoroughly could give feedback to the user of the parts of the document that may need to be re-read for better understanding of the content. On the other hand, feedback about how a document is read can provide information to the author as to how easily it is read and understood. In turn, the author can revise the document to make it easier to read and comprehend. In education feedback often comes from assessment results. Presentation of course content may be in the form of slides, readings, tutorials, all of which are increasingly presented online. Feedback about how students comprehend and read these documents may offer insight into assessment results. This kind of feedback could provide invaluable information to instructors about how to better present course content.
1.2 Primary Research Questions
The central research question of this thesis is:
Can eye tracking be used to make eLearning environments more effective for first and second language English readers?
This is a broad question, which is broken down to look at ways in which eye tracking could be used to make eLearning environments more effective for reading English by both first and second language readers. Primarily the investigation in this thesis will be the use of eye tracking data to make predictions about reading comprehension and text properties. In this way, the broad question is divided into sub-questions that are addressed throughout this thesis:
4
Introduction
 1. Can outcomes of eye gaze analysis be used to optimise the layout of reading materials in eLearning environments for learning outcomes? How does the layout compare for L1 and L2 readers?
2. Can eye gaze be used to provide feedback about learning behaviour in eLearning environments for L1 and L2 readers?
3. Can eye tracking data be used to predict reading comprehension scores in eLearning environments for L1 and L2 readers?
a. Does presentation of text affect predictions of comprehension?
b. Does text difficulty affect predictions of comprehension?
4. Can participants predict text difficulty and can we predict text difficulty from their eye gaze?
5. Can eye gaze data be used to differentiate between L1 and L2 readers and to derive a measure of text difficulty?
These questions all investigate a sub-component of the overall question of whether eye tracking can be used to make eLearning more effective. In all cases we investigate this for both L1 and L2 readers, whereby we compare the outcomes for two groups. In this way, the investigation is a comparison of first and second language readers.
1.3 Hypotheses
The overall hypothesis is that using eye tracking to analyse the reading and learning behaviour of first and second English language readers can be used to improve reading and learning in eLearning environments. Within each chapter we explain the hypotheses for the investigation carried out in that chapter. However, an overview of these hypotheses is:
1. Layout of text and questions will affect eye gaze and learning outcomes as well as affect L1 and L2 readers in the same way even though there will be differences between the two groups.
2. Eye gaze can provide feedback about implicit learning behaviours, in particular, answering behaviours.
3. Different formats and different levels of text difficulty will affect prediction outcomes of reading comprehension.
4. Eye gaze data can be used to predict text difficulty.
1.4 Methodology
The research carried out for this thesis is based on data collected from user studies where the eye gaze of participants was tracked using video based eye tracking placed at the base of the display monitor. Participants were asked to sit on a chair in front of the monitor and were able to reach the keyboard and mouse. The eye tracker recorded eye gaze and pupil dilation data.
5
Introduction
 Questionnaire data was also gathered from the participants. There are two main studies that were carried out in the thesis. Each study involves in-depth analysis that is covered by more than one chapter.
1.5 Thesis Outline
The research presented in this thesis is aimed at answering the overarching question of how eye tracking can be used to make eLearning more effective. The thesis is organised in a way that follows the order of the research sub-questions.
Chapter 1: Introduction
This chapter introduces the thesis, the motivation, the research questions that will be explored, hypotheses and outline of the thesis.
Chapter 2: Literature review
The literature review presents an overview of current knowledge of eye gaze analysis and adaptive eLearning. Eye gaze has been used extensively to study the reading process. With this background on the reading process we move to the discussion of using eye gaze to make eLearning environments adaptive to students. This thesis seeks to build upon previous research and enhance the current state of adaptive online learning environments.
Chapter 3: Effect of presentation on reading behaviour
Chapter 3 addresses the first research question of whether eye gaze can be used to find appropriate layouts of reading materials in eLearning environments. This chapter describes a user study that investigated how different sequences of text and assessment questions affect performance outcomes, eye movements, and reading behaviour of L1 and L2 readers. The results from the study show that different presentation sequences induce different performance outcomes, eye movements, and reading behaviour. The presentation sequence impacts participants’ ability to accurately perceive their own understanding, in addition to inducing specific reading behaviours, such as thorough reading. The outcomes from this study can be used to influence how students interact with the learning environment as well as how they learn the material.
Chapter 4: Answering questions in eLearning tutorials
A subset of the data presented in Chapter 3 is explored further by investigating the situation where participants are provided with the opportunity to read text whilst answering the questions. The eye movements that occur as a result of this presentation are characterised by transition between the questions and text to find the correct answer, or to reassure the participant that they have the correct answer. We term these eye movements as answer-seeking behaviour, and present a method for measuring and comparing this behaviour. We propose using the degree of answer-seeking
6
Introduction
 behaviour to measure how question difficulty and as an implicit measure of how difficult a participant finds a tutorial and quiz.
Chapter 5: Effects of presentation on prediction of comprehension
Using the data collected from the user study described in Chapter 3 we explore how presentation formats affect the prediction outcomes of reading comprehension from eye movements. The hypothesis being that the different eye movements caused by the formats will cause different levels of prediction accuracy. The chapter incorporates three components of analysis; the first component builds on previous work of using fuzzy output error (FOE) as an alternative performance function to mean square error (MSE) for training ANNs, as a means of improving reading comprehension predictions. The use of FOE-ANN produced better classification results compared to MSE-ANN. Additionally, the FOE trained ANN outperforms other comparison machine learning techniques. Finally, clustering of the more complex formats revealed reading behaviour properties.
Chapter 6: Effects of text difficulty on prediction of comprehension
Continuing from Chapter 5, this chapter focuses on predicting reading comprehension of text that is shown without comprehension questions. We extend the work by investigating the effect of text difficulty and machine learning techniques on prediction accuracy. Another user study was carried out to collect data from L1 and L2 participants as they read texts with differing degrees of difficulty. The grades of difficulty are based on different levels of readability and conceptual difficulty. We hypothesised that text difficulty and reader type would affect prediction accuracy. We found that neither had a significant effect on the accuracy of the k-nearest neighbour (kNN) classifier used. Whilst this is the case, we did manage to improve the classification accuracy to on average 80% for the L1 group and 73% for the L2 group, which is a substantial improvement from the 44% correct classification obtained in the previous chapter for format C. These results were achieved by using genetic algorithms (GA) for feature selection, which were significantly higher than the results produced when no feature selection is performed.
Chapter 7: Perception and prediction of text difficulty
We investigate prediction of text difficulty from eye gaze using machine learning techniques and compare these to participants’ perceptions of difficulty. We show that predictions from eye tracking data are more accurate than the participants’ perceptions of both readability and conceptual difficulty. We then show that prediction of participants’ perceived ratings of readability and conceptual difficulty from the eye tracking data are significantly better than prediction of the predefined values. This indicates that the eye gaze measures and pupil dilation data may be more aligned with the participants’ perceptions of difficulty rather than the predefined difficulty of the text. Further analysis of participants’
7
Introduction
 perceptions showed that they are poor at predicting predefined text difficulty, especially when the readability and the conceptual difficulty are not the same. Additionally, the text difficulty affected comprehension scores and confidence levels of the L1 readers.
Chapter 8: Deriving text difficulty from eye gaze
The eye tracking data from the user study in Chapter 6 was used to investigate whether L1 and L2 readers’ eye gaze are distinct, and whether eye gaze measures can be used to derive text difficulty. The investigation involves clustering eye movement measures from participants using kmeans clustering. Whilst there are clusters of different reading behaviours for different levels of text difficulty, such as skimming and thorough reading, the L1 and L2 groups are not distinct. Instead, there is a tendency for L2 readers to read more thoroughly compared to skimming. The average eye gaze measures for each text were clustered using kmeans. The clusters show that there are distinct reading behaviours and that the average eye gaze measures can be used to rate the texts based on the derived reading difficulty for the L1 and L2 groups. These findings can be used to provide feedback for the purpose of adapting learning material.
Chapter 9: Discussion and Implications
This chapter discusses the results from the preceding chapters, each of which addressed a sub-question of whether eye tracking can be used to make learning more effective in eLearning environments. This overall question is essentially approached from two directions. The first approach is by investigating whether eye tracking can immediately make eLearning environments better suited to the individual learner. The demonstration of these results is through the use of adaptive eLearning whereby the system adapts to the student’s understanding levels and perceptions of difficulty. The second approach is the use of historical eye tracking data to make eLearning more effective. This is through the use of eye tracking to provide information to the author of the text and comprehension questions regarding their difficulty. This information can in turn be used to improve the quality of online texts and more accurately define their complexity. To show this we have tied the results from each chapter together in the presentation of a dynamic text selection method to make eLearning environments adaptive.
Chapter 10: Conclusion
The thesis is concluded with a summary of the research findings, and a discussion of the limitations of the research and how it can be improved and extended.
Appendix A: Experiment materials for eye gaze in eLearning environments
The participant information form, consent form, texts and questionnaire used in the experiment explained in Chapter 3.
8
Introduction
 Appendix B: Experiment materials for adaptive eLearning and digital images
The participant information form, consent form, run sheet, texts and questionnaire used in the experiment explained in Chapter 6.
Appendix C: Dealing with eye gaze data that is imperfect
This appendix explains the post calibration used in the first study.
Appendix D: Reading in distracting digital environments
This appendix outlines preliminary results from a user study on reading in distracting environments.
1.6 Acronyms
The following is a list of acronyms used throughout this document: ALE Adaptive eLearning Environment
ANOVA Analysis of variance
ANN Artificial Neural Network
FOE Fuzzy Output Error
FOE-ANN Feed-forward ANN trained using backpropagation with FOE
as the performance function FMF FOE Membership Function
HCI Human Computer Interaction KNN k-nearest neighbour
L1 First English language reader
L2 Second English language reader
MANOVA Multivariate analysis of variance MCR Misclassification rate
MOOC Massive Open Online Course
MSE Mean Squared Error
MSE-ANN Feed-forward ANN trained using backpropagation with MSE as the performance function
RF Random Forest
9
1.7 Glossary
This is a glossary of the terms used within this thesis.
Cloze question
eLearning
Eye gaze pattern Eye gaze point
Eye tracker Fixation
Assessment questions that can be a sentence or paragraph with words removed thereby requiring the reader / participant to fill them in. For example, “This is an ________ of a cloze question” were the missing word is example.
Learning materials presented using digital technology and usually via the Internet or Intranet.
The combination of all the eye gaze points recorded for a participant for each screen showing a text.
Eye gaze trackers take measurements of where the participants’ eye is looking on the screen at regular intervals. Gaze points are used to determine fixations and saccades.
Equipment used to measure eye gaze location.
When the eye finishes a saccade and stays relatively still to take in visual information for processing.
Introduction
 Flesch-Kincaid Grade Level A readability test that returns the minimum education level (based on the USA education system) needed for the
Readability
Saccade Wattle
reader to understand the text.
Refers to an explicit measure of text readability as calculated by readability formulae, which typically counts syllables, words, and sentences to determine readability. The readability formula used throughout this thesis is the commonly used metric Flesch-Kincaid Grade Level.
A rapid movement of the eye as it jumps from one fixation to another. Little to no visual information is taken in during a saccade.
The online eLearning environment used at the Australian National University. Accessible via: https://wattle.anu.edu.au/
 10
Chapter 2
Chapter 2. Literature Survey
This chapter reviews a range of research on the physiology and psychology of reading through to the practical use of human computer interaction (HCI) for eLearning. The central focus of the discussion is on the use of eye tracking to record and analyse eye gaze. Since its invention, eye tracking has proven to be an effective way of analysing human behaviours. This is particularly true for reading, as the eyes have been shown to move in a unique way during reading. These movements consequently reveal much about the underlying cognitive functions involved in reading (review by Rayner (1998)).
Eye tracking is a relatively recent technology (Huey, 1968) but advances in hardware and software for eye tracking have seen an increased popularity of eye tracking for many uses. Initially eye tracking was primarily used for reading analysis, but this technology has proven to be useful in usability testing and HCI (Jacob & Karn, 2003; Poole & Ball, 2005). Reading in a digital environment is now ubiquitous. Concurrently, eLearning technologies have become popular. Given that a primary form of educational material is text and that eye tracking provides an invaluable method of analysing reading behaviour, this raises the question of how eye tracking can be used to make the learning process more effective in eLearning.
The review begins with the discussion of the reading process. This leads to the discussion of how the eye moves during the reading process to reveal the cognitive process of reading. With this background on the reading process we move to the discussion of using these research findings to make eLearning environments adaptive to eye movements.
11
Literature Survey
 2.1 Attention and Effort
Attention and effort are two cognitive experiences that impact what will be discussed in this thesis. In later sections discussion of the human eye, its functions, and how they apply to reading behaviours will be centred somewhat on attention and effort. This brief discussion about attention and effort is focussed on its applicability to reading.
Attention is focused awareness and can be thought of as the allocation of cognitive resources to deal with some stimuli over others (Buscher et al., 2012; Kahneman, 1973). Selective attention is conscious; an example being that one can move one’s head and/or eyes to either look or not look at something. Reading is an example of selective attention where the reader has to consciously choose to allocate attention to the task of looking at the page and reading. On the other hand, involuntary attention occurs when one’s attention is allocated to a sudden change in the environment with no conscious control over this allocation. Hearing a loud or surprising noise and turning to see what made it, where it came from, and if it is a threat, is an example of involuntary attention.
There are limited cognitive resources in the human brain and thus limits to human attention. When attention is subjectively allocated it reflects, at least in some way, the person's preference. A person will focus on what they consider most relevant, interesting, or useful in a given situation (Buscher et al., 2012). For humans a reliable measure of attention is eye movement (Henderson, 2003).
2.1.1 AttentionandVisualProcessing
Due to the anatomy of the human eye, humans do not
view scenes in full; they only view parts of it, and only the
essential parts in detail. This leads to the intriguing fact
that just because the eyes are directed upon a stimulus
does not guarantee that all parts of the stimulus are seen;
only that which is needed. This observation has been
demonstrated many times; perhaps most famously in The Invisible Gorilla experiment (Simons & Chabris, 1999). In the experiment, participants watched a short film where two teams, wearing black and white shirts respectively, were passing basketballs between members of their own teams. The players are moving around rapidly, weaving in between one another. Participants are asked to count the number of passes made only by the white team. Halfway through the video a person wearing a gorilla suit crosses the court, thumps their chest and moves on. What they found was that half of the participants did not see the gorilla. This is a demonstration of selective attention where participants are forced to focus on a task and become effectively blind to everything else, termed inattentional blindness.
This experiment has been replicated many times, in different settings and confirms that about half of the observers never see an unexpected stimulus. The obvious question is whether the observers actually looked at the gorilla at all. In a study by Memmert (2006) eye tracking was used to record the eye gaze of the
  The illusion of of "seeing"
 12
Literature Survey
 observers in the gorilla experiment. Participants who did not perceive the gorilla had their gaze fixed upon the gorilla for about a second, which is the same time as those who did perceive it. Furthermore, factors such as age and expertise in dealing with certain stimuli are correlated with perceiving unexpected stimuli. So looking at something does not equate to perceiving it; something that has to be kept in mind when analysing eye gaze data.
Inattentional blindness is related to change blindness, a phenomenon where humans are seemingly blind to visual change in a stimulus, not always caused by focusing on an absorbing task. Simons and Levin (1998) showed this in a remarkable experiment where an experimenter initiated a conversation with a pedestrian and half way through the conversation the experimenter was replaced by another person. Only half the participants realised that the experimenter had been changed. Even if the eyes are directed upon a stimulus there is no guarantee that all parts will be seen. We have included an example that is designed to show this point. In the triangle2 figure shown on the previous page most people are not aware that there are is a duplication of the word "of" until it is pointed out to them. The choice of how eye movements are used in real world situations has to take this non-direct relationship into account.
The last point of this subsection is that not all features of a visual stimulus can be reportable. In short, shown the string "aaaaaaa" one could quite easily report that it is a group of a's but most likely not that is it a group of 7 a's without taking a longer to look in order to count them. These phenomena illustrate that the brain does not need to know everything and in fact would not be as efficient if it did. Instead it calculates what it needs only when it is needed. This is why attention and effort are important concepts. Eye gaze provides the remarkable ability to actually identify what is seen in fine detail and to some degree where attention lies. The goal of this research is not to investigate attention; however these concepts must be kept in mind when undertaking reading analysis. Just putting text in front of someone will not guarantee that it is read or even seen.
2.2 The Human Eye
Eyes are the small but complex, organs that enable vision in humans. The human eye is capable of responding to a portion of the electromagnetic spectrum, referred to as visible light. In most cases, reading is possible because our eyes give us the ability to see. This section contains an overview of the physiology of the human eye as an introduction to how humans can take in visual information to be processed by the brain. Finally, how the eye moves to take in information is discussed.
2.2.1 VisualProcessinginHumans
The sensory organs collect information about the environment and physical state. The brain processes all of the complicated information that streams in from the sensory organs and then decides what to do with it. Evolutionary processes favour
 2 Taken from (Eagleman, 2011) page 26.
13
Literature Survey
 brains that can process the complex information in the most beneficial and effective ways in order to promote survival or even more significantly, reproductive capacity. A large portion of the human brain is used in visual processing, which itself is a complex array of neural processes. Consequently, vision actually occurs in the brain and not in the eyes, which are just there to take in the information (Gehring, 2005).
Figure 2.1. The layers of the retina. Image taken from (Dyer & Cepko, 2001).
To illustrate this fact, when humans who have been blind for most of their lives are given surgery to give them eyesight, such as corneal implants, they do not miraculously start "seeing" the world in the way that someone with normal vision from birth does. Instead they have to learn to see; the neural networks in the brain have to be reorganised to provide this ability. If vision was lost early in life or a person is blind from birth, it is believed parts of the visual processing system never completely develop to the extent of an individual with unimpaired vision (Cohen et al., 1997). This highlights the fascinating point that some brain function is dependent upon input from the sensory organs. From an evolutionary point of view it is very likely that the eyes came before the brain (Gehring, 2005). Intuitively this is because there is no point having such an intricate information-processing unit if it has no information to process.
When humans read, the eyes are the starting point of the process, (excluding reading Braille). Given that the eye is a critical part of reading we discuss further how letters on a piece of paper or on an electronic display make their way into the human brain for interpretation. Firstly, light enters the eye and is passed through the cornea and projected onto the retina, which is a light sensitive layer of tissue at the back of the eye. The cornea is a transparent covering of the iris and pupil at the front of the eye. The iris dilates and constricts the pupil to regulate the amount of light that enters the eye and the lens focuses this light onto the retina (Burton et al., 2009).
 14
Literature Survey
  Figure 2.2. The optic tract in the human brain. Image taken from https://senseofvision.wikispaces.com/ (Last accessed: 8th November 2015).
The retina transforms light that enters the eye into an electrical signal using photoreceptors. The retina is a complex multilayer structure, as shown in Figure 2.1 and the following is an overview of how the light that enters the eye then makes its way to the brain. The photoreceptor layer in the human eye contains two types of light receptors: rods and cones, shown at the top of Figure 2.1. Rods are responsible for vision in low level light and are used in peripheral vision, cones are responsible for vision in higher levels of lights and for the ability to see colour. When a rod or cone absorbs light energy, an electrical signal is generated. These signals are passed through a layer of bipolar cells onto ganglion cells that integrate the electrical signals from many photoreceptors. The resulting signals are transmitted through the long axons of the ganglion cells that bundle together to form the optic nerve. The optic nerve transmits the signal to the brain via the optic chiasma where information from the left half of each visual field goes to the right hemisphere and similarly for the right (Schwarz & Schmückle, 2002).
The optic tract projects to three major subcortical structures (Schwarz & Schmückle, 2002) that make use of the visual information for different purposes. These structures are: the pretectum which controls pupillary reflexes; the superior colliculus which controls saccadic eye movements; and, the lateral geniculate nucleus (LGN), which is a thalamic nuclei and is the major relay for input to the visual cortex (Schwarz & Schmückle, 2002).
The sensory information from the eyes first goes through the LGN and then proceeds to the primary visual cortex in the occipital lobe at the back of the brain. Most sensory and motor information only reaches the cortex via the thalamus; there are exceptions to this, such as smell. Visual information then flows through the hierarchy of the visual cortex, where V1 is the point of entry of the visual sensory
15
Literature Survey
 information that is then passed through to V2, and so on to V53. Complexity of the neural representation increases as the information flows through the cortical hierarchy (Dehaene, 2009).
2.2.2 TypesofVision
The eye is capable of two types of vision, peripheral and detailed. Peripheral vision is hazy and occurs outside of the centre of gaze. Whilst peripheral vision is not very good at distinguishing colours and shapes, it is sensitive at detecting movements, and mostly used to gather information about the present surroundings. The human brain prioritises the information to give attention only to what it somehow deems important. For example, Itti and Baldi (2009) found that humans orient their attention and gaze toward surprising stimuli in the context of watching television. The reason for this orientation towards a stimulus is so that detailed vision can be used to examine the stimuli further and to manage the limited resources at hand in order that the most important stimuli are tended to first. The peripheral region of the visual field encompasses the whole retina apart from the foveal and parafoveal regions.
Figure 2.3. Diagram of the anatomy of the eye. Image taken from: https://nei.nih.gov/sites/default/files/nehep-images/eyediagram.gif (Last accessed: 29th January 2016)
Detailed vision is handled by the fovea and to some degree, the parafovea. The fovea is the small central region of the retina that is sensitive to fine detail. The fovea only sees the central 2° of the visual field (Rayner & Bertera, 1979), and comprises of a region of only cone photoreceptors. The parafovea extends 10° of the visual field around the centre of gaze and provides less detailed visual information than the fovea but more than the periphery (Rayner & Bertera, 1979). Whilst the fovea takes up less than 1% of the retina, the processing of this information accounts for over 50% of the activity of the visual cortex in the brain (Mason & Kandel, 1991). The fovea is necessary in humans for reading (Rayner & Bertera, 1979). Since only a very small part of the eye is capable of seeing in detail, the eye is constantly on the move to assimilate information about the visual environment. How and why the eye moves in the way that it does will be discussed in the following subsection.
3 also known as the middle temporal area (MT) 16

Literature Survey
 2.2.3 TypesofEyeMovements
Eye movement is somewhat sporadic and complex, with the eyes moving at high velocity before stopping for a period to take in information before moving on again. Louis Emile Javal first described this process in 1879 by direct observation. It was not until almost a century later that Edmund Huey developed the first eye tracker (Huey, 1968). To explain this phenomenon, we must consider detailed vision again. The foveal region is where 2° of visual acuity extends across the fixation point (Rayner, 1998; Rayner & Bertera, 1979; Underwood & Batt, 1996). The parafoveal region is just outside the foveal region and it comprises 5° on either side of the fixation point (Rayner & Bertera, 1979). The peripheral region is the rest of the visual field.
Due to this limited area of detailed vision, the eyes are constantly on the move so that the fovea can be oriented upon different parts of the environment. The visual information is taken in when the eye has been reoriented and is relatively still; this is termed a fixation. The rapid jumps between fixations are termed saccades, and little to no visual information is taken in then (Rayner, 1998). Humans, therefore, do not view an image of the environment or scene as a whole, instead it is viewed in parts and in differing detail depending on where the centre of gaze is oriented (Henderson, 2003). The attention given to certain stimuli can be quite dependent upon the reasons for looking at them. This was shown by Alfred Yarbus in his early work on eye movements in scene perception (Yarbus, 1967). In his work, he showed that an individual's eye gaze was dependent upon the question they were asked.
Gaze control is influenced by many factors, including information about the environment or stimulus and several cognitive systems (Henderson, 2003). This includes past memories of the scene, whether the individual is searching or memorising the scene, and its spatial and semantic properties. This type of gaze control is said to be knowledge-driven (Henderson, 2003). More precisely, the spatial and semantic properties of the scene refer to the fact that you can anticipate where a particular object will be found. For example, you would expect to see a stapler on a desk and not on the floor. Further, there is a difference in the distribution of fixations and their durations based on whether the individual is trying to memorise or scan the scene (Henderson, 2003). Short, sparse and highly distributed fixations are observed for scanning and frequent, long and clustered fixations are observed for memorisation.
The control of when and where a fixation will occur involves coordination of information from several areas of the brain. In a general sense, we can say that it is the oculomotor system that oversees the process of directing the fovea to particular regions of interest. To accomplish this task, six different control systems are involved, which are grouped into two classes of gaze control mechanisms; intentional gaze shifting mechanisms and reflex gaze stabilizing mechanisms (Schwarz & Schmückle, 2002).
The gaze shifting mechanisms include saccadic movements, smooth pursuit movement, and vergence movement. Saccadic eye movements are the rapid ballistic
17
Literature Survey
 movements that move the fovea to another point of fixation (Purves et al., 2001). Pursuit eye movements keep the fovea on a moving target and are slower than saccades (Purves et al., 2001). Finally, vergence eye movements change the orientation of the eyes in accordance with the distance from which a target is being viewed. That is, the eyes rotate toward the nose when looking a close target.
The gaze stabilising mechanisms include vestibular eye movements and optokinetic eye movements (Schwarz & Schmückle, 2002). Vestibular eye movements are rotations of the eye produced in order to maintain vision in the same direction when there are head and body movements (Purves et al., 2001). Optokinetic eye movements are the combination of saccade and pursuit eye movements. These eye movements are seen when the observed target is moving fast across the visual field.
The fixations are characterised by the relative stillness of the eye to take in visual information. Although fixations are characterised by suppression of gaze shifting eye movements, the eye actually never stays completely still. This is due to three types of small eye movements: tremors, drifts, and microsaccades (Martinez-Conde, 2006). The eye constantly tremors; these are the smallest of any eye movements and are hard to record (Martinez-Conde, 2006). Drifts and microsaccades are larger movements, but are still quite small. Drifts appear to be random and caused by instability of the oculomotor system (Martinez-Conde, 2006). Microsaccades, similar to saccades, are jerking motions. They are differentiated from saccades as being the movements that happen whilst you are fixating. These small movements are usually regarded as noise as it is the larger eye movements that are of importance, especially in reading.
The main types of eye movements to consider when investigating reading are saccades, fixations and regressions. Saccades4 are high velocity ballistic movements of the eyes. At the end of a saccade the eye stays relatively still for a period of time; (a fixation) and is the only point during reading that visual information is encoded. Since visual information is taken in during fixations, there is often a focus on analysis of fixations, in particular the duration and location. No visual information is taken in during saccades under normal reading conditions (Underwood & Batt, 1996). However, they cannot be discounted, as lexical processing occurs during saccades (Yatabe et al., 2009) and that during long saccades, readers perform more lexical processing than during short saccades.
Saccades are motor movements and therefore require time to plan and execute. Saccade latency is the period associated with making a saccade (Rayner, 1998). Saccade latency still exists even if uncertainty about where and when to move the eyes is eliminated so saccade programming is believed to be done in parallel with comprehension processes during reading (Rayner, 1998). Engbert and Kleigl (2001) found that initiations of saccades are not completely driven by lexical processing and that in fact saccades can be autonomous (with foveal inhibition).
4 Saccade is French for jump 18

Literature Survey
 2.2.4 Pupillometry
Movements are not the only source of information about cognitive processes that can be gathered from eye tracking. Pupil dilation provides abundant information about the cognitive state of the person. Experiments have shown that pupil size correlates to cognitive load or effort, where the pupil dilates further as effort increases and constricts as it decreases (Kahneman & Beatty, 1966). Pupil dilation is a good indicator of effort (Kahneman, 1973; Pomplun & Sunkara, 2003) and as a result pupil dilation has become widely used as an involuntary indicator of mental effort and cognitive load.
Whilst pupil dilation as an indicator for mental effort was first described by Hess and Polt (1964), it has been greatly studied after popularisation by (Kahneman & Beatty, 1966). The correlation between cognitive load and pupil dilation has been confirmed in many contexts since including assessing task difficulty in response preparation (Moresi et al., 2008), software development (Fritz et al., 2014), listening comprehension (Engelhardt et al., 2010; Zekveld et al., 2014), as well as to detect decision to change task (Katidioti et al., 2014) and difficulties in making decision (Satterthwaite et al., 2007).
The rest of this section is concerned with outlining how pupil response can be used as an indicator of learning and in terms of HCI. Work on pupillary response as an index of learning (Sibley et al., 2011) show that pupil diameter drops as participants learn tasks reflecting decreased effort required in performing that task and increases at the beginning of another level of difficulty. The implication is that pupillary response could be used to assess whether an individual has learned a task sufficiently or if they need more training. Further, the pupillary response could be used to speed up and slow down training procedures by judging the rate at which the individual is learning. This is significant due to the implications of the use of pupil response in adaptive learning and training environments - a major aspect of this research.
Pupil response could therefore be used as an accurate indicator of task difficulty (Iqbal & Bailey, 2004; Pomplun & Sunkara, 2003; Zekveld et al., 2014). However, the averaged value of pupil response ignores the effects of the fluctuations of pupil dilation due to lower and higher loads of mental effort within tasks (Iqbal & Bailey, 2004). Pupil dilation can therefore be used to measure the changes in workload during a task (Iqbal et al., 2005). The pupil is seen to increase is dilation during a subtask but decrease when the subtask is finished. These results have interesting implications on our current research, as averaging of pupil response cannot be considered a viable measure in assessments of reading comprehension. This differs from the eye movements where commonly averaged numbers of fixations, saccade lengths, etc., are used as measures.
2.3 Reading
Language is one of the important characteristics that have set humans apart from any other organism on the planet. This extensive communication device has enabled
19
Literature Survey
 intricate social behaviour that has seen the human race flourish. Along with tool making and teaching, humans have become masters of invention. Amongst the greatest of these inventions is that of writing and reading language, and is indeed a very important part of human behaviour.
When a human reads, the eyes quickly and almost unconsciously move to acquire the text on display so that the brain can piece them all together and make logical sense out of it. Reading requires numerous cognitive processes to work together including visual information processing, word recognition, attention, language processing, and oculomotor control.
Up to 30% of Australian children have difficulty learning to read even with normal schooling (Burton et al., 2009). The process of learning to read is less natural than learning to speak, as written language is a much later addition to spoken language. Reading requires complex interpretation of symbols in order to derive meaning from them. This is termed comprehension and is the main objective of reading. Proficient readers quickly and unconsciously recognise words; if a word is not familiar it requires more cognitive processing in order to discern the meaning of the word. Reading, therefore, requires continuous education to ensure this processing time is minimised.
2.3.1 HumanLanguage
Language is a complex communication system. Humans created written language as a way to communicate through time and space. Before reviewing written language, which is the foundation of reading, there is a short discussion about human language to present the foundations of language. A language system is made up of a set of symbols, sounds, meanings (semantics), rules (syntax) and interpretation (pragmatics). Language can be conceptualised in a hierarchical structure, consisting of basic elements at the lowest level called phonemes. Phonemes are the smallest elements of sound that form coherent speech such as how vowels and consonants are pronounced in English. Phonemes make up morphemes, which are the smallest units of meanings, e.g. words. Morphemes make up phrases that in turn combine with more words to make up sentences. The rules that govern these combinations are called syntax. Syntax is a part of grammar, which is the system of generating correctly structured expressions. Semantics is used alongside syntax to understand meaning of expressions (Burton et al., 2009). Semantics are the rules behind the meanings of the morphemes, words, phrases and sentences. Language is generative and diverse, allowing humans to express themselves in a potentially infinite number of ways. From a finite set of elements that make up language (phonemes) a very large number of words, phrases and sentences can be generated. Human languages are forever growing, changing and evolving to humans needs.
2.3.2 WrittenLanguage
Human language developed at least 45,000 years ago whereas written language only occurred about 3500 BC (Barton, 2007; Burton et al., 2009). Children very easily
20
Literature Survey
 learn to speak a language but must be taught how to read and write. Writing systems were built on to spoken language and have evolved according to the neural network capabilities of the human brain (Dehaene, 2009). More precisely the human brain allows neuronal recycling so that new activities can be learnt by humans to deal with new or changing situations. This means that new activities are constrained by the limits of the brain structures. Reading and written language are as mentioned very recent in human history. Neither our eyes nor brains have evolved to read or write, instead it is our reading and writing system that have developed according to the constraints imposed by our visual system and brains’ capabilities (Dehaene, 2009).
Writing was invented independently in three different areas; the Fertile Crescent of Mesopotamia and Egypt, China, and pre-Columbian America (Barton, 2007). Cuneiform is the earliest known writing system, which can be dated back to about 3500 BC in Mesopotamia. There are now numerous writing systems, however, they share much in common because they are limited by the same brain structures (Dehaene, 2009). Characters from all writing systems have visual features that rely on basic shapes that provide optimal contrast of contours on the retina.
2.3.3 EyeMovementduringReading
We have already discussed the types of eye movements in the preceding sections. This section discusses the movements in the context of reading and is broken down into three subsections; types of eye movements observed during reading (saccades, fixations and regressions) and the reasons for why these movements are observed, perceptual span and parafoveal preview, and finally, where and when fixations and saccades occur.
2.3.3.1 Reading and the Fovea: Why and What
The human brain and eyes have not evolved to read. Instead written language has been constrained by the anatomy of our eyes and brains (Dehaene, 2009). The fovea, which is essential in the reading process (Rayner & Bertera, 1979), developed an extremely long time before language in general was even conceived. It is the fovea and paraforveal regions that are critical for reading (Rayner & Bertera, 1979; Rayner & McConkie, 1976). Masking foveal and parafoveal vision have showed this experimentally (Rayner & Bertera, 1979). The results showed that although participants could see words, they would get the words wrong in the sentence. Longer fixation times were recorded when vision was masked and reading time increased. The larger the mask, the greater the percentage of words incorrectly identified. Even though the participants were aware that words were in the parafovea and peripheral view they could not report what the words were. Masking of the fovea resulted in severe reading difficulties compared to masking of only the parafovea. Rayner & McConkie (1976) concluded that information necessary for meaningful identification of a word is obtained from the fovea and near parafovea. Additionally, information such as that used to guide the eye to the next location in the text is collected by the parafovea.
21
Literature Survey
 The further a word is presented from the fovea, the greater the decrease in ability to identify that word (Rayner & McConkie, 1976). The eye must move frequently in order to orient the fovea for highly detailed vision in different places to assimilate information. This is why the eyes do not move in a smooth pattern taking in a constant amount of information. Contrary to what may be assumed, the eyes do not necessarily move from left to right, line by line. Therefore the eye movements generated during the reading process can be quantified into measures that can be used to infer cognitive processes that are required for reading. The variability of these measures then reflects real time processing (Rayner, 1998).
Generally when reading English, fixation duration is around 200-300 milliseconds, with a range of 100-500 milliseconds and saccadic movement is between 1 and 15 characters with an average of 7-9 characters (Liversedge & Findlay, 2000). The majority of saccades are to transport the eye forward in the text when reading English, however, a proficient reader exhibits backward saccades to previously read words or lines about 10-15% of the time (Rayner, 1998). These backward saccades are termed regressions. Short regressions can occur within words or a few words back and may be due to problems in processing the currently fixated word, overshoots in saccades, or oculomotor errors (Rayner, 1998). However, longer regressions occur because of comprehension difficulties, so the reader tends to send their eyes back to the part of the text that caused the difficulty (Rayner, 1998). Frazier and Rayner (1982) demonstrated this elegantly by showing that when readers were subjected to garden-path sentences regressions could be systematically induced. A garden path sentence5 is designed in such a way to mislead the reader into incorrectly interpret the sentence though the sentence is grammatically correct. They are used in psycholinguistics to illustrate the fact that during reading, humans process language one word at a time. Readers make regressions back to the point of difficulty and then re-interpret the sentence (Frazier & Rayner, 1982).
2.3.3.2 Reading and the Parafovea: Perceptual Span and Parafoveal Preview
As mentioned earlier, the parafoveal region includes 10° of the visual field around the point of fixation. Although the parafovea is not responsible for fine detail vision it plays an important role in reading. In this subsection we will discuss two of these roles in regards to reading: perceptual span and the effect of parafoveal preview.
The perceptual span in reading is the region of the visual field where visual information is encoded. In previous sections we have distinguished between the areas of the visual field: foveal, parafoveal and peripheral. It is established that the foveal region is where fine detail vision is encoded, but the parafoveal region also encodes visual information that is useful, though in less detail then the foveal region. Information from the parafoveal region is gathered on most fixations (Rayner, 1998). An approach used to assess the perceptual span in reading is using a
5 An example of a garden path sentence is “The old man the boat.” 22

Literature Survey
 gaze-contingency paradigm6 technique called the moving window paradigm (McConkie & Rayner, 1975). In this paradigm, the experimenter controls the visual information available at each fixation as the text within a defined window is distorted in some way (see Figure 2.4). Although the reader is free to look anywhere, the letters outside of a window spanning a given number of character spaces are distorted. It is possible to determine the perceptual span of the reader by changing the size of the window and making its location dependent on where the reader is looking. A notable observation from experiments using this technique is that given the correct window size and properly functioning equipment, participants are not aware of the changed text outside of the window (Pollatsek & Rayner, 2009).
Figure 2.4. Figure 2 from (Rayner, 1998) examples of the moving window paradigm
Studies have shown that when the window extends 14-15 character spaces to the right of the fixation point readers can read alphabetic text, such as English, without disturbance (McConkie & Rayner, 1975; Rayner & Bertera, 1979). The same has been shown for the window extending just 3-4 characters spaces to the left of the fixation point (Rayner & McConkie, 1976). This observation shows that there is an asymmetry to the perceptual span, which is most likely language specific as in English readers read from left to right and in other languages where the direction of reading is different, so too is the perceptual span (Rayner, 1998; Reichle et al., 2003). However it is important to note that word encoding does not occur outside of 7-8 characters to the right of the fixation, only information about letter shape and word length is taken in (Rayner, 1998; Reichle et al., 2003).
This leads to the next point of discussion: parafoveal preview. Studies have shown that even before a word is fixated upon, orthographic and phonological processing already has begun (Rayner, 1998; Reichle et al., 2003). The parafoveal preview of a word can therefore decrease subsequent fixation duration of a word. This is seen most prominently by the observation that even when a word has not been fixated upon it is still processed (Rayner, 1998). This was demonstrated in a
6 Gaze-contingency
   techniques are where the display on a computer screen is changed as function of
 where the viewer is looking.
23
Literature Survey
 study where an initial group of participants had their eye gaze monitored as they read text. Another group read the same text with the words that the first group skipped removed. The study showed that the second group had difficulty understanding the modified text (Rayner, 1998). Word skipping can therefore be induced by parafoveal information that allows the word to be identified when in the parafoveal visual region.
The effects of parafoveal preview are seen most prominently in the observation that predictable words are skipped more than unpredictable words and that short function words are skipped more than content words. The effect of the parafoveal preview is diminished when processing of the fixated word is difficult. There is some support of this in that durations of the fixations before and after a skip are longer (Rayner, 1998; Reichle et al., 2003).
2.3.3.3 Eye movements: Where and When
Where the eyes move is based largely on low-level visual information such as word length and spacing between words. When the eyes move, the movement is believed to be largely based upon lexical processing. That is, until lexical processing has concluded the eye will not move to the next word. Discussion in the previous subsection has already introduced the fact that fixations are not evenly distributed over the words in the text read. Firstly, not all words are fixated upon during reading, with many being skipped. Secondly, some words are fixated upon more than once. Interestingly, content words are fixated upon 85% of the time and function words are only fixated upon about 35% of the time (Rayner, 1998). Rayner and McConkie (1976) found that there is a relationship between the probability of fixating upon a word and its length, so as word length increases so too does the probability of fixation. Since function words are usually short words this is one explanation for why they are fixated upon less than content words.
Word length is also useful in determining where in the word a fixation will occur. The first fixation on a word has been shown to, in general, be between the beginning and the middle of the word (McConkie et al., 1988, 1989; O'Regan, 1981). This was termed the preferred viewing location. Later, the optimal viewing position of a word was defined as the location in a word at which recognition time is minimised. The optimal viewing position is closer to the centre of the word. The likelihood of re-fixation increases as the fixations become further away from the optimal viewing position, which is termed the re-fixation effect (O'Regan, 1984). However, as word length increases, first fixations tend to occur near to the beginning of the word and a second fixation will occur toward the end of the word.
The length of the fixated word and the word to the right of the fixation influence saccade length. The fixation position on a word is dependent on the fixation position on the currently fixated word. When readers have no information about where the spaces are between upcoming words, saccade length decreases and reading is slowed considerably (McConkie & Rayner, 1975).
Linguistic processing also has a great bearing on how long a fixation is on a particular word. Evidence for this is that low frequency words are fixated upon for
24
Literature Survey
 longer that high frequency words. In addition to this, numerous studies have shown that predictable words are more likely to be skipped than unpredictable words and fixations are more likely to occur on low frequency words (Rayner, 1998). Furthermore, longer fixations have been observed for misspelled words (Rayner, 1998), which in essence is a product of word frequency and word predictability.
The effect of the text and the presentation of the text can be observed in eye movements. Factors such as the quality of the print, line length, and letter spacing influence eye movements (Rayner, 1998). The format in which text is presented in terms of length can have effect on eye movements observed during reading. Sharmin et al. (2012) showed that by altering text presentation length from paragraphs, to individual sentences and to line-by-line presentation of text that fit a computer screen, made a considerable effect on fixation duration, number of fixations per minute and number of regressions. Furthermore, as text becomes more difficult to understand, an increase in fixation duration is seen along with decreases in saccade length and increased frequency of regressions (Rayner et al., 2006).
2.3.4 EyeMovementMeasures
Eye tracking produces a considerable amount of data. As established in the previous subsections, the basic units of analysis when considering eye gaze data are fixations and saccades. Converting eye gaze data into fixation points first reduces the eye gaze data. However, the fixation data is still quite large and fixations and saccades, alone, tell us very little about the nature of reading behaviour. Eye movement measures are a way to reduce the amount of data to investigate the nature of reading. There are many commonly used measures, which are described in Table 2.1. Typically word-based measures are used, especially when investigating lexical access and syntactic parsing. Eye movement measures are used to identify different patterns in the data to tell us different facts about the data. An eye movement measure is a form of description about the fixations and saccades that are observed for given section of text, which may be a word, sentence, paragraph, etc.
When considering global text processing, recognition of individual words that have been read is not appropriate for assessing comprehension. This is because global text comprehension not only involves the assimilation of words in individual sentences to form a conceptual meaning and build relationships between sentences in the text. Listed in Table 2.1 are eye movement measures that can possibly be used in assessing global text processing. These measures include eye movement matrices and regional gaze duration.
The majority of the research using eye gaze to analyse reading behaviour is on small units of text such as words, phrases, or sentences for the purpose of studying lexical access and syntactic parsing (Hyona et al., 2003). Models such as the E-Z reader model (Reichle et al., 1998; Reichle et al., 2006; Reichle et al., 1999, 2003, 2012; Reichle et al., 2009) are used for local processing analysis. However, in real life situations, in particular HCI situations, often the text being read is quite a lot longer, being paragraphs, articles, books, etc. When analysing the comprehension of such long pieces of text, global text processing must be assessed. Global text processing is
25
Literature Survey
 where relationships are identified and symbolised in constructing a mental model of the meaning of the text (Hyona et al., 2003). The relationship not only spans sentences, but also paragraphs. This type of text processing requires recall of not just working memory but short-term memory, and often long term memory.
Table 2.1. Eye movement measures
Measure       Definition
Number of fixations       The number of fixations can be affected by the reading behaviour, text difficulty, and reading skill (Rayner,
   Average fixation duration
Average forward saccade length
Regression ratio
Average Regression Length
Coherently read text length
Thorough reading ratio
Total fixation time
1998).
The sum of the durations of all fixations on a paragraph divided by the number of fixations on that paragraph. This measure has been used to predict reading comprehension (Underwood et al., 1990).
The average length of the left to right saccades. Saccade length is affected by characteristics of the text (Rayner, 1998).
The number of regressions divided by the total number of saccades on a paragraph. There is evidence that when reading harder text more regressions are observed (Rayner et al., 2006).
The average length of regressions. Regressions are affected by text complexity and inconsistencies in the text (Rayner et al., 2006).
The length of text in characters that has been read without skipping any text in between according to the reading detection algorithm (Buscher et al., 2012). Used in assessing whether users find a piece of text relevant or irrelevant. However, the assumption for this thesis is that the longer the length of text read the more likely that the text has been understood.
The length of text that has been detected as read by a reading detection algorithm divided by the length of read or skimmed text (Buscher et al., 2012).
Sum of all fixations on complete text. This measure is useful in global text processing analysis because this measures immediate as well as delayed effects of comprehension (Hyona et al., 2003).
     2.3.5 ReadingComprehension
Reading comprehension is the capacity to make sense from written language. In alphabetic languages such as English, this requires assimilating symbols to make them into words and then sentences, and deducing meaning of the bigger picture. Simply looking at the alphabetic symbols on a page, electronic display, packaging, etc. involves little cognitive ability as our eyes have evolved to take in fine detail
26
Literature Survey
 information (see section 2.2). Interpreting these symbols requires somewhat more skill. This involves knowledge of the alphabet system and language so that individual letters can be recognised as words, that is, identification of the orthographic form of a word and lexical processing of that word to identify phonological and/or semantic forms. However, the individual words and letters often have little importance on their own; it is their combination that carries value including meaning (Snow, 2002; Underwood & Batt, 1996). Reading for the most part requires making inferences both locally and globally in the text and conceptualising the ideas expressed in the text. The reader then incorporates their knowledge and experience to build a model of the ideas being expressed within the text (Kintsch & Rawson, 2005). This is why reading comprehension is dependent on many different variables, not just knowledge and experience but also motivation and context.
Reading comprehension is a skill that must be taught and requires constant education. It is a skill that requires making relationships between not only the words in a sentence but also in the multiple sentences and paragraphs by concurrently finding and forming meaning from what has been read (Snow, 2002; Underwood & Batt, 1996). When understanding language, we integrate ideas in the text and form a mental model that is an abstraction of the conglomeration of ideas (Kintsch & Rawson, 2005; Underwood & Batt, 1996). The actual text read is not remembered verbatim, it is the ideas and constructed representation that are remembered (Bransford & Franks, 1971; Kintsch & Rawson, 2005; Underwood & Batt, 1996).
One of the main parts of reading comprehension is inference. This is often referred to in terms of inferring co-reference but can also be inferring meaning from context. Words can often have multiple meanings (lexical ambiguity e.g. bank, right) and multiple words that are spelt differently and mean different things have the same sound (phonological ambiguity, e.g. homophones such as to, too and two). Furthermore, phrases and sentences can have different meanings (syntactic ambiguity). An example of this is the sentence "Visiting relatives can be boring". This sentence is ambiguous because it can both be interpreted as relatives that have come to visit are boring or the act of going to visit relatives is boring. .
In general, semantically and phonologically ambiguous words are resolved by context. This is more prominent with semantic ambiguity where the meaning of the word is derived from the context from the sentence, or prior sentences. Of course this may not always be the case, where both meanings may be equally probable, and it has been found that the more prominent meaning of the word is usually inferred. If the inference is wrong then the reader often directs his eyes back to the word to re-interpret (Frazier & Rayner, 1982).
Reading comprehension involves several levels of processing (Kintsch & Rawson, 2005). The first and most basic level has just been described; this is the linguistic level where word recognition and parsing occurs. The next level is to derive meaning from the text, the semantic analysis of the text, which requires inference. A classic example of inference is anaphoric co-reference, where basic
27
Literature Survey
 inference is required to resolve quite simple meaning. An example of this is the sentence: “The carnation won a prize. It was the best flower in the show.” (Underwood & Batt, 1996). These are two simple sentences that are easy to understand. However, the two sentences only make sense in combination when the conclusion is made about the pronoun “it” refers to the carnation in the first sentence. Resolution of anaphoric references is fundamental in sentence comprehension.
The example of an anaphor given above is simple, however,
Linguistic factors include context such as gender information, for example, from (Kintsch & Rawson, 2005): "Leonard handed Michael a sandwich. Then he passed Carla an apple. Then Carla passed him an apple.” In
this sentence is it
Semantic factors that affect anaphor resolution include implicit causality of verbs. Take for example the two sentences: “John questioned Chris because he wanted the correct answers. John praised Chris because he knew the correct answers.” Here the verb question in the first sentence implies that “he” refers to John, in contrast with the second sentence where the verb praise implies that “he” refers to Chris. Another factor is pragmatic plausibility, where contextual information has implications on the interpretation of anaphors. For example: “Scott stood watching while Henry fell down some stairs. He ran for a doctor.” Here, the referent “he” is most likely to be Scott because it is less plausible that Henry, who has just fallen down the stairs, is able to run to find a doctor.
These are basic relationships and are local in effect. Sections of text are often related as well, which requires recognition of global interrelationships. It is the combination of the local and global relationships within the text that represent the meaning of the text. This requires identifying important themes or topics in a text. The ideas expressed in the text are tied together, which results in a meaningful interpretation of several sentences. This involves abstraction of ideas and their integration into an overall mental model of the text. So ideas are not kept in isolation, but integrated together to form general meaning. In fact, the original piece of text is most likely not remembered verbatim and instead the abstract ideas are remembered in the mental model.
This has elegantly been shown experimentally by Bransford and Franks (1971) who presented several sentences to participants. Following the first presentation, another set of sentences is shown to the participants and they are asked if the sentences in the second presentation had been presented in the first presentation as well. The interesting part of this study is that the first set of sentences only contained singular ideas that where semantically related so could be linked together to form an overall coherent idea. In the presentation of the second set of sentences, the participants were more likely to say that they had been presented with sentences that incorporated all of the ideas, even though they never had been.
  pragmatic information.
 28
more likely that the pronoun “he” in the second sentence is
there are many
 factors that can affect how anaphors are resolved, such as linguistic, semantic, and
 resolved to refer to Leonard and the pronoun “him” in the last sentence is resolved
 to refer to Michael?
Literature Survey
 2.3.5.1 Testing Comprehension
Reading comprehension is not a straightforward quantity or characteristic that can be measured because it is not an explicit process that the brain performs. It is the product of a number of cognitive processes, including visual processing, lexical processing, linguistic and semantic processing, high-level integration of concepts, memory, and reasoning. It is the products of these processes that observed and from which assumptions about reading comprehension must be made.
Unfortunately, this is not the only reason why reading comprehension is hard to measure. A person’s understanding of text is reliant on many factors including their overall cognitive capabilities (intelligence), motivation, knowledge, experiences, and even the purpose of reading (Snow, 2002). Variations in any of these factors can attribute to different measurements of comprehension. Both the text and the type of assessment should be considered when making conclusions about reading comprehension (Fletcher, 2006). This section begins with an example to highlight a critical problem that must be addressed when testing for comprehension.
In a study that examined speed-reading, normal readers and speed-readers were required to read a text and then answer a reading comprehension test. It was found that normal readers had a higher percentage of questions answered correctly compared to the speed-readers, 72% to 68% respectively (Crowder & Wagner, 1992). However, the interesting part of this example is that the same comprehension test was given to individuals who had not even read the text. These individuals managed to on average correctly answer 57% of questions on the test. Guessing and common sense was enough for the participants to pass the comprehension test. Although the point of this study was to show that speed-readers underperform in reading comprehension compared to normal readers, this example serves to highlight the necessity of appropriately testing comprehension. The remainder of this section will discuss current methods for measuring reading comprehension as well as the advantages and disadvantages of each method.
Typical informal7 methods for assessing reading comprehension include: question-answer tests, recall procedures, oral passage reading measures, and cloze8 techniques (Fuchs et al., 1988). These are all relatively simple to construct and can be tailored to the purpose of the teacher or experimenter. Usually one method of assessment (e.g. multiple choice, cloze, etc.) is used in assessing reading comprehension making the assessments one-dimensional. However, this is often not sufficient to accurately assess comprehension in reading. Cutting and Scarborough (2006) showed that results from different assessment methods produced differing levels of comprehension for the same material. Their results suggest that commonly used tests of reading comprehension may not require the same cognitive processes to complete. Keenan et al. (2008) demonstrated similar results in testing different
7 As opposed to standardised tests, informal methods can be constructed by a teacher or experimenter within their own bounds and are more flexible in what is tested.
8 Cloze techniques refer to methods where words are deleted from text and replaced with blanks. Students then insert words into the blank spaces to complete and construct meaning from the text. This procedure can be used as a diagnostic reading assessment technique. Also, note that this is used as a method of calculating predictability of words in context.
   29
Literature Survey
 standard reading comprehension tests and found that even though the tests were supposedly measuring the same outcome, the results from each test were only modestly correlated to the others. Furthermore, Francis et al. (2006) showed that any single, one-dimensional attempt to assess reading comprehension is inherently imperfect. This is because only parts of the comprehension process can be observed from which conclusions are made, so inherently these conclusions may not be representative of the true quality of the comprehension. In brief, this illustrates that the method of assessment used to evaluate reading comprehension can be a determinant of the conclusions drawn.
Further to this point, the difficulty of text as well as its characteristics (e.g. semantic, syntactic) can play a large role in whether an individual will understand it or not. Therefore, the text plays a key role in determining level of comprehension (Fletcher, 2006). This is demonstrated in the eye movement study by (Rayner et al., 2006) and the attempt to minimise the role of certain text characteristics in Francis et al. (2006). It is important to understand that text variability is a determinant of the inferences made about reading comprehension.
2.3.5.1.1 Standardised Reading Comprehension Tests
A common standardised test for reading comprehension is the Critical Reading section of the Stanford Achievement Test Series (SAT). The SAT is a widely used achievement test set in the American schooling system. It is used to measure academic knowledge of elementary and secondary school students. The whole set of test covers subjects including mathematics, science, social science, spelling, listening comprehension, and importantly for this analysis, reading comprehension.
Another standardised achievement test is the Wechsler Individual Achievement Test Second Edition (WIAT-II). This test is different from the SAT in that is can be used to assess academic achievement of children right through to adults (ages 4 to 85), where SAT is designed for school students. WIAT-II is used to assess the four general areas of: Reading, Math, Writing, and Oral Language. The reading comprehension subtests include: matching a written word with its representative picture, reading passages and answering content questions, and reading short sentences aloud, and responding to comprehensive questions.
Other standardised tests of reading comprehension include: Passage Comprehension from Woodcock-Johnson-III, Diagnostic Assessment of Reading Comprehension (DARC) (which are both analysed in (Francis et al., 2006)), the Gates-MacGinitie reading test, and the Gray Oral Reading test (which are both analysed in (Cutting & Scarborough, 2006)).
The materials for these standardised achievement tests are not openly available and must be purchased. Furthermore, they cannot be altered to fit a specific situation, such as those found in experiments. The main reason for listing some standardised tests is to show that there are many ways to assess reading comprehension and no particular way may be better than another. In fact, Cutting and Scarborough (2006) showed that individual tests vary in assessment of different measures of comprehension.
30
Literature Survey
 2.3.5.2 Eye Movements and Comprehension
Eye movements can be used to understand the on-going cognitive processes that occur during reading (Rayner, 1998). Models for reading based on the premise that lexical processing is driving eye movements have been built on these findings, such as the E-Z Reader (Reichle et al., 1998; Reichle et al., 2006; Reichle et al., 1999, 2003; Reichle et al., 2009) and SWIFT model (Engbert et al., 2002; Engbert et al., 2005). These models serve as default models for the reading process where lexical processing is assumed to drive eye movements. However, lexical processing is not the only factor that affects eye movements; comprehension of the text can have significant effects on the eye movements observed.
As a number of studies have shown, there are numerous variables that are largely based around comprehension functions that can have influence on eye movements during reading. The variables include: semantic relationships between words, anaphora and co-reference, lexical ambiguity, phonological ambiguity, discourse factors and stylistic conventions, and syntactic disambiguation (Rayner, 1998). These variables have different effects on eye movement that cause them to deviate from the default reading process. For example, garden-path sentences are syntactically ambiguous and induce regressions to resolve the comprehension problems (Frazier & Rayner, 1982). Amongst these findings it is believed that as text becomes increasingly hard to understand, fixation duration and number of regressions is observed to increase along with shorter saccades observed. This is due to the fact that higher order comprehension processes supersede the default reading process.
Indeed, experimental results show that eye movements reflect text difficulty (Rayner et al., 2006). As the difficulty in comprehending text increases so too does average fixation duration, the number of fixations and the total time taken to read the text (Rayner et al., 2006). Furthermore, this study showed that there is a higher probability of regressions when text was difficult. It is important to note that the text difficulty in this experiment was assessed independently by a group of students who had to rate the passages between 1 and 10. The authors note that although there was some correlation between poor comprehension and text difficulty, it was not statistically significant. The method of testing comprehension was not specified. Although there was no statistically significant correlation between text difficulty and comprehension, there was no mention of whether there were correlations between eye movement measures and comprehension. Nevertheless, the results from the Rayner et al. (2006) study confirm that eye movements are affected by overall text difficulty and that regressive eye movements can indicate comprehension failures.
Eye movements can reveal much about readers such as when readers encounter difficulties in reading (Frazier & Rayner, 1982), and text incomprehension (Okoso et al., 2015). Furthermore, fixation duration has been shown to be a predictor of reading comprehension (Underwood et al., 1990). Yet, the task of predicting quantified measures of reading comprehension has been attempted with poor results (Copeland et al., 2014b; Martínez-Gómez & Aizawa, 2014).
31
Literature Survey
 2.4 Eye Tracking
The discussion so far has been on eye movements, but we have not discussed how such movements are recorded. This section provides a brief overview of how some eye-tracking systems work followed by a discussion of some of the challenges faced dealing with this type of data. Put simply, an eye tracker is a device that captures eye position at regular time intervals to give estimates of where a person’s gaze is (Morimoto & Mimica, 2005; Poole & Ball, 2005). Eye tracking is a relatively recent technology; the first being invented by Edmund Huey, for the purpose of reading analysis, and whose results were published in the late 1960’s (Huey, 1968). This tracker, along with many early eye trackers, was intrusive and designed specifically for scientific research (Morimoto & Mimica, 2005). It involved the use of contact lenses that are embedded with a device such as a magnetic field sensor to estimate the person’s eye gaze. These systems are very accurate but expensive and invasive (Morimoto & Mimica, 2005).
Figure 2.5. Pupil and corneal reflection are tracked with camera-based eye tracking to estimate eye gaze. Image take from (Poole & Ball, 2005).
At present, camera based eye trackers are most often used commercially. These types of eye trackers are usually non-intrusive to the user where a camera and light source are situated in front of the person whose gaze is being tracked (Poole & Ball, 2005). The person is usually not restricted in any way. However, sometimes to increase accuracy of tracking head mounted cameras are used (Morimoto & Mimica, 2005). Camera based eye trackers track at least one feature of the eye, such as the corneal reflection method. Many camera based eye trackers at present use light, usually infrared, to track the pupil and corneal reflection, as shown in Figure 2.5 (Goldberg & Wichansky, 2003; Morimoto & Mimica, 2005). An infrared light is targeted at the eye that generates a reflection off the surface of the eye and causes the pupil to appear as a bright disk (Poole & Ball, 2005). This is because the pupil reflects almost all of the infrared light. A camera can then be used to capture images of the eye and then the information is used to determine the eye rotation. However, most current camera based eye trackers are based on pupil-corneal reflection (Morimoto & Mimica, 2005). Corneal reflection is a glint on the cornea surface, also referred to as the first Purkinje image. The corneal reflection and the centre of the pupil are used to track the eye and determine where gaze is directed (Morimoto & Mimica, 2005). Specialised image processing software is needed to generate such results (Poole & Ball, 2005). Camera based tracking systems have to be calibrated to the participants’ eyes for every session and potentially multiple times per session (Goldberg & Wichansky, 2003). This requires the participant to watch a dot appear
 32
Literature Survey
 in several different locations of the screen. An example of this is shown in Figure 2.6.
 Figure 2.6. Example of calibration screen for “The Eye Tribe” eye tracker. Image taken from The Eye Tribe website: http://dev.theeyetribe.com/start/.
Head mounted eye tracking is useful for situations where the participant needs to move around the environment (Goldberg & Wichansky, 2003). However remote eye tracking is often used to study onscreen eye motion. This method has the disadvantage of requiring the participant to stay relatively still and often can be quite susceptible to equipment error or noise (Hornof & Halverson, 2002).
Recently remote camera based eye trackers have become inexpensive. Examples are the Tobii EyeX9 and The Eye Tribe10 which at the time of this writing were $USD 139 and $USD 99, respectively. The accuracy of the Eye Tribe is 0.5° – 1° which means it is able of determining the on-screen gaze position with only a 10mm error. This device is small, as shown in Figure 2.7, and attachable to tablet devices. These affordable devices expand the use of eye tracking from research or commercial use to the wider community.
 Figure 2.7. The Eye Tribe eye tracker. Image taken from https://theeyetribe.com/order/ Last accessed: 27th January 2016
Remote eye trackers can also be used to measure pupil diameter (Klingner et al., 2008). In the fields of psychology and cognitive science many studies performed on pupillary response use specialist pupillometry systems. However, eye trackers are
9 http://www.tobii.com/en/eye-experience/eyex/ Last accessed: 22nd August 2015 10 http://theeyetribe.com/ Last accessed: 22nd August 2015
 33
Literature Survey
 more readily available and inherently perform the task of tracking where the eye is positioned so it makes sense to use the eye trackers for recording pupil diameter as well. Klinger et al. (2008) demonstrated that classic pupillometry studies (Kahneman & Beatty, 1966) performed with specialist pupillometry equipment could be replicated with a video based remote eye tracker.
The output of eye trackers is time series data that is in the form of coordinates where the eye gaze position was captured at regular intervals (Goldberg & Wichansky, 2003). This is usually in x-y coordinates and a time stamp along with any other measurements that are requested by the experimenter such as pupil diameter. The following subsections will outline analysis of such data.
2.4.1 DealingwithError
Modelling eye movement patterns is challenging. To add to this problem, analysis of fixation locations is often complex for several reasons: equipment noise, user variability and the size of the data set. The gaze data is often cleaned up and inference must be made about where fixations actually occur subsequent to data collection. First, we will consider the issues of eye tracking inaccuracies, which can be due to: inaccuracy of the equipment; the participants moving in front of the tracker causing drift from calibration; or simply that the participants eyes are hard to track (Hyrskykari, 2006). There are methods for adjusting and recalibrating the eye tracker during use such as the use of implicit required fixation locations (RFLs) (Hornof & Halverson, 2002). Implicit RFLs are locations on a screen that a participant must look at as part of a task and therefore provide a location from which the eye gaze data can be recalibrated if deviation has been encountered. Other algorithms such as those presented by Hyrskykari (2006) are highly related to reading tasks and involve using lines of text as the locations where fixations are reference points for mapping of the gaze data. This algorithm is used in real time as part of a reading aid called iDict and allows for manual corrections to be made if the fixations are not mapped to the right words (Hyrskykari, 2006). This algorithm focuses more on the vertical disposition of gaze points rather than the horizontal disposition. For post-collection recalibration of data, inference about where the fixations should occur can use the same logic as the above examples of recalibration of eye gaze trackers during experimentation.
2.4.2 FixationIdentification
Eye tracking can result in large data sets from monitoring even quite short tasks. Trackers typically sample many times per second, such as 50 to 60Hz. This means that even for a 10-minute task sampling at 60Hz there will be 36,000 data points generated. Fixation and saccade identification is the first essential step to take when analysing eye gaze data and can reduce the data considerably (Salvucci & Goldberg, 2000). However, fixation identification can have a large impact on results (Jacob & Karn, 2003). There is no standard fixation identification algorithm in current use, (Salvucci & Goldberg, 2000) so it is hard to compare results regarding fixations across experiments that do not use the same algorithms or even the same parameters (Jacob & Karn, 2003).
34
Literature Survey
 To complicate things further, during fixations the eye does not stay completely still. The eye can make very small rapid movements, or occasional drifts and sometimes microsaccades to bring the eye back to the original position (Salvucci & Goldberg, 2000). These mean very little to high level analysis of eye movements such as in reading analysis. Nevertheless they can make it harder to establish when fixations begin and end. Poor fixation identification may result in too few or too many fixations being extracted from the sequence of gaze points, which could in turn have dramatic effects on observations and further analysis.
Salvucci & Goldberg (2000) performed a comparison study on fixation identification algorithms. They divided them into two characteristic groups, spatial and temporal. Spatial algorithms are based on the velocity of saccades or based on dispersion of gaze points. Alternatively, temporal algorithms are time sensitive. In this thesis we use the dispersion-threshold based identification (I-DT) algorithm that is described in generic terms by Salvucci & Goldberg (2000). This algorithm is straightforward to understand and implement, as it relies on the underlying nature of fixations and saccades. That is, when the eye fixates on an object in the visual field it remains relatively still. This means that eye gaze points that are in close proximity, for a specific time frame, are likely to make up a fixation. Gaze points that are sparse are therefore more likely to be part of a saccade. There is average fixation duration of about 200–250ms and a range from 100ms to over 500ms, so on average about 12 to 15 gaze points make up a fixation with a range of about 5 to 30 fixations in each trial. Since there is a minimum of the range a fixation duration, the I-DT algorithm uses a minimum duration threshold to ensure a fixation meets the duration criterion. To check for fixation, a moving window approach is used. The initial window is set to encompass the minimum duration threshold and then the dispersion of points within the window is checked. If the distance between points is less than the dispersion threshold, the window is expanded to enclose more gaze points until the dispersion threshold is reached. At this point the fixation is closed off and a new window is created. For each fixation that is identified, a centre point and encompassing diameter must be calculated.
2.4.3 InterpretationofEyeMovements
There are two difficulties faced when interpreting eye movements that are due to human visual physiology. These difficulties are incidental fixations and off centre fixations (Salvucci, 1999). Although this has less impact on analysis of reading eye gaze patterns, it is important to keep in mind whilst modelling the data.
Incidental fixations are fixations that are accidental; these types of fixations are not of much interest when looking at reading eye gaze patterns (Salvucci, 1999). Gaze points recorded by eye trackers can be off centre over visual targets. This creates off centre fixations. Also humans can fixate within 1° visual angle of the target and still encode information in the fovea. To add to this, eye trackers have a typical accuracy of approximately 1°. This adds to the problem of mapping user actions to user intentions based on eye movement (Salvucci, 1999). This is a problem in terms of analysis of reading eye gaze patterns because calibration needs to be done to bring the fixation points in line with what the participant is actually fixating
35
Literature Survey
 on. If the points are not brought in line with actual fixation points, there could be misinterpretation of the gaze patterns.
Interpretation of eye gaze data can take many different approaches and is often based upon the purpose of the original research and the area that the research falls under. For instance, in usability studies in HCI eye tracking is often used to measure relative visual attention and how users look at a specific stimulus. In this field, there are approaches taken that are considered top-down and bottom-up interpretation of the data (Jacob & Karn, 2003). The top-down approach can be either based on cognitive theory or a hypothesis about the design. Analysis of eye gaze is therefore based upon either a cognitive theory such as longer fixations imply difficulty interpreting the interface or observations that change of a design causes longer fixations and therefore is harder to understand and use (Jacob & Karn, 2003). The bottom-up approach is used when there are no hypotheses before recording the data and instead patterns in the data are found and extrapolations can be made from there.
Eye movement patterns can be quite different depending on what task is being performed. The task a person is performing can be predicted based on their eye movement (Iqbal & Bailey, 2004; Simola et al., 2008). Even in the general task of reading there are differences in eye movement patterns (Fahey, 2009; Gustavsson, 2010; Vo et al., 2010). In most cases, there appears to be a difference in eye movement patterns when you visually compare gaze paths for reading the paragraphs to reading the questions. Copeland (2011) showed the types of movements (forward, backward and no movement) were statistically significantly different when comparing paragraphs to questions.
Figure 2.8 shows there is a difference between eye movements of participants recorded reading paragraphs compared to reading questions. This difference is expected, as when individuals answer questions they may study the questions and the answers more closely than they study the paragraph material the questions are based on.
Figure 2.8. Eye movement trajectories of one participant; to the left is the eye movement whilst reading a paragraph and the right is the eye movement pattern whilst reading a question. Images taken from (Fahey, 2009).
  36
Literature Survey
 Salvucci & Anderson (2001) put forward the idea of eye movement as protocols, which they describe as “tracing”, which is plotting eye movements to predictions of a cognitive model. The three tracing methods are target, fixation and point tracing. These methods can be used differently in applications such as equation solving, reading and eye typing. They indicated that for reading, only fixation and point tracing are relevant. These three scenarios are an example of the different patterns that can be generated from eye movement. All three are essentially related to normal reading; when solving an equation you must first read the equation and when typing you must read the letters before you type. They generate quite different patterns where fixation trends tend to be focussed on the elements of the equation or the keyboard. There are many more applications of analysis of eye movement in tasks such as viewing faces, driving, watching television where complex patterns can be seen in eye movement. Salvucci & Anderson (1998) showed that “tracing” eye movement data is effective at interpreting the intent of eye movements using hidden Markov models. Tracing has been shown to generate accurate interpretations of these actions in areas such as eye typing and has been proposed to improve flexibility and design of eye based user interfaces (Salvucci, 1999).
2.5 The Use of Eye Tracking in HCI
Eye tracking has been used extensively in human computer interaction (HCI). Eye tracking can be utilised for quite different purposes and outcomes. This includes using eye movements to perform useability evaluation or using the eye movements as inputs to drive interaction with the system. The topics that will be discussed in this section include analysis of eye gaze data, usability analysis, and eye movement interactions with interfaces.
2.5.1 EyeGazeAnalysisinHCI
Eye gaze data can provide a wealth of knowledge about different tasks, not just about the cognitive functions that occur during reading. This subsection is highly related to the current research as it focuses on analysis of eye gaze data for the purpose of drawing conclusions about the nature of the eye movements and not the cognitive processes that make them occur. This will lead into the final subsection in this section that centres on providing feedback based on eye gaze. Different data analysis techniques have been used to achieve different outcomes. These will be discussed in the following subsections.
2.5.1.1 Task Identification
Tracking a person’s eye gaze while they perform a task produces a pattern in which they view a visual scene. The patterns in which the eyes move can vary greatly between tasks, making eye tracking a useful tool for discriminating between tasks. Eye gaze patterns have been used to detect the following: what kind of task the participant is performing (Iqbal & Bailey, 2004; Salojarvi et al., 2005; Simola et al., 2008); when a person is viewing particular expressions on an individual (Kozek, 1997); and when a person is reading or not reading (Campbell & Maglio, 2001).
37
Literature Survey
 Previous studies have shown that even within the activity of reading, eye gaze patterns can be used to differentiate when individuals are reading different types of content (Vo et al., 2010) and that there is a correlation between the eye gaze patterns observed from reading (subjectively) hard or easy content (Rayner et al., 2006).
Figure 2.9. The scoring system for fixation transitions for the reading algorithm outlined in (Buscher et al., 2008). Taken from (Buscher et al., 2008).
Reading detection algorithms use eye gaze to detect reading, skimming and scanning behaviour (Buscher et al., 2008; Campbell & Maglio, 2001). The reading detection algorithm put forward by Campbell and Maglio (2001) uses averaged gaze points to differentiate between reading and scanning. The algorithm uses cumulative evidence for reading that is assigned for both the horizontal and vertical movements. The system starts in scanning mode. Points are associated to these movements and when enough evidence has been accumulated the system goes into reading mode. The system can be reset back into scanning mode by encountering a scan jump.
The reading detection algorithm put forward by Buscher et al. (2008) is an extension of algorithm just described. Buscher et al.'s (2008) reading detection algorithm uses sequences that are separated by reset jump features or unrelated moves, as defined in Figure 2.9, to separate behaviour. A reading and a skimming score are kept for each sequence, denoted %& and %' in Figure 2.9. The sum of each score is found for each sequence and if that sum is above a given threshold11, (& = 30 and (' = 20, that is, / ∈ 23 %- . > (-. If only one detector is above the threshold, then behaviour is defined as detected. If both reading and skimming behaviours are detected, the algorithm moves to the next line to discern between the two behaviours. The scores for the detection algorithm are shown in Figure 2.9. Further differences in the algorithms are that the parameters do not need to be set, as they are defined by the authors; gaze points are not used, instead fixation points are; the
11 Authors specify that threshold is based on the literature 38

Literature Survey
 movement in the y axis is confined to movement between lines of text; and finally skimming behaviour is also detected as well as reading.
2.5.2 UsabilityTesting
Usability testing involves a wide range of methods and techniques. Methods include heuristic evaluation, cognitive walk through, pluralistic walk through and task analysis (Ramakrisnan et al., 2012). Techniques used include interviews, questionnaires, direct observation, video recorded observation and eye gaze tracking (Ramakrisnan et al., 2012). Often the methods and techniques are somewhat interrelated and more than one technique is often used. The technique that will be discussed in this section is eye gaze tracking.
Eye gaze tracking provides the ability to observe implicit behaviour during a task; that is, the user may not be aware they are exhibiting a certain behaviour or method to completing a task. Eye movements are the product of complicated cognitive and oculomotor processes, which have provided researchers with a bridge to the underlying workings of the human brain. This is what makes eye movement analysis such a diverse tool in HCI. Useability studies based on the implicit feedback of eye movements give researchers a unique method of assessing where people look, what catches their visual attention, and more importantly what they do not look at or miss in a visual stimulus An example of an application for analysing people's eye gaze as they view web pages is WebGazeAnalyzer (Beymer & Russell, 2005).
Many usability studies are designed to discover differences in expert and novice users, or investigate how users search for something in the interface. Results from both types of investigations can have great impact in the design and layout of an interface as well as give insight into training for use of an interface. An example of discovering design issues of an interface is analysis of the design of learning management systems (LMS) (Ramakrisnan et al., 2012). LMSs now play an important role in education as Web based delivery of content has become ubiquitous. The LMS interface must therefore be usable and beneficial to the learning process, as hindrance would detract from the learning process. In the study by Ramakrisnan et al. (2012), several design issues with the experimental LMS were discovered using eye tracking. From this information the authors outlined suggestions for improvements and potential guidelines for designing LMS interfaces.
In terms of marketing, this gives companies an important tool in assessing whether their marketing campaign is designed in such a way to attract people’s attention to the right parts of the advertisement. As a tool in interface design, eye movement analysis can result in redesign of interfaces or displays to reduce error, increase efficiency or appeal. Somewhat related to this is the investigation of eye gaze whilst searching through Web search engine results. Whilst Web search engines return organic results they also display ads and pay-per-click placements of search results. Eye tracking allows researchers to see if there are factors that affect how those results are viewed. Interestingly, the visual attention paid to an ad is
39
Literature Survey
 dependent on not only its quality but also the quality of the ads in prior searches (Buscher et al., 2010). In the same study, Buscher et al. (2010) found that the visual attention paid to the organic search results, was dependent upon the task type and ad quality. Insights like these allow search engines to optimise their search results and advise how to produce effective ads.
2.5.3 EyeMovementInteraction
Eye gaze can also be used as an input device for human-computer communication, as opposed to a keyboard and mouse. This type of interaction has been investigated for the purpose of providing interaction with computers for disabled users (Jacob & Karn, 2003). However, as gaze tracking becomes more readily available and cheaper so too does the possibility of using eye movements to control hands free devices and computers in new and proactive ways.
The advantages of using eye gaze as an input medium for interaction with a computer are that eye movement is fast, natural, and the user will most likely have to look at a visual stimuli, such as a button, menu item, and so on, in order to click it anyway. The problem, however, with using eye movements as a control medium is in finding ways to respond to the eye movement inputs appropriately, that is, how do you differentiate eye movements for viewing the scene to eye movements for control purposes (Jacob & Karn, 2003). A common problem with using eye movements as a control device is how to design the interface so that it does not over-respond (Jacob & Karn, 2003). This is called the “Midas Touch” whereby everything the user looks at turns into a command. Of course the opposite of this is that the interface under-responds and the user has to look at something so long for the command to be issued that the interface becomes unnatural and too slow to use. The use of eye gaze to control on-screen keyboard input has been investigated (Lankford, 2000; Salvucci & Anderson, 2001), as well as to control a clicking device12 with the eyes (Lankford, 2000; Murata, 2006).
Eye gaze in gaming has become more popular as eye-tracking technology becomes better and less expensive. The use of eye gaze in entertainment mediums such as in gaming not only could benefit people with disabilities but there is the possibility of using eye gaze as a sort of secondary input that rather than controlling the game, provides a more dynamic experience. A review of the use of eye tracking in gaming is given by Isokoski et al. (2009) and will not be discussed further in this thesis as it has little relevance to reading analysis and instead serves as an illustration the diverse uses of eye gaze.
2.5.3.1 Attention Aware Systems
Attention aware systems are where the eye gaze is integrated into the use of an interface in an implicit way, so that the user may not even be aware it. An example of this is in gaze-based rendering where high-resolution display is only rendered at the point of the users fixation (Jacob & Karn, 2003). This type of display exploits the fact that the fine detail vision occurs only in the fovea and so only high resolution of
12 Analogous to a mouse controlled by the hand 40

Literature Survey
 image display is necessary at the point of fixation. Another example is the use of eye gaze in interactive environments. Gedeon et al. (2008) demonstrated that fuzzy signatures could be used to infer actions based upon eye gaze in an interactive task.
To an extent these systems are more of an amalgamation of usability and control based on eye gaze, whereby, they can provide feedback or assistance. The main goal of these systems is to act almost as a transparent interface whose input is eye gaze but where feedback is based upon use. An example of an implicit feedback system is an “attentive” document (Buscher et al., 2012). Eye gaze is recorded to give implicit information about the users’ perceived relevance of pieces of text in a document. Two experiments are presented in this paper (Buscher et al., 2012); the first looks at providing implicit feedback to users about the ways in which they read documents in relation to how relevant or important they deem the documents. The second demonstrates the effect of implicit feedback for personalising web search. The aim is to work toward “attentive” documents that keep track of how they are read.
The Text 2.0 framework enables applications to use eye gaze to be used in real time to provide help with comprehension difficulties (Biedert et al., 2010; Biedert et al., 2010). The framework allows applications that analyse eye gaze to plug-in to gaze handlers. Several applications have been created to aid in reading comprehension for different purposes. An example is the use of eye gaze in creating footnotes that contain information about words to assist when reading in foreign languages. For further information see Biedert et al. (2010).
There are several applications that are used in reading assistance. iDict is a reading aid designed to help readers of a foreign language (Hyrskykari et al., 2000). iDict uses eye gaze to predict when a reader is having comprehension difficulties. If the user hesitates whilst reading a word then a translation of the word is provided along with a dictionary meaning. This is somewhat similar to The Reading Assistant (Sibert et al., 2000), which uses eye gaze to predict failure to recognise a word. The Reading Assistant then provides auditory pronunciation of the word to aid in reading.
2.6 Digital Text and eLearning
Digital environments are dynamic and immersive. The rise of the Internet, and ever growing expansion of the World Wide Web, has seen an increase in reading in many countries (Bohn & Short, 2009). This increase is growing with the proliferation of mobile technology such as smart phones and tablets. The Internet is now available almost anywhere at any time given you have a smart device. The debate on the effects of digitisation and rapid access to vast quantities of information ranges from ergonomics (Dillon, 1992, 2004), effect on memory (Sparrow et al., 2011), reading comprehension and effects on learning (DeStefano & LeFevre, 2007; Dillon & Gabbard, 1998; Mangen et al., 2013; Rockinson-Szapkiw et al., 2013). This section will begin with a discussion of these debates, and then lead into making learning environments adaptive.
41
Literature Survey
 2.6.1 Electronictext(eText)
Electronic text (eText) is the general term for digital presentation and storage of text. eText is read via digital devices, such as a computer, laptop, tablet, smart phone, or eReader. The advent of these devices has meant that eText is becoming more prevalent. The digitalization of text has spawned a great deal of research into what effects this has on the reading process. Initially, much research went into comparing reading digital to paper based texts (Dillon, 1992; Rho & Gedeon, 2000). We now give a brief overview of differences that have been found in the context of educational materials.
Hypertext is a prominent form of eText, in that it is the primary delivery of information on the web. Broadly, a hypertext document enables the reader to navigate via links to other resources or pieces of text. The resulting structure of hypertext documents can be complex and requires the reader to make decisions about where to go next. The consensus now is that hypertext structure negatively impacts the reading processes due to increased cognitive demand needed for decision-making and visual processing (DeStefano & LeFevre, 2007).
Hypertext is of course not the only form of eText. Quite often documents are read that are linear, such as PDFs (portable document format) or eBooks (electronic books). Such eTexts are therefore much closer to traditional print media. When print and PDF text comprehension was tested on students it was shown that students who read the print version of the text achieved significantly higher comprehension results than those who read a PDF version (Mangen et al., 2013). However, looking at the issue more abstractly, it has been shown that students who purchase electronic textbooks perform no differently in a university course (Rockinson- Szapkiw et al., 2013).
Paper offers advantages over digital presentation that has been studied to provide design suggestions for better reading technologies (O'Hara & Sellen, 1997). These include supporting annotation, quick and easy navigation, as well as control of spatial layout. Meanwhile, eText does itself have advantages over paper that include increased accessibility, easy storage and retrieval, ubiquity, and flexibility. Flexibility refers to the ability to dynamically change the way text is read. Changes can be simple, for example changing of font size, colour, or typeface. Changes can also be complex, such as verbalizations of the text, embedded definitions, and links to background information (Anderson-Inman & Horney, 2007). The ability to dynamically change eText presents the opportunity to make transformations to promote learning and comprehension. (Anderson-Inman, 1999) produced a typology of resources for supported eText that consists of presentational, navigational, translational, explanatory, illustrative, summarizing, enrichment, instructional, notational, collaborative, and evaluation resources. The typology is a list of ways in which eText can be supported; they vary vastly in method and purpose. Perhaps for this reason there is no consensus which supports should be provided (Anderson-Inman & Horney, 2007).
42
Literature Survey
 Many studies have examined navigation through eTexts, as it is often non-trivial (Dillon, 2004). Studies have investigated navigation in eBooks (McKay, 2011) and periodicals (Marshall & Bly, 2005) as well as the impact of screen size on document triage (Marshall & Bly, 2005). Navigation can be affected by the medium and familiarity with the book, whereby there is no difference in search efficiency between paper books and PC however the same task is performed significantly slower on a tablet PC (Shibata et al., 2015).
Additionally, the effects of highlighting, hyperlinks, fonts, distractions such as alerts, as well as embedded videos and sounds have long been investigated. The insight gained from these studies is beneficial in designing online reading materials. Inappropriate highlighting of words negatively affects reading comprehension whereas appropriate highlighting enhances comprehension (Beymer & Russell, 2005). The effects of font and font size used in eText have been investigated, where the focus has been on comparing serif and san-serif fonts (Bernard & Mills, 2000; Beymer et al., 2008; Mansfield et al., 1996). Smaller font sizes tend to induce slower reading speeds (Bernard & Mills, 2000; Beymer et al., 2008). This was found to result from increased fixation duration (Beymer et al., 2008).
The increased ease at which we can now locate information has changed the way in which we remember information (Sparrow et al., 2011). Knowing the information can be gathered from the Internet almost anywhere and anytime means that we often do not remember what we read on the Web and instead remember where to find information. People learn that the Internet “knows” certain information, which results in the tendency to not remember that information, instead remembering only the information that cannot found on the Internet (Sparrow et al., 2011).
2.6.2 eLearning
Associated with the rapid increase in digitised media is the rapid rise of eLearning. There are clear benefits to providing learning materials in digital format on the Web, namely making these materials accessible virtually anywhere at any time, to a wide and varied audience. In tertiary education, eLearning materials are becoming increasingly ubiquitous. This is due in part to increased accessibility and availability of computer technologies, but also because of the problems that large class sizes cause, such as limiting class discussions, assessment and time for teacher-student interaction (Longmore et al., 1996). Universities now frequently offer online and off- campus degrees where students may have little or no face-to-face interaction with their instructors or other students. Students are increasingly skipping lectures in favour of accessing digital copies or recordings of the lectures. The availability of lecture webcasts and PowerPoint slides negatively impacts student attendance (Traphagan et al., 2010). Whilst missing face-to-face tuition can have a negative effect on learning (Romer, 1993; Woodfield et al., 2006), webcasts of lectures actually nullify the negative effects on student performance and are instead associated with higher learning experience satisfaction (Traphagan et al., 2010). This means that eLearning has potential for making the learning process more enjoyable whilst increasing the amount learnt.
43
Literature Survey
 The advent of massive open online courses (MOOCs) has also increased the importance of designing effective eLearning materials. MOOCs have become popular in the past couple of years. The goal of MOOCs is to provide free or low cost but quality education that is available to anyone who wishes to take part. There are now many examples of reputable websites that offer MOOCs, such as Udacity, Coursera, edX, and Khan Academy. Whilst on one hand MOOCs do achieve the goal of making educational resources available to people who would not have access to them otherwise, they suffer from extremely low completion rates. An analysis of edX’s first MOOC, Circuits and Electronics 6.002x, completion rate was below 5% (Breslow et al., 2013). One of the problems identified with MOOCs is that they are indeed massive, making them easy to get lost in and likely to end up unhappy, frustrated or overwhelmed. Students that are likely to succeed in completing MOOCs tend to be self-motivated, self-directed, and independent; they tend to be students who would succeed in a classroom setting and who are probably doing the MOOC out of interest rather than necessity (Howland & Moore, 2002).
The problem of how to make eLearning effective to a wide and varied audience is significant especially when learning materials come in many types and forms, quite often dependent on the subject being taught. For example, a mathematics course would have mathematical exercises as opposed to a history course, which would be more likely to have text-based materials. One solution is to use technology to provide personalised learning to students. The focus of this study is on text-based materials with assessment questions, and the use of eye gaze.
2.6.3 ProvidingAdaptivityineLearning
Adaptive learning is the modification of educational material to suit a student’s needs. Traditionally, a skilled instructor, who would observe a student’s performance, would change the learning material to reflect the student’s needs. However, the advent of computers allows for the automation of such a process and takes away the responsibility of a human instructor to make such judgments. Broadly these types of software packages are termed adaptive learning environments (ALEs) although they are also referred to as adaptive learning management systems (ALMS) and intelligent tutoring systems (ITS).
Adaptive eLearning has already started to show great promise in improving education. Adaptive tutorials have been harnessed to decrease failure rates in early year engineering subjects and drastically increase student enrolment and satisfaction (Prusty & Russell, 2011). In the area of learning from information visualisation, adaptivity to the user using innervations has been shown to improve performance (Carenini et al., 2014). For MOOCs adaptive support has been shown to improve user’s acceptance as well as to isolate areas for improvement (Kardan & Conati, 2015). Finally, analysing attention of users in educational games increases performance when providing hints to help learning and completion (Conati et al., 2013).
Adaption can be performed by the student or by the system. Adaptability on the other hand is the term used when the student performs the adaption (Surjono,
44
Literature Survey
 2014). In these environments, the student is in control of how the adaption occurs by changing certain parameters. Adaptivity is the term used when the system performs the adaption (Surjono, 2014). In these environments student characteristics are detected and used to determine the adaption. These characteristics are determined using non-trivial means such as intelligent algorithms and machine learning. This thesis will focus on environments that provide adaptivity.
ALEs are typically composed of different components referred to as models (Paramythis & Loidl-Reisinger, 2003). These models include the expert model, which contains the learning material. The student model tracks and acquires information about the students’ behaviour. The instructional model is where the learning material is delivered, and finally the instructional environment is the user interface for the ALE (Kareal & Klema, 2006). The student model is the main driver for how the system will be adapted. It is important that the model be as accurate as possible because the adaption performed can only be as good as the model.
There has been work to create adaptive learning environments in many different respects. Some examples of adaptive learning systems include InterBook (Eklund & Brusilovsky, 1999), which is a web-based adaptive tutoring system that allows textbooks to be navigated in multiple ways. The navigation of the textbook is personalized to assist the learner. Another example is Web-based Intelligent Design and Tutoring System (WINDS), which uses a student’s past and current behaviour to predict their knowledge and goals, as well as record progress (Specht, Kravcik, Klemke, Pesin, & Hüttenhain, 2006). This information is used to provide adaptive learning material by annotating material and guiding students to suitable learning material. Generic Responsive Adaptive Personalized Learning Environment (GRAPPLE) project (De Bra et al., 2013) is another adaptive learning environment through adaptive guidance and personalized learning content. The authors of GRAPPLE show how they can integrate their system with currently used LMSs such as Claroline, Moodle, Sakai, Clix and learneXact. Other frameworks take into account students with learning problems such as dyslexia (Alsobhi et al., 2015). The Dyslexia Adaptive eLearning (DAEL) framework is designed to tailor learning materials according the dyslexia type (Alsobhi et al., 2015).
There are also companies actively involved in producing adaptive learning technology. There are several examples of commercially available adaptive learning technology; two such examples include DreamBox13 and Smart Sparrow14. DreamBox is an adaptive learning platform that provides individualized learning paths based on the users measured skill level and use of gamification. Smart Sparrow is a web based adaptive learning environment that allows instructors to create, deploy and report on adaptive learning material. It is an intelligent tutoring system in which adaption comes from the answers that the student provides to questions.
13 http://www.dreambox.com/ Last accessed: 7th January 2016
14 https://www.smartsparrow.com/ Last accessed: 7th January 2016
 45
Literature Survey
 2.6.4 MakingeLearningadaptiveusingpsychophysiological data
The basis of what drives changes in an eLearning environment can be based on different factors such as the learners current understanding, emotional state, such as stress (Calvi et al., 2008; Porta, 2008), emotions (Jaques et al., 2014), learner style (Mehigan et al., 2011; Spada et al., 2008; Surjono, 2014), cognitive load (Coyne et al., 2009), learning rate (Bondareva et al., 2013; Kardan & Conati, 2013), and skill level (Chen, 2008). The methods for determining these factors also vary and using the use of such information also varies. Methods for gathering user state and deducing these factors include the use of biometric technology (Mehigan et al., 2011; Spada et al., 2008) and psychophysiological response data (Rosch & Vogel-Walcutt, 2013), especially eye tracking (Alsobhi et al., 2015; Barrios et al., 2004; Bondareva et al., 2013; Calvi et al., 2008; Conati et al., 2013; Conati & Merten, 2007; Kardan & Conati, 2013; Merten & Conati, 2006; D'Mello et al., 2012; Gütl et al., 2005; Mehigan, 2014; Mehigan & Pitt, 2013; Mehigan, 2013; Mehigan et al., 2011; Porta, 2008). Development in technologies for measuring these signals and understanding of psychophysiological responses now provide the unique opportunity of adapting eLearning environments in real time.
Whilst learning style can be determined via questionnaire (Surjono, 2011) this interrupts the student with non-learning based assessment. Progressively more research indicates that measures of the students’ behaviour and biometric technology can predict learner style, therefore alleviating the need to have student input (Mehigan et al., 2011; Spada et al., 2008). Mouse movement patterns have been shown to have a high correlation with global / sequential learning style (Spada et al., 2008). Eye tracking has also been shown to be a potential way of identifying visual/verbal learner style (Mehigan et al., 2011). Eye movements in areas of interest on the page were related to measures of learner style in that investigation. Similar uses of eye tracking have been used to compare learning behaviours between novice and advanced students when learning SQL (Liu, 2005). This study revealed that advanced students look at the database schema more than novice students. Studies such as this are useful for identifying this difference in order to provide more help for novice students. The concept of adaptive eLearning also extends to mobile learning. The MAPLE framework uses a combination of eye tracking and accelerometer data to determine learner style in both mobile and online environments (Mehigan & Pitt, 2013).
Adaption is not only provided via detection of learning style. Eye tracking can be used to detect many facets of human behaviour. Eye gaze patterns have been used to detect what kind of task the participant is performing (Iqbal & Bailey, 2004) or whether a person is reading or not (Campbell & Maglio, 2001) as well as if they are reading or skimming (Buscher et al., 2008), and their cognitive load (Rosch & Vogel-Walcutt, 2013). Eye movement measures have been shown to be effective at distinguishing between readers with low and high level of understanding as well as predicting English language skill (Martínez-Gómez & Aizawa, 2014). Eye gaze has also been used to investigate parts of text that readers are failing to comprehend.
46
Literature Survey
 Results from this investigation indicate that eye gaze features such as number and duration of fixations can be used to determine reading incomprehension (Okoso et al., 2015). Eye tracking can also be used to analyse how multiple-choice questions are answered (Nugrahaningsih et al., 2013; Tsai et al., 2012) and to predict student performance of physics concepts when presented as text or images (Chen et al., 2014).
Eye tracking has been used in multiple ways to provide adaptivity to eLearning. A classic example of the use of eye tracking in eLearning is AdeLE (Adaptive e- Learning with Eye-Tracking). The AdeLE project sets out a structure for how an adaptive eLearning environment could be constructed using eye tracking data such as blink rate and how open the eyelid is (Gütl et al., 2005).
Detection of a student’s state are frequently investigated in adaptive eLearning, such as boredom and curiosity (Jaques et al., 2014), emotional state (Calvi et al., 2008), disengagement (D'Mello et al., 2012). An interesting approach to identifying students’ engagement comes from the use of type-2 fuzzy logic based system (Paramythis & Loidl-Reisinger, 2003). This novel method gauges degree of engagement to adapt the learning environment. Results show that using the system to adapt material causes a significant improvement in average scores compared to other methods of adaption and no adaption.
Prediction of a student’s learning rate is another important way in which a learning environment can react and adapt to the student. Prediction of learning rate has been shown to be effective using eye gaze data (Bondareva et al., 2013; Kardan & Conati, 2012). Similarly, eye gaze has been effective at predicting learning rate and initial experience with information visualisations (Lallé, Toker, Conati, & Carenini, 2015),. Additionally in the area of information visualisation, performance and user’s cognitive abilities (Steichen, Conati, & Carenini, 2014) as well as confusion in processing the visualisation (Lallé, Conati, & Carenini, 2016) can be predicted with eye tracking data. Eye gaze is also effective as identifying parts of visualisations that are not conducive for associated tasks (Toker & Conati, 2014). Importantly though, these application are all in the form of prediction of user state to adapt the visualisation to the user, thus being of high relevance to this thesis.
Eye tracking is also used to analyse reading in eLearning environments. One example is iDict, a reading aid designed to help readers of a foreign language that uses eye gaze to predict when a reader is having comprehension difficulties (Hyrskykari et al., 2000). If the user hesitates whilst reading a word then a translation of the word is provided along with a dictionary meaning. Similarly, the Reading Assistant (Sibert et al., 2000) uses eye gaze to predict failure to recognize a word. The Reading Assistant then provides an auditory pronunciation of the word to aid reading. Eye movements have been used in combination with measuring pupil size as a means of gauging mental workload (Lach, 2013).
Adaption of reading material has been shown to be beneficial to young students (Dingli & Cachia, 2014). Adaptive eBooks involves detection of reading difficulty, currently based on measures such as out load reading speed, and dynamically
47
Literature Survey
 simplifying the text for the students. The system is designed for year 4 students and an initial study shows that such modifications can improve reading performance. However, the authors’ note that the reading detection currently used is in the system is not sufficient and should be replaced, noting also that eye tracking would be a good solution.
In summary there is a broad range of scenarios that these adaptive technologies can be directed at helping students, such as plugging into traditional eLearning environments (Barrios et al., 2004; De Bra et al., 2013), or providing adaption in mobile environments (Mehigan & Pitt, 2013), or accounting for dyslexia (Alsobhi et al., 2015) and foreign language reading (Hyrskykari et al., 2000), and indeed eye tracking has shown to be an effective driver for these adaptions.
2.7 Summary
This literature review covered topics ranging from the physiology of the eye and visual information processing in the human brain to the use of eye tracking in adaptive eLearning. eLearning has extended the reach of teaching and learning from the classroom to a wide and varied audience that has different needs, backgrounds, and motivations. This gives rise to the question of how to make eLearning more effective through adaptivity. Whilst there are existing methods of providing adaptivity, eye tracking has been shown to be an effective way of analysing various human behaviours, particularly reading. Eye tracking is especially useful for analysing the implicit differences between different types of readers. This review sets the scene for the research presented in this thesis. We use this current knowledge to build from and produce advances in the integration of eye tracking technology into eLearning environments.
48
Chapter 3
Chapter 3. Effect of Presentation on Reading Behaviour
 “What this means is that we shouldn't abbreviate the truth
 The presentations of learning materials affect how we learn. In this chapter, we use eye tracking to investigate how different sequences of text and comprehension questions can affect performance outcomes, eye movements, and reading behaviour for first (L1) English language and second (L2) English language readers. We show that different presentation sequences induce different performance outcomes, eye movements, and reading behaviour. The sequence can affect how a participant reads the text as well as their perceptions of how well they understood what they read. For instance, if questions and text are not shown together, this improves participants’ ability to accurately perceive their comprehension and promotes thorough reading. Alternatively, showing questions before the text promotes skimming behaviour. Importantly, the presentation sequence affects both L1 and L2 readers in the same way. We observe L2 reader take longer to read text but have the same comprehension levels as L1 readers, this difference comes primarily from longer fixation durations. The results from this study can be used to design learning materials in eLearning environments to influence how students interact with the learning environment as well as how they learn. The purpose of this investigation is to make informative decisions about designing adaptive eLearning environments. This chapter builds on work presented at OzCHI 2013 (Copeland & Gedeon, 2013a), OzCHI 2014 (Copeland & Gedeon, 2014a), and IHCI 2014 (Copeland & Gedeon, 2014b) and is largely based on work published in IEEE Transactions on Emerging Topics in Computing (Copeland & Gedeon, 2015).
but rather get a new method of presentation.”
 ― Edward Tufte
49
Effect of Presentation on Reading Behaviour
 3.1 Introduction
The way in which learning materials are presented to students can have great bearing on the outcomes of comprehension. It has been established that the presentation of images with text increases comprehension (Clark & Mayer, 2011). Moreover, pretesting with multiple-choice questions improves subsequent learning of materials (Little & Bjork, 2012). In this chapter we explore further how presentation of learning materials affects reading and learning behaviour. We investigate the effects that test questions have on learning by presenting questions and text in different sequences. Furthermore, we investigate if the effects of sequence are different for L1 and L2 readers since there is a growing diversity in the audiences of eLearning courses. Henceforth we will refer to the presentation sequences as formats. To make this comparison, the different formats are investigated to assess how eye movements and learning performance are affected. The central question being asked in this chapter is therefore:
Can outcomes of eye gaze analysis be used to optimise the layout of reading materials in eLearning environments for learning outcomes? How does the layout compare for L1 and L2 readers?
We explore this question by conducting a user study to compare four formats. These formats are manipulations to the order in which text and quiz questions are shown to a student. In the user study, participants’ eye gaze was recorded, using eye tracking technology, as they read text and answered questions. Eye tracking has been shown to be an effective way of analysing various human behaviours, particularly reading (see review by (Rayner, 1998)). Eye movements are unique in reading and can reveal when readers encounter difficulties in reading (Frazier & Rayner, 1982) as well as text difficulty and comprehension (Rayner et al., 2006). Eye tracking is especially useful at analysing the implicit differences between different types of readers. One example is in comparing first (L1) and second (L2) English language readers, which reflects an increasing diversity in audiences of online learning materials. Kang (2014) found that L1 and L2 readers performed no differently in comprehension tests and that there was no difference in attention distributions when reading or in eye gaze patterns. L2 readers took longer to read the text and longer to find answers cues in the text. However, this study did not look into differences of eye gaze measures, so more is still to be understood about the differences in eye movement and learning behaviours of L1 and L2 readers. For instance, it may be that there are different methods of presentation of learning materials, which are optimal for L1 and L2 readers.
We hypothesize that the format of the text and comprehension questions will: 1) affect L1 and L2 readers in the same way even though there will be differences between the two groups; 2) have an effect on participants' performance, in terms of time and quiz score, and perceived understanding of the text; 3) cause differences in eye movements and induce different reading behaviour.
50
Effect of Presentation on Reading Behaviour
 The background for this analysis is covered in the literature survey chapter, so this chapter is organized into the following sections: user study method; results and analysis; discussion and recommendations; and conclusions and further work.
3.2 Method 3.2.1 Design
Our study used a between-subjects design where participants were shown one of four formats of tutorial and quiz content. The independent factors of the experiments are the presentation type and English as a first language (L1) or second language (L2). Participants were permitted to take as long as they desired to complete the tutorial and quiz, with no time limit imposed.
3.2.2 Materials&Procedure
The user study conducted involved tracking participants’ eye gaze as they read a text and answer comprehension questions. The text and questions are taken from a tutorial and quiz that is coursework from a first year Computer Science course run at the Australian National University. There are 9 screens of text, each covering a specific area about the main topic of the tutorial (“Web Search”). Each screen is 400 words long and has an average Flesch Kincaid Grade readability level of 11.5. This indicates that participants need around a 12th grade education level. This is a suitable readability level as the slides are targeted at first-year university students. For each screen there were two comprehension questions; one of the questions was multiple-choice and the other was cloze (fill-in-the-blanks). These two types of questions were used because they can be used to assess different forms of comprehension (Fletcher, 2006). The scores that the participants can receive for each question are 0, 0.5 and 1, corresponding to incorrect, half correct and correct respectively. Participants were not given any time restrictions on reading the text or answering the questions.
Upon completion of the quiz (but before being shown results) participants were asked to subjectively rate their overall comprehension on a scale of 1 to 10 with 10 being complete understanding. The text and questions are presented to participants in four formats to measure the effect of presentation on participants’ eye gaze and answering behaviour. The formats are based on the presentation of quiz questions in relation to the text. These formats are described below:
Format A (5 → 5/6). The tutorial text slide (T) Figure 3.1 is first shown to participants followed by a slide with both questions and the tutorial text (T/Q see Figure 3.2). Since there are 9 topics, 18 slides in total are displayed in this part of the study. In this format participants are required to read the text before being able to read the questions relating to it.
Format B (5/6). A slide containing both the questions and tutorial text (T/Q) is shown to participants. An example of this is seen in Figure 3.2. Since there are 9 topics, 9 slides in total are displayed in this part of the study. In this format
51
Effect of Presentation on Reading Behaviour
 participants are no longer required to read the text before they see the questions. Our question is: is there a difference in quiz performance when participants can immediately answer the questions without reading the text?
Format C (5 → 6). The tutorial slide (T), shown in Figure 3.1, is first shown to participants followed by the questions slide (Q) but no access to the text, see Figure 3. Since there are 9 topics, 18 slides in total are displayed in this part of the study. This format can be considered to be a control presentation method. In this format the reference text is removed from the questions slide so the participants are forced to answer the questions from understanding and memory. We expect that the worst comprehension scores will be observed for this format. Format C is the most commonly used in on-line quizzes.
Format D (6 → 5 → 6). The last presentation format consists of displaying a slide with only the questions (Q) on it, as seen in Figure 3.3, followed by the tutorial text slide (T) Figure 1, and then again presenting them with the questions slide (Q) as in Figure 3. Since there are 9 topics, 27 slides in total are displayed in this part of the study. The reasoning for this format is to mimic a situation where the participants knew what the comprehension questions are but have no access to them as they read. The hypothesis is that participants will read the text differently than for formats A and C.
  Figure 3.1. Example of text only tutorial page (T).
52
Effect of Presentation on Reading Behaviour
   Figure 3.2. Example of text and comprehension question tutorial page (T/Q).
 Figure 3.3. Example of comprehension questions only tutorial page (Q).
53
Effect of Presentation on Reading Behaviour
 3.2.3 Participants
The study included 60 participants who were divided equally into the four groups, each of which was shown one of the presentation formats. The breakdown of participants into groups is as follows:
Format A. 15 participants (6 female, 9 male) with an average age of 22.3 years (standard deviation 4.1 years, range 17-31 years). English was not the first language for 4 of the participants.
Format B. 15 participants (6 female, 9 male) with an average age of 22.7 years (standard deviation 6.0 years, range 18-41 years). English was not the first language for 4 of the participants.
Format C. 15 participants (5 female, 10 male) with an average age of 23.5 years (standard deviation 5.3 years, range 18-37 years). English was not the first language for 6 of the participants.
Format D. 15 participants (7 female, 8 male) with an average age of 22.2 years (standard deviation 3.3 years, range 17-28 years). English was not the first language for 5 of the participants.
 Figure 3.4. Experiment set up; Participant to the left with the experimenter’s laptop and view to the right.
3.2.4 ExperimentSetup
The tutorial quiz was accessible via Wattle (a Moodle variant) the online learning environment used at ANU. A copy of the texts used for the experiment, along with the participant information sheet, consent form, and other experiment resources are found in Appendix A. The study was displayed on a 1280x1024 pixel Dell monitor. Eye gaze data was recorded at 60Hz using Seeing Machines FaceLAB 5 infrared
54
Effect of Presentation on Reading Behaviour
 cameras mounted at the base of the monitor. This is shown in the left half of Figure 3.4 along with the laptop the experimenter used to monitor the eye tracking quality in the right half.
This eye tracker has a gaze direction accuracy of 0.5-1° rotational error and measures pupil diameter as well as blink events. The study involved a 9-point calibration prior to data collection for each participant. As the data recorded is a series of gaze points, EyeWorks Analyze was used to pre-process the data to give fixation points. The parameters used for this were: a minimum duration of 60 milliseconds and a threshold of 5 pixels.
3.2.5 DataPre-processing
The raw eye gaze data consists of x, y-coordinates recorded at equal time samples (60Hz). Fixation and saccade identification was performed on the eye gaze data. From this data many other eye movement measures are derived. The measures used in this analysis are:
Number of fixations: The sum of fixations recorded for each page. The number of fixations can be affected by the reading behaviour, text difficulty, and reading skill (Rayner, 1998).
Maximum fixation duration (seconds): The maximum duration of the longest fixation recorded for a tutorial page. Longer fixations can be an indicator of difficulties in processing particular words or due to linguistic and/or comprehension difficulties (Rayner, 1998).
Average fixation duration (seconds): The sum of the duration of all fixations on a paragraph divided by the number of fixations on that paragraph. This measure has been used to predict reading comprehension (McConkie & Rayner, 1975).
Total fixation duration (seconds): The sum of all fixations on complete text. This measure is useful in global text processing analysis (Hyona et al., 2003) because it measures immediate as well as delayed effects of comprehension.
Number of regressions and regression ratio: The number of regressions divided by the total number of saccades on a paragraph. There is evidence that when reading more difficult text more regressions are observed (Rayner et al., 2006).
Reading analysis: Using our combination of two reading detection algorithms (Buscher et al., 2008; Campbell & Maglio, 2001), this is the percentage of saccades classified as being part of reading (read ratio), skimming (skim ratio), and scanning/searching (scan ratio).
Participants’ quiz outcomes are measured to assess how well they performed under different conditions. The measures of participants’ performance are:
Subjective comprehension: a self-rated measure between 0 and 10, where 10 is comprehensive understanding of the material.
55
Effect of Presentation on Reading Behaviour
 Comprehension question scores: the multiple-choice questions are graded as 0 (incorrect) or 1 (correct) and the cloze questions are scores as 0 (incorrect), 0.5 (one word was correct) or 1 (correct). The maximum total score for the quiz is 18.
Time taken: the total time it took each participant to complete the tutorial and quiz is recorded.
3.3 Result & Analysis
The first part of this section contains a statistical analysis of participants’ performance (score, time taken and perceived comprehension) under each of the experimental conditions. Additionally, L1 and L2 readers are compared under each condition. The second part of this section contains the statistical analysis of the eye movement measures derived from the participants' eye gaze under each of the experimental conditions. Once again, the L1 and L2 readers are compared.
3.3.1 Doesformataffectperformance?
The question of whether format affects reader performance incorporates two hypotheses that will be explored in this subsection. These hypotheses are:
1. The different presentation formats will affect participants' scores, time taken to complete, and perceived understanding, and these effects will be the same for both L1 and L2 readers.
2. Only time taken to complete will be different between the L1 and L2 readers.
The mean and standard deviations for the quiz grade is shown in Figure 3.; the time taken (minutes) to complete the tutorial and quiz is shown in Figure 3.; and the
participants’ subjective understanding is shown in Figure 3..
To address the above hypotheses a MANOVA is used to determine if there are any statistical differences between the formats and L1/L2 readers. The correlations between the dependent variables are within the acceptable limits for MANOVA outcomes, i.e. the correlations lie between r=-0.4 and r=0.9. To test for normality in the dependent variables the Shapiro-Wilk Test is used, as it is more appropriate for small sample sizes. The quiz scores are normally distributed for all formats (all p>0.05). The times taken are normally distributed for the formats A, B and C (all p>0.05), it is just the times taken for D (p=0.026) which is still relatively normal and should not impact the MANOVA as the assumption is for approximately normal distributions. Whilst the subjective scores for B are normally distributed and the scores for C are very close to being normally distributed, the scores A and D could be a problem. Finally, the homogeneity of variance-variance-covariance matrices is satisfied as the Box's M value of 69.73 (p=0.165).
56
Effect of Presentation on Reading Behaviour
   Figure 3.5. Means and standard deviations of quiz scores for each format (A: 5 → 5/6; B: 5/6; C: 5 → 6; D: 6 → 5 → 6)
  Figure 3.6. Means and standard deviations of time taken to complete the tutorial for each format (A: 5 → 5/6; B: 5/6; C: 5 → 6; D: 6 → 5 → 6)
  Figure 3.7. Means and standard deviations of subjective understanding scores for each format (A: 5 → 5/6; B: 5/6; C: 5 → 6; D: 6 → 5 → 6)
57
Effect of Presentation on Reading Behaviour
 3.3.1.1 Effect of format on Performance Measures
The results in Figure 3.5, Figure 3.6, and Figure 3.show that whilst there are some differences between L1 and L2 readers, the effect of format is relatively consistent for both L1 and L2 readers. This indicates that the format does affect performance outcomes for both groups. This is supported by the results from the MANOVA that show there is a statistically significant difference in performance variables based on the format shown to participants, F(9,121.8)=4.036, p<0.0005; Wilk's λ=0.530, partial η2=0.191. There is no statically significant effect of interaction between the format and reader type. This indicates that format affects both L1 and L2 readers in the same way.
Since statistically significant results have been found, we use ANOVAs to assess if the formats have an effect on the dependent variables. Format has a statistically significant effect on both the quiz grade (F(3,52)=6.078; p=0.001, partial η2=0.260) and on time taken (F(3,52)=5.552; p=0.002, partial η2=0.243), however format did not affect the subjective comprehension score. Tukey's HSD tests are used to make pairwise comparison of the formats. Figure 3.5 shows that Formats A and B have similar quiz scores, as do formats C and D. There is no significant difference in quiz scores between formats A and B or between formats C and D. These two groups correspond to similarities in presentation formats whereby formats A and B show the questions with the tutorial text and Formats C and D do not. Two conclusions can be made from this observation; firstly, the lack of difference between formats A and B illustrates that reading the tutorial text before being presented with the questions does not improve comprehension scores. Secondly, when comparing formats C and D, the knowledge of the questions before reading the text also does not improve quiz results.
However, formats A and B have significantly higher quiz scores than formats C and D, (formats A and C (p=0.006), A and D (p=0.003), B and C (p=0.005), and B and D (p=0.002)). For formats C and D the participants did not have access to the content as they answered the questions and therefore had to rely on memory and their understanding of the material.
Format A takes significantly longer to complete than formats B (p=0.011) and D (p=0.002). There is no significant difference between the other formats. For format A, participants were asked to read the text and then move to the next page with the questions, where they also had the option to re-read the content. The lack of significant difference between formats A and C could be accounted for by the participants reading the text on the text only page before the questions and text page, which is analogous to format C.
The format has no significant effect on subjective comprehension scores. However, for formats C and D there are strong positive correlations between the quiz scores and the subjective comprehension scores (r=0.9 and r=0.8, respectively). In these formats participants estimate their comprehension level more accurately compared to other formats. Participants shown formats A and B seem unable to estimate their own comprehension levels (r=0.3 and r=-0.1, respectively). An important part of the learning process is awareness of skill (Dunlosky & Lipko,
58
Effect of Presentation on Reading Behaviour
 2007). Under-estimation of understanding can lead to students wasting time on material already understood instead of using the time to learn more material. On the other hand, overestimation of understanding will results in students not learning what they need to and not realizing their lack of understanding.
For format C, participants are asked the comprehension questions after having read the content and cannot refer back to the text. The participants can seemingly gauge whether they know the answers or not. Interestingly, this effect extends to format D where once again the participants did not have access to the content whilst they answered the questions. However, the difference in this format is that participants knew the comprehension questions before reading the content and so could target their reading goals for answering those questions. For formats A and B the participants have access to the text whilst answering the questions. Participants accordingly do not fully read the content and thus fail to find key concepts in the text. In this case the participants have a false sense of confidence.
3.3.1.2 L1 versus L2 readers
The second hypothesis is that the only difference expected between L1 and L2 readers will be in time taken. The MANOVA shows that there is a statistically significant difference in performance variables between L1 and L2 readers, (F(3,50)= 5.79, p=0.002, Wilk's λ=0.742, partial η2=0.258).
Between-subjects ANOVAs are used to compare the groups for each performance variable. The difference between L1 and L2 readers is statistically significant for time taken (F(1,52)=13.135; p=0.001, partial η2=0.202) but has no significant effect on subjective comprehension or quiz score. This confirms our expectations and is analogous to existing research that has shown that although L2 readers take longer to read, they perform no differently to L1 readers in comprehension (Kang, 2014). We have also found there is no difference in their subjective comprehension.
3.3.1.3 Summary
The interim conclusion made from this analysis is that presentation formats affect students’ performance. In concordance with current research it was found that L2 readers took longer to complete the quiz but performed no differently to L1 readers. Additionally, the differences in measures caused by formats are consistent for both L1 and L2 readers. The presentation format can be manipulated in the same way for both L1 and L2 readers to optimize the performance outcomes of students in order to increase their understanding.
3.3.2 Doesformataffecteyemovements?
The overall hypotheses are that presentation format affects eye movements and that the eye movements of L1 and L2 readers will be different. To address these overall hypotheses, the two central differences in presentation formats are analysed separately. That is, first the tutorial text when shown without the questions will be analysed and then the tutorial text when shown with the questions. Finally, aspects
59
Effect of Presentation on Reading Behaviour
 of the answering process that derive from the nature of the presentation format will be analysed, namely, reading intensity of paragraphs.
3.3.2.1 Text Pages
Two types of behaviour are hypothesized for reading the tutorial text without the questions:
1. Participants presented with format C will take more care reading the text, as they know they cannot refer to it again whilst answering the comprehension questions;
2. Participants presented with format D will not read the text thoroughly, rather will skim the text to find the paragraphs where they believe the answers are located and read only those paragraphs thoroughly.
3. L2 readers will be observed to read the text for longer, i.e. more fixations and longer fixation duration.
The final hypothesis is a deeper analysis into the observation that L2 readers have longer read times than L1 readers.
Table 3.1. Comparison of eye movement measures for text only (T) pages (Mean ± Standard Deviation) (A: 5 → 5/6; C: 5 → 6; D: 6 → 5 → 6).
                      Format Type
Num. Fixations
Max fixation dur (s)
Ave fixation dur (s)
Num. regressions
74±7
83±11
75±7
106±9
66±7
66±10
    A C D
L1       241±21     1.1±0.2     0.17±0.02
L2 311±35
L1 245±23
L2 351±28
L1 178±22
L2 221±31
2.1±0.3 1.3±0.2 1.6±0.2 1.0±0.2
1.9±0.3
0.25±0.03 0.21±0.02 0.23±0.02 0.17±0.02
0.26±0.03
                      A MANOVA was used to check for statistical significance of
measures between formats and reader type. The correlations between the dependent variables are all within the range of r=-0.4 and r=0.9. Additionally, the majority of the dependent variables are normally distributed according to the Shapiro-Wilk test for normality for both reader type and format. The total fixation duration time was excluded from the analysis, as it did not have a normal distribution. The Levene’s test for equality of variances shows that there is homogeneity for all dependent variables (p>0.05). Additionally, the Box’s M value of 98.1 (p=0.025) is interpreted as non-significant so we can be satisfied that we have homogeneity if variance- variance-covariance matrices. The means and standard deviations for the eye movement measures are shown in Table 3.1, whilst the means and standard deviations of reading ratios for each format are shown graphically in Figure 3.5.
60
eye movement
Effect of Presentation on Reading Behaviour
   Figure 3.5. Means and standard deviations of reading ratios (% of eye movements detected as reading) for text only page which are in formats A, C and D
(A: 5 → 5/6; C: 5 → 6; D: 6 → 5 → 6).
                                                                                                                                                                                                                                                       Figure 3.6. Example of fixations recorded from reading text only page in format A (5 → 5/6)
There is a statistically significant difference in eye movement measures based on the presentation format the participant was exposed to (F(10,72)=3.043, p=0.003, Wilk's λ=0.486, partial η2=0.303). Furthermore, there is a statistically significant difference in eye movement measures between L1 and L2 readers (F(5,35)=3.623, p=0.010; Wilk's λ=0.659, partial η2=0.341). There was no statistically significant effect of interaction between the format and reader type. Once again, format affects both L1 and L2 readers in the same way.
61
Effect of Presentation on Reading Behaviour
 ANOVAs are used to determine how the eye movements differ for the formats and languages. Format has a statistically significant effect on the number of fixations (F(2,39)=7.262; p=0.002; partial η2=0.271), and number of regressions (F(2,39)=4.234; p=0.022; partial η2=0.178), but no effect on maximum fixation duration, average fixation duration or the read ratio.
Tukey’s HSD tests are used to make pair-wise comparisons of the formats. There is a statistically significant difference between number of fixations for Formats A and D (p=0.034) and between formats C and D (p=0.002). There is a statistically significant difference between the number of regressions for formats C and D (p=0.030), but not between formats A and D or A and C. There is no significant difference between formats A and C for any of the eye movement measures.
Figure 3.7. Example of fixations from reading text only page in format C (5 → 6)
It was predicted that, for format C, participants would read the text more thoroughly. However, the statistical analysis shows that there is no difference between formats A and C, so participants are actually reading format A as thoroughly as they are reading C. The hypothesis is partially supported as there are significantly fewer fixations recorded for format D, so even though there is no difference in the read ratio there is less overall reading of the text compared to formats A and C. This can be observed visually in the comparison of three different participants’ fixations as they read the same text under different formats in Figure 3.6, Figure 3.7, and Figure 3.8. Whilst the eye movements in Figure 3.6 (format A) and Figure 3.7 (format C) are different, they do show coverage of the entire text. These differences can also be put down to individual variance in reading. However, the eye movements shown in Figure 3.8 (format D) are substantially different from those in Figure 3.6 and Figure 3.7, where only the first paragraph is read. All images are indicative of the reading behaviour observed for each of the formats.
                                                                                                                                    62
Effect of Presentation on Reading Behaviour
 Finally, L2 readers have significantly more fixations (F(1,39)=11.395; p=0.002; partial η2=0.226) than L1 readers as well as longer maximum fixation duration (F(1,39)=13.840; p<0.001; partial η2=0.262) and longer average fixation duration (F(1,39)=11.527; p=0.002; partial η2=0.228). Also, L2 readers also have significantly higher read ratios for each format compared to L1 readers (F(1,39)=4.951; p=0.032; partial η2=0.113). This outcome agrees with the observation that L2 readers have longer read times than L1 readers. The analysis of eye gaze shows that this is due to higher numbers of fixations that are also for longer duration.
Figure 3.8. Example of fixations recorded from reading text only page for format D (6 → 5 → 6)
In conclusion, the eye movements and reading behaviours that are observed for the formats A, C and D reflect the participants’ overall intentions in reading the text and the goals set for the participants. The purpose of this analysis was to assess the hypotheses that the different presentation formats would affect the eye movements observed and therefore the reading behaviours observed. These hypotheses have been confirmed by this study. The implications of these findings can be used to support design decisions for eLearning environments. That is, if the teacher wants to promote thorough reading, the goals placed on the reader should not be targeted at certain parts of the text as in format D. Instead, thorough reading is observed where the goal was to understand the text overall.
3.3.2.2 Questions and Text Pages
Format A consists of two presentations of the text, first on its own and second with the questions. The hypothesis is that the first read through of the text in format A will help participants answer the questions and they will need less reference to the text compared to Format B. The means and standard deviations for the eye
                                                                                           63
Effect of Presentation on Reading Behaviour
 movement measures are shown in Table 3.2, whilst the means and standard deviations of reading ratios for each format are shown graphically in Figure 3.9.
A MANOVA is used to test for statistical significance of eye movement measures between formats and reader type. The correlations between the dependent variables are all within the range of r=-0.4 and r=0.9. All of the dependent variables are normally distributed according to the Shapiro-Wilk test except average fixation duration, which is therefore excluded from the analysis. Levene’s test for equality of variances shows that the homogeneity for all dependent variables (p>0.05). Box’s M value of 45.8 (p=0.005) is interpreted as non-significant so we can be satisfied that there is homogeneity in the variance-variance-covariance matrices.
Table 3.2. Comparison of eye movement measures for Questions and Text pages (Mean ± Standard Deviation) (A: 5 → 5/6; B: 5/6)
               Format Type
dur (s)
dur(s)
regressions
Num.       Max fixation     Total fixation     Num.
Fixations
L1       225±37     0.97±0.13     38±8     97±14
  A B
L2 246±61
L1 350±37
L2 429±61
1.65±0.21 1.31±0.13
1.85±0.21
54±14 64±8
102±13
95±23 149±14
167±23
                        Figure 3.9. Means and standard deviations of reading ratios (% of eye movements detected as reading for text and questions pages) for Formats A and B. (A: 5 → 5/6; B: 5/6)
There is a statistically significant difference in eye movement measures based on presentation format, (F(5,22)=3.142, p=0.027; Wilk's λ=0.583, partial η2=0.417. Also there is a statistical difference between L1 and L2 readers, F(5,22)=3.309, p=0.022; Wilk's λ=0.571, partial η2=0.429. However, there is no statistically significant effect of interaction between reader type and format. There is no difference in how L1 and L2 readers are affected by the presentation format.
ANOVAs are used to compare each of the eye movement measures separately. Format has a statistically significant effect on number of fixations (F(1,26)=9.279, p=0.005), total fixation time (F(1,26)=10.924, p=0.003), and the number of regressions (F(1,26)=10.827, p=0.003) but not on maximum fixation duration or the read ratio. Thus, format B has more observed fixations and therefore a longer total fixation
64
Effect of Presentation on Reading Behaviour
 time as well as more regressions. This confirms the hypothesis that less eye movements would be observed for Format A. This can also be seen visually in Figure 3.10 and Figure 3.11. Both figures show the eye movements for reading and answering questions on the questions and text tutorial pages. For format A it can be seen that whilst participants did use the text to answer the questions (Figure 3.10) there are fewer fixations and therefore less reading of the text compared with Format B (Figure 3.11).
Figure 3.10. Example of eye movements from reading and answering questions on questions and text tutorial page for Format A (5 → 5/6)
L2 readers have significantly longer maximum fixation durations (F(1,26)=12.230, p=0.002) and higher read ratios (F(1,26)=4.350, p=0.040) compared to L1 readers. Additionally, L2 readers have significantly longer total fixation durations than L1 readers (F(1,26)=5.870, p=0.023). However now there is no difference between the numbers of fixations observed for L1 and L2 readers and there is no significant difference between the numbers of regressions observed for L1 and L2 readers. This is an interesting result as no difference in the number of fixations between L1 and L2 readers indicates that the increase in time taken for L2 readers is due primarily to increased fixation duration.
The conclusion from this analysis is that pre-reading of the text before questions is asked (Format A) decreases the time needed to answer the questions. This means
                                                                                                             65
 Effect of Presentation on Reading Behaviour
that the participants are using their knowledge of the text to answer the questions as well as checking the text for the correct answers.
Figure 3.11. Example of eye movements from reading and answering questions on questions and text tutorial page for Format B (B: 5/6)
3.4 Discussion and Implications
The objective of this chapter is to investigate whether the presentation of text and comprehension questions in an eLearning environment the performance outcomes of participants and how those participants implicitly interact with the learning materials. Extending this question further we also investigate these effects on two groups of readers, L1 and L2 readers. The availability of learning materials to a wide and varied audience is becoming more common with the growth of online eLearning environments and online courses, such as MOOCs. More learners are reading materials written in their non-native language. The effects of this need to be explored further, the importance is only growing as accessibility to foreign language materials is becoming easier.
The results generally confirm that whilst L2 readers take longer to read content their comprehension is no different to L1 readers (Dednam et al., 2014; Kang, 2014). Delving deeper into there is a discrepancy in read times, we move to eye movements for insight. When reading text with no questions present, L2 readers are
66
Effect of Presentation on Reading Behaviour
 observed to have higher numbers of fixations for longer durations than L1 readers. The divide between L1 and L2 readers is primarily due to fixation duration once the questions and text are presented together.
The hypothesis that format affects performance outcomes and eye movements was confirmed. The formats elicited distinct eye movement and reading behaviours. The presentation format can therefore be manipulated to promote specific behaviours. In the next subsection, recommendations are made based on these observations. There were some surprising results on the effects of presentation format that will now be discussed.
The scores from format D are somewhat surprising given that pretesting with multiple-choice questions has been shown to benefit subsequent learning (Little & Bjork, 2012). In fact, just memorizing the pre-test questions instead of answering them has been shown to improve recall of information (Little & Bjork, 2012). This is analogous to format D where participants were given the comprehension questions before they read the material to answer them. Participants were told that they should read the questions and were welcome to answer them if they wished. Yet our results showed no improvement in comprehension scores compared with the control presentation format which required participants to rely purely on their memory of the text to answer the questions (format C).
Furthermore, format A had two surprising effects on participants’ behaviour. The first was that for this format there was no correlation between participants’ quiz scores and their subjective ratings of understanding. This is surprising because for formats C and D, where the participants had to answer the questions without the text being available, there were very strong correlations. The effect of showing the text before asking the questions was believed to, at least, partially mimic these formats thereby partially enhancing the ability to subjectively rate understanding. This however was not the case. The second surprising effect of format A was that it was hypothesized that participants would read format C more thoroughly, in terms of fixations and read ratio than format A. This was not found either; instead participants read the text in format A as thoroughly as participants did in format C.
3.4.1 Recommendationsforpresentingtextandassessment questions
This section outlines recommendations based on the observations from this study for 1) educators designing courseware in eLearning environments, and 2) design considerations for developers of eLearning environments. The analysis has established that the presentation of text and evaluation resources, such as quiz questions, impacts learning outcomes and reading behaviour. The presentation format can be manipulated to optimize the performance outcomes of students, thereby increasing their understanding.
Formats C and D were shown to promote more accurate self-assessment of comprehension, which minimizes both under- and over-estimation of knowledge. Formats A and C were shown to promote more thorough reading of the learning
67
Effect of Presentation on Reading Behaviour
 materials compared to D, therefore in the context of learning this is a more optimal outcome. Given the aims promote thorough reading and accurate self-assessment format C is thus optimal.
The differences in eye movement measures and reading behaviours reflect the overall purpose and goals placed on the reader. If an educator wants to promote thorough reading, the goals placed on the reader should not be targeted with the use of quiz questions. In this case, students only read the parts of the text that they think contains the answers. However, not showing the text with the questions means that the students have to rely too heavily on short-term memory and this impacts their quiz scores. The happy medium is format A where the students are requested to read the text and then move on to answer the compression questions. Of course this raises the question of how to make students read the text before moving on to the questions and text page. This is where eye tracking can be utilized. The eye tracker can be integrated into the learning environment so that it can monitor reading behaviour. Once the student has read the text then the learning environment would allow the student to move on to the questions.
3.5 Conclusion and Further Work
The study presented in this chapter was designed to increase our understanding of how text and comprehension questions presented in eLearning environments affect eye movements and performance outcomes. These effects are investigated for L1 and L2 readers. We found that presentation of text and comprehension questions affect L1 and L2 readers consistently. There is a difference between L1 and L2 readers, where L2 readers take longer to complete the task. However, L1 and L2 readers are otherwise no different. Following on this observation we observe that L2 readers have consistently longer fixation durations and in the situation where reading is the primary task, L2 reader have more fixations than L1 readers.
Importantly, making participants rely on memory to answer assessment questions promotes more accurate subjective ratings of understanding. When participants are asked comprehension questions after reading the content and have no reference back to the text they can more accurately gauge their understanding. When shown the text with the assessment questions participants are unable to gauge their own understanding.
The primary finding is that different presentation sequences of text and comprehension questions affect performance outcomes and eye movements of participants. The order in which text and comprehension questions are presented to students can therefore be manipulated to optimize performance outcomes and / or reading behaviour.
A limitation of this study is that only two types of questions were investigated in this analysis, being multiple-choice and cloze questions. These are commonly used question type but not the only types generally available in eLearning environments, so further research should investigate what effect other question types have on the observed behaviour.
68
Effect of Presentation on Reading Behaviour
 Further exploration of presentation formats on mobile devices would be beneficial given the prevalence of this technology, as this study only considers reading from a computer screen in a university setting.
One might ask why formats in which only question or only text were not used. In the former, we would be able to assess the answering behaviour and performance as a baseline to assess intuition and prior-knowledge. It would be quite informative to test. For the purposes of this experiment we were highly focused on reading behaviour of the text foremost, and questions second, so the case was omitted. However, follow-up should be run because the implications for adaptive eLearning are quite useful in that is we could predict from a student’s eye gaze whether they know the answer to a question or are confident in answering a questions, the learning material could be adapted to help them. This information is useful in addition to the answer correctness as it would indicate areas which the student needs help with, or conversely, already excels at. With the latter case it would be interesting to examine baseline reading behaviour in the absence of any test to see what that behaviour look likes, and compare it to the observed behaviour in this chapter.
The next step in our investigation of how students read in eLearning environments involves predicting their reading comprehension from their eye movements. The first point of call is further analysis of this data set. In particular, a specific pattern in eye movements is observed for the questions and text pages. The unique eye movements seen for these pages are analysed further in the proceeding chapter as a method of estimating learning in eLearning environments, and therefore to provide feedback to developers of eLearning environments. Following on from this we investigate prediction of comprehension scores from eye movements. This would allow for the removal of comprehension questions as well as the dynamic change of textual material based on predicted behaviours.
69
Effect of Presentation on Reading Behaviour
 70
Chapter 4
Chapter 4. Answering Questions in eLearning Tutorials
 “Google can bring you back 100,000 answers. A librarian
 In Chapter 3 we investigated how different sequences of text and comprehension questions affect eye movements and learning outcomes. Two of these formats, A and B, provided participants with the opportunity to read text whilst answering the questions. The eye movements that occur as a result of these presentation formats are characterised by transitions between the questions and the text to find or confirm the correct answer. We term these eye movements as answer-seeking behaviour. In this chapter we describe answer-seeking behaviour and present a method for measuring and comparing this behaviour. We propose using the degree of answer-seeking behaviour as an implicit measure of question difficulty. The end of the chapter explains how the use of eye movement to predict implicit question difficulty can benefit the design of eLearning environments. This chapter includes work that was presented at CogInfoCom 2013 (Copeland & Gedeon, 2013b) and work presented in IEEE Transactions on Emerging Topics in Computing (Copeland & Gedeon, 2015).
4.1 Introduction
Eye tracking provides the capability of providing feedback about students answering behaviour in eLearning environments. This feedback can then be used to monitor student learning behaviour as well as to improve learning materials. The use of eye tracking to analyse answering behaviour in eLearning environments has been attempted. Results are promising given that it has been shown that eye movement measures can be used to predict student performance on certain
can bring you back the right one.”
 ― Neil Gaiman
71
Answering Questions in eLearning Tutorials
 problems such as answering physics problems (Chen et al., 2014). Eye tracking has also been used to analyse how multiple-choice questions are answered giving information on how eLearning environments can be designed in order to exploit such behaviour (Nugrahaningsih et al., 2013; Tsai et al., 2012).
We investigate eye movement measures for analysing reading behaviour and reading comprehension when text and comprehension questions are presented on the same page. The situation of presenting text on the same page as comprehension questions elicits unique eye movement behaviour. In this chapter we move toward answering the question of whether eye movements can be used to predict how difficult students’ find a task and if this can be used to provide feedback about that task. Our objective is to identify eye movement measures that will be useful for providing feedback in eLearning. The central question for this chapter is therefore:
Can eye gaze be used to provide feedback about learning behaviour in eLearning environments for L1 and L2 readers?
Primarily, we hypothesise that there will be varying levels of answer-seeking behaviour for each question as well as between participants. It is for this reason we propose the use of this measure for evaluating text and question difficulty. Additionally, we propose using this measure for evaluating students reading and answering behaviour to analyse their ongoing performance.
There are two formats, A and B, under investigation. Given the results from the previous chapter, the eye movements observed will likely be affected by format. In format A there is pre-exposure to the text before being presented with the questions and the text. We hypothesise that this pre-exposure to the text will induce less answer-seeking than is observed when there is no pre-exposure (i.e. Format B).
In this chapter we further analyse a subset of the data from the user study described in Chapter 3. The data used is that collected for formats A and B (A: ! → !/$; B: !/$). As this is an extension of the user study presented in the previous chapter we omit the details of the user study methodology and description of data set. For further details on these data sets refer to Chapter 3. The bulk of the chapter is an analysis of answer-seeking behaviour. We present the uses of answer-seeking behaviour for feedback in eLearning environments. Finally, we will conclude and indicate how these results will be used in the future work of developing an adaptive eLearning environment.
4.2 What happens when text is presented with questions?
Formats A and B (A: ! → !/$; B: !/$) provide participants with the opportunity to check the text whilst answering the questions. Participants exhibit specific eye movement behaviours as a result. This analysis outlines an investigation of these behaviours. It begins with looking solely at the data from format A as it provides insight into answering behaviour after a participant has read the text. In the case of format B, participants have no knowledge of the text before being presented with
72
Answering Questions in eLearning Tutorials
 the questions and the behaviour exhibited is to find the answers. We then move to defining this behaviour as a method of providing informative feedback in eLearning.
4.2.1 Eyemovementsandansweringbehaviour:towards Answer-Seeking
Format A offers an interesting case where participants have already read the text before they see the questions. We have established in Chapter 3 that participants do read the text less on the questions and text page for format A than they do for format B. The hypothesis is that this would mean that participants would read the second presentation of the text less than the first.
For format A, there are negative correlations between each of the following measures to the score for the multiple-choice question: the number of fixations (r=- 0.8), the total fixation time (r=-0.8), and the number of regressions (r=-0.7). The results are similar for the cloze questions, where there are negative correlations to the following measures to the score for the cloze question: the number of fixations (r=-0.8), the maximum fixation duration (r=-0.8), the total fixation time (r=-0.8), the number of regressions (r=-0.8), and the regression ratio (r=-0.8). These correlations indicate that participants tended to do worse on the quiz if they read both the questions and the second appearance of the text more. The definition of more reading is that there are high numbers of fixations and regressions as well as a longer total fixation time.
Additionally, there are correlations between the eye movement measures observed when reading each type of question and the reading behaviour seen when reading the second display of the content. We hypothesise that those participants who re-read the question more are having difficulty answering the question and would therefore exhibit similar behaviour when reading the text for the second time. We found positive correlations between the number of fixations observed for reading the multiple-choice question (r=0.7) and the cloze question (r=0.6) to the number of fixations observed for the second display of reading the content. Similarly, a positive correlation was found between total fixation time (r=0.7 and r=0.6, for multiple-choice and cloze questions respectively) and number of regressions (r=0.8 and r=0.7, for multiple-choice and cloze questions respectively) observed when comparing the eye movement recorded for the multiple-choice and cloze question to the second display of the content.
More reading is indicative of the participant’s lack of understanding of either the questions or the content. Time spent reading questions and referencing text for the questions is related to the participant’s understanding whereby longer time spent answering the questions indicates less understanding.
The participants who do not understand the question or the content well enough to answer the question, seek to find the answer by re-reading both the question and the content. We term this answer-seeking behaviour. Answer-seeking behaviour is indicative of the participant’s lack of confidence in answering the questions. The
73
Answering Questions in eLearning Tutorials
 participant’s confidence is related to his actual understanding of the content, his perceived familiarity with the subject matter, as well as his confidence in his or her abilities to answer the questions correctly.
4.2.2 Answer-Seekingbehaviour
There are many reasons for why a participant would seek an answer, namely they do not know the answer and / or they are not confident with the answer. It is beneficial to measure such behaviour so that feedback can then be given based on the existence and the extent of answer-seeking observed.
Figure 4.1. Example of answer-seeking behaviour
This behaviour can be seen visually in Figure 4.1, where there are large jumps between the questions and the text regions, which are followed by extensive reading of the text. Reading only occurs in certain paragraphs of the text, where the participant thinks the answer is. There are also heavy fixations on the questions indicating re-reading of the questions. We propose measuring answer-seeking behaviour by recording the jumps between question and text regions and the reading behaviour recorded after each scan jump. The reading behaviour is detected
                                                                                                                                                           74
Answering Questions in eLearning Tutorials
 and recorded using a combination of reading detection algorithms (Buscher et al., 2008; Campbell & Maglio, 2001).
4.2.3 Effectofformatonanswer-seeking
We have now established a definition for answer-seeking behaviour and how to measure it. Answer-seeking behaviour can be recorded for formats A and B. We now investigate the hypothesis that pre-exposure to the text will induce less answer- seeking than is observed when there is no pre-exposure (i.e. Format B). The answer- seeking behaviour for each of the formats and each reader type are show in Table 4.1. The results show that there is little difference between the L1 and L2 readers, however more jumps are observed for format B as compared to format A. Furthermore, more answer-seeking behaviour is observed for the cloze questions compared with the multiple choice questions.
Table 4.1. Mean ± standard deviation answer-seeking behaviour for formats A and B (A: 5 → 5/6; B: 5/6)
              Format
A B
Reader Multiple Choice
Type Jumps       Reading Saccades
Cloze
Jumps Reading
    Saccades L1 8.3 ± 1.6       68.6 ± 17.2     10.2 ± 1.8     87.4 ± 18.4
    L2 6.3 ± 2.7 L1 10.3 ± 1.6
L2 11.8 ± 2.7
62.1 ± 28.5 136.6 ± 17.2
142.7 ± 28.5
11.1 ± 3.0 15.1 ± 1.8
19.5 ± 3.0
98.6 ± 30.5 124.4 ± 18.4
160.1 ± 30.5
                      A MANOVA was used to test for statistical significance of eye movement measures between formats. The correlations between the dependent variables are all within the range of r=-0.4 and r=0.9. Levene’s test for equality of variances shows that there is homogeneity for all dependent variables (p>0.05) and the Box’s M value of 17.092 (p=0.207) is interpreted as non-significant so we can be satisfied that there is homogeneity in the variance-variance-covariance matrices.
There is a statistically significant difference in eye movement measures based on the presentation format to which the participant was exposed to, F(4.23)=4.199, p=0.011; Wilk's λ=0.578, partial η2=0.422. There is no statistical difference between the L1 and L2 readers and no significant effect of interaction.
ANOVAs are used to determine how the eye movements differ for the formats. Format has a statistically significant effect on the number of reading saccades recorded after scan jumps for multiple-choice questions (F(1,26)=10.006, p=0.004), and scan jumps for cloze questions (F(1,26)=7.050, p=0.013), but not on multiple- choice scan jumps or cloze reading saccades after scan jumps.
For format B there is significantly more reading after a jump between the multiple-choice questions and the text than for format A. Yet this is not true for the cloze questions, contrary to the hypothesized behaviour. Additionally, format B has significantly more jumps between the cloze questions and the text than format A and once again this is not true for multiple-choice questions. Format does not affect
75
Answering Questions in eLearning Tutorials
 the reading behaviour for the cloze questions. This is an interesting outcome, reading the text before having access to the questions does not help participants when answering the cloze questions. Furthermore, reading the text before knowing the questions (Format A) does not reduce the number of jumps between the multiple-choice questions and the text.
The difference in the observed effect of pre-reading the text between the two types of questions is presumably due to the nature of the questions. Multiple-choice questions are more of a pattern matching exercise that promotes reading the multiple options and scanning through the text to try to find a similar phrase. We hypothesised that fewer of these jumps would be required for format A as the participants had already read the text and would presumably have some knowledge about the answers. The fact that there is significantly more reading after these jumps for Format B compared to A is supportive of this hypothesis.
Conversely, cloze questions require the participant to come up with a word to fill in the blank. This is comparative to a search task where the participants are looking to find words in the text. However, since they have to come up with a word, it does require a certain level of comprehension of the text in order to come up with the word so we would expect less jumping between the questions and text and more reading of both the questions and the text. This is what was found; for format A participants had significantly less jumps between the cloze questions and the text than format B. In this case, irrespective of the amount of reading, the fact that there are fewer jumps for format A confirms the hypothesis that reading the text before seeing the questions (format A) helps answer the cloze questions.
This analysis partly confirms the hypothesis that participants would on average show less answer-seeking behaviour when presented with format A compared to B. Although pre-reading the text before seeing the questions does not decrease the amount of answer seeking observed for the multiple-choice questions, it does for the cloze questions, which confirms the hypothesis for that kind of question.
There is no significant difference between the L1 and L2 readers in answer- seeking behaviour. This contrasts with the other analyses in the previous chapter where a clear difference was evident. This is an important finding: as the aim is to level the playing field for readers, and providing questions and text together will do so.
4.2.4 UsingAnswer-SeekingforFeedbackineLearning
We propose two purposes for measuring answer-seeking behaviour. The first is as a feedback tool for instructors and / or authors concerning the nature of how students read and answer questions. The second is to provide feedback to instructors about how individual students are performing.
4.2.4.1 Feedback about answerability of questions
This discussion will begin by elaborating on the first use mentioned above, feedback about comprehension questions for Format A. The average number of jumps
76
                          Answering Questions in eLearning Tutorials
 between question and text, the average reading behaviour and the average score for that question for both formats are shown in Table 4.2. Since the first part of the analysis showed that there was no statistically significant difference between L1 and L2 readers in this behaviour, we have not separately the reader types for this example.
Table 4.2. Answer-seeking behaviour averages per question for format A (A: 5 → 5/6)
          Quest. No.
1 (MC) 2 (CL)
3 (MC) 4 (CL)
5 (MC) 6 (CL)
7 (MC) 8 (CL)
9 (MC) 10 (CL) 11 (MC) 12 (CL) 13 (MC) 14 (CL) 15 (MC) 16 (CL) 17 (MC) 18 (CL)
Format A
Scan Jumps
11.1 ± 9.5 12.2 ± 9.2 3.3 ± 3.4 5.9 ± 2.6 4.6 ± 4.6 4.3 ± 3.2 10.6 ± 8.0
19.5 ± 13.6 6.1 ± 5.0 11.2 ± 8.1
17.3 ± 12.9 17.3 ± 11.5 4.6 ± 4.1 7.9 ± 5.6 5.8 ± 7.7 7.1 ± 4.2 5.5 ± 4.2 9.3 ± 5.3
Format B
           Reading
after jump               after jump
Score
Scan Jumps
Reading
Score
0.6 ± 0.5 1.0 ± 0.0 1.0 ± 0.0 0.9 ± 0.2 0.9 ± 0.3 1.0 ± 0.0 0.9 ± 0.2 0.9 ± 0.2 0.9 ± 0.2 1.0 ± 0.0 0.5 ± 0.5 1.0 ± 0.0 0.7 ± 0.4 1.0 ± 0.0 0.7 ± 0.4 1.0 ± 0.1 0.8 ± 0.4 1.0 ± 0.0
      85.9 ± 79.4 115.6 ± 118.6 23.9 ± 36.7 65.5 ± 52.8 36.9 ± 40.3 32.0 ± 20.8 58.6 ± 47.7 203.3 ± 173.5 73.5 ± 90.9
77.5 ± 55 148.1 ± 107.9 146.7 ± 120.6
57.3 ± 60.6 65.9 ± 80.4 64.8 ± 100.4 61.2 ± 46 33.7 ± 32.7 71.7 ± 42.8
0.7 ± 0.5 0.9 ± 0.2 1.0 ± 0.0 1.0 ± 0.0 0.8 ± 0.4 1.0 ± 0.0 0.9 ± 0.4 0.9 ± 0.2 0.7 ± 0.5 1.0 ± 0.1 0.7 ± 0.5 1.0 ± 0.1 0.8 ± 0.4 1.0 ± 0.0 0.7 ± 0.5 1.0 ± 0.1 0.9 ± 0.3 0.9 ± 0.2
9.8 ± 5.5 20.7 ± 15.9 6.5 ± 3.3 9.2 ± 7.3
12.5 ± 9.2 8.8 ± 5.5 9.7 ± 3.5
22.1 ± 10.1 14.7 ± 11.5 20.9 ± 7.6 18.4 ± 13.4 23.5 ± 13.1
9.8 ± 5.0 12.3 ± 9.9 8.7 ± 6.0 12.5 ± 6.9 5.9 ± 3.8 16.3 ± 6.3
178.9 ± 118.3 166.6 ± 136.1 92.7 ± 55.7
105.7 ± 99.3 178.2 ± 96.6 50.1 ± 37.9 81.9 ± 79.5
214.2 ± 134.1 178.5 ± 101.0 150.6 ± 70.9 189.8 ± 102.8 202.7 ± 138.7 124.5 ± 79.4
90.8 ± 80.3 128.3 ± 64.8 107.1 ± 101.4
91.4 ± 79.3 117.3 ± 50.3
There
above the number of jumps observed for format B are higher than for format A. The same observation can be said the reading behaviour after each jump. For format A, there is a minimum average of 3.3 jumps and 23.9 reading transitions for question 3 and a maximum average of 19.5 jumps and 203.3 reading transitions for question 8. From these observations, question 3 was the easiest question on average to answer as fewest jumps and lowest amount of reading was needed to answer the question. Question 8 was the most difficult question on average to answer as the most jumps and the most reading were needed to answer the question. For format B however, the lowest number of jumps is 5.9 for question 17 but the least reading behaviour after a jump was recorded for question 6 at 50 reading transitions. The most jumps and reading behaviour were not recorded for the same question either where the most jumps of 23.5 was for question 12 and the most reading of 214 for question 8. Although the answer-seeking results for the formats do not exactly match one another, they are roughly similar. Questions appear to have similar relative answer- seeking observed under both formats. This is why answer-seeking should be recorded for both the jumps and reading behaviour after the jumps as it provides additional context to the answering behaviour.
is a sizeable range in the average jumps for each question. As we found
77
Answering Questions in eLearning Tutorials
 On average, there were more jumps and reading observed for some questions; that is, more answer-seeking recorded for particular questions. This indicates that some questions are more difficult to answer than others. This difficulty could be for several reasons, such as ambiguity in the question or technical difficulty of the question. There is little correlation between the number of jumps or the amount of reading transitions observed to the score obtained for the question (r=-0.1, for both measures for format A and r=0.2, r=-0.3 for jumps and reading for format B). Therefore, performance on the question is not an accurate measure of how difficult the participants found the questions. Instead the answer-seeking behaviour is a measure of how difficult participants found the questions to answer as well as how much attention they gave to the question. We propose the use of answer-seeking behaviour be used in combination with answer correctness to describe how difficult a question is.
The large standard deviations seen in Table 4.2 show that there is a large variation in the observed answer-seeking behaviour. This is expected, as there is a large variation in eye movement behaviour observed between individuals (Rayner, 1998). Furthermore, we are only considering average performance on questions, as we would expect that some individuals would find questions easier to answer than others. This leads us to the discussion of using answer-seeking behaviour to quantify individual student performance.
4.2.4.2 Feedback about student performance
We now investigate the use of answer-seeking behaviour to analyse participant learning performance. We will only use the data from Format A as a case study for this proposed use as this is sufficient for showing its use.
The average number of region jumps between question and content, the average reading behaviour, and the total score for each participant are shown in Table 4.3. Note that in Table 4.3 the participants are listed in ascending order of average number of jumps. This ranking of participants’ shows the extent of the variance of answer-seeking behaviour each participant exhibits. Once again we can use this information to extrapolate how difficult the individual participant found the tutorial and quiz.
As we would expect, the L2 participants did not have higher answer seeking than the L1 readers. The L2 readers are distributed between the L1 readers. Participant 15 showed quite a high amount of answer-seeking behaviour whilst the participant 1 showed about a seventh of the number of jumps and reading behaviour. There is a small negative correlation between the average number of region scans and the participants’ total score (r=-0.3). This indicates that the participants who displayed less answer-seeking behaviour were not necessarily correct and may be over confident with their answers. Therefore, answer-seeking behaviour does not guarantee that the correct answer is selected by the participant, and neither does the lack of answer-seeking behaviour. This could be a by-product of the laboratory setting of the experiment where some participants, knowing they are being watched, will read more thoroughly and carefully to avoid making errors. Further investigation of this behaviour should be considered in an in-the-wild type
78
Answering Questions in eLearning Tutorials
 study. However, we propose the use of answer-seeking behaviour, in combination with answer correctness, as an implicit measure of how difficult a participant finds the text and quiz.
Table 4.3. Average answer-seeking behaviour per participant for format A (A: 5 → 5/6)
               Participant ID
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Region jumps       Transitions as Reading
classified
Total Score L1/L2
17 L1 18 L1 17 L1 17 L2 16.5 L1 13 L2 17 L1 14.5 L2 18 L1 14.5 L1 15 L1 13.5 L1 16 L1 15.5 L2 16 L1
         2.9 ± 3.2 3.2 ± 3.6 5.0 ± 3.4 5.4 ± 4.7 6.4 ± 4.7 6.8 ± 5.8 6.9 ± 5.9 7.8 ± 6.8 8.6 ± 5.6 10.6 ± 11.4 11.4 ± 6.9 12.3±8 12.3 ± 9.7 14.8 ± 8.6 22.1 ± 14.3
24.9 ± 34.9 23.7 ± 21.9 67.4 ± 68.3 46.6 ± 55 40.3 ± 38.9 47.1 ± 43.1 39.4 ± 43.4 95.8 ± 108.1 60.2 ± 51 89.7 ± 103.6 136.2 ± 57.9 56.7 ± 51.9 143.7 ± 163.8 139.4 ± 95.1 173.9 ± 129.6
              Once again there is high variation in the observations as shown by the standard deviations. This is a reflection of the differing difficulty of the 18 questions as already discussed and shown in Table 4.2. The standard deviations for each participant can be used to evaluate how consistently difficult that participant found the questions. For example, a low standard deviation indicates low variability and therefore that the participant consistently showed similar answer-seeking behaviour. This result indicates that the participant found each question to be similar in difficulty. This information can be used to construct questions of similar or differing difficulty.
4.3 Using Answer-Seeking Behaviour for Feedback
We have established a definition of answer-seeking behaviour of recording the large jumps between questions and text combined with the amount of reading that is performed in the question and text areas. We propose the use of this measure as an indicator to the instructor of question difficulty as well as the participant’s implicit difficulty in completing the quiz. We will now establish the benefits of such information.
There is a range in the answer-seeking behaviour seen for each of the questions. This shows that some questions were harder to answer then others. The use of answer-seeking behaviour as a measure of question difficulty can be used as a
79
Answering Questions in eLearning Tutorials
 feedback system to an instructor. Such information can be used to gauge the difficulty of questions. This difficulty could be due to factors such as the technical nature of the material, and ambiguity in the material. Conversely, the instructor could see that the question is too easy and change it to be more challenging. This information could also be used to weight questions so that more difficult questions are weighted higher than those that are less difficult.
Furthermore, there is a range of answer-seeking behaviour seen among the participants. Some found the quiz more challenging than others. It is beneficial for learning for students are challenged equally in respect to one another, so that some students aren’t being under-challenged whilst others are over-challenged. Under- challenged students may get bored and lose interest in the material whilst over- challenged students may become anxious and disheartened by the material. In either case, there is a negative impact on the learning process. Using answer-seeking behaviour as an implicit measure for a student’s confidence in the material can provide the framework for an adaptive online learning environment. Such an environment can use input from the eyes to measure the answer-seeking behaviour and alter the learning material and questions in response to the student’s behaviour. That is, if a student is found to be having no difficulty completing a quiz, then the material can be altered to be more advanced and technical. Conversely, if a student is having difficulty then the material can be altered to be less technical and more basic.
4.4 Conclusion and Further Work
In this chapter we have investigated answer-seeking behaviour as a method of evaluating text comprehension for a tutorial and quiz. Answer-seeking behaviour is the eye movement behaviour exhibited when students are presented with questions and text on the same page. Answer-seeking behaviour is characterised by jumps between the questions and the text to find the correct answer, or to reassure the participant that they have the correct answer. We hypothesised that the pre- exposure to the text before being asked the questions would affect the reading behaviour observed when presented with the text and questions, and therefore induce less answer-seeking than is observed when there is no pre-exposure (i.e. Format B). However, we found that pre-exposure to the text does not decrease answer-seeking behaviour for multiple-choice questions, although it does for the cloze questions, which partly confirms the hypothesis.
An interesting point found from the study was confirmation of the hypothesis that the presentation format affected the L1 and L2 participants in the same way. Additionally, there is no significant difference between the L1 and L2 readers in answer-seeking behaviour. This is an important finding as it means that any conclusions regarding how presentation format affects students can be generalized for both reader types and it is not an additional factor that creators of learning materials have to take into account.
Additionally, we hypothesised that there would be varying levels of answer- seeking behaviour for each question as well as between participants. We have
80
Answering Questions in eLearning Tutorials
 proposed the use of answer-seeking behaviour to describe how difficult a question is to answer and as an implicit measure of how difficult a participant finds the tutorial and quiz. The eventual goal is to create a tool that will provide feedback to instructors about implicit behaviour of students performing a reading task through an online learning environment. For example, if the instructor receives feedback that multiple students are failing to understand specific parts of the text then the instructor can dedicate more time explaining these concepts during face to face teaching time, or could re-word the content to make it easier to understand. Furthermore, the instructor can be given feedback about how students are reading questions and be able to deduce if questions are appropriately worded or are ambiguous and hence causing low scores or confusion. Finally, the information about reading behaviour can also be used to dynamically alter tutorial content to personalize the learning experience where students familiar with or excelling at specific content can be given more advanced content to read compared to students that are not familiar with the content or who find it harder to understand.
As stated in the previous chapter, a limiting factor of the study is the use of only two question types. The answer seeking behaviour from multiple choice and cloze questions varies so it is pertinent that different questions be assessed. Additionally, participants were not asked how difficult they found the questions to answer. This is a limitation of the study that we deal with in Chapter 7 where we investigate participants’ perceptions of difficulty.
The results from this study will be used as the foundation for uses of applying eye tracking in adaptive eLearning. Eye tracking can be used to determine learning rates and behaviour during reading so that learning can be adapted to students’ needs as well as increasing the quality of the materials in the environment. The next step in the investigation is to predict reading comprehension scores from eye movements, including answer-seeking behaviour.
81
Answering Questions in eLearning Tutorials
 82
Chapter 5
Chapter 5. Effects of Presentation on Prediction of Comprehension
Predicting reading comprehension from eye gaze data is a difficult task. In this chapter we investigate the effects of presentation format on prediction accuracy of reading comprehension measures. The data from the user study outlined in Chapter 3 is used to explore the problem of predicting reading comprehension from eye gaze using machine-learning techniques. Chapters 3 and 4 established that presentation format affects eye movements and reading comprehension. The hypothesis examined in this chapter is that the different formats will cause different levels of prediction accuracy. The investigation begins by using artificial neural networks (ANNs) to predict reading comprehension scores. To help increase prediction accuracy of the ANN we investigate the use of fuzzy output error (FOE) as an alternative performance function to mean square error (MSE) for training ANNs as a means of improving reading comprehension predictions. The results show that the use of FOE provides more accurate predictions. Additionally, the FOE trained ANN outperforms other comparison machine learning techniques. Nevertheless, we deduce there are complex relationships between eye movements and reading comprehension as three hidden neuron layer ANNs provided the best classification results. We encountered problem with imbalanced data sets that requires further investigation. In this chapter we present research that extends work presented at ICONIP 2014 (Copeland, Gedeon, & Mendis, 2014a) and based on work published in AIR journal (Copeland, et al., 2014b).
83
Effects of Presentation on Prediction of Comprehension
 5.1 Introduction
In this chapter the data from the user study conducted in Chapter 3 is used to investigate methods for predicting reading comprehension from eye gaze using machine-learning techniques. The task of predicting quantified measures of reading comprehension has been attempted with poor results (Martínez-Gómez & Aizawa, 2014). Additionally, little prior work has been done to predict reading comprehension via machine-learning techniques. Prediction of reading comprehension has classically been made using statistical analysis of eye movement measures that have been derived from the eye gaze signal such as fixation duration (Underwood et al., 1990) and regressions (Rayner et al., 2006). Current applications of eye tracking in reading analysis only take into account assessment of reading behaviour such as using fixation time to predict when a user pauses on a word (Hyrskykari et al., 2000; Sibert et al., 2000) and finding word relevance (Loboda, Brusilovsky, & Brunstein, 2011). Instead, we look at combining eye movement measures to make more complex predictions about reading behaviour. The central question being asked in this chapter is:
Can eye tracking data be used to predict reading comprehension scores in eLearning environments for L1 and L2 language readers?
However, we know that predicting reading comprehension is not trivial so we approach this question in two chapters, this chapter and the next. In each of the two chapters we investigate factors that could affect prediction accuracy. In this chapter we also investigate the effect of text presentation on prediction accuracy of comprehension:
Does presentation of text affect predictions of comprehension?
We explore this question by investigating different methods of increasing prediction accuracy. Initially, we build on previous work that involves prediction of reading comprehension from eye gaze using fuzzy output error (FOE) as the performance function for back-propagation training of feed-forward artificial neural networks (ANNs) as this showed promising results (Copeland, Gedeon, et al., 2014a). We extend this research by exploring different membership function shapes (FMFs) for calculating FOE and compare these results to using mean square error (MSE) as the performance measure for training. The next part of the analysis is comparison of the results from the ANN classification to comparative classification techniques to deduce whether this is the optimal technique. We then move to assess prediction of the different questions types, multiple-choice and cloze. Finally, we perform cluster analysis of the more complex formats.
As this chapter uses the eye gaze data collected from the user study explained in Chapter 3, we will not restate the details of the methodology. Please refer to Chapter 3 for more details about the user study. This chapter begins with a background review of classification and clustering techniques that will be used throughout the chapter, which is followed by an introduction to fuzzy output error; method for
84
Effects of Presentation on Prediction of Comprehension
 analysis; followed by a discussion and implications for eLearning; finally, the conclusion and further work are outlined.
5.2 Making Predictions
Prediction involves modelling input data to produce an output that reflects the input in some way. Traditionally, types of machine learning problems are dependent on the learning strategy used. These are primarily supervised, unsupervised and reinforcement learning (Russell, Norvig, Canny, Malik, & Edwards, 2003). Supervised learning is where outputs are known and used to train the model. Unsupervised learning is where the outputs are not known so a structure in the inputs has to be found by the learning algorithm. An example of unsupervised learning is clustering. This section provides background information on several prediction techniques, which are used in this chapter. However, this is by no means to full coverage of prediction methods and is rather a short list of some commonly used methods.
The main analysis is the use of artificial neural networks (ANN) to which we compare to decision tree based and k-nearest neighbour (kNN) classifiers as they are commonly used. The advantage of using an ANN is that they allow for prediction of two output values, which suits the problem given that there are two comprehension questions. Additionally, the use of ANNs has shown promise for this type of problem (Copeland et al., 2014a). The problem of classifying reading comprehension from eye movements is difficult; indeed a three-hidden layer topology generates the optimal predictions, namely the [12 6 3] topology for both FOE and MSE (Copeland et al., 2014a).
5.2.1 DecisionTrees
Decision trees are commonly used predictive models that map inputs to outputs. Two types of decision tress are classification and regression trees. There are several learning algorithms for constructing decision trees such as CART (Breiman, Friedman, Stone, & Olshen, 1984), ID3 (Quinlan, 1986), and C4.5 (Quinlan, 2014). Decision trees are easy to interpret and fast to learn.
5.2.2 EnsembleLearning
The premise of ensemble learning is the use of many weak learning algorithms in combination to improve predictive power (Rokach, 2010). An ensemble combines a set of supervised learning algorithms, but is itself a supervised learning algorithm as it is trained to make predications. Common weak learners are decision trees and k-nearest neighbour as they are quick to train. However, ensembles can be made from any predictor such as ANN (Hansen & Salamon, 1990). Bagging and boosting are common ensemble techniques (Rokach, 2010).
85
Effects of Presentation on Prediction of Comprehension
 5.2.3 Boosting
Most boosting algorithms consist of making currently misclassified examples more important in the next round of classification. Therefore, a new weak learner is added that focuses on previously misclassified examples (Freund et al., 1999). Boosting is quite often applied to overcome the problem of imbalanced data sets such as using under-sampling, over-sampling, and other forms of sampling to reduce the imbalance. AdaBoost is a common boosting algorithm used for binary classification (Freund et al., 1999), which has been extended for multi-class situations (Zhu et al., 2009). The boosting algorithm used in this chapter is RUSBoost (Seiffert et al., 2010), which uses a mix of random undersampling (RUS) and boosting to deal with imbalanced data sets, a problem prevalent in the data sets used for this analysis.
5.2.4 BootstrapAggregation(Bagging)
Bootstrap aggregation, also known as bagging, is the straightforward strategy of creating multiple predictors for multiple subsets of observations and / or features sets (Breiman, 1996, Rokach, 2010). The bootstrap samples of the data set are chosen at random with replacement from the training set. The result is many diverse classifiers that are aggregated together to find the end prediction of an unseen sample. In the case of regression, the aggregation is an average of the predictors’ outcomes and majority vote for the case of classification.
5.2.4.1 Random Forests
Random forests are a special case of bagging where bagging of both observations and feature sets is performed (Breiman, 2001). As described above, bootstrap samples of observations are generated from which decision trees are constructed for each sample. The decision tree algorithm is modified so that at each candidate split in the decision tree learning process a random subset of features is used as the pool of options for the split. If there are features that are strong predictors of the output variable, then many trees will include this feature thereby being correlated. The modification of the bagging method to include feature bagging helps alleviate the correlation of trees thereby making a stronger ensemble.
5.2.5 K-NearestNeighbour(kNN)Classification
The k-nearest neighbour (kNN) algorithm for classification is conceptually quite easy to understand. The algorithm works by having a set of k training instances for which the test instances are compared (Peterson, 2009). The training instance(s) that are closest to the test instance, as defined by a distance metric such as Euclidean distance, are used to vote for the winning class. In the case of regression, an average is used instead of the mode.
5.2.6 AgglomerativeHierarchicalClustering
Hierarchical clustering is a clustering technique that builds hierarchies of clusters. Agglomerative refers to a bottom-up approach, which all data instances start in
86
Effects of Presentation on Prediction of Comprehension
 their own cluster that are then merged together into larger clusters, until they are in the same group (Xu & Wunsch, 2008). In order to merge the smaller clusters into the larger clusters a distance metric is used to measure dissimilarity between the clusters, or at the base level, data instances (Rokach & Maimon, 2005). Typical distance metrics include Euclidean, Manhattan, hamming, and cityblock distances. Comparing the distances using a linkage criterion performs merging of clusters. Typical linkage methods are single-, complete-, and average-link clustering (Rokach & Maimon, 2005). This choice of distance metric and linkage criterion affects the clusters that are produced from clustering.
5.2.7 Back-propagationArtificialNeuralNetworks(ANN)
ANNs are models of biological neural networks. The basic idea of an artificial neuron is that a set of weighted inputs are fed into a neuron and summed together. This sum is called the neuron’s activation and if it is above a threshold then the neuron fires. This is analogous to a biological neuron. To calculate if the activation is above a threshold an activation function is used, which can be as simple as a binary threshold function but most commonly sigmoid functions are used (Jain et al., 1996).
Many artificial neurons can be joined together to create networks of artificial neurons. Feed-forward networks are those where there are no loops, and the inputs follow in only one direction, forward. These networks need to be trained to perform the tasks for which they are intended. This usually involves the learning of weights. Different learning algorithms are required for different network architectures (Jain et al., 1996). The most common method of training multi-layered feed-forward ANNs is using the back-propagation algorithm, which is a supervised learning algorithm. These types of networks can be used to perform classification or regression tasks. Back-propagation works by passing a training example through the network, calculating the error from the results output, and using this error to change the weights. Thus, the error is propagated back through the network.
5.3 Fuzzy Output Error (FOE)
Fuzzy Output Error (FOE) (Gedeon et al., 2012) is an extension of FYCLE and SYCLE (Mendis & Gedeon, 2008). FOE uses a fuzzy membership function to quantify the difference between the predicted and the target values, i.e. the error, rather than assign the difference a value of 0, 0.5 or 1, as is done in FYCLE. As opposed to MSE, FOE describes the error in a fuzzy way and then sums the fuzzy errors together to get the total error.
FOE is defined as follows for a data set of n records with matching pairs of target and predicted values for each record 1 to n.
789= ? 1−<=−= BhDEDF∈N. (1) >@A > >
where <() is the membership function of a desired classification and its complement describes the error. The membership function is termed the FOE Membership Function (FMF). The FMF is used to describe the output of a fuzzy classification (or a regression) in regards to how close that output is to the target
    87
Effects of Presentation on Prediction of Comprehension
 output. The membership function itself represents the fuzzy set for good classification15. The value of < J   gives the degree of membership of the error in the good classification fuzzy set and consequently the complement of < J gives the error measure. Therefore, < = − = = 1 and hence there is no error when there is perfect classification. The more <(J) tends toward 0 the higher the error since the difference is larger. The FMF shapes used in this analysis will be trapezoidal or triangular membership functions. FMF’s can be created in any shape in order to describe the output of a function.
It is important to note that the difference between target and predicted values is not taken as the absolute value of the difference (i.e. = − =|). Although this would make the FMF simpler because it would only need one side of a piecewise linear function, not using the absolute value of the difference provides more flexibility in describing the types of error. For example, false negatives may be considered a much worse error than false positives when screening for diseases.
5.3.1 ApproximationofFMFsusingsquashingfunctions
There are many different ways to construct membership functions as described in (Dombi, 1990), however, commonly piecewise linear functions are used as they are easy to handle (Dombi & Gera, 2005). The problem with these functions is that optimisation of parameters via gradient-based methods become complicated, as they do not have continuous derivatives. One of the solutions to this problem is to approximate piecewise linear functions using combinations of sigmoid functions called squashing functions (Dombi & Gera, 2005, 2008; Gera & Dombi, 2005).
A sigmoid function is an s-shaped function that is commonly used as an activation function of artificial neurons, as well as in economic and biological models. The definition of a sigmoid function is shown in Equation 2.
L MN J = 1 ( 1 + D P N Q P M ) ( 2 )
The parameter R controls the steepness of the sigmoid curve, that is, varies the function from a shape either close to linear or more like a step function. The parameter S controls where the centre of the curve, L J   = 0.5, is on the horizontal axis. More precisely, J − S will move the centre to S and J + S will move the centre to –S. These two parameters play an important role in how the sigmoid function will be shaped to approximate the piecewise linear membership functions.
To approximate one half of a trapezoidal or triangular function, we integrate the difference between two sigmoid functions on an interval [a, b] (József Dombi & Gera, 2005, 2008). The definition of the squashing function on interval [a, b] is shown in Equation 3.
WNJ=12Zln(LPNJLPN J)AN (3) M,Y M]Y MPYQ
15 Good classification refers to a level of error between the predicted and the desired that is within a threshold that users accept as either correct or close to correct classification.
                       88
Effects of Presentation on Prediction of Comprehension
 Where S gives the centre of the squashing function and Z gives the steepness of the squashing function. The parameter Z is referred to as the fuzziness parameter and R the approximation parameter. The larger R is the closer the approximation to the trapezoidal function being modelled.
A piecewise linear membership function can therefore be approximated with the combination of two squashing functions using the conjunction operator. The following equation defines the approximation of a trapezoidal membership function (József Dombi & Gera, 2005, 2008).
WN(WN (J)+WPN(J)−1) (4) ^,^ `^,-^ `_,-_
__
When aA = bA = −1⁄2 and ac = bc = 1⁄2 the squashing function approximates a triangular membership function. All FMF shapes are represented in this form throughout the analysis so that gradient descent methods can be used to optimise the error function.
5.3.2 FMFshapesusedtocalculateFOE
In this chapter we utilise 7 FMF shapes, denoted as FMF1 through to FMF7. FMF1 (Figure 5.1(a)) is designed to be a cross between FYCLE and the shape of an MSE curve. The difference between the predicted value and target value is within ±0.2 so is not considered an error and therefore considered correct classification. The difference between the predicted and target value is considered to be erroneous after ±0.2. FMF2 (Figure 5.1 (b)), is designed to be a model of FYCLE. FMF3 (Figure 5.1(c)) is a triangular membership function that is designed to resemble the shape of an MSE curve. The difference between target and predicted values is a lower value for membership in the good classification set the further the difference progresses to -1 or 1.
FMF4 (Figure 5.2(a)) and FMF5 (Figure 5.2(b)) are asymmetrical FMFs that are inverses of each other. They are both a combination of half of FMF1 with the opposite half of FMF2, and were trialled to investigate the effect of asymmetric FMFs, which may have benefit in some applications. The shape of FMF6 (Figure 5.2(c)) is a variant of the FYCLE approximation FMF2. It has a smaller region that defines the difference between the predicted and target values as being completely in the good classification set, i.e. < = − =   = 1. This region is when the difference is between ±0.1 instead of ±0.2. Again, this is to make the error output closer to zero as described above. FMF7 (Figure 5.2(d)) is a variation of FMF1 but is also a combination of FMF1 and FMF3. Again the variation is that there is a smaller region that defines the difference between the predicted and target values as being completely in the good classification set, i.e. < = − = = 1. This region is when the difference is between ±0.05 instead of ±0.2.
             89
Effects of Presentation on Prediction of Comprehension
  Figure 5.1. Plots of : (a) FMF1; (b) FMF2; and (c) FMF3
Figure 5.2. Plots of (a) FMF4; (b) FMF5; (c) FMF6; and (d) FMF7 5.4 Description of data sets
For further details on the user study methodology and the associated data sets please refer to Chapter 3. The raw eye gaze data consists of x,y-coordinates recorded
 90
Effects of Presentation on Prediction of Comprehension
 at equal time samples (60Hz). Fixation and saccade identification was performed on the eye gaze data. Eye movement measures are derived from the fixation and saccade data, which are explained in Chapter 3. These measures are:
• Number of fixations,
• Maximum fixation duration (seconds),
• Average fixation duration (seconds),
• Total fixation duration (seconds),
• Number of regressions and regression ratio,
• Average forward saccade length (pixels) and,
• Reading analysis statistics (read, skim and scan ratios).
We include two additional measures not explained in Chapter 3; these measures are:
Regional Analysis: The fixation-to-word and duration-to-word ratios are measured for the paragraphs where the answers are located. These are measures for how long the participant spent in the area containing the answer to the question. The hypothesis is that there is a relationship between the proportion of attention participants give to the answer paragraphs and the answers they provide.
Answer-seeking behaviour: The behaviour of jumping between the questions and the text to find the answers, discussed in Chapter 4.
Table 5.1 summarises the properties of the data sets used in the predictive analysis. Starting from the first row in the table, the features refer to the number of inputs for the ANNs, and other classifiers. These features are the eye movement measures just discussed and vary depending on the presentation method as the inputs are generated from the pages that the participant viewed. This means that for format A, as the participants view the tutorial content page and then the questions and content page, the inputs are generated from both pages for the scores obtained from the questions and content page. Note that since there is a large difference in the ranges for each of the inputs they are normalized to a range of [0,1].
Table 5.1. Properties of each data set (A: 5 → 5/6; B: 5/6; C: 5 → 6; D: 6 → 5 → 6)
                  Properties
Features
Size (N)
Comprehension scores Class Split %: 2/1.5/1/0.5/0
Format A 36
135
74/7/18/1/0
Format B 20
135
77/1/22/0/0
Format C 28
135
44/19/21/7/9
Format D 40
135
45/8/39/1/6
                       * The class split refers to the split in marks, i.e. for format A, 81% of participants answered the multiple-choice questions correct and 92% of participants answered the Cloze questions correct.
The size of each data set is consistent at 135 instances; this is because there are 15 participants for each format who each viewed 9 topics. The final row refers to the
91
Effects of Presentation on Prediction of Comprehension
 distribution of outputs. The outputs are the total comprehension score from the questions. The classes of comprehension scores are 2, 1.5, 1, 0.5, and 0. The outputs are assigned based on the answers to the multiple-choice and cloze questions. That is, the multiple-choice score can take values of 0 or 1, corresponding to an incorrect or correct answer for the question. Similarly, for the cloze question except that in this case half marks can be achieved so the output that is assigned can take the values 0, 0.5 or 1. The reason for half marks being achieved for the cloze questions is because two gaps requiring a word each for each question, so if a participant got one word correct and not another, they received 0.5 out of 1.
As shown in Table 5.1 the ratio of the number of data instances in each class varies considerably between the formats. We can observe that for formats A and B there are imbalances in the scores for cloze questions, where most people answered the cloze questions correctly for these formats. The cloze question scores for formats C and D are less imbalanced as more participants answered the questions incorrectly. For the multiple choice questions there are slight imbalances in scores for all formats, where the majority of people answered these questions correctly.
5.5 Results and Analysis
The analysis consists of three components; first is the use of ANNs to predict reading comprehension from eye movements, second is the use of common classification techniques to predict reading comprehension, and finally cluster analysis. The results section is organized to reflect this analysis.
5.5.1 ANNpredictionsofreadingcomprehension
The focus of the analysis is on finding a satisficing technique for prediction of reading comprehension from eye gaze data. In previous work we considered the use of a novel performance function for training of the ANNs called Fuzzy Output Error (FOE) (Copeland et al., 2014a). This showed promising results that led to improved predictions. We investigate the use of FOE further in this investigation. In the previous work only one FMF shape was investigated (Copeland et al., 2014a). We extend this investigation to look at the use of 7 FMF shapes to calculate FOE, described in Section 5.3.2.
The ANNs were trained using the scaled conjugate gradient algorithm (Møller, 1993) with the performance function set to be either FOE or MSE. From this point on we denote ANNs trained using FOE as FOE-ANN and ANNs trained using MSE as MSE-ANN. The analysis is performed Matlab R2012a using the Neural Network toolbox. FOE was implemented as a custom performance function. The default training method is the Levenberg-Marquardt algorithm (Hagan & Menhaj, 1994) however this training method will not accept custom performance functions. The scaled conjugate gradient algorithm has been shown to perform faster than other methods available (Møller, 1993).
The number of inputs for each presentation format is outlined in Table 5.1 and all networks have 2 outputs. From initial testing it was found that a single layer
92
Effects of Presentation on Prediction of Comprehension
 network performed poorly for both FOE and MSE. We have chosen two and three layer topologies for the analysis. The following topologies were tested: [10 5], [20 10], [30 15], [12 6 3], [16 8 4], [20 10 5], and [30 20 10]. The notation [X Y Z] indicates neurons in the first hidden layer to the third hidden layer. The analysis revealed that the topology that generates the best predictions is [12 6 3] for both FOE and MSE.
The results from this analysis are reported in Table 5.2, as the average and standard deviations of misclassification rates (MCR) from 10-fold cross validation with standard deviations. The analysis revealed that the topology that generates the best predictions is [12 6 3] for both FOE and MSE. We restrict our presentation of these results to report only average results for all topologies and the optimal topology.
Table 5.2. Misclassification rate (MCR) comparison: FOE versus MSE as the performance function for ANN training (A: 5 → 5/6; B: 5/6; C: 5 → 6; D: 6 → 5 → 6)
                   Format
A [1263]
Average
B [1263]
Average
C [1263]
Average
D [1263]
Average
FOE-ANN
FMF Mean±std 3 0.15±0.03
0.23±0.12 5 0.12±0.02 0.22±0.09 2 0.49±0.09 0.57±0.12 7 0.57±0.10 0.58±0.11
MSE-ANN Mean±std 0.27±0.08 0.27±0.10 0.20±0.10 0.27±0.10 0.63±0.10 0.64±0.11 0.62±0.11 0.64±0.13
                                                      On average the MCR from FOE-ANN is lower
performance function, for all formats. These results are an improvement on the results from previous work where FMF2 was used to calculate FOE (Copeland, Gedeon, et al., 2014a). We found previously that on average the MCRs for formats A and B were both 0.28. We have improved these results, in particular, for format A when the [12 6 3] topology is used an average across the cross validation results of 85% correct classification is achieved (MCR=0.15). This is a 46% reduction in MCR compared to when MSE is used (MCR=0.27). Similarly, for Format B when the [12 6 3] topology is used an average of 88% correct classification is achieved (MCR=0.12), which is a 39% reduction in MCR compared to when MSE is used (MCR=0.2).
The results from this analysis indicate that prediction of reading comprehension using ANNs is most accurate for Formats A and B. Indeed, there are quite high MCRs for formats C and D. The results can be compared to a simple majority prediction in which the highest class is predicted, where for format A the largest class of output is a score of 2 which is 74% of all scores and a total of 4 output classes. We are able to outperform this by gaining an average of 85% correct
than from MSE-ANN as the
93
Effects of Presentation on Prediction of Comprehension
 classification. For format B the largest class of output is a score of 2 as well and this accounts for 77% of the scores, yet we are able to obtain an average accuracy of 88%. For format B there are only 3 output classes of which one of the class’ only accounts for 1% of the total set of outputs so it is highly unlikely that we are predicting this class effectively and so further research should go into better predicting these minimal classes. However, in both cases we are not simply predicting the majority class of output.
The breakdown of outputs for formats C and D are much more spread out with both formats having 5 classes of outputs. The majority output being 2 for both formats, with of all outputs being 2 being 44% and 45%, for formats C and D respectively. We obtained quite poor classification results for formats C and D. Using FOE as the performance function, for format C we were able to achieve 51% correct classification (MCR=0.49) and for format D we were able to achieve 43% correct classification (MCR=0.57). Given that this is in total a 5-class classification task chance classification is 20%, thus we are achieving double chance classification. However, when taking into consideration the output class breakdown we are achieving above majority prediction for format C, but not for format D, for which classification is actually below the majority class. Further investigation of format D is required.
We hypothesised that it would be easier to predict reading comprehension from format D compared to format C as participants had knowledge of the questions before they read the text. They therefore know what it is that they are looking for in the text. However, from this analysis we cannot confirm this hypothesis. In the next sections we investigate methods for improving these classification results.
5.5.2 Comparisontootherclassificationtechniques
The second part of the analysis is the comparison of other classification techniques to the ANN results. The four supervised learning techniques are classification trees, boosted classification trees, random forests, and k-nearest neighbour. The average and standard deviations of the MCRs from 10-fold cross validation are reported for each format are reported in Table 5.3.
The results reported in Table 5.3 are suboptimal compared to using ANNs. The MCR values are double that from using ANNs for Formats A and B. Of the classification techniques used, the random forest ensemble produces the lowest MCR rates for all formats. As in the first analysis, Formats C and D both have poor classification results.
However, the results are that on average the random forest are not performing much better than a majority classifier. For example, for format A on average 71% correct classification (MCR=0.29), but the majority answer output accounts for 74% of the outputs. It could be that these techniques are not optimal for dealing with the particular data sets so further investigation of other machine learning techniques should be considered, such as support vector machines and algorithms for training ensembles that primarily deal with imbalanced data sets.
94
Effects of Presentation on Prediction of Comprehension
 Table 5.3. Comparison of Misclassification (MCR) results for predicting total comprehension scores for all eye movement measures (A: 5 → 5/6; B: 5/6; C: 5 → 6; D: 6 → 5 → 6)
      Classification Format Technique A
B 0.39±0.15
0.40±0.19
0.30±0.13
C 0.76±0.10
0.73±0.09
0.61±0.05
D 0.57±0.13
0.78±0.13
0.46±0.13
                Classification
Tree
Boosted
Classification 0.47±0.13 Tree
0.32±0.10
                  Random
Forest 0.29±0.08 (Classification)
kNN 0.37±0.12 Best FOE-ANN
Result*
Note in bold are the lowest techniques.
0.39±0.12       0.65±0.11     0.55±0.13
             0.15±0.03
0.12±0.02
0.49±0.09
0.57±0.10
              average MCRs for each format from the comparison
5.5.3 ClusterAnalysis
Making predictions on the eye gaze data collected for Formats C and D has proven to be quite challenging. Exploration of these data sets using clustering is performed to see if there are any natural clusters in the data to which we can apply classification techniques and from which we can make conclusions. In both cases agglomerative hierarchical clustering is used. This type of clustering starts with every observation in its own cluster and then merges the groups together until they are in the same group (Xu & Wunsch, 2008). In both cases the distance measure is set to cityblock and linkage set to average. The eye movement measures from both the text page and the questions page were used in the clustering. The Statistics Toolbox in Matlab R2012a is used to perform the cluster analysis.
5.5.3.1 Format C (5 → 6)
The results of the clustering are shown in Figure 5.3. From the clustering there is evidently an outlying point. The outlying point has 691 fixations recorded for reading the tutorial page and a total fixation time of 176 seconds. This is above what is expected given that the text contains 400 words. If this outlier is removed and the rest of the data set is considered, there are three unequal clusters of data at a high level. Comparisons of the averages of eye movement measures for these clusters are shown in Table 5.4.
The results from the cluster analysis indicate that Cluster 1 represents the readers who spent little time to read the text. We infer that they spend less time to read due to the combination of low numbers of fixations, total fixation times and reading ratios, as well as longer forward saccades and higher skimming and scanning ratios. This cluster also corresponds to higher question scores. This would indicate that this cluster has grouped together the instances where there was prior
95
clustering of format C data (5 → 6)
Measures
Cluster Size
Number of fixations
Average fixation duration (s) Total fixation duration (s) Regression ratio
Average forward saccade length Longest reading sequence
Read ratio
Skim ratio
Scan ratio
Multiple Choice Score
Cloze Score
Combined score
Cluster
1 2 3
41 72 21 171 295 467 0.17 0.24 0.24 28.3 69.8 110.9 35% 28% 32% 123 96 110 48.27 88.99 74.05 58% 78% 71% 23% 11% 15% 19% 10% 14% 0.85 0.74 0.67 0.78 0.55 0.71 1.63 1.28 1.38
Effects of Presentation on Prediction of Comprehension
 knowledge of the subject and so less reading is needed to achieve understanding, and therefore high comprehension scores.
 450
400
350
300
250
200
150
100
Heirarchical Clustering of Format C
                                             630 3 4242625 1 729 227 513232814171618101922112112 8 91520
Figure 5.3. Hierarchical clustering of eye movement measures for Format C (5 → 6)
Table 5.4. Comparison of average eye movement measures for clusters obtained from hierarchical
                                    Clusters 2 and 3 have similar comprehension scores to each other yet different observed eye movements. Cluster 2 has a higher numbers of fixations and longer total fixation times than observed for Cluster 1 but lower and shorter, respectively, than observed for Cluster 3. This cluster also has the most instances so can therefore be considered to represent the average eye movements for the group. Cluster 3 has
96
Distance
Effects of Presentation on Prediction of Comprehension
 the highest numbers of fixations and the longest total fixation times. This cluster groups together the instances where the readers spent more time on the text.
The outlier was removed from the data set and the remaining data was split into the three clusters described in Table 5.4. Using random forest ensembles the question scores were predicted for within each cluster. The average and standard deviations of MCR are shown in Table 5.5 from 10-fold cross validation.
Table 5.5. Comparison of Misclassification (MCR) results for predicting questions scores for text only pages eye movement measures from Format C using Random Forest Ensemble Classification
Cluster Combined
1 0.39±0.17
2 0.81±0.13
3 0.61±0.21
The hypothesis was that grouping together like eye movements would create more accurate predictions of the question scores. However, this was not validated.
5.5.3.2 Format D (6 → 5 → 6)
The results of the clustering are shown in Figure 5.4. The cluster analysis reveals that there are once again three clusters, one of which only contains 3 data instances. Comparing the averages of eye movement measures for these clusters are shown in Table 5.6. The cluster with only three data instances has considerably more observed fixations than the other clusters. Additionally, these instances have considerably lower comprehension scores than the other two clusters. We discount these data instances as outliers for the classification analysis.
                350
300
250
200
150
100
Heirarchical Clustering of Format D
                                              2 5
1 3
6 7
9 12 30 11 16 18 17 19 29 20 21 15 22 23 25 27 8
4 10 28 14 13 24 26
Figure 5.4. Hierarchical clustering for eye movement measures from format D (6 → 5 → 6)
For Format D we expect fewer fixations and less reading of the text given that
participants had knowledge of the reading comprehension questions before reading 97
Distance
Effects of Presentation on Prediction of Comprehension
 the text. As a result, the hypothesised behaviour was for the participant to search the text for the answers and only read the apparently relevant sections of the text. The remaining two clusters appear to fit with this assumption, whereby there is a smaller cluster where more reading of the text occurred. There is however no difference in the scores between Clusters 1 and 3 so the difference really is in the reading behaviour. Given the increase in fixations and reading ratio in Cluster 1 compared to Cluster 3, the behaviour grouped together in Cluster 1 seems to be that of more normal reading behaviour as the characteristics of the measures are similar to Cluster 2 for Format C. In these cases the readers must not have skimmed the text and rather read it in a “normal” manner. Cluster 3 however has dramatically lower numbers of fixations and reading ratio, which is more characteristic of the behaviour that was hypothesised.
Table 5.6. Cluster details for Format D (6 → 5 → 6)
      Cluster
1 2 3
45 3 87 286 445 137 0.26 0.18 0.17 76 81 23 29% 35% 40% 98 121 143 89 48 30 77% 58% 45% 13% 22% 28% 10% 20% 27% 0.84 0.33 0.77 0.60 0.33 0.67 1.44 0.67 1.44
As for the Format C data set, random
question scores for Clusters 1 and 3. The
from 10-fold cross validation are shown in Table 5.7. These results show little improvement from the results obtained without clustering. This indicates that the hypothesis of grouping together similar eye movements is not an effective method for increasing prediction accuracy.
Table 5.7. Comparison of Misclassification (MCR) results for predicting questions scores for text only pages eye movement measures from Format C using Random Forest Ensemble Classification
Cluster Combined 1 0.39±0.23 3 0.51±0.21
     Measures
Cluster Size
Number of fixations
Average fixation duration (s) Total fixation duration (s) Regression ratio
Average forward saccade length Longest reading sequence
Read ratio
Skim ratio
Scan ratio
MC Score
Cloze Score
Combined Score
                         forest ensembles were used to predict the average and standard deviations of MCR
              98
Effects of Presentation on Prediction of Comprehension
 5.6 Discussion
The central question of this investigation is whether eye tracking data can be used to predict reading comprehension. We focus on the prediction of reading comprehension from eye movements recorded from reading text shown in different formats. This allows us to investigate if the presentation of the text and questions affects reading comprehension prediction. The analysis involved three components; the first was an investigation of reading comprehension prediction using ANNs. This was an extension of previous work where we investigated the benefits of using FOE as the performance function for training. The second component was a comparison to other machine learning technique, and finally clustering of two of the data sets, formats C and D.
Generally, the results reflect the fact that the data sets are quite hard to classify, especially the formats C and D data sets. The best classification results were obtained using three layers of hidden neurons, indicating complex relationships between the eye movement measures and reading comprehension scores. In saying this, the results from the analysis show that predictions for formats A and B are the most accurate. Whilst this could indicate that there are relationships between eye movements and the reading comprehension scores in these formats, it is also probably a by-product of the fact that formats A and B are highly imbalanced. Formats C and D on the other had have wider distributions of comprehension scores and are less imbalanced than formats A and B.
Note that the 10-fold cross validation is not done on a participant basis, but rather a data point basis. The implications of this are that the modelled data is for the current participant base and the results may be quite different if completely new participants are tested. This is an area that requires further work, where we test the model on new participants to check for versatility. In practice, we would expect that the trained model would be constantly updated to account for new students and for new scenarios.
The second part of the analysis was an investigation of other methods of classification. FOE-ANNs outperform the other classification techniques used when predicting the combined comprehension scores. The conclusion made from this part of the analysis is that ANNs are the most appropriate classification technique, from the set tested, for predicting reading comprehension scores. However, we note that the satisficing classification technique from the set of four is the random forest ensemble and further investigation of machine learning techniques should be carried out. In particular techniques that can be sued to deal with the imbalance in the data sets.
Martínez-Gómez and Aizawa (2014) found it difficult to predict comprehension of text where no significant result could be found in the regression task of prediction understanding scores. Further analysis could be run on the current data set to make predictions using regression analysis rather than classification analysis. Our work adds to this showing that to some extent eye movements can be used to prediction
99
Effects of Presentation on Prediction of Comprehension
 reading comprehension scores. However, this still needs further investigation to improve accuracy.
One of the advantages of using FOE is that it is a flexible error function that can be tailored to data sets and problems. Specifying the shape of the FMF used to calculate FOE does this. However, there is no simple way of constructing an FMF. In this analysis we only investigated 7 predefined FMFs, however, a beneficial approach would be to determine the most appropriate FMF shape from the data set. An area of further exploration is how to apply the learning of the FMF shape when using other classifiers such as neural networks.
Notably, it was hypothesised that predictions from Format D would be better than Format C as participants were shown the questions before being shown the text. Participants therefore knew what to look for in the text in order to answer the questions. However, this was not found to be the case. In both formats, participants would have read the text to the point at which they deemed they understood the text. This is subjective and dependent on a number of factors including prior knowledge, familiarity with the subject matter, current state (mood, arousal, etc.) as well as their motivations. This could account for the variability in eye movement measures and the reading comprehension outcomes. Participants’ subjective reading comprehension ratings were not recorded for each of the individual texts. Future work, will be in recording this information and exploring relationships between eye movements and subjective comprehension for each text.
The clustering of both formats C and D reveals interesting patterns of eye movements and reading behaviours that are evident in the data. In particular, there was no relationship between reading speed and reading comprehension but there is high variability in reading styles, which is consistent with Underwood et al. (1990). The clustering of both formats C and D demonstrates that there are clear changes in reading behaviour observed for participants. The clusters were composed of data samples from different participants, indicating that participants to some extent changed their reading behaviour to reflect the text. The change is from low reading behaviour (lower numbers of fixations and reading ratios, as well as shorter total fixation time), to medium reading behaviour, up to high reading behaviour, which is indicated from higher numbers of fixations and reading ratios, as well as longer total fixation times.
5.6.1 ImplicationsforeLearning
The analysis shows that there are relationships between eye movements and reading comprehension, albeit complex relationships. These relationships are strongest when the questions are shown along with the text, which in the context of practical use for eLearning is not so significant. Nevertheless, this is an important finding as it shows that there are relationships.
The goal of reading comprehension detection is to incorporate eye tracking into eLearning environments and use the eye tracking data as a form of adaption. Consequently, the content and the presentation of content can be altered to reflect
100
Effects of Presentation on Prediction of Comprehension
 the student’s current state. The product of reading comprehension prediction is twofold; first, if students are given text to learn, instead of explicitly assessing their comprehension, eye tracking could be used to assess their understanding thus reducing time, workload, and potentially stress or anxiety of the students. Secondly, predicting students’ comprehension using eye tracking would allow the learning environment to, 1) adapt the questions asked of students about the content, and, 2) alter the learning path to reflect the students’ current understanding levels.
The second point can be elaborated upon, as this is the main advantage of predicting comprehension from eye gaze. Take for instance the case where a student has read some learning materials but does not understand it. He is then asked the same comprehension questions as all other students. Not understanding the text possibly increases the student’s anxiety about the learning material, causing him to be disheartened. Two solutions arise from this, first is that the questions themselves are modified to be easier, perhaps covering more superficial understanding of the content. Text with more explanation of the content that was not understood could then be given, after which they are assessed on the original comprehension questions. Secondly, instead of asking comprehension questions at all, the text with more explanation could be provided to the student.
If we now consider the converse case where a student has a high level of understanding, as is the case when the student has prior knowledge on a certain topic, this student may become frustrated or bored by being presented with easy content and unchallenging questions. Again, either the questions or the content could be altered to present these students with hard subject matter and questions that require much more thought and insight.
Furthermore, the use of a technology such as eye tracking gives rise to the possibility of monitoring implicit behaviours related to reading and learning in eLearning environments. As seen, especially from the cluster analysis, there are differences in eye movement and reading behaviours within the formats. From previous work, we have shown that differences in eye movements and related measures can be used to measure how difficult or interesting a student is finding certain texts (Copeland & Gedeon, 2013b). This information can then be used by the instructor, or writer, of the learning materials to find the implicit difficulty of the questions and text, to get a ranking of how the students are performing, as well as any other information such as the rate at which students are developing (Copeland & Gedeon, 2013b).
The eye movement measures can also be used to determine if a student is having problems reading materials over a longer term so that remedial assistance can be provided. In the opposite case of a student who consistently skims text due to high levels of prior knowledge and understanding of a given topic, this student can be helped by being either moved up a level in the course or provided with more challenging tasks.
101
Effects of Presentation on Prediction of Comprehension
 5.7 Conclusion
This chapter is an investigation of predicting reading comprehension from eye movements. The eye gaze data that was collected from the user study in Chapter 3 showed that presentation format affects eye movements and reading behaviour. In this chapter, we explored how those differences cause variations in prediction outcomes of reading comprehension from eye gaze. We found prediction of reading comprehension measures was most accurate for formats of presentation where the text and questions are shown together. For these formats, denoted A and B, we could achieve 85% and 88% correct classification, respectively, using FOE-ANN. Prediction from the formats where questions and text are shown separately to one another proved to be more challenging, where poor classification results were obtained. Further work is required to investigate other machine learning techniques, especially those that could be used to deal with imbalanced data better.
The extension of previous work of the use of FOE as a performance function for training ANNs has shown that FOE-ANN provides better prediction results than the use of MSE-ANN. However, further research needs be carried out to explore the nature of this performance function and the creation of the FMF shapes used to calculate FOE. Additional data sets and problems should be trialled as well to investigate if these results generalise.
102
Chapter 6
Chapter 6. Effect of Text Difficulty on Prediction of Comprehension
 Prediction of reading comprehension scores is a difficult task, as we have already seen in Chapter 5. In this chapter we extend the work from the previous chapter by investigating the effect of text difficulty and machine learning techniques on prediction accuracy. To this end, a user study was carried out to collect data from L1 and L2 participants as they read texts with differing degrees of difficulty. The grades of overall difficulty are based on different levels of readability and conceptual difficulty. We hypothesised that text difficulty and reader type would affect prediction quality. We found that neither had a significant effect on the accuracy of the k-nearest neighbour (kNN) classifier used. However, we did improve the classification accuracy to on average 80% for the L1 group and 73% for the L2 group, which is a substantial improvement from the 44% correct classification obtained in the previous chapter for format C. These results were achieved by using genetic algorithms (GA) for feature selection, which were significantly higher than the results produced when no feature selection is performed. We found that readability affects normalised number of fixations (NNF) but not regression ratio. We also found that there is a significant difference between the L1 and L2 readers NNF and tendency to regress. Although the significant difference between the groups is what we would expect, from past research and the findings of this thesis, there was no interaction effect between the reader groups and the text difficulty. This indicates that in this study the readability and conceptual difficulty of the text affect the two groups similarly. This chapter builds on work presented at CogInfoCom 2015 (Copeland et al., 2015).
“Prediction is very difficult,
 especially if it's about the future.”
 ― Niels Bohr, Danish Physicist
103
Effect of Text Difficulty on Prediction of Comprehension
 6.1 Introduction
The eye moves in specific patterns when reading, making it not only possible to detect when a person is reading from their eye gaze (Campbell & Maglio, 2001) but also how they are reading (Buscher et al., 2008), what task they are performing (Iqbal & Bailey, 2004), how relevant they find text (Buscher et al., 2012; Vo et al., 2010), and even their cognitive load (Iqbal, Zheng, & Bailey, 2004). Unsurprisingly, given this list, eye movements also provide insight into how difficult text is to read (Rayner et al., 2006) and comprehend (Martínez-Gómez & Aizawa, 2014; Underwood et al., 1990). As text increases in difficultly, the number of fixations increases, fixation duration increases, saccade size decreases, and regressions increase (Rayner, 2009; Rayner et al., 2006; Staub & Rayner, 2007). Text characteristics have also been shown to affect comprehension for which, in the context of legal documents, making text simpler would benefit vulnerable populations (Scherr et al., 2015). This can be extended to considering the differences of students in eLearning, where some students may be supported by simpler texts.
Eye movement measures have been shown to be effective at distinguishing between readers with low and high level of understanding as well as predicting English language skill (Martínez-Gómez & Aizawa, 2014). Eye gaze has also been used to investigate those parts of text that readers are failing to comprehend, indicating that eye gaze features, such as the number and duration of fixations, can be used to identify reading incomprehension (Okoso et al., 2015). Prediction of reading comprehension derived from eye movements would make the current model of adaptive eLearning more versatile. It would allow for the eLearning environment to change dynamically based upon implicit behaviour. This would result in decreased time for the student to learn the material as well as not contributing to their over- or under-confidence in actual understanding of the learning materials.
However, the task of predicting reading comprehension from eye gaze is not a simple one, as we have established in the previous chapter. Clearly, the current method of prediction is inadequate so we explore methods of increasing prediction quality. It has been shown that the number of fixations increases as text difficulty increases and the number of regressions increases when inconsistencies are introduced into texts (Rayner et al., 2006). Indeed, there are many factors in texts that affect readers’ eye movements. With this in mind we postulate that the differences induced by text with differing degrees of difficulty will i cause significant differences in eye movements and therefore will have differential effects on predictions of reading comprehension scores. As in the previous chapter the central research question is:
Can eye-tracking data be used to predict reading comprehension scores in eLearning environments for L1 and L2 readers?
Once again we also investigate factors that could affect prediction accuracy. In this chapter we investigate the effect of text difficulty on prediction accuracy of comprehension:
104
Effect of Text Difficulty on Prediction of Comprehension Does text difficulty affect predictions of comprehension?
We explore this question by investigating factors that influence prediction performance of reading comprehension scores from eye tracking data. These factors are text readability, conceptual difficulty of the text, and whether the reader is a first (L1) or second (L2) English language reader. To perform this investigation, we conducted a user study to collect eye gaze data from participants as they read text with differing degrees of difficulty. We hypothesise that predictive performance is affected by text difficulty and reader type, in particular that, 1) more accurate predictions will be obtained for L1 readers compared to L2 readers, and 2) more accurate predictions will be made when the text is most difficult. We explore two additional methods of increasing predictive accuracy; the first being the inclusion of pupil dilation data into the feature set, and the second being another way of generating the feature set. This involves breaking the task into smaller windows are generating the eye movement measures for these windows rather than the whole task. This technique, in combination with feature selection using genetic algorithms, has been used to improve stress prediction (Sharma & Gedeon, 2013b) as well as for biofeedback (Gedeon, Zhu, Copeland, & Sharma, 2015). Our hypothesis is that breaking the task up into smaller windows and calculating measures for each window will improve predictive accuracy.
This chapter is organized into the following sections: background information; user study method; results and analysis; discussion and implications; and conclusion and further work.
6.2 Feature selection using genetic algorithms
The majority of the background material for this chapter has been covered in the literature review and in the previous chapter. We introduce a new technique in this chapter: the use of feature selection, specifically the use of genetic algorithms to perform feature selection. Feature selection is the selection of a subset of features is selected before modelling occurs. This may be done for several reasons, such as when data sets contain hundreds or thousands, or indeed hundreds of thousands of features, however it may be that many of these features hinder the model’s accuracy because they outweigh the useful features (Guyon & Elisseeff, 2003). Removing redundant or irrelevant features and only using the most “useful” features can improve the quality of the model created as well as speeding model creation (Guyon & Elisseeff, 2003; Siedlecki & Sklansky, 1989). An example of this can be seen when random forests are used to model high dimensional data. Poor results are often found since random sampling of the feature set to create the ensemble often results in subsets of only irrelevant features (Amaratunga et al., 2008).
One method of feature selection that has been shown to be effective is genetic algorithms (GA) (Garrett, Peterson, Anderson, & Thaut, 2003; Yang & Honavar, 1998). Before proceeding to discuss the use of GAs to perform feature selection we will introduce the concept of GAs. GAs are search algorithms that are inspired by natural evolution (Whitley, 1994). In particular, are based on the fundamentals of genetic evolution to search the solution space. GAs are often considered a “global”
 105
Effect of Text Difficulty on Prediction of Comprehension
 search tool because they do not usually suffer from the disadvantages of optimisation methods such a gradient decent, e.g. getting stuck in a local minima, however, GAs should be thought of as a search process rather than an optimisation process (De Jong, 1993).
The foundation of a GA is that there is a population of individuals used to search the solution space. Each individual in the population represents a potential solution to the problem, which is represented as a chromosome. A chromosome represents the characteristics of an individual, which refer to the variables of the search problem. The chromosome is composed of a set of genes whose indices are termed loci. Each gene can have one or more values, which are termed alleles. An important step in the design of a GA is to find an appropriate representation of the chromosomes. The genotypes are often represented as simple data types such as a bit string or numerical representation of a chromosome.
The main driving operators for GAs are selection and recombination, through application of a crossover operator, with mutations to add diversity. The focus of genetic algorithms is generally on recombination of existing chromosomes in the population so mutation rates are usually set to less than a 1 probability (Whitley, 2001). The population is usually initialised to a random set of chromosomes and then crossover functions are used to create the next generation of chromosomes. Types of crossover functions include random selection, proportional selection, tournament, and elitism; however there are many other types.
GAs have been used successfully for feature selection for neural network classification of a number of University of California Irvine (UCI) machine learning repository data sets16 (Yang & Honavar, 1998). These data sets mostly have feature sets below 20, with a maximum range of 60. This is quite a low number of features when physiological data is considered such as pupil dilation, EEG, ECG, GSR, amongst other signals. Selecting features from larger feature sets and features derived from physiological data have also been successful (Garrett et al., 2003; Schroder et al., 2003). In particular, feature selection has been shown to be beneficial in predicting stress during reading tasks from physiological signals, including eye gaze data (Sharma & Gedeon, 2012, 2013a).
6.3 Method 6.3.1 Participants
The eye gaze of 70 participants (47 male, 23 female) was recorded. Participants had an average age of 25 years (9 years standard deviation, range of 18 to 60 years). Of the participants 46 stated that English was the first language they learnt to read in and the remaining 24 stated a language other than English. Participants were mostly (n=44) sourced from a first year Web design and development course offered at the university (ANU). All other participants were sourced from the university more widely.
16 Available at http://archive.ics.uci.edu/ml/ Last accessed: 29th January 2016 106

Effect of Text Difficulty on Prediction of Comprehension
 6.3.2 Design
Participants’ eye gaze was tracked as they read and completed a tutorial on the topic of “Digital Images”. The tutorial was taken from a first year computer science course on web design and development offered at the ANU. The tutorial was composed of 9 texts of approximately 240 words (standard deviation of 20 words) in length. An example of the tutorial text is shown in Figure 6.4. Since participants were mostly sourced from the course, they were provided with an incentive to do well on the tutorial to gain marks for the course. Additionally, the task was similar to tasks the majority of participants were used to performing throughout the course, so participants were not given practice texts to read. Reading was self-paced; participants were told to read the text and that after reading the text they would be asked comprehension questions.
Figure 6.1. Description of the text property breakdown
Two variables in the text were altered in each new text; the readability and concept difficulty. Each variable has 3 values; “easy”, “medium”, and “difficult”, giving a total of 9 combinations (see Figure 6.1). The readability was measured using the Flesch-Kincaid Grade Level (Kincaid et al., 1975). Each increase in readability level is about 3 years of education, so we start at a level that all participants should comfortably be able to read and finally move to a grade that signifies postgraduate level studies (see Figure 6.2). The COH-Metrix L2 Readability index is designed to rate the readability of text for L2 readers. The L2 readability index for each text was generated using COH-Metrix 3.0 (McNamara et al., 2013). Since, it has been shown that the L2 readability index is more appropriate for describing the readability of texts for L2 readers (Crossley et al., 2008), we check that the L2 readability indices are consistent with the Flesch-Kincaid grade levels, in that there is a consistent increase in L2 readability indices as there is a decrease in Flesch- Kincaid grade levels. More specifically, the increase in difficulty in readability is consistent for L1 and L2 readers.
 107
Effect of Text Difficulty on Prediction of Comprehension
   20
18
16
14
12
10
8 6 4 2 0
L2 readability FK grade level
                                                      Easy
Medium
Readability Category
Difficult
Figure 6.2. The Flesch-Kincaid readability grade level and COH-Metrix L2 readability for each level of readability.
Table 6.1 Example of chunking concepts to derive the levels of concept difficulty for Topic 3 - Photo Credibility
      Chunk 1
2
3
4
Conceptual level
Basic       Intermediate     Advanced
       Photography was invented in 1850s, photos have been modified since then
Manipulating images used to be difficult, now it is easy. People do it to critique others or make money
There is something called digital forensics that run some tests on photos, but there is no authenticate process
Forensics are important because manipulated photos can affect people and industries
That altering is actually tampering. That deliberately false photos are created
That image manipulation is called post-processing. That these images can easily be distributed given our networked society. That they go around the world quickly
That image manipulation causes manipulation artefacts.
Forensics is a quickly growing field because of these impacts
That tampering is done to deceive. That photos were publishing their deceptive images into the public record.
That these manipulated images are now part of our knowledge base and daily experience
It is not possible to positively authenticate an image
Despite forensics, digital photographs cannot be guaranteed to be real
                                An expert and educator in digital images wrote texts as teaching material for web design course the participants were sourced from. Since conceptual difficulty in a discipline is largely a qualitative judgment often best measured by a subject matter
108
Calculated Readability
Effect of Text Difficulty on Prediction of Comprehension
 expert, it is difficult to measure with automated tools. Whilst the concept level was defined by the expert’s judgments, she based her concept content design on the idea of conceptual chunking (Miller, 1956). Table 6.1 provides an example of how chunking was used to create the different levels of conceptual difficulty. Additionally, the text difficulty was designed using the following schema and principles:
i. systematic chunking, including increased number of concepts presented in each level of expository text, refer to Table 6.1 for an example of this;
ii. consideration of information scaffolding (what participants could be expected to already know); and
iii. expanded knowledge demands presented in each level of expository text (Initiative, 2010).
The 9 texts were shown in groups of 3; each set of 3 texts covered a topic. The 3 topics are “Working with Digital Images”, “Copyright and Intellectual Property”, and “Photo Credibility”. Each text had differing degrees of difficulty, whereby the readability and the concept difficulty was changed. The descriptions of each text’s properties are shown in Figure 6.1. Each text is given an alphabetic label. Each participant was given a sequence of texts to read. These sequences are described as paths and that the first text is always A, the second text is always B, E or D (one square in the grid away from A), and finally the last text was one of B, C, D, F, G, H, or J. The paths are as follows: A>E>J; A>E>H; A>E>F; A>B>D; A>B>G; A>D>C; and, A>D>B, and are graphically shown in Figure 6.3.
Figure 6.3 Process used to generate the paths
The process used to generate the paths was that of a hypothetical eLearning environment that changed the next text based upon answer correctness from the corresponding comprehension questions. Text A is always the start of the path. In the situation where the student answers the comprehension questions correctly for text A, then the system would present text E to the student. If the student then answered the comprehension questions for text E correctly then the system would be presented with text J. If the student answered the comprehension questions for text E incorrectly, then the system would present with another text, either F or H, each one step away from E. If the student answered the comprehension questions for A wrong, then texts B or D are shown, each one step away from A. If the system presents text B and the student answers are correct then the system presents text D, which is an increase in conceptual difficulty but same level of readability. If the student answers incorrectly, then the system presents text G, which is a decrease in
 109
Effect of Text Difficulty on Prediction of Comprehension
 conceptual difficulty but increase in readability to check for whether increase in readability will influence the student’s ability to comprehend the text. If the system presents text D, the opposite texts are shown to B.
The reasons these paths were chosen were to 1) start all participants on common ground, 2) only have subtle increases in text difficulty so that it would not be obvious what the text difficulty is, and 3) not have all increases as linear increases in difficulty, to elicit if one of the text properties has an influence over the subjective ratings.
Participants’ prior knowledge on the subject area was not tested however participants were asked to rate how familiar they were with each topic after they read the text. The participants’ ratings of familiarity to each topic are shown in Table 6.2. The ratings indicate that across the topics there are consistent percentages of all familiarity evaluations, with the largest percentage is that about 50% of participants have somewhat familiarity to all three topics.
Table 6.2. Participants’ ratings of familiarity to each topic.
              Familiarity evaluation
Familiar Somewhat Familiar Not Familiar
Topic 1
19% 51% 31%
Topic 2
19% 52% 27%
Topic 3
17% 49% 32%
                  After each text was read, participants were asked two comprehension questions to assess their understanding of the text. This is analogous to format C in the first user study (see Chapter 3). However, after being asked the two comprehension questions participants were then asked four qualitative questions related to the text they read:
1. How well do you think you understood the text?
(Very well / Well / Somewhat / Not at all)
2. How confident were you answering the questions?
(Very confident / Confident / Not Confident)
3. How difficult did you find the text to read?
(Easy / Moderate / Hard)
4. How complex was the concept being explained in the text?
(Basic / Intermediate / Advanced)
6.3.3 ExperimentSetup
The texts and questions were implemented in the online learning environment used at ANU, called Wattle (a Moodle variant). A Moodle quiz module was used to implement the process. The text was presented to the participants as shown in Figure 6.4. A copy of the texts used for the experiment, along with the participant information sheet, consent form, and other experiment resources are found in Appendix B. All participants had knowledge of the learning environment and had
110
Effect of Text Difficulty on Prediction of Comprehension
 used it prior to the experiment. The study was displayed on a 1280x1024 pixel Dell monitor and the set up was identical to the set up used in Chapter 3.
 Figure 6.4. Example of text presented in the Wattle online eLearning environment
Eye gaze data was recorded at 60Hz using Seeing Machines FaceLAB 5 infrared cameras mounted at the base of the monitor. This eye tracker has a gaze direction accuracy of 0.5-1° rotational error and measures pupil diameter as well as blink events. The study involved a 9-point calibration prior to data collection for each participant. As the data recorded is a series of gaze points, EyeWorks Analyze was used to pre-process the data to give fixation points. The parameters used for this were a minimum duration of 60 milliseconds and a threshold of 5 pixels.
6.3.4 DataPre-processing
The raw eye gaze data consists of x,y-coordinates of where the participants’ eyes were looking. Fixation and saccade identification were performed on the eye gaze data. From this data many other eye movement measures are derived. Given that there are 70 participants and 9 texts there is a total of 630 eye gaze sets for the prediction analysis. Due to problems in collected data, 12 of these eye gaze sets had to be removed resulting in 618 eye gaze data sets for the prediction analysis. For each piece of text, eye movement measures and pupil dilation measures are calculated.
6.3.4.1 Inputs
Many of the eye movement measures have already been discussed; however, we introduce the use of pupil dilation data in this chapter. The measures used in this investigation have in the most part already been explained and so will not be elaborated upon here (see Chapter 5 for more details about eye movement
111
Effect of Text Difficulty on Prediction of Comprehension
 measures). The pupil dilation measures are calculated for the left and right eyes separately and then an average is calculated for the two eyes. These measures are:
• Average, minimum, maximum and range of pupil diameter
• Average, minimum, maximum and range of pupil area
The average pupil diameter for a fixation is calculated with the average standard deviation. Pupil dilation has been known to increase with increased cognitive load (Kahneman et al., 1969). Additionally, changes in pupil dilation have been found to reflect learning (Sibley et al., 2011).
A list of the 28 measures used is as follows:
1. Normalised number of fixations
2. Maximum fixation duration
3. Average Fixation duration
4. Normalised total fixation duration 5. Number of regressions
6. Regression ratio
7. Average saccade length
8. Reading ratio
9. Skimming ratio
10. Scanning ratio
11. Left eye - Average pupil diameter 12. Left eye - Average pupil area
13. Left eye - Minimum diameter
14. Left eye - Maximum diameter
15. Left eye - Range of diameter
16. Right eye - Average pupil diameter 17. Right eye - Average pupil area
18. Right eye - Minimum diameter
19. Right eye - Maximum diameter
20. Right eye - Range of diameter
21. Both eyes - Average pupil diameter 22. Both eyes - Average pupil area
23. Both eyes - Minimum diameter
24. Both eyes - Maximum diameter
25. Both eyes - Range of diameter
26. Both eyes - Minimum area
27. Both eyes - Maximum area
28. Both eyes - Range of area
We consider two cases in this investigation; the first is the same as in the previous chapter where the measures are calculated for the entire task and the second is windowing the task by dividing the task into quarters and calculating the measures for each quarter, sixth and eighth. In this way, there are a total of 112 features for each piece of text when 4 windows are used, 168 when 6 windows are used, and 224 when 8 windows are used.
6.3.4.2 Outputs
Reading comprehension score: The outcome variables are in the form of the participants’ reading comprehension scores. After each piece of text the participant was asked two comprehension questions. Possible scores are 0, 1 or 2 for which the distribution of scores in the sets are described in Table 6.3.
As can be seen there is an imbalance of scores across the texts. The majority class is usually 2, however as the texts become more difficult the majority class shifts towards 1. Furthermore, the imbalance is more prominent for the L1 data set. Note that the differences in the comprehension scores will be investigated further in the next chapter.
112
Effect of Text Difficulty on Prediction of Comprehension
 Table 6.3. Distribution (%) of comprehension scores for each text and for the L1 and L2 data sets
L1       L2
TextID0 1 2       0 1 2 A 4 28 69       6 38 57
                         B 10 41 48 C 5 29 67 D 7 51 42 E 9 27 64
12 53 35 20 50 30 14 34 51 11 57 32 9 36 55 22 33 44 43 43 14 50 40 10
F 27 G 15 H 18 J 22
33 40 50 35 45 36 39 39
                    6.3.5 Featureselectionmethod
Genetic algorithms (GA) are used to perform feature selection. The chromosome length is the total number of features in the set, i.e. 112 when 4 windows are used with the 28 measures outlined in Section 6.3.4.1. A chromosome is represented as a binary string as implemented by Oluleye et al. (2014). A bit represents each feature on the chromosome, and the bit value indicates whether the feature was used. The initial population was constructed using the algorithm specified by Oluleye et al. (2014). Finally, the fitness function used was the quality of the prediction from the predictors when that feature set was used. Further details regarding the GA parameters are outlined in Table 6.4.
Table 6.4. GA parameter settings for feature selection
GA Parameter       Value/Setting Population type       Bitstrings
   Population size Generations Crossover rate Crossover
Mutation
Mutation Probability Selection
Elite count
50
25
0.8
Arithmetic Crossover Uniform Mutation 0.1
Tournament of size 2 2
     Note that the number of generations and population size is similar to that reported by Yang and Honavar (1998). Furthermore, preliminary analysis showed that this number of generations provided improvement in prediction accuracy.
6.4 Results
The data collected from the user study allows us to investigate if text complexity and reader type affects predictions of reading comprehension. Recall that our
113
Effect of Text Difficulty on Prediction of Comprehension
 hypotheses are that, 1) more accurate predictions will be obtained for L1 readers compared to L2 readers, and 2) more accurate predictions will be made when the text is most difficult. In this chapter we investigate two additional methods of increasing predictive power; the first being the inclusion of pupil dilation data to the feature set, and the second being the use of windowing of tasks to increase the feature set and then use of feature selection. Our hypothesis is that breaking the task up into smaller windows and calculating measures for each window will improve predictive accuracy. The first part of this section is the use of no feature selection and no windowing, which is analogous to the previous chapter. We can therefore delve into the factors of text difficulty and reader type. The second part of the analysis is looking at using windowing of the task and GA feature selection.
The classification techniques used in the following analyses are ANNs, KNNs, and random forests. All ANNs used have a 2-hidden layer topology with 10 neurons in the first layer and 5 in the second, using MSE as the performance function. The kNN classifiers use k with square root the number of data instances in the set. All analyses were performed using Matlab R2013a.
6.4.1 Predictionwithoutwindowing
The first part of the analysis uses eye gaze features that are calculated from the entire task. The classification rates (%) from the random forest ensembles, ANN, and kNN are shown in Table 6.5 respectively. The results were generated from 10- fold cross validation.
Table 6.5. Classification rates (%) from no windowing or feature selection
Text Text Properties       L1 L2
ID Read. Concept       ANN kNN       RF ANN kNN       RF A Easy Basic       66 58       63 47 46       61
                         B Mod. Basic
C Diff. Basic
D Easy Int.
E Mod. Int.
F Diff. Int.
G Easy Adv.
H Mod. Adv.
J Diff. Adv.
Average           41     46     48     36     41     51
41 34 30 40 47 32 55 54 28 57 50 35 30 45 30 50
50 39 29 56 30 33 67 44 48 35 28 32 53 43 42 45 47 14 43 43 53 50 30 60 30 40 45 75 55 20 40 40
                      The results from this analysis are poor, analogous to what we found in the previous chapter for format C. The results are similar, or worse than, majority class prediction. The best classification comes from the random forest classifier where for the L1 group an average of 48% correct classification was achieved and 51% for the L2 group.
114
Effect of Text Difficulty on Prediction of Comprehension
 Furthermore, there is no obvious pattern caused by text difficulty on the predictions. Additionally, there is no obvious difference in prediction accuracy for the L1 and L2 data sets. If anything, contrary to our hypothesis, the L2 data set leads to greater accuracy of predictions.
6.4.1.1 Prediction with feature selection
Genetic algorithms are used to perform feature selection. Random forests, ANNs and kNN are then used to predict the reading comprehension score. The correct classification results (%) from GA-RF, GA-ANN and GA-kNN are shown in Table 6.6. Note that feature selection is performed with nest 10-fold cross validation, that is, the cross validation is performed within the GA.
Table 6.6. Classification rates (%) using feature selection and no windowing
Text Text Properties       L1 L2
ID Read. Concept       ANN kNN       RF ANN kNN       RF A Easy Basic       65 71       68 58 65       67
                         B Mod. Basic
C Diff. Basic
D Easy Int.
E Mod. Int.
F Diff. Int.
G Easy Adv.
H Mod. Adv.
J Diff. Adv.
Average           72     80     78 65 73       67
69 70 71 66 65 68 81 83 82 50 80 50 75 85 83 57 66 73 75 77 78 65 75 77 80 95 80 60 57 50 65 80 75 80 80 90 77 77 83 80 90 70 67 85 80 70 90 60
                         The use of feature selection significantly increases the prediction quality for all three classifiers. This time the best results are obtained when using the kNN classifier which are significantly higher than when no feature selection is used (t(17)=11.176, p<0.0005). We are now able to achieve classification results above majority class prediction, especially for the L2 data set and the more difficulty tests which do not have as severe imbalance as compared to the simpler texts.
To assess if the text difficulty, or the reader group, have significant effects on the prediction accuracies we use ANOVA. The kNN accuracy rates are normally distributed (using the Shapiro-Wilks normality test, p=0.923). The dependent measure as the kNN accuracy and the L1/L2 reader groups and the readability and conceptual difficulty as the independent variables. Neither text readability (F(2,4)=1.68; p=0.296), nor conceptual difficulty (F(2,4)=3.82; p=0.118), nor reader group (F(1,4)=3.18; p=0.149) have an effect on the kNN accuracy. This implies that there is no relationship between the degree of accuracy from the kNN classifier with either text difficulty or reader group. There is no statically significant effect of interaction between any of the three independent variables.
Our hypothesis regarding text difficulty is that more accurate predictions would be achieved when the text is most difficult; perhaps because more cognitive
115
Effect of Text Difficulty on Prediction of Comprehension
 resources are committed to the task hence fewer distractions are possible. However, this hypothesis is not validated by the results gained above.
There is a significant correlation between conceptual difficulty and the kNN accuracy (r=0.5, p=0.049). Whilst the correlation is not large there does appear to be a small effect of conceptual difficulty on kNN accuracy.
6.4.2 Predictionwithwindowing
The next part of the prediction analysis is using eye gaze features that are calculated from windowing the task. By this we mean that we divided the entire reading take up into smaller segments to calculate the eye movement measures. The effect of the number of windows on prediction accuracy is investigated in this section. In this part we conflate the datasets without controlling for text complexity, i.e. “all texts”. Feature selection is performed with nest 10-fold cross validation, that is, the cross validation is performed within the GA. The number of windows used to divide the data set into smaller segments are 2, 3, 4, and 6. The results are generated using kNN classifications as shown in Table 6.7.
Table 6.7. Correct classification (%) of reading comprehension for different windows
Text       Text Properties L1 L2
ID Read. Concept 2 3 4       6 2       3 4       6 A       Easy Basic 75 73 72       69 67       67 66       66
                              B Mod.Basic 85 66 73
C Diff. Basic 80 75 75
D Easy Int. 71 68 66
E Mod. Int. 79 71 73
F Diff. Int. 77 88 87
G Easy Adv. 83 87 82
H Mod. Adv. 90 70 70
J Diff. Adv. 85 85 75
Average           81     76     75     75     73     69
71 76 70 70 66 70 75 69 82 73 78 78 80 80 80 77
59 69 62 70 67 67 62 62 67 66 75 77 75 83 77 75 72 75 82 80 78 65 65 73
   71       71
                          To assess if the window size or the reader group, have significant effects on the accuracies we use ANOVA again. The kNN accuracy rates are normally distributed (using the Shapiro-Wilks normality test, p=0.333). The dependent measure as the kNN accuracy and the L1/L2 reader groups and window size as the independent variables. The window size does not have a significant effect on the kNN results (F(3,64)=1.88; p=0.141; partial η2=0.081), but the reader group does have an effect (F(1,64)=11.87; p=0.001; partial η2=0.156). It is interesting that the L2 participants have significantly lower prediction accuracies than the L1 participants in this case given that there was no significant difference in the previous test, which requires further analysis. Additionally, there is no statically significant effect of interaction between any of the three independent variables. This implies that the window size does not actually affect the accuracy of the result from the kNN classifier, showing that there is no benefit by windowing the task. When 2 windows are used the
116
Effect of Text Difficulty on Prediction of Comprehension
 performance remains the same as when no window is used. However, after this the classification accuracy starts to decline; adding more windows actually impedes classification performance.
6.4.3 Effectoftextdifficultyoneyemovements
Given that the results are not as we had hypothesised, we now delve into looking at the eye movements themselves and whether they differ significant due to the text difficulty. In this section we investigate if the grades of readability and / or conceptual difficulty affect eye movements. The assumption, based on past research, is that there will be significant differences in eye movements between L1 and L2 readers as well as between the different grades of text difficulty. To address these assumptions, we use MANOVA analysis determine if there are any statistical differences between text properties and reader type. The two eye movements that are analysed in this section are the normalised number of fixations (NNF) and the regression ratios as both are known to be affected by text difficulty (Rayner et al., 2006). The NNF and regression ratios are shown in Figure 6.4 and Figure 6.5, respectively.
   1.40 1.20 1.00 0.80 0.60 0.40 0.20 0.00
L1 L2
                                                        ABCDEFGHJ
Text ID
Figure 6.5. Normalised number of fixations (NNF) for each text
The correlation between the NNFs and regression is within the acceptable limits for MANOVA. The Levene’s test for equality of variances shows that there is homogeneity for both dependent variables (p>0.05). Additionally, the Box’s M value of 73.68 (p=0.04 > 0.005) is interpreted as non-significant so we can be satisfied that we have homogeneity in the variance-variance-covariance matrices.
There is a significant difference in eye movement measures between L1 and L2 readers (F(2,592)=6.017, p=0.003; Wilk's =0.980, partial η2=0.020). Readability affected eye movements (F(2,593) = 4.074, p=0.017; Roy’s λ=0.014, partial η2=0.014), however, conceptual difficulty did not affect eye movements (F(2,593)=2.299, p=0.101; Roy’s
117
Normalised number of fixations
Effect of Text Difficulty on Prediction of Comprehension
 =0.008, partial η2=0.008). There is no significant effect of interaction between conceptual difficulty, readability, and reader type.
  50% 45% 40% 35% 30% 25% 20% 15% 10%
5%
0%
ABCDEFGHJ
Text ID
L1 L2
                                                                     Figure 6.6. Regression ratios for each text
Between-subjects ANOVAs are used to determine how the eye movements differ for each text type as well as between L1 and L2 readers. L1 readers have lower NNFs (F(1,593)=10.972; p=0.001; partial η2=0.018) and higher regression ratios compared to L2 readers (F(1,593)=5.668; p=0.018; partial η2=0.009). This result confirms the observations made from inspections of Figure 6.5 and Figure 6.6, that L2 readers have higher NNF values and lower regression ratios.
The ANOVA reveals that the readability only affects the NNFs observed (F(2,593)=3.45; p=0.032; partial η2=0.012) but not the regression ratio (F(2,593)=0.181; p=0.835; partial η2=0.001). Tukey’s multiple comparison tests were used to further investigate the effect that readability has on NNFs. This reveals that the difference lies in the easy versus the difficult readability (p<0.005) however the medium readability results in NNFs that are not statistically different from the easy or difficult text.
Note that the NNF is a ratio of fixations to words. There is generally an uneven distribution of fixations on words whilst reading English (Rayner, 1998). The NNF values for normal reading behaviour are therefore expected to be less than 1. In fact, Carpenter and Just (1983) found that readers fixate on average on 67.8% of words. The closer a value is to 0 the greater the skimming of the text. Values above 1 correspond to more fixations than there are words in the paragraph and are indicative of re-reading of some of the text. The NNF values for each text are shown in Figure 6.5.
From the MANOVA analysis we observe that there is a significant difference between L1 and L2 readers. We can observe from Figure 6.5 that the difference
118
Regression ratio
Effect of Text Difficulty on Prediction of Comprehension
 comes from the L2 readers tending to have higher NNFs compared to the L1 readers. This is expected from past research (Dednam et al., 2014; Kang, 2014). The MANOVA analysis also shows that text readability has a significant effect on NNFs. We can see from Figure 6.4 that as readability difficulty increases, so too does the NNFs. This is what would be expected from past research (Rayner, 1998; Rayner et al., 2006). As we would expect from the MANOVA, there is no similar increase in NNF values as the concept level increases.
Additionally, the MANOVA shows that there is a significant difference between the regression ratios for L1 and L2 readers, averages shown in Figure 6.6. Contrary to what we would expect, the L2 readers have lower regression ratios compared to the L1 readers. That is, the L1 reader tend to regress more in comparison to forward saccades. This could be due to the fact that they simply have fewer forward saccades so this need to be further investigated. As we would expect from the MANOVA there is no relationship between regression ratio and text difficulty.
The analysis shows that there is significant effect of readability on NNF values. However, conceptual difficulty had not effect on either eye movement measure, contrary to our hypothesis. We found that readability affects normalised number of fixations (NNF) but not regression ratio. We also found that there is a significant difference between the L1 and L2 readers NNF and tendency to regress. However, there was no interaction effect between the reader groups and the text difficulty. This indicates that in this study the readability and conceptual difficulty of the text affect the two groups similarly.
6.5 Discussion and Implications
The overall research question for this chapter, and the previous, was whether reading comprehension can be reliably predicted from eye tracking data. In the previous chapter we established that predicting reading comprehension scores from eye movements is not trivial. We explore the question of whether text difficulty affects prediction accuracy. The premise is that text difficulty causes differences in eye gaze (Rayner et al., 2006). Therefore, we hypothesise that increased text difficulty will increase prediction accuracy from eye tracking data. Furthermore, these investigations were performed with respect to L1 and L2 readers. We hypothesized that predictive performance would be different for L1 and L2 readers.
The analysis shows that there are differences in prediction accuracy between L1 and L2 groups. On average, prediction accuracies for the L2 groups are lower than for the L1 group. However, text difficulty was not found to have a significant effect on prediction accuracy. This requires further analysis since, even though there is no statistically significant difference, there is much less of an imbalance in the scores in the more difficult texts. So obtaining similar prediction results to cases with an imbalance in scores indicates that the prediction quality must be improved somehow.
Even though our hypotheses were not validated in this analysis we did improve classification accuracy to on average 80% for the L1 group and 73% for the L2
119
Effect of Text Difficulty on Prediction of Comprehension
 group, which is a substantial improvement from the 44% correct classification obtained in the previous chapter for format C. These results were achieved by using genetic algorithms (GA) for feature selection, which were significantly higher than the results produced when no feature selection is performed.
The analysis of the eye movements for each text, somewhat, supported this conclusion. We had hypothesised that we would observe a much greater difference in eye movement caused by the difficulty of the text. However, what we see is that there is a significant effect of readability on NNF but not on regression ratio, and the conceptual difficulty does not affect either measure. Further investigation should be carried out to investigate this further, including looking further into the pupil dilation data and whether the pupil is affected more by conceptual difficulty.
We added pupil dilation measure to the feature set as pupil dilation is affected by cognitive load (Beatty, 1982; Iqbal et al., 2004; Kahneman & Beatty, 1966), whereby the pupil dilates under increased load and constricts under decrease load. Furthermore, it has been shown that pupil dilation is also affected by repeated exposure to a task, or more precisely, learning (Kahneman & Beatty, 1966; Sibley et al., 2011). This would appear to be an appropriate candidate measure for reading comprehension prediction. However, average pupil dilation over a task can negate any of the observed changes in pupil dilation caused by the task (Iqbal et al., 2004). This introduces the idea that windowing the task into smaller chunks would improve predictive accuracy. The windowing results in high numbers of features so we also introduced feature selection. Windowing physiological data and then using feature selection has been shown to be beneficial in predicting stress during reading tasks (Sharma & Gedeon, 2012, 2013a). However, the use of task windowing did not provide any significant improvement; it was instead the GA feature selection that provided the substantial improvement. However, we have only considered a very short reading task. Each text had on average 240 words, which took on average 84 seconds to read. This is not a long task when we consider many online collections of learning materials. Further analysis of windowing should be investigated for longer tasks.
In this chapter we investigate ANNs, kNNs, and random forests as predictors. One of the disadvantages of using ANNs and random forests in the need for longer training times, which is made significantly lower when combined with GA feature selection. KNN on the other hand does not suffer from the same problem. The results from the study indicate that in this case kNN is optimal for predicting reading comprehension from eye gaze measures when GA feature selection is used. Sharma and Gedeon (2013a) found that using GA feature selection and support vector machines (SVM) provided high classification rates of stress during reading. The use of GA-SVM should also be considered in this case. Additionally, as noted by Sharma and Gedeon (2013a), even though better classification results were obtained using GA feature selection, the execution time is substantially longer than for other methods. In our investigation we utilised smaller populations and much fewer generations to increase speed. Further exploration using larger populations and more generations should be investigated to determine if there is a general optimal trade-off between accuracy and training time.
120
Effect of Text Difficulty on Prediction of Comprehension
 6.5.1 Usecase
The main goal of reading comprehension detection is to incorporate eye tracking into eLearning environments and use the eye tracking data as a form of adaption. The use of reading comprehension prediction to perform adaption was discussed in Chapter 5, where we outlined how eLearning material can be tailored to a student’s current level of understanding. The main advantage of using eye-tracking data over explicit answers to questions is that the comprehension questions could be altogether removed. In turn this speeds up the learning session as well as alleviating stress and anxiety over not knowing answers or questions being too simple. Instead, an individualised dynamic learning path could be followed and students would not even be aware that what they are reading is possibly different from their peers.
Another use of comprehension prediction in eLearning is similar to the concepts put forward by Buscher et al. (2012) and Okoso et al. (2015) whereby part of the documents are labelled based on reading behaviour. In the first case, Buscher et al. (2012) propose using eye tracking to annotate parts of a document that contain many eye movements. Okoso et al. (2015) propose finding parts of documents that are not comprehended. Conflation of the two ideas with the current research leads to the notation of real time detection of comprehension levels and annotation of the learning documents with these levels. The student would not be aware of these annotations but the learning environment could adaptively reshow parts of the text that are not well understood. In this way, when a student clicks a Next button to move to the next text in the sequence, the next text could be dynamically selected to also reflect the parts of the text that were not well understood. In this way, it is quite similar to the use proposed in the previous chapter, however, the idea is refined in this case so that instead of the whole text being re-shown to students (and thereby giving them information they already understand), only the subsections of text that are not well understood could be re-shown together with new information.
Alternatively, instead of re-showing the students text plainly, the use of questions and text presentation (as discussed in Chapter 3) could be used to exploit the student’s answer-seeking behaviour and reading behaviour to encourage them to read certain parts of the text more thoroughly. That is, instead of giving students another page of text, giving them a page of text with questions as well. The questions themselves could be related only to the parts of the text that were not well understood based on the eye tracking predictions. This could encourage re-reading of these sections to answer the questions. In the case that it does not, the feedback of getting the questions wrong will then encourage re-reading of these sections.
6.6 Conclusion and Further Work
The goal of this chapter was to discover techniques for increasing prediction performance for reading comprehension. We investigated the effects that reader type and text difficulty have on predicting reading comprehension from eye gaze data. In the analysis we were able to achieve 80% classification accuracy for the L1 group and 73% for the L2 group, which is a substantial improvement from the 44% correct classification obtained in the previous chapter for format C. We did not find
121
Effect of Text Difficulty on Prediction of Comprehension
 that the text difficulty improved predictive accuracy, however, the use of feature selection did provide significantly higher predictions then without. We also experimented with the addition of pupil dilation feature set. The analysis cannot confirm that the addition of this feature was significant. Further work considering the accuracies of the classification using only the pupil dilation feature set should be carried out. This would explore if there are added benefits of using these features. Furthermore, use of only these feature set should be considered to see if this is better than the eye movement measures altogether.
Whilst there was no significant difference in prediction accuracy found due to the text difficulty, we did show that the text readability has a significant effect on eye movements, whereas, conceptual difficulty has no effect on eye movement. This should be investigated further as it was hypothesised that the conceptual difficulty would also affect eye movements. The implications of only the surface variables of the text affecting eye movement is interesting and has important side effects in the context of eLearning.
We also investigated the use of task window and GA feature selection. This showed that whilst windowing provided no improvement, the GA feature selection did improve predictions. Additionally, we found that the best predictor, for this problem type, from the set that we investigated is kNN. Given the results from the windowing, further investigation should be considered where longer documents are read. In our case the documents were quite small; barely longer than the abstract of this chapter, or approximately the length of this paragraph. The hypothesis is that increased length will make windowing useful. However, the current windowing results show a reasonable possibility for labelling paragraphs or sections of text with a level of comprehension. In this way we could move towards real time comprehension detection of small sections of documents. This requires further work, again by investigating longer documents.
Up to this point we have only considered predicting reading comprehension based on eye gaze data. The data collected from this user study allows us to make predictions about the text difficulty. Given the interesting results obtained from the eye movement analysis several questions arise; 1) if text difficulty affects L1 and L2 readers eye movements differently, are their perceptions of text difficulty also affected differently? 2) Since the eye movements are not as we hypothesised, are the participants’ perceptions of difficulty more accurate than their eye movements at predicting readability? Finally, 3) given that the readability and conceptual difficulty have different effects on the L1 and L2 groups, is this reflected in their perceptions? In the next chapter we investigate the comparison of text difficulty prediction from eye gaze data versus participants’ perceptions and also perform more analysis on the effect of text difficulty on the L1 and L2 readers.
122
Chapter 7
Chapter 7. Perception and prediction of Text Difficulty
  Up to this point we have only considered predicting reading comprehension from eye gaze data. Given the results in Chapter 6, several questions arise around whether participants can predict text difficulty. In this chapter, we investigate prediction of text difficulty from eye gaze using machine learning techniques, and compare these to participants’ perceptions of difficulty. We show that predictions from eye tracking data are more accurate than the participants’ perceptions of both readability and conceptual difficulty. We then show that prediction of participants’ perceived ratings of readability and conceptual difficulty from the eye tracking data are also better than prediction of the predefined values. This indicates that the eye gaze measures and pupil dilation data may be more aligned with the participants’ perceptions of difficulty rather than the predefined difficulty of the text. Further analysis of participants’ perceptions showed that they are poor at predicting predefined text difficulty, especially when the readability and the conceptual difficulty are not the same. The readability and conceptual difficulty of a text interact with each other to distort participants’ perceptions of overall text difficulty. Further analysis of text difficulty on participants’ perceptions shows that text difficulty does not affect participants subjective understanding but does have a significant effect on comprehension. However, the effect is minimal, where the only significant difference is the scores for the easiest (A) compared to the hardest (J). This suggests that comprehension score alone is not a sufficient indicator of text difficulty. Nevertheless, L1 readers scored higher on comprehension questions compared to L2 readers, contrary to past research, and text difficulty did not affect L2’s confidence in answering the questions. The analysis highlights that there are
“Two people can look at the same thing
and see it differently.”
  ―
Justin Bieber
123
Perception and prediction of Text Difficulty
 significant differences in perceptions of L1 and L2 readers, which must be considered when designing texts for education.
7.1 Introduction
Reading online materials is essential as it is a primary way of accessing many forms of information. Much has been done in researching effective ways of presenting learning materials in learning environments (Clark & Mayer, 2011). There has also been headway on investigating the growing diversity of students in eLearning, specifically by linguistic background. It has been established that first (L1) and second (L2) English language readers have different reading behaviour (Dednam et al., 2014; Kang, 2014). However, how we perceive a task does not always match our performance on that task and people often see the same thing differently. For example, it has been shown that people who are unskilled are also unaware of their deficiency and so overrate their abilities in comparison to the appropriate cohort; especially, they think they are above average. Conversely, skilled people tend to know their shortcomings and underrate their abilities in comparison to the cohort (Dunning et al., 2003; Ehrlinger et al., 2008; Kruger & Dunning, 1999). Indeed, task complexity and perceptions of task complexity are distinct but are related to one another (Robinson, 2007). Task complexity affects perceptions of difficulty as well as confidence levels (Robinson, 2007). A complex task does not guarantee that perception of that task will be that it is complex. This is especially true when comparing readers who have different levels of expertise in the language they are reading.
In Chapter 3 we touched on the perceptions of students in eLearning where we recorded their perceptions of their understanding. We found that there was no significant difference between L1 and L2 readers in their perceived comprehension, but there is a difference in accuracy of these perceptions based on the presentation format the participant was shown. That is, when the comprehension questions are shown in isolation from the text, participants were more likely to be able to correctly perceive their understanding as opposed to when the questions were shown on the same page as the text. However, the texts had the same level of difficulty in that study, so we could not investigate how changing the difficulty affects participants’ predictions. So whilst L1 and L2 participants had the same perceived understanding in the previous study, this does not imply that both groups found it equally challenging. The user study described in Chapter 6 provides the opportunity to investigate the question of whether participants can predict text difficulty and whether the we can predict text difficulty from participants’ eye tracking data. Therefore, the question being investigated in this chapter is:
Can participants predict text difficulty and can we predict text difficulty from their eye gaze?
Text difficulty in this context is a combination of readability and conceptual difficulty. The use of eye gaze has shown potential for predicting task difficulty (Rayner et al., 2006; Victor et al., 2005). In particular, we know that pupil dilation information is related to task difficulty (Engelhardt et al., 2010; Iqbal et al., 2004;
124
Perception and prediction of Text Difficulty
 Pomplun & Sunkara, 2003; Zekveld et al., 2014). We therefore hypothesise that both eye gaze and pupil dilation data can be used to predict text complexity. Does text difficulty affect perceptions of participants, and does it do so in the same way for L1 and L2 readers? We hypothesize that changes in text difficulty will be reflected by changes in perceived difficulty but that participants’ eye gaze data will be more accurate at predicting the text difficulty than their perceptions.
This chapter is organized into the following sections: background information; prediction results; perception analysis; discussion and implications; and finally the conclusion and further work.
7.2 Background
7.2.1 Definingtextdifficulty
The definition of text difficulty that we will refer to in this chapter is adapted from the Common Core State Standards (Initiative, 2012) which is an educational initiative in the United States. This standard defines text difficulty as a combination of three components: qualitative, quantitative, and reader and task considerations. The quantitative component is based on the text structure and calculated from a formula using word and sentence structure; examples are the Flesh-Kincaid readability test calculating the education a reader needs to comfortably read a piece of text (Kincaid et al., 1975). Tests with more dimensions include COH-Metrix, which produces measures defining the cohesion of a document as well as the readability for L2 readers (Crossley et al., 2008; McNamara et al., 2013). The qualitative component refers to the levels of meaning and knowledge demands. The last component of text difficulty is not so much related to the text itself but to the reader and the task being performed. That is how motivation, prior knowledge, task and purpose influence the text difficulty (Bunch et al., 2014).
7.2.2 DifferencesbetweenL1andL2readers
The differences between L1 and L2 readers has growing importance given the wide spread and pervasive use of the Internet and World Wide Web. Access to texts that are not written in a reader’s native language is now easy and often required especially for study. The impact of this on learners is of great importance for designers of eLearning environments, as they must take into consideration the differences between L1 and L2 readers. There are differences in eye movements as well, for example L2 Afrikaans readers exhibit more fixations and for longer duration than L1 readers (Dednam et al., 2014). This is consistent with what we found for L1 and L2 English readers in Chapter 3.
The differences between L1 and L2 readers can be seen in their reading behaviours. Kang (2014) found that L1 and L2 English readers performed no differently in comprehension tests and that there was no difference in reading attention distributions or eye gaze patterns, but L2 readers took longer to read the text and longer to find answer cues in the text. Notably, L1 readers tend to deal with increases of text difficulty with increased reading efficiency, whereas, L2 reading
125
Perception and prediction of Text Difficulty
 efficiency decreases (Dednam et al., 2014). Text characteristics should be considered differently for L1 and L2 readers since they have differential effects on reader type (Zhang et al. 2013).
7.2.3 Predictionoftextreadability
Text characteristics include word count, syllable count and number of words in a sentence, which are often used to calculate readability. The readability formula used throughout this thesis is the Flesch-Kincaid grade level which is the most widely used readability test, taking into account only the total number of words, sentences and syllables (Kincaid et al., 1975). The such characteristics have been linked to greater difficulty in reading, according to eye movements, which is also linked to lower comprehension (Scherr et al., 2015). Since readability has an effect on reading behaviour and comprehension, it is important to consider how it is calculated. Most automated tools for detecting text difficulty focus primarily on the readability of the text, based on the syntactic nature of the text. That is, the traditional formulas rely on counting words, word length, sentences length, and syllables. The Flesch-Kincaid grade level is one the most widely used readability test, taking into account only the total number of words, sentences and syllables. Whilst this formula is quick and easy to use in practice there are two potential problems with it, firstly it only deals with the surface properties of the text, not accounting for the conceptual difficulty and secondly it is generally aimed at English text for native English readers (Zhang et al. 2013).
COH-Metrix is an important tool in this context as it provides a bridge to overcome these faults. COH-Metrix measures text cohesion at various levels of selected language, discourse, and conceptual analysis to provide a measure of readability from a cognitive view (Crossley et al., 2008; Graesser et al., 2011; McNamara et al., 2014). COH-Metrix has been found to be better at predicting the reading difficulty than traditional readability formulas (Crossley et al., 2008). Additionally, COH-Metrix produces a measure of readability for second language readers, the L2 Readability Index (Crossley et al., 2008; Graesser et al., 2011; McNamara et al., 2013). This has been shown to be useful in assessing the difficulty of texts and highlighting differences between L1 and L2 readers (Zhang et al., 2013). Note that we investigate the L2 Readability Index further in Chapter 8.
7.2.4 Perceptionsoftaskcomplexity
Perceptions can play an important part in learning and how students approach study. Perceptions of heavy workload with inappropriate assessment promotes surface learning whereas perceptions of good teaching and appropriate assessment promotes deep learning and are a stronger predictor of learning outcomes (Lizzio, Wilson, & Simons, 2002). Additionally, confidence in performing the underlying task influences perceptions of task difficulty. This has been shown in the area of programming studies where computer confidence has a significant effect on perceived task difficulty (Chang, 2005). Importantly, managing perceptions can help alleviate anxieties in learning. Task complexity affects perceptions and confidence (Robinson, 2007). The importance of managing perceptions of L1 and L2 readers is
126
Perception and prediction of Text Difficulty
 necessary given that L2 readers have great perceived difficulties with hard texts (Dednam et al., 2014).
7.3 Method
7.3.1 Datacollection
The method for the data collection user study was described in Chapter 6 and will not be repeated here. Instead this section will outline prediction of text properties from eye gaze measures. Firstly, we will recap the text difficulty properties, as they are crucial in this investigation. The texts had differing levels of difficulty that are a combination of three different levels of readability, as measured from the Flesch- Kincaid Grade Level (Kincaid et al., 1975), and three different levels of conceptual difficulty, constructed independently by a colleague. This resulted in nine texts with differing difficulty, which is described by the grid system in Figure 7.1.
Figure 7.1. Description of the text difficulty
Note that there is no correlation between the Flesch-Kincaid Grade Level and the conceptual level for each text in each topic (r=-0.1, for all topics). The Flesch-Kincaid Grade Level does not account for any changes in the conceptual level of the text, which was not only expected but also necessary for the above grid system construction of text complexity.
After each piece of text, participants were asked two comprehension questions to assess their understanding of the text and four qualitative questions related to the text they read. These questions are:
5. How well do you think you understood the text?
(Very well / Well / Somewhat / Not at all)
6. How confident were you answering the questions?
(Very confident / Confident / Not Confident)
7. How difficult did you find the text to read?
(Easy / Moderate / Hard)
8. How complex was the concept being explained in the text?
(Basic / Intermediate / Advanced)
 127
Perception and prediction of Text Difficulty
 7.3.2 Predictionmethod
Once again windowing is used along with GA feature selection. The GA parameters and explanation for feature selection are described in Chapter 6. In this analysis we use k-nearest neighbour (kNN) classification and GA feature selection method used by Oluleye et al. (2014). The classification results were generated from nested 10- fold cross validation. All analyses were carried out using Matlab R2013a.
7.3.3 DataPre-processingforprediction
7.3.3.1 Inputs: Eye gaze and Pupil dilation data
The inputs to the classifier are the same as those defined in Chapter 6. Please refer to Chapter 6 for details regarding the inputs.
7.3.3.2 Outputs
Each text has a readability level of Easy, Medium, and Difficult and a conceptual level of Basic, Intermediate, or Advanced. In this analysis we look at the prediction of both the readability and the conceptual levels. In the final part of the analysis we look at the overall text difficulty which is the product of both the readability and the conceptual difficulty. These outputs refer to the text IDs, as shown in Figure 7.1, A through J.
7.4 Predicting text difficulty
In this section we analyse whether participants are able to predict text difficulty, and compare their perceptions of text difficulty to predictions of text difficult from their eye movements. Each text has a difficulty that is a product of the readability and the conceptual levels, which are first considered separately, and then the combination is considered to assess overall perceptions of text difficulty.
7.4.1 Predictionsofconceptualdifficulty
The first part of the investigation is prediction of the conceptual difficulty. We control for the readability of the text. Where the Easy, Medium, and Difficult are the three grades of readability. For the kNN classifier this is a three-class problem where the conceptual difficulty can be: Basic, Intermediate, or Advanced. However, the readability also differs for each text by three variables. We consider three cases since we control for the readability of the text. We therefore predict the conceptual difficulty for each of the readability levels. The average correct classification rates (%) from the nested 10-fold cross validation for the GA-kNN classification are shown in Figure 7.2.
Figure 7.2 indicate that the predictions of text difficulty from the GA-kNN are more accurate than participants’ ratings of conceptual difficulty, for every level of readability. One explanation for this could be that whilst participants are not consciously aware of the correct level of difficulty, they are non-consciously aware of the difference since their eye movements reflect the difficulty to a higher degree.
128
Perception and prediction of Text Difficulty
   Prediction of conceptual difficulty for each readability level
80
70
60
50
40
30
20
10
Easy Medium Difficult
                     0
Participants' perception GA-kNN
        L1 L2 L1 L2
   Figure 7.2. Participant versus GA-kNN predictions of conceptual level (for each level of readability)
ANOVA of the predictions shows that there is a significant difference between participants’ perceptions of text difficulty compared to the predictions from the GA- kNN (F(1,2)=33.51; p=0.029). However, there is neither a significant difference between the L1 and L2 reader groups (F(1,2)=0.02; p=0.892) nor between the different levels of readability (F(2,2)=0.02; p=0.892).
The participants’ predictions of conceptual difficulty are poor; given that there are three classes, chance identification is 33%, participants’ ratings are close to chance. We see that for the L1 participants’ predictions of conceptual difficulty, as the readability becomes more difficult participants’ ratings of conceptual difficulty decrease in accuracy. However, this is not seen as evidently for the L2 participants since the ratings for the concept level is (almost) the same for both the Medium and the Difficult levels of readability. This could be why we see no significant difference between the levels of readability.
The GA-kNN predictions from the eye tracking data also gets worse for each readability level. This might imply some sort of weak interaction between readability and conceptual difficulty, by which the difficulty in readability is masking the difficulty in conceptual level. The eye tracking data provide slightly better predictions of conceptual difficulty for the L2 group compared with the L1 group. Prediction of the conceptual difficulty is about double chance (64%) for the L2 participants. In this case, whilst the L2 participants are slightly worse at predicting the conceptual difficulty of the text, their eye movements, at least in the easier levels of readability, reflect the conceptual difficulty to a higher extent compared to the L1 participants.
Notably the predictions from the eye tracking data for the L1 group are similar to the L2 participants’ ratings where the predictions for the Medium and Difficult
129
% correct prediction
Perception and prediction of Text Difficulty
 texts are the same. However, there a clear difference between the predictions for the Medium and Difficult but not between the Easy and the Medium texts from the eye tracking data from the L2 participants. Perhaps L1 readers use the same eye movement strategies for Medium and Difficult text while L2 readers use consistent strategies for Easy and Medium texts.
7.4.2 Predictionsofreadability
The second part of the analysis is prediction of the readability level of the text. Once again we consider three cases since we control for the conceptual difficulty of the text in this section. We therefore predict the readability for each of the conceptual levels: Basic, Intermediate, or Advanced. The average correct classification rates (%) from the nested 10-fold cross validation for the GA-kNN classification are shown in Figure 7.3.
  Prediction of readability for each level of concept difficulty
80
70
60
50
40
30
20
10
0
Basic Intermediate Advanced
                       L1 L2 Participants' perception
L1
L2
   GA-kNN
       Figure 7.3. Participant versus GA-kNN prediction of readability level (for each level of conceptual difficult)
ANOVA of the predictions shows that there is a significant difference between participants’ perceptions of text difficulty compared to the predictions from the GA- kNN (F(1,2)=170.88; p=0.0058) and between the levels of conceptual difficulty (F(2,2)=34.79; p=0.0279). However, there is no significant difference between the L1 and L2 reader groups (F(1,8)=0.25; p=0.6332).
The results of the analysis shown in Figure 7.3 are similar to what was found in Section 7.4.1. Once again, the GA-kNN predictions are more accurate predictions of readability than participants’ prediction of readability, for each level of conceptual difficulty. Similarly, the accuracy of predictions of readability decrease as the concept level increases in difficulty, however, this time the difference is significant. This is an interesting finding as it appears that might be some sort of interaction between readability and conceptual difficulty, by which the conceptual difficulty masks the participants’ ability to detect difficulty in readability.
130
% correct prediction
Perception and prediction of Text Difficulty
 The L1 and L2 groups rate the readability level with quite similar accuracy. This is also true for the predictions from the eye tracking data. That is, the readability appears to affect both the L1 and L2 participants in similar ways, no matter what the conceptual level.
7.4.3 Predictionofexplicitperceptionofdifficulty
An important question to now ask given the results in Sections 7.4.1 and 7.4.2 is whether the eye tracking data is more related to the predictions of the participants, rather than to the predefined levels of readability and difficulty. In this section we assess this question. The results from prediction of participants’ perceptions of conceptual difficulty from their eye tracking data using the GA-kNN, with nested 10-fold cross validation, for each level of readability are shown in Figure 7.4.
  80
70
60
50
40
30
20
10
0
Easy Medium Difficult
Prediction of participants' perception vs predefined conceptual difficulty, for each level of readability
                L1 L2 L1 L2
Prediction of participants' Prediction of text difficulty perception of text difficulty from physiological data
   Figure 7.4. Classification of perceived conceptual difficulty versus predefined conceptual difficulty from eye tracking data
ANOVA of the predictions shows that there is a significant difference between prediction of the participants’ perceptions of conceptual difficulty to prediction of the predefined conceptual difficulty, both based on their eye tracking data (F(1,8)=6.09; p=0.0389) but that there is no difference between the L1 and L2 readers (F(1,2)=2.32; p=0.1665). From Figure 7.4 we can see that the prediction accuracy for the participants’ perceptions of conceptual difficulty from their eye physiological data are higher than prediction of the predefined text difficulty, using the same data. This indicates that eye tracking data might be more linked to the participants’ perceptions of conceptual difficulty rather than the actual conceptual difficulty. This could explain the lack of significant difference in eye movement measures between conceptual difficulty levels, as discussed in section 6.4.3.
We now consider prediction of participants’ ratings of readability level whilst controlling for the conceptual difficulty. The results from prediction of participants’
131
% correct classification
Perception and prediction of Text Difficulty
 ratings of readability from their eye tracking data using GA-kNN, with nested 10- fold cross validation, for each level of conceptual difficulty, are shown in Figure 7.5.
  Prediction of participants' perception vs predefined readability, for each level of conceptual difficulty
80
70
60
50
40
30
20
10
0
L1 L2 L1 L2
Prediction of participants' Prediction of text difficulty perception of text difficulty from physiological data
     Basic Intermediate Advanced
              Figure 7.5. Classification of perceived readability level versus predefined readability level from eye tracking data
ANOVA of the predictions shows that there is a significant difference between prediction of the participants’ perceptions of readability to prediction of the predefined readability, both based on their eye tracking data (F(1,8)=10.57; p=0.0117) but that there is no difference between the L1 and L2 readers (F(1,2)=3.1; p=0.1163). We observe in Figure 7.5, that the prediction accuracy of participants’ perceived level of readability are higher compared to the prediction of predefined readability using participants’ eye tracking data. Once again, this could indicate that eye tracking data is more linked to the participants’ perceptions of readability rather than the actual text difficulty. Whilst we found that readability affected eye NNFs, but it did not affect the regression ratios, and the only difference found in the NNFs was between the easy level of readability and the difficulty level of readability, and not the medium level of readability. The lack of difference between the easy and medium as well as the medium and the difficult levels indicates that the readability did not have a large effect on the NNF. The higher accuracy of participants’ perceptions of readability compared to the predefined readability could explain the lack of significant difference in eye movement measures between all readability levels, as discussed in section 6.4.3.
7.4.4 Predictionofoveralltextdifficulty
One of the questions raised from the analysis of eye movement data in Chapter 6 is that given readability and conceptual difficulty have different effects on the L1 and L2 groups, is this difference reflected in their perceptions? In this section we will explore this question by delving deeper into participants’ explicit perceptions, as
132
% correct classification
Perception and prediction of Text Difficulty
 well as further investigating the use of eye tracking data to predict overall text difficulty.
7.4.4.1 Prediction of text difficulty
In this section we investigate if there are interactions or effects of readability and conceptual difficulty on perceptions. A Chi-square test shows that the text difficulty affects perceptions of conceptual level for both groups (c(16)=51.7, p=0.001 for L1 readers, and c(16)=53.3, p=0.001 for L2 readers). Text difficulty also has an effect on perceived readability level for L2 readers, but not on L1 readers (c(16)=19.4, p=0.247 for L1 readers, and, c(16)=47.0, p=0.001 for L2 readers). These results highlight two key points; the perceptions of text difficulty are different between the L1 and L2 readers and changes in text difficulty are reflected in perceived difficulty.
Table 7.1. Expected versus reported text difficulty for L1 readers
      Perceived Text Difficulty (%)
A B C D E F G H J A Easy Basic       4513117193001 36 10 0 19 26 5 0 2 2 3850191950140 375210372250 225018385254 7 13 0 20 27 7 13 7 7 350020355000 32 5 0 23 27 5 0 5 5 1160174406170 297018304362
Given that the perception scale is the same as the scale used to rate the texts by the author, we can put these ratings together to come up with the same grid references as shown in Table 7.1 for L1 readers and Table 7.2 for L2 readers. We compare the expected to the reported percentages of text difficulty. A Chi-square test for independence shows that there is a strong relationship between the text difficulty and the perceived text difficulty for the L1 readers (c(64)=92.7, p=0.01) and the L2 readers (c(64)=99.1, p<0.005). However, it is clear that participants, both L1 and L2, are poor at perceiving the predefined text difficulty. This is signified by the main diagonal (in bold) and contains very low percentages. Instead the difficulty affected the participants’ perceptions in other ways, which we will elaborate upon in the section below.
As just stated, L1 readers are poor at perceiving the actual text difficulty. With the combined variables of text difficulty, L1 participants correctly classify 47% of the texts, which is well above chance prediction of 11%. As the Chi-square test showed, text difficulty and perceived text difficulty are not independent, so we would expect that participants perform above chance. Most L1 readers rate texts as A, D or E. That is, mostly participants think the texts have an easy level of readability with either an
Actual Text Difficulty ID Read. Conc.
                                             B Mod. Basic C Diff. Basic D Easy Int. E Mod. Int.
F   Diff. Int.
G Easy Adv.
H Mod. Adv.
             J Diff. Adv.
                 Average
                             133
Perception and prediction of Text Difficulty
 easy or intermediate level of conceptual difficulty, or a moderate readability with intermediate conceptual difficulty. The interesting issue about this is that A and D have an easy readability but different levels of conceptual difficulty and E has both intermediate readability and conceptual level. However, L1 readers seem unable to distinguish the readability levels from the conceptual difficulty.
Table 7.2. Expected versus reported text difficulty for L2 readers
Actual Text Difficulty Perceived Text Difficulty (%)
ID Read. Conc. A B C D E F G H J A Easy Basic       36 15 0 6 40 1 0 1 0 12 6 3 6 59 6 0 3 6
                                           B Mod. Basic
C Diff.
D Easy
E Mod.
F Diff.
G Easy Adv. H Mod. Adv.
J Diff. Adv.
Average
0 0 0 0 50 20 0 10 20 1414311430096 7 19 0 4 52 0 0 15 4
1890027901818 22 0 0 0 56 11 0 0 11 14 0 0 29 29 0 0 0 29 10 0 0 0 30 0 0 20 40 157164350815
Basic Int. Int. Int.
                                              Furthermore, L1 readers appear to see texts as being simpler than they are, as many of the ratings are in the lower half of the diagonal. More specifically, when the readability and conceptual levels are at opposite extremes to one another (texts C and G) we see interesting interactions that reveal much about the nature of the interaction between readability and conceptual levels. That is, when the readability was difficult and the concept basic (text C) the majority of L1 readers rated the text with intermediate and advanced concept levels and varying degrees of readability. Conversely, when the readability is easy but the concept is advanced (text G) no L1 participant rated the text with advanced conceptual level and instead the majority rate it with intermediate concept level and differing degrees of readability. Very few L1 participants rated the texts as having difficult readability (C, F and J). There are also low ratings for B, G and H. Essentially participants are poor at perceiving the most complex texts as well as the interactions between the readability and conceptual difficulty of the text. There is an interaction that masks the two variables resulting in a rating somewhere in the middle. More specifically, these texts are rated as moderate in readability and intermediate in conceptual difficulty (E). Participants extrapolate the difficulty as being somewhere in the middle of the two variables. This poses an interesting question, how distinguishable is readability from conceptual level to the reader?
With the combined variables of text difficulty L2 participants can correctly classify 45% of the texts, similar to the L1 group, this is well above chance, as we would expect. Again, we see that many L2 readers rate texts as E, both intermediate readability and conceptual level, which is similar to the L1 readers. However, the perceptions of L2 readers are somewhat different from L1 readers. There is a spread
134
Perception and prediction of Text Difficulty
 of complexity ratings for the texts, with many more L2 readers rating texts as more difficult than they are. The knock on effect of this is that the L2 readers are capable to perceive the most difficult text J compared to L1 readers. As hypothesised, L2 readers tended to rate texts with higher difficulty compared to the expected difficulty.
L2 readers do however show the same behaviour as the L1 readers whereby the perceived complexity of the text is mostly conflated so that there is a significant over estimation of texts being rated as both Moderate readability and Intermediate conceptual level (E). More generally, L2 readers mainly rated texts as A, E and J, where the readability and conceptual levels are the same levels.
To summarise, the key highlights of the perception analysis are:
1. L1 and L2 readers have different perceptions:
a. L2 readers tend to overestimate difficulty of the text
b. L1 readers tend to underestimate the difficulty of the text
2. Both groups over estimate complexity as E where the readability and conceptual difficulty are both in the middle of the scale
3. Both groups tend to conflate the levels of readability and conceptual difficulty, thus under estimating all texts surrounding the main diagonal, especially as complexity ratings C and G.
4. Their eye movements are a reflection of both the predefined and explicitly perceived text difficulty
The two extremes in the readability and conceptual level do not mix well when they are inverses of one another, (texts C and G). The interaction between the two variables results in an underestimation of one variable and overestimation of the other. If the desired effect is to make a concept appear harder or easier, then the readability can be changed to achieve this. An example of this is underestimation of text B. Conveying a basic concept in text with difficulty readability will cause perceptions of the conceptual difficulty to be overestimated.
7.4.4.2 Predictions of predefined text difficulty from eye tracking data
To contrast the results of the explicitly perceived text difficulty, we investigate the predictions of predefined text difficulty from the eye gaze and pupil dilation data. GA-kNN classification is used once again to predict the predefined text difficulty, denoted as A through to J. The average correct classification rates (%) from nested 10-fold cross validation are summarised in Table 7.3 for the L1 readers and Table 7.4 for the L2 readers.
For the L1 data set we obtained an average correct classification of text difficulty of 49% from 10-fold cross validation. This is roughly the same compared with the explicit perceptions of participants, where the correct classification rate is 47%. The GA-kNN also has a tendency to predict texts as being simpler than they in fact are which is consistent with the perception analysis. Given the lack in substantial difference between eye movements based on text difficulty as shown in Chapter 6 (section 6.4.3), we would not expect the predictions from the eye gaze and pupil
135
                                                                   Perception and prediction of Text Difficulty
 dilation data to be substantially more accurate compared to participants’ perceptions.
Table 7.3. Average correct classification rates (%) of text difficulty for the L1 group from GA-kNN classification from eye tracking data
Actual Text Difficulty Predicted Text Difficulty (%) ID Read. Conc. A B C D E F G
                           H J A Easy Basic       90313201       00
                     B Mod. Basic C Diff. Basic D Easy Int. E Mod. Int.
F Diff. Int.
G Easy Adv.
H Mod. Adv.
J Diff. Adv.
Average
00 00 00 00 07 00 90 017
434129222 4329195500 463046500 4716092700 5371313070 6051055015 591455900 5611011006 5514512613       13
                                                         Table 7.4. Average correct classification rates (%) for the L2 group of text difficulty group from GA- kNN classification from eye tracking data
Actual Text Difficulty Predicted Text Difficulty
ID Read. Conc. A B C D E F G H J A Easy Basic 96 3 0 1 0 0 0 0 0 47 29 0 18 6 0 0 0 0 50 10 40 0 0 0 0 0 0 63302660003 2511014460004 64009918000 67 22 0 11 0 0 0 0 0 57 29 14 0 0 0 0 0 0
                                                      B Mod. Basic C Diff. Basic D Easy Int. E Mod. Int. F Diff. Int. G Easy Adv. H Mod. Adv.
  J Diff. Adv.
Average
20 40 0 0 20 0 0 0 20 54 16 6 9 10 2 0 0 3
For the L2 data set we obtained an average correct classification of text difficulty of 50% from cross validation. Compared to the explicit perceptions of participants, which have a classification rate of 45%, this is a slight improvement. Similar trends in prediction accuracy for each text are seen where the easier texts are best predicted.
136
Perception and prediction of Text Difficulty
 7.5 Effects of text properties on understanding and confidence
We now move to analysing whether text difficulty affects participants’ comprehension, perceived understanding, and confidence in answering questions, and if so, is it in the same way for L1 and L2 readers? We hypothesise that harder texts will be associated with lower comprehension scores, and that L2 readers will have lower comprehension compared to L1 readers for the harder texts. Comprehension is a quantitative measurement so we will also look into the qualitative data and consider what text difficulty does to participants’ confidence and perceptions of understanding the text. Task difficulty is known to effect perceptions of difficulty as well as confidence levels (Robinson, 2007) so our hypothesis is that harder texts will be associated with lower ratings of confidence and perceived understanding.
7.5.1 Comprehensionversusperceivedunderstanding
Participants were asked to rate their understanding level in answering the questions. These ratings are described in Figure 7.6 for L1 readers and Figure 7.7 for L2 readers. The ratings of understanding were recorded on a Likert scale of Very Well, Well, Somewhat, and Not at all. Using Chi-square test for independence we observe that text difficulty does not affect the ratings of subjective understanding for L2 readers (c(24)=23.59, p=0.485) or L1 readers’ ratings (c(24)=35.81, p=0.06).
  70% 60% 50% 40% 30% 20% 10%
0%
ABCDEFGHJ
Text ID
Very well Well Somewhat Not at all
               Figure 7.6. L1 readers’ subjective understanding on the text
L1 readers rarely answered that they did not understand the text. Most L1 readers rated that they understood the text Well; for all but 2 texts over 50% of participants rated their understanding as Well. What we do see is that there is a pattern whereas the text gets harder this rating goes down and we see an increase in rating understand as Somewhat.
137
% answered
Perception and prediction of Text Difficulty
   70% 60% 50% 40% 30% 20% 10%
0%
ABCDEFGHJ
Text ID
Very well Well Somewhat Not at all
               Figure 7.7. L2 readers subjective understanding ratings
L2 readers much more often answered that they did not understand the text well, with 4 texts with 10% or more of participants rating that they did not understand the text. L2 participants are more likely to rate their understanding as Somewhat however there are still many answers of Well.
  2.5 2.0 1.5 1.0 0.5 0.0
L1 L2
                                                    ABCDEFGHJ
Text ID
Figure 7.8. Average comprehension score per question
Participants’ measured comprehension is shown in Figure 7.8. For the majority of questions, L1 participants received higher scores than the L2 participants. On average, L1 participants scored 1.51 (SD=0.65) on the comprehension test and L2 participants scored 1.36 (SD=0.61). ANOVA analysis shows that whilst the difference is small L2 readers did have statistically significantly lower comprehension scores compared to L1 readers (F(1,599)=6.56; p=0.011; partial η2=0.011). The text difficulty also has a statistically significant effect on the
138
Mean Comprehension Score % answered
Perception and prediction of Text Difficulty
comprehension scores (F(8,599)=3.46; p=0.001; partial η2=0.044). There is no
significant effect of interaction between the reader type and text difficulty.
This result is contrary to what we would expect from past research as well as what we found throughout this thesis, that L1 and L2 participants should perform the same in comprehension tests (Kang, 2014). The results from this investigation indicate that when the degree of difficulty of text is altered then differences in comprehension between L1 and L2 readers occur. L2 participants’ perceptions matched the comprehension levels to an extent given that the L2 readers had higher ratings of Somewhat understanding.
Tukey’s HSD tests were used to perform pairwise comparisons of the texts to further investigate the effects on text difficulty on scores. The pairwise comparison shows that there is a significant difference between texts A and J (p=0.006) and there is a weak difference between texts A and D (p=0.056). However, the difference between texts A and J is consistent with the findings that text difficulty, in particular readability, affects eye movements, and that the eye movements are somewhat related to comprehension. Both A and J are at either ends of the spectrum of text difficulty. The lack of difference between other texts suggests that there is a spectrum of change with only the ends being statistically significantly different.
7.5.2 Confidencelevels
Participants were asked how confident they were with their answers to questions. The ratings are shown in Figure 7.9 for L1 participants and Figure 7.10 for L2 participants. Text difficulty did not have an effect on L2 readers’ confidence levels (c(16)=7.85, p=0.95) although it did affect L1 readers’ confidence levels (c(16)=30.8, p=0.015).
   80% 70% 60% 50% 40% 30% 20% 10%
Very Confident
Confident
           0%
ABCDEFGHJ
Text ID
 Figure 7.9. L1 participants’ confidence ratings
139
% answered
Perception and prediction of Text Difficulty
   70% 60% 50% 40% 30% 20% 10%
Very Confident
Confident
           0%
ABCDEFGHJ
Text ID
 Figure 7.10. L2 participants’ confidence ratings
For the L1 participants, whilst we can observe that there is a slight trend of the Not confident rating as the text becomes more difficult, the majority of participant state that they are Confident in answering the questions no matter what the difficulty is. Therefore, there does not seem to be a clear trend that text difficulty affects confidence ratings of L1 participants.
Whilst again the majority of L2 readers state they are Confident similarly to L1 readers, there is a larger number of L2 readers that state they are Not confident compared with the L1 readers. Therefore, a larger subset of L2 readers compared to L1 readers had less confidence and thus overrated difficulty, which is consistent with past research (Chang, 2005). Given the lower levels of confidence this accounts for the L2 readers, on average, perceiving the texts as being more difficult than L1 readers.
7.6 Discussion and Implications
In this chapter we analysed participants’ perceptions of text difficulty, which showed that that participants’ perceptions of text difficulty do not align with the predefined text difficulty. In particular, participants are poor at predicting text difficulty. The GA-kNN predictions, from participants’ eye tracking data, of readability and conceptual difficulty were significantly higher that the participants’ predictions. However, using the participants’ eye tracking data the results from predicting the participants’ perceived conceptual difficulty and readability were significantly higher than predictions of the predefined values. This indicates that participants eye tracking data may be also aligned with their perceptions of difficulty rather than just the predefined difficulty.
Further analysis of participants’ perceptions of text difficulty shows that the readability and conceptual difficulty of the text interplay to cause deviations of perceptions from the predefined difficulty. For both L1 and L2 readers, the perceptions of text difficulty are worst when the readability and the conceptual
140
% answered
Perception and prediction of Text Difficulty
 levels do not match. This is observed most obviously when the readability is easy and the concept is advanced (G) or the readability difficult and the concept is basic (C). Participants seem unable to distinguish the properties in these cases. There is an interaction that masks the two variables resulting in a rating somewhere in the middle. The result is that L2 readers mostly rated texts as moderate in readability and intermediate in conceptual difficulty (E), and L1 readers mostly rated texts as easy, with both readability and conceptual difficulty (A). For L2 readers, this could be due to participants not really knowing what the levels of difficulty are and therefore extrapolating or approximating as being somewhere in the middle. This highlights the second hypothesis of the study that L2 participants will have an inflated perception of text difficulty compared to the L1 participants. The results from the study support this hypothesis but also the results show that in general L1 readers underestimate text difficulty. Finally, we hypothesised that the eye gaze and pupil dilation data would be better at predicting the text complexity than the participants’ perceptions. The analysis did not provide evidence that this was the case.
The second part of this chapter investigated the effect that text difficulty has on comprehension and confidence. Past research has shown that whilst L1 and L2 readers have different eye movements during reading, they have the same comprehension levels. However, this only dealt with text at a constant level of difficulty. The results from the study indicate that L2 readers have significantly lower comprehension scores to L1 readers. Moreover, text difficulty was found to have a significant effect on comprehension, but the difference is only between the very easiest and the very hardest of the texts, A and J. There is no other clear effect that text difficulty affects comprehension. This indicates that the comprehension scores from the texts are not entirely reliable indicators of difficulty. Given that our analysis showed that predictions for participants’ perceptions of readability and conceptual difficulty from eye tracking data are higher than the predictions the predefined levels, this introduces the idea that eye tracking data could be used as an indicator of difficulty.
Finally, we hypothesised that harder texts would be associated with lower ratings of confidence and perceived understanding. This is because task complexity is known to affect perceptions of difficulty as well as confidence levels (Robinson, 2007). The results show that while our hypothesis was incorrect about subjective understanding ratings, there is a difference between L1 and L2 readers, where L2 readers express lower subjective understanding than L1 readers. L2 readers are also less confident than L1 readers, no matter what text is read and hence text difficulty has no significant effect on L2 readers. This is an interesting finding and could be due to skill level of the readers, so that the L2 readers are too challenged by all of the texts and therefore no effect can be seen. However, this is not true for L1 readers where text difficulty was found to significantly affect confidence, where the harder the text becomes, the less confidence readers have.
141
Perception and prediction of Text Difficulty
 7.7 Conclusion and Further Work
In this chapter we investigated how eye tracking data and participants’ perceptions compare at predicting text difficulty. We take into consideration both the readability and the conceptual difficulty of the text and assess how L1 and L2 readers differ. Participants were poor at predicting text difficulty however we found that using GA-kNN to predict readability and conceptual difficulty, from their eye gaze and pupil dilation data, is significantly more accurate. However, prediction of participants’ perceived ratings of readability and conceptual difficulty from the eye tracking data are significantly better than prediction of the predefined values. This indicates that the eye gaze measures and pupil dilation data may be more aligned with the participants’ perceptions of difficulty rather than the predefined difficulty of the text.
Whilst comprehension scores were found to be effected by the text difficulty. the effect is minimal, where the only significant difference is the scores for the easiest (A) compared to the hardest (J). Combining both findings indicates that comprehension score alone is not a sufficient indicator of text difficulty but that eye tracking data could be used in combination to determine the overall difficulty.
Finally, L1 readers scored higher on comprehension questions compared to L2 readers, and text difficulty did not affect L2’s confidence in answering the questions, highlighting that there are significant differences in perceptions of L1 and L2 readers and not just their reading behaviour. These difference need to be considered when designing texts for education.
Further research into the use of physiological signals could reveal more accurate predictions of text difficulty. In particular, using cognitive load as a measure of text difficulty could provide more accurate text difficulty predictions from eye gaze and pupil dilation data. Cognitive load has been successfully predicted from both eye gaze and pupil dilation data (Rosch & Vogel-Walcutt, 2013), which is useful because cognitive load has been used to predict task difficulty (Waniek & Ewald, 2008). Furthermore, looking into the use of physiological signals such as electrocardiogram (ECG), galvanic skin response (GSR), electroencephalogram (EEG) could prove to be useful in predicting text difficulty as these signals have been used prediction of stress whilst reading documents of difficult degrees of difficulty and stressfulness (Sharma & Gedeon, 2012, 2013a, 2013b).
The results from this chapter indicate that the ranking of text difficultly might be insufficient. We propose the use of eye tracking data to classify texts according to complexity measures that reflect students’ perceived difficulty of the text. We will explore this further in the next chapter.
142
Chapter 8
Chapter 8. Deriving text difficulty from eye gaze
The eye gaze data from the user study in Chapter 6 was used to investigate the differences between L1 and L2 readers’ reading behaviours as well as whether eye gaze measures can be used to derive text difficulty. The investigation involves clustering eye movement measures from participants using kmeans clustering. The results indicate that whilst there are clusters of different reading behaviours for different levels of text difficulty, such as skimming and thorough reading, the L1 and L2 groups were not found to be distinct from each other. Instead, there is a tendency for L2 readers to exhibit more thorough reading compared to skimming. The previous chapters established that eye movements are related to both the predefined and readers’ perceptions of the text difficulty and that the readability and conceptual difficulty interplay to cause deviations from expected text difficulty. This raises the question of whether the ratings of text difficulty are adequate for defining the actual difficulty of the text. The average eye gaze measures for each text were clustered using k-means. The clusters show that there are distinct reading behaviours and that the average eye gaze measures can be used to rate the texts based on the derived reading difficulty for the L1 and L2 groups. These findings can be used to provide feedback to the author for the purpose of adapting learning material. As in previous chapters, this feedback will be in two forms; first on an individual basis to provide feedback regarding reading and thereby aid personalised learning, and secondly, on a cohort basis to provide feedback about reading difficulty of particular texts.
143
Deriving text difficulty from eye gaze
 8.1 Introduction
Individual students have different prior knowledge and expertise as well as different levels of reading abilities. From the Chapter 7 that we can see that an individual’s perception of text difficulty is likely to be affected by several factors, such as reading skill, prior knowledge, motivation, and arousal or interesting in a given topic. Alternatively, the definition of text difficulty may not be flexible enough to deal with the differences between the L1 and L2 groups as well as within those groups. In this case the problem becomes how to determine a robust method of determining text difficulty. One method is to ask students how difficult text is for them to read. However, this method does not support real time changes, is disruptive to the learner, and people are poor at perceiving their abilities (Kruger & Dunning, 1999). Inexperienced people are unaware of their lack of expertise resulting in them overrating their abilities in comparison to a cohort, whereas accomplished people tend to know their shortcomings and underrate their abilities in comparison to the cohort (Dunning, Johnson, Ehrlinger, & Kruger, 2003; Ehrlinger, Johnson, Banner, Dunning, & Kruger, 2008; Kruger & Dunning, 1999). In Chapter 3 we found that this can be somewhat mitigated by presentation method, consequently students’ perceptions of difficulty cannot be relied upon to gain insight into the level of difficulty of learning material. The use of physical and physiological data can be used to predict cognitive load (Rosch & Vogel-Walcutt, 2013) which in turn can be used to dynamically change an eLearning environment in real time (Coyne et al., 2009). In particular, eye tracking has been used to measure cognitive load during reading, where longer reading times indicate greater cognitive load (Rosch & Vogel-Walcutt, 2013), which is consistent with the finding that eye movements reflect comprehension processes (Rayner et al., 2006).
The main goal of this chapter is to investigate ways of providing feedback regarding text difficulty based upon eye gaze data, similarly to how answer-seeking behaviour was used to provide feedback in Chapter 4. This chapter’s approach is similar in that eye gaze measures will be clustered and analysed to provide feedback about how individual students read as well as how groups of students read certain texts, in order to provide a measure of difficulty derived from reading behaviour. This follows on from the results from Chapters 6 and 7, which raised the important question of whether the way in which text difficulty has been defined in this thesis is in fact suitable. Therefore, the research question of the chapter is:
Can eye gaze data be used to differentiate between L1 and L2 readers and to determine derived difficulty of text?
The importance of differentiating between L1 and L2 readers comes from the results from Chapter 7 that the two groups have different perceptions of text difficulty and therefore different measures of text difficulty. Additionally, the two groups are known to have different eye gaze behaviours during reading. Given that we propose using eye gaze to measure derived difficulty, it is imperative that the two are differentiated before calculating measures of derived difficulty. This also
144
Deriving text difficulty from eye gaze
 provides us with the opportunity to further investigate the differences in eye gaze and reading behaviour between L1 and L2 readers.
The potential of measuring derived text difficulty is that in an adaptive environment the text could be tailored to students’ needs and reading behaviours. It is not just important to identify that participants understand text but it is also crucial to know the level of difficulty at which they are either understanding, or not understanding, so that the content can be changed accordingly. This is touched upon in Chapter 4 where we propose the measure of answer-seeking behaviour to identify how difficult text and the related comprehension questions are, and where a ranking of how hard the participant found the questions is provided. The work in this chapter differs in that we attempt to measure text difficulty without using questions or asking students to state how difficult they found the text using a rating system. We hypothesise that for simpler texts there will be a spectrum of eye movements where L1 and L2 readers are not easy to differentiate. However, as the text gets more difficult, clusters of the L1 and L2 readers’ eye movements will become more distinct. Additionally, we hypothesise that each text will induce different average eye movement measures that can be used to find the average reading behaviour of readers of that text to then use as a derived measure of text difficulty.
This chapter does not include a background section as the literature has been covered in previous chapters. The rest of the chapter is organised to firstly cover the analysis of differentiating L1 and L2 readers; then to investigate how clustering of eye movement data can be used to provide a measure of text complexity; finally these result will be discussed in relation to their implications for adaptive eLearning.
8.2 Method
The eye tracking data used in this analysis was recorded in the user study conducted in Chapter 6; refer to section 6.3 for further details. The analysis in this chapter is primarily through k-means clustering of the data using Matlab R2016a. The first part of the investigation looks at clustering the eye tracking data recorded from three texts, A, E, and J, to see if there are natural clustering between L1 and L2 readers and thus distinct reading behaviours. The second part of the analysis looks at the use of eye movement data in rating the texts on derived difficulty.
We used the silhouette method to evaluate the quality of the clusters. Using Matlab’s evalclusters() function we found the optimal number of clusters for the given data set. We then used the optimal number to cluster the data using k-means clustering. After this the average silhouette width for the total data set was calculated and reported. The average silhouette width provides an evaluation of the clusters to support the choice in number of clusters, where the close to 1 the average silhouette width, the stronger the clustering structure should be (Rousseeuw, 1987).
145
Deriving text difficulty from eye gaze
 8.2.1 Eyemovementmeasures
The eye gaze measures that we analyse in this section are the same as used throughout this thesis: normalised number of fixations (NNF), maximum fixation duration (MFD), average fixation duration (AFD), normalised total fixation duration (NTFD), regression ratio, and average forward saccade length (AFSL). Refer to Chapter 6 for more details on these measures.
8.3 Differentiating L1 and L2 readers
Throughout the thesis there has been an assumption that L1 and L2 readers are distinct. However, many of the analyses have shown that a difference does not exist. An example of this is that there is no difference in answer-seeking behaviour between the L1 and L2 groups. In other cases, differences exist and are statistically significant; however, just because the groups are statistically different does not imply that they do not have some overlap. That is, there may be some L2 readers that are similar to L1 readers and some L1 readers that are similar to L2 readers.
In this section, cluster analysis is used to investigate if there are distinct clusters of eye movements between the L1 and L2 readers. Clustering of the eye movement data from three texts A, E, and J is performed. We hypothesise that for the simplest text, A, instead of having distinct clusters, a spectrum of eye movements will be observed. However, as the text gets more difficult, as in texts E and J, the clusters will become more distinct as the differences between the L1 and L2 participants grow.
8.3.1 EasyText(A)
The eye gaze data recorded from reading text A is clustered in this section. The optimal number of clusters was determined to be 2, see Table 8.1, where the largest average silhouette width is 0.790, which is for 2 clusters.
Table 8.1. Average silhouette widths for clustering of A
      Number of clusters 2 3 4 5 6 7 8 9 10
Ave. silhouette width 0.790 0.732 0.741 0.735 0.691 0.680 0.695 0.697 0.695
        146
Deriving text difficulty from eye gaze
 To assess whether the clustering has separated the L1 and L2 readers, we will examine the contents of the clusters. The average measures for the two clusters are shown in Table 8.2. What we observe is that there is not a clear distinction between the L1 and L2 readers’ data points, for the simplest text, A. However, in saying this, the majority of the L2 data points 85% (61 of 72 points) are in cluster 1. This cluster appears to be a clustering of what we can consider as more thorough reading compared to cluster 2. This can be concluded from the higher NNF, longer MDF, AFD, and NTFD, and shorter forward saccades in cluster 1 compared to cluster 2.
Table 8.2. Eye movement averages from clusters for text A
Measure
Number of L1 readers
Number of L2 readers
Total number in cluster
Mean normalised number of fixations (NNF) Mean maximum fixation duration (MFD)
Mean average fixation duration (AFD)
Mean normalised total fixation duration (NTFD) Mean regression ratio
Mean average forward saccade length
Mean comprehension score
Mean readability
Mean conceptual difficulty
Cluster 1 86 (59%) 61 (41%) 147
0.81 1.63 s 0.24 s 0.2 0.33 102.08 1.59 1.5 1.45
Cluster 2 48 (81%) 11 (19%) 59
0.46 0.97 s 0.16 s 0.08 0.45 165.7 1.69 1.39 1.39
                            Note that the points within each clusters are note individual participants but the texts that the participants read. So whilst there are 70 participants, each participant read 3 versions of text A totalling 210 texts read, however due to removal of corrupt data 4 of these were removed totalling 206 texts analysed in this section.
There is generally an uneven distribution of fixations on words whilst reading English (Rayner, 1998). The NNF values for normal reading are therefore expected to be less than 1. In fact, Carpenter and Just (1983) found that readers fixate on an average of 67.8% words. The closer the NNF is to 0, the more this indicates skimming or scanning of the text, whereas values above 1 correspond to more fixations than there are words in the paragraph which is indicative of re-reading of some of the text. The mean NNF for cluster 1 is 0.81 which is above the expected value just stated. The mean NNF for cluster 2, however, is considerably lower than the expected value being on 0.49. This would indicate that cluster 1 is a clustering of reading behaviour that is above average reading behaviour and cluster 2 is a clustering of reading behaviour that is well below the average reading behaviour.
The majority of the L2 data points lie within cluster 1, and the L2 data points make up almost half of the cluster. We conclude that the majority of L2 participants were reading above the average reading behaviour. However, the majority of the L1 data points are also in this cluster, so the behaviour of above average reading is not unique to the L2 readers. Therefore, we can also conclude the L1 readers are more likely to skim, or have well below average reading behaviour, compared to the L2 readers, given that 81% of the data points in cluster 2 come from L1 readers.
147
Deriving text difficulty from eye gaze
 Finally, the clustering did not find any distinct differences between the participant’s ratings of readability and conceptual difficulty, which are all, on average, between easy/basic and moderate/intermediate. There are no differences in the comprehension scores between the clusters either (t(204)=-1.18, p=0.238). This indicates that the main difference between the clusters is the reading behaviour, which can be described as thorough reading, for cluster 1, and skimming for cluster 2. Moreover, neither type of reading behaviour is characteristic of reading groups, however it is more likely that L1 readers will skim compared to L2 readers.
8.3.2 ModerateText(E)
We now cluster the eye movement measures from text E. The optimal number of clusters was determined to be 3, see Table 8.3, where the largest average silhouette width is 0.799, which is for 3 clusters.
Table 8.3. Average silhouette widths for clustering of E
      Number of clusters 2 3 4 5 6 7 8 9 10
Ave. silhouette width 0.779 0.799 0.773 0.724 0.752 0.650 0.691 0.696 0.718
        The contents of each cluster are shown in Table 8.4, which indicate that there is no specific differentiation between the L1 and L2 data points. However, in saying this, the differences between the L1 and L2 data points has grown. As for text A, cluster 1 is comprised almost equally by L1 and L2 data points. In cluster 2 we can see that the distribution of L1 and L2 points is now skewed towards L1 points, as with text A. Finally, in the extra cluster, 3, only L1 points comprise this cluster. This indicates that whilst there is not a clear distinction between the L1 and L2 data points, there are some differences in their eye movements, and this distinction becomes more prominent when the text increases in difficulty.
Looking deeper into the reading behaviour represented in the clusters, we once again see that cluster 1 is representative of thorough reading, and most L2 readers are part of this cluster, as for text A. The mean NNF for this cluster is 0.91, which means that on average 91% of words were fixated, which is above the standard fixation rate. Combined with increased fixation durations and smaller forward saccade lengths, this is indicative of increased text difficulty, which is observed for text E. Cluster 2 has reduced reading compared to cluster 1, with values that are indicative of average reading behaviour. Finally, cluster 3 is similar to cluster 2 for
148
Deriving text difficulty from eye gaze
 text A, where we can observe skimming behaviour. This cluster is made up solely of L1 data points. This is consistent with what we found for text A, where L1 readers are more likely to be the readers to skim.
Table 8.4. Eye movement averages from clusters for text E
Measure
Number of L1 readers
Number of L2 readers
Total number in cluster
Mean normalised number of fixations (NNF) Mean maximum fixation duration (MFD)
Mean average fixation duration (AFD)
Mean normalised total fixation duration (NTFD) Mean regression ratio
Mean average forward saccade length
Mean comprehension score
Mean readability
Mean conceptual difficulty
Cluster 1 29 (58%) 21 (42%) 50
0.92 1.78 0.26 0.25 0.32 95.4 1.41 1.76 1.92
Cluster 2 21 (75%) 7 (25%) 28
0.71 1.25 0.20 0.14 0.40 140.3 1.5 1.64 1.71
Cluster 3 5 (100%) 0 (0%)
5
0.34 0.73 0.15 0.06 0.47 201.2 1.6
2 2
                                       Again there is little difference is the participants’ ratings of readability and conceptual difficulty, which are still between easy/basic and moderate/intermediate. ANOVA of the comprehension scores shows that there is no significant difference between the clusters (F(2,204)=1.4, p=0.238), even though there seems to be a slight trend is the comprehensions being higher as the clusters go up.
8.3.3 DifficultText(J)
Finally, we cluster of the eye movement measures recorded from reading text J. The optimal number of clusters was determined to be 4, see Table 8.5, where the largest average silhouette width is 0.825, which is for 4 clusters.
The contents of each cluster are shown in Table 8.6. Whilst there are 4 clusters for this text, one of the clusters contains only 1 data point, which is an outlier for the data set. There is, once again, no clear distinction between the L1 and L2 data points, as for the previous texts. In fact, there is an almost even distribution of L2 data points between the 3 other clusters. That is, unlike in the previous sections, there is no cluster (other than cluster 4) that contains only or close to only L1 data points. This indicates that the harder the text gets, the harder it is to differentiate L1 and L2 readers. Perhaps the text becomes too difficult and as a result the L2 readers are unable to cope with the task.
As with the previous texts, the clusters show different reading behaviours that range from thorough reading (cluster 1) to skimming (Clusters 3 and 4). Moving from text A to E there is an increase in the NNF and fixations durations, as well as a
149
Deriving text difficulty from eye gaze
 decrease in forward saccade length, all indicative of more reading. We see that cluster 1, which is representative of the most thorough reading, has an average NNF with a much higher than expected, indicated that words are fixated on more than once. This is the highest average NNF that we observe and since J is the most difficult text to read we would expect more thorough reading compared the A or E.
Table 8.5. Average silhouette widths for clustering of J
      Number of clusters 2 3 4 5 6 7 8 9 10
Ave. silhouette width
   0.776 0.757 0.825 0.822 0.720 0.769 0.683 0.747 0.768
     Table 8.6. Eye movement averages from clusters for text J
                  Measure
Number of L1 readers
Number of L2 readers
Total number in cluster
Mean normalised number of fixations (NNF)
Mean maximum fixation duration (MFD)
Mean average fixation duration (AFD)
Mean normalised total fixation duration (NTFD)
Mean regression ratio
Mean average forward saccade length
Mean comprehension score
Mean readability
Mean conceptual difficulty
Cluster 1 7 (58%) 5 (42%) 12
1.10
2.22 s
0.32 s
0.33 s
0.29
86.7
1.0 1.8 2.3
Cluster 2 7 (70%) 3 (30%) 10
0.94
1.13 s
0.20 s
0.19 s
0.39
114.5
1.2 1.8 2.3
Cluster 3 3 (60%) 2 (40%) 5
0.63
1.05 s
0.16 s
0.1 s
0.42
148.5
1.3 2.2 2
Cluster 4 1 (100%) 0 (0%)
1
0.46
0.58 s
0.15 s
0.07 s 0.53 213.06
2 2 2
                                The remaining clusters represent gradually less reading behaviour, where cluster 2 is similar to cluster 2 from text E, which is still indicative of thorough reading. Cluster 3 is the closest of normal reading behaviour with an average NNF
150
Deriving text difficulty from eye gaze
 of 63 (compared the stated average of 67.8%). However, given that this is the most difficult text to read we would not expect to have many people reading it as a normal text. Finally, the outlier point indicates skimming behaviour. Given that this data point has a high comprehension score (2 out of 2), the skimming behaviour could be due to the fact that the reader had a high level of prior knowledge in the area and therefore did not need to read the text thoroughly.
For the remaining clusters there appears to be little difference in the average comprehension scores, which are on average quite low, and little difference between the other subjective ratings. ANOVA of the comprehension scores, for all of the clusters, shows that there is no significant difference between the clusters (F(3,27)=0.94, p=0.438), even though there seems to be a slight trend is the comprehensions being higher as the clusters go up. Again, there appears to be little difference between the ratings of readability and conceptual difficulty between the clusters.
8.4 Deriving text difficulty from eye gaze
From the first part of the analysis in this chapter, we observe that there are differences in reading behaviours for each text. It is clear from the previous chapter that participants are poor at identifying the predefined difficulty of the text. The results indicate that this could be due to the fact that difficulty is different for everyone, and therefore everyone has different perceptions of difficulty. This is evident given that students have different prior knowledge and expertise, as well as different levels of reading abilities.
Given that eye movements are not a complete reflection of perceptions or predefined text difficulty; we now question whether the predefined definition of text difficulty is suitable for adaptive eLearning? That is, are these definitions flexible enough to deal with the differences between not only the L1 and L2 groups, but also the differences within these groups? Since the eye movements are affected by both perceived and predefined text difficulty, but not completely governed by either, this suggests that eye movements are reflections of the derived difficulty of the text. We suggest that each text will have different average eye movement measures. These can be used to find the average reading behaviour of that text and then use as a measure of text difficulty.
As described in Chapter 6, there are 27 texts used in total for the study; from 3 topics, and each topic containing 9 versions of text, based upon the grid system, labelled A through J. For more information, refer to the Method (section 6.3) in Chapter 6. For each of the 27 texts the average eye movement measures, as described in section 8.2.1, were calculated for the L1 and L2 groups. These measures were clustered using k-means clustering for the L1 and L2 groups separately.
8.4.1 L1derivedtextdifficulty
The optimal number of clusters for the L1 text averages was determined to be 4, see Table 8.7, where the largest average silhouette width is 0.787, which is for 4 clusters.
151
Deriving text difficulty from eye gaze
 The average eye movement measures and outcome measures for each cluster are shown in Table 8.8. There is a spectrum of mean eye movement measures across the clusters indicating that there are different average reading behaviours observed for different texts. Starting with cluster 1, this cluster has the fewest texts within it, but is also the cluster that is associated with thorough reading. For the L1 readers, we can see that they did not find a lot of the texts difficult to read as the majority of the texts are associated average reading behaviours.
Table 8.7. Average silhouette widths for clustering of average eye movement measures for each text
      Number of clusters 2 3 4 5 6 7 8 9 10
Ave. silhouette width 0.670 0.768 0.787 0.724 0.697 0.643 0.628 0.694 0.754
        Table 8.8. Averages of measures for each clusters for L1 readers, based on text averages
                  Measures
Total number in cluster Mean normalised number of fixations (NNF)
Mean maximum fixation duration (MFD)
Mean average fixation duration (AFD)
Mean normalised total fixation duration (NTFD) Mean regression ratio
Mean average forward saccade length
Mean comprehension score Mean perceived readability Mean perceived conceptual difficulty
Cluster 1 3
0.95
1.34
0.22
0.22
0.33 105.7 1.1
1.6 2.2
Cluster 2 10
0.77
1.25
0.20
0.17
0.37 126.1 1.3
1.6 1.8
Cluster 3 7
0.75
1.35
0.21
0.16
0.37 116.4 1.6
1.5 1.6
Cluster 4 7
0.65
1.08
0.18
0.12
0.39 138.0 1.7
1.5 1.5
                                There does not appear to be a large difference between clusters 2 and 3, which are indicative of reading behaviour that is slightly above the average. Cluster 2 has slightly short fixation durations, but more fixations and therefore longer total
152
Deriving text difficulty from eye gaze
 fixation duration, and longer forward saccades, compared to cluster 3. This would seem to indicate that the main difference between the two clusters is that the texts in Cluster 3 have more concentrated reading compared to cluster 2, which is why we see longer fixations and shorted forward saccades. Cluster 2 has the most texts within it and given the nature of the eye movements this is most likely the normal reading behaviour of participants for this set of texts.
Finally, cluster 4 contains 7 texts which have reading behaviour that is diminished compared to the other 3 clusters. Whilst we cannot describe the average reading behaviour as explicitly skimming, the reading behaviour is below the expected level. Therefore, the texts in this cluster are easier to read.
We use MANOVA to determine if there are any statistical differences between the clusters. The correlations between the dependent variables are within the acceptable limits for MANOVA outcomes, i.e. the correlations lie between r=-0.4 and r=0.9. To test for normality in the dependent variables the Shapiro-Wilk Test is used, as it is more appropriate for small sample sizes. All variables are normally distributed (p>0.05). Levene’s test for equality of variances shows that there is homogeneity for all dependent variables (p>0.05) Finally, the homogeneity of variance-variance-covariance matrices is satisfied as the Box's M value of 111.46 (p=0.038>0.001).
There is a statistically significant difference in average eye movement measures between the clusters, F(18,51)=7.42, p<0.0005; Wilk's λ=0.530, partial η2=0.701. ANOVA shows that the clusters have a statistically significant effect on all measures, NNF (F(3,26)=3.81; p=0.024; partial η2=0.332), MFD (F(3,26)=4.55; p=0.012; partial η2=0.372), AFD (F(3,26)=9.75; p<0.0005; partial η2=0.560), NTFD (F(3,26)=7.33; p=0.001; partial η2=0.489), regression ratio (F(3,26)=7.63; p=0.001; partial η2=0.499), and AFSL (F(3,26)=129.7; p<0.0005; partial η2=0.944). This is not surprising given that the eye movement measures were used to create the clusters. However, this does indicate that we can use this eye movement measures to rank these texts into distinctive groups based on reading behaviour.
Perhaps more informative is an analysis of how the clusters are related to predefined readability and conceptual difficulty as well as the resulting measures of comprehension and perceived readability and conceptual difficulty. Firstly, Chi- square test for independence shows that there is no evidence of relationship between clusters and predefined readability (c2(6)=3.685, p=0.719) and predefined conceptual difficulty (c2(6)=7.371, p=0.287). The clusters, and therefore reading behaviours, are not related to the predefined readability or conceptual difficulty. This is what we hypothesised, and expect based on previous analysis in this thesis.
Considering now the comprehension scores and perceived readability and conceptual difficulty MANOVA is used. The correlations between the dependent variables are within the acceptable limits for MANOVA outcomes, i.e. the correlations lie between r=-0.4 and r=0.9. To test for normality in the dependent variables the Shapiro-Wilk Test is used, and all variables are normally distributed (p>0.05). There is a statistically significant difference in average resulting measures
153
Deriving text difficulty from eye gaze
 for each text between the clusters, F(9,51)=2.20, p=0.037; Wilk's λ=0.530, partial η2=0.233. Interestingly, the differences lie in the comprehension scores (F(3,23)=3.556; p=0.03; partial η2=0.317) and the perceived conceptual complexity (F(3,23)=4.01; p=0.012; partial η2=0.373), but not on the perceived readability (F(3,23)=0.336; p=0. 799; partial η2=0.042). Whilst the perceived conceptual complexity appears to be associated with the clustering of eye movements, there is no relationship to the predefined levels of complexity.
Table 8.9. Texts within each cluster, for L1 averages for text
Cluster       Characteristic reading     Texts in cluster behaviour
1       Thorough     T1-C, T3-F, T1-J
     2 3 4
Average
Average, more concentrated Below average
T1-A, T2-A, T3-A, T1-E, T1-F, T2-G, T1-H, T2-H, T3-H, T2-J
T2-B, T2-C, T1-D, T2-D, T3-D, T3-E, T3-J
T1-B, T3-B, T3-C, T2-E, T2-F, T1-G, T3-G
        NOTE: T1 refers to topic 1, T2 refer to topic 2, and T3 refers to topic 3.
Table 8.9 shows the texts that are within each cluster. The clusters give a measure for the average reading behaviour observed for the text and can be used as feedback to the author or designer of eLearning material to obtain the derived difficulty of the text. That is, texts with low levels of reading are simpler to read, also have less perceived conceptual difficulty, and therefore less thorough reading is observed. We can see that for cluster 1, the text associated with the most thorough reading behaviour on average, the all of these texts have the highest concept difficulty. Yet these texts are only a subset of all texts with the same level of conceptual difficulty, and these texts all have different levels of readability. In this way, the clustering may be surprising to the author as the reading behaviours for the texts are not associated in the ways we would expect to the predefined readability and conceptual difficulty. Since it has been shown that as text becomes more difficult to read, eye movements are seen to reflect the difficulty. This implies that the predefined difficulties are not entirely associated with the reading difficulty.
8.4.2 L2derivedtextdifficulty
We now consider the L2 averages for each text. The optimal number of clusters for the L2 text averages was determined to be 2, see Table 8.10, where the largest average silhouette width is 0.991, which is for 2 clusters. However, when we inspect the clusters further this clustering results in an outlier text in its own cluster and the rest of the texts clustered together. For this reason, we move to using 3 clusters to describe the texts, as the average silhouette width for 3 clusters is 0.802, which is indeed higher than the average silhouette width for the optimal number of clusters for the L1 text averages.
154
Deriving text difficulty from eye gaze
 Table 8.10. Average silhouette widths for clustering of average eye movement measures for each text
      Number of clusters 2 3 4 5 6 7 8 9 10
Ave. silhouette width 0.991 0.802 0.621 0.613 0.737 0.726 0.687 0.638 0.517
        Table 8.11 shows the average eye movement measures for the texts within the 3 clusters. Examining the contents of the clusters for the L2 averages for the texts shows that the outlying text in a cluster of its own is a text that on average the eye movements that signify skimming behaviour. That is, L2 participants only seemed to skim one text, rather than the 7 texts that the L1 participants are seen to skim. The text that the L2 participants skim is unexpected; instead of being a text with easy readability and easy conceptual difficulty (e.g. text A) it is text H (from Topic 3), which has difficult readability and intermediate conceptual difficulty, therefore being one of the most difficult tasks to read. This text also does not correspond with the texts that the L1 readers had below average reading behaviour for.
Table 8.11. Averages of measures for each clusters for L1 readers, based on text averages
                     Total number in cluster
Mean normalised number of fixations (NNF) Mean maximum fixation duration (MFD)
Mean average fixation duration (AFD)
Mean normalised total fixation duration (NTFD) Mean regression ratio
Mean average forward saccade length
Mean comprehension score
Mean perceived readability
Mean perceived complexity
Mean COH-Metrix L2 readability
Cluster 1 1
0.60 0.87 0.17 0.10 0.44
212.7 1.0 0.0 1.0 11.3
Cluster 2 9
0.85 1.55 0.24 0.22 0.36
112.1 1.2 2.3 2.2 7.6
Cluster 3 17
0.91 1.92 0.29 0.28 0.33 99.3
1.3 2.5 2.3 9.0
                  The remaining two clusters show that L2 participants
thorough reading behaviour, on average, compared to the L1 participants. We see that cluster 3 has the most texts within it and yet this cluster has similar eye movement averages to the thorough reading cluster for the L1 participants. Cluster
do indeed have more
155
Deriving text difficulty from eye gaze
 2 has the remaining 9 texts within it with the average eye movement measures being less than those in cluster 3 but still above the normal level we would expect.
Table 8.12. Texts within each cluster, for L1 averages for text
Cluster       Characteristic reading     Texts in cluster behaviour
       1 2
3
Outlier – below average       T3-H
Average/thorough Thorough
T1-A, T1-B, T2-B, T3-B, T3-A, T1-E, T1-G, T1-H, T2-F, T2-J
T2-A, T1-C, T2-C, T3-C, T1-D, T2-D, T3-D, T2-E, T3-E, T3-F, T1-F, T2-G, T3-G, T2-H, T1-J, T3-J
        NOTE: T1 refers to topic 1, T2 refer to topic 2, and T3 refers to topic 3.
As above, we use MANOVA to determine if there are any statistical differences between the clusters. The correlations between the dependent variables are within the acceptable limits for MANOVA outcomes, i.e. the correlations lie between r=-0.4 and r=0.9. To test for normality in the dependent variables the Shapiro-Wilk Test is used, as it is more appropriate for small sample sizes. All variables are normally distributed (p>0.05). Levene’s test for equality of variances shows that there is homogeneity for all dependent variables (p>0.05). Finally, the homogeneity of variance-variance-covariance matrices is satisfied as the Box's M value of 43.5 (p=0.114>0.001).
There is a statistically significant difference in average eye movement measures between the clusters, F(12,38)=21.208, p<0.0005; Wilk's λ=0.017, partial η2=0.870. ANOVA shows that the clusters have a statistically significant effect on all measures except NNF; NNF (F(2,26)=1.795; p=0.188; partial η2=0.130), MFD (F(2,26)=6.07; p=0.007; partial η2=0.336), AFD (F(2,26)=4.43; p=0.023; partial η2=0.270), NTFD (F(2,26)=3.66; p=0.041; partial η2=0.234), regression ratio (F(2,26)=13.64; p<0.0005; partial η2=0.532), and AFSL (F(2,26)=403.6; p<0.0005; partial η2=0.971). This time the clusters do not completely differ statistically, as compared to clustering from the L1 participants eye movements. This indicates that the NNF is not a good measure for predicting reading difficulty as the values must not vary enough between the texts. However, as above, it is not surprising that there are significant differences between the clusters given that the eye movement measures were used to create the clusters. This does show that we can use measures such as fixation duration and forward saccade length to classify the texts based on their derived difficulty.
Analysis of how the clusters are related to predefined readability and conceptual difficulty as well as the resulting measures of comprehension and perceived readability and conceptual difficulty. Firstly, Chi-square test for independence shows that there is no evidence of relationship between clusters and predefined readability (c2(4)=3.13, p=0.535) and predefined conceptual difficulty (c2(4)=5.88, p=0.208). As with the L1 averages, the clusters, and therefore reading behaviours, are not related to the predefined readability or conceptual difficulty.
156
Deriving text difficulty from eye gaze
 Considering now the comprehension scores and perceived readability and conceptual difficulty MANOVA is used. The correlations between the dependent variables are within the acceptable limits for MANOVA outcomes, i.e. the correlations lie between r=-0.4 and r=0.9. To test for normality in the dependent variables the Shapiro-Wilk Test is used, as it is more appropriate for small sample sizes. All variables are normally distributed (p>0.05). Levene’s test for equality of variances shows that there is homogeneity for all dependent variables (p>0.05). Finally, the homogeneity of variance-variance-covariance matrices is satisfied as the Box's M value of 11.78 (p=0.131>0.001).
MANOVA shows that there is a statistically significant difference in average resulting measures for each text between the clusters, F(6,44)=2.26, p=0.001; Wilk's λ=0.386, partial η2=0.378. In contrast to the analysis on the L1 text clusters, the difference lies in the perceived readability (F(2,26)=5.85; p=0.009; partial η2=0.328) and not in the comprehension scores (F(3,26)=0.842; p=0.443; partial η2=0.066) or the perceived conceptual complexity (F(3,26)=1.11; p=0.345; partial η2=0.345). Whilst the perceived conceptual readability appears to be associated with the clustering of eye movements, there is no relationship to the predefined levels of readability. This finding implies that readability of the text has a greater effect on L2 readers’ eye movement than on L1 readers’, and conversely, conceptual difficulty has a great effect on L1 readers’ eye movements than L2 readers’.
Interestingly, there are correlations between the predefined conceptual difficulty and the L2 perceived readability (r=0.5, p=0.017) and L2 perceived conceptual difficulty (r=0.6, p<0.0005). In both cases, as the predefined conceptual difficulty gets harder, the L2 participants perceptions of both readability and conceptual difficulty get higher. However, the predefined readability has no significant correlations to either the perceived readability or the perceived conceptual difficulty. An interesting correlation is between the perceived conceptual difficulty and the perceived readability (r=0.8, p<0.0005). This implies that the two have a strong relationship, and even though there is no significant difference in perceived conceptual complexity between clusters, overall the two perceptions are related.
Finally, we investigate the readability of the texts further as this is an important factor on L2 readers’ eye movements. The COH-Metrix L2 Readability Index is designed to rate the readability of text for L2 readers. We introduced this index in Chapter 6, but now investigate how these values differ in the clusters. The properties for each text were generated using COH-Metrix 3.0 (McNamara et al., 2013). It has been shown that the L2 readability index is more appropriate for describing the readability of texts for L2 readers (Crossley et al., 2008). However, for these texts it is not a consistent indicator for the derived difficulty based upon the clustering of eye movements, as there is no significant difference in the L2 indices between clusters 2 and 3 (t(24)=-0.7495, p=0.460).
There is a correlation (r=-0.6, p=0.002) between the L2 readability indices and the Flesch-Kincaid grade level for each text, so the two are somewhat related even though the L2 Readability Indices take into consideration more than the lexical structure of the text. There is also a correlation between the L2 readability indices
157
Deriving text difficulty from eye gaze
 and predefined conceptual difficulty (r=-0.7, p<0.0005), however, there is no significant correlation to the predefined readability. This implies that the L2 readability indices are more related to the predefined conceptual difficulty than readability. However, when we consider the perceived variables of the text we see that the L2 readability indices have no significant correlation to the perceived readability and conceptual difficulty. This indicates that the use of the eye movements to calculate the derived text difficulty is useful as it takes into consideration more than the superficial nature of the text and is versatile for dealing with both L1 and L2 readers.
There are, however, strong correlations between the Flesch-Kincaid grade level and the perceived readability (r=0.5, p=0.008) and perceived conceptual difficulty (r=0.7, p<0.0005). As the Flesch-Kincaid grade level goes up so too does the participants perceptions of readability and conceptual difficulty. This implies that the Flesch-Kincaid grade level is perhaps still useful for assessing readability for L2 readers, as there are a relationship between this measure and the perceptions of readability and conceptual difficulty.
8.5 Discussion and Implications
The goal of the chapter was to investigate the clustering of eye movement measures to provide feedback based upon reading behaviour. The purpose was to first investigate the distinction between L1 and L2 groups as well as reading behaviours of participants for different texts based upon eye movements. Leading on from this, the average eye movement measures for each text were clustered to rate each text’s derived difficulty. This provides feedback about how texts are read by the cohort to the author of the text.
Not all readers have the same reading skills, whether they are L1 or L2 readers. There can be variance within each group, not just between the groups. Some L2 readers may actually behave like L1 readers because they have been reading the language for so long. For this reason, we hypothesised that for the simplest text, A, there would a spectrum of eye movements and no clear distinction between the L1 and L2 groups. This was indeed what we observed. However, while there was no clear distinction between the two groups there are trends in the reading behaviours where the majority of the L2 readers tended to read thoroughly. However, there are many L1 readers that also read thoroughly, there is just a lower proportion of L1 readers that read thoroughly.
We further hypothesised that, as the text became more difficult, the clusters would become more distinct as the reading differences between the L1 and L2 participants should grow. This was not validated, and the distinctions between L1 and L2 readers declined as complexity increased. However, similar patterns in reading behaviour were observed, in that there are participants who read more thoroughly than others. So whilst the analysis did not reveal natural clusters between L1 and L2 readers, it did reveal that there are differences in reading behaviours. Furthermore, L2 readers are likely to read more thoroughly than L1 readers in easy to moderate texts, but not necessarily when the text is very difficult.
158
Deriving text difficulty from eye gaze
 As text difficulty increases there is an observable increase in thoroughness of reading behaviour and time spent looking at the text, as we would expect. However, we only examined three texts in the first half of the analysis; the three that we know from Chapter 6 to have demonstrable differences in eye movements and from Chapter 7 to have perceptions more aligned to the predefined difficulty. Yet we saw from the analysis of the other texts that the expected differences in eye movements did not exist. Additionally, the perceptions of texts that lay outside of the main diagonal (A, E, and J) were poorest. This raises the question of whether the degree of difficulty assigned to the text based on conceptual difficulty and readability actually reflects of the true difficulty.
The problem becomes how to identify a robust method of determining text difficulty. Asking students to rate texts on difficulty is one method of obtaining a rating of the perceived text difficulty. However, this requires explicitly asking students to rate the texts on difficulty, which is time consuming and inconvenient to students. Calculating the difficulty of text based on students’ behaviour, as measured by physiological and physical responses, would solve this problem. Eye gaze measures have been successfully used to indicate cognitive load (Rosch & Vogel-Walcutt, 2013). The proposition is that cognitive load of the learner should be neither too high nor too low, as this degrades learning outcomes (Paas et al., 2004). Additionally, eye gaze measures have been correlated with task complexity in visual tasks such as navigation (Waniek & Ewald, 2008) and search (Crosby et al., 2001). Coupling with the aforementioned effects of the predefined and perceived text difficulty on eye gaze measures, the use of eye gaze measures to calculate derived text difficulty is appealing.
We propose the use of eye gaze measures to calculate the derived difficulty of text for the purpose of providing feedback to the author of the text. This is similar to the use of answer-seeking behaviour to provide feedback regarding the derived difficulty of questions, discussed in Chapter 4. The categorisation of text from students’ perceptions and eye gaze reveals that the students did not reflect the expected difficulties of the texts, on average. This supports the hypothesis that eye gaze measures can be used to provide a more accurate rating system for the derived reading difficulty of the text. Furthermore, there are differences between the L1 and L2 readers that make it necessary to categorise the texts for the two groups separately.
It is clear from this chapter, and the previous, that not all students neither rate nor perceive the text in the same way, even within their language groups. Students have different prior knowledge and expertise as well as different levels of reading abilities. The true power of this method of finding the derived difficulty is to be able to individually determine how difficult a student finds a text. In an adaptive environment this provides a wealth of information about the student as well as the materials. Additionally, in accordance with the cognitive load theory for the design of eLearning materials, it also provides the ability to deliver the correct level of difficulty to students, thus personalising the learning path.
159
Deriving text difficulty from eye gaze
 8.6 Conclusion and Further Work
In this chapter we first investigated the differences between L1 and L2 readers using eye gaze measures. This revealed that there is no clear distinction between the two groups. L2 readers are more likely to read normally or thoroughly than to skim text. L1 readers have a broad spectrum of reading behaviours, and so reading behaviour does not distinguish the groups. Furthermore, we used cluster analysis of eye gaze measures to assess the derived reading difficulty of text. This is a useful tool for authors of eLearning materials and also allows consideration of individual differences between students. Given that we found that there are substantial differences between readers even within the same language group, it is necessary for any adaptive eLearning system to account for this and ensure that the appropriate level of difficulty of text is shown to the student. Therefore, the contributions of this chapter are a necessary part of the design of any adaptive text based eLearning.
Deriving text difficulty from average reading behaviour is useful for providing more information to the author about the derived difficulty of the text and for adapting a learning environment on a cohort basis. Leading on from this point is the idea of individually detecting a student’s reading behaviour and adapting the system to their reading ability. That is, using the individual’s eye movements to derive how difficult he or she finds the text to read. Whilst this was not investigated in this chapter it is an area of further research as the implications are important for adaptive eLearning.
However, the participants were all sourced as computer science students and the topic of the texts is a computer science topic. Changing the content matter to be something completely different from what they are used to, (such as biology or chemistry), would increase the difficulty even more. In this case would the eye gaze measures observed be different from what was observed in this study? Furthermore, in this situation would L1 readers exhibit reading behaviour that is more similar to the L2’s reading behaviour? Further work should also be carried out to observe the effects of dynamically assessing the text difficulty whilst monitoring their cognitive load as well as reading comprehension.
160
Chapter 9
Chapter 9. Discussion and Implications
Increasingly students are turning to online resources; however, the one-size-fits-all approach predominantly used in this medium is not effective for catering to the needs of all students. There has been much work on effective ways of presenting learning materials in learning environments (Clark & Mayer, 2011). Eye tracking is a useful method of investigating the reading process (Rayner, 1998) as well as cognitive load (Rosch & Vogel-Walcutt, 2013). In this chapter we look back at the studies and results presented throughout this thesis and tie them together to finally discuss the main research question of the thesis:
Can eye tracking be used to make eLearning environments more effective for first and second language English readers?
To do so, this chapter is divided into 2 sections; the first discusses how the results compare to the current literature, the second discusses application into an eLearning environment. To discuss the application, we propose the architecture of an eLearning environment that provides dynamic text selection and presentation based on eye movements. The students’ eye gaze would be used to predict their comprehension level and the text difficulty altered to reflect this. This can be used to influence how students interact with the learning environment as well as how they learn the material, streamlining the learning process and optimising learning outcomes. The latter half of the discussion is based on work presented at IHCI 2014 (Copeland, Gedeon, & Caldwell, 2014) and throughout this thesis in the implications parts of the chapters.
161
Discussion and Implications
 9.1 Eye tracking in eLearning
The two propositions behind the research question are firstly, educational materials are being offered through online and electronic media more frequently. Universities are now frequently offering online and / or off-campus courses and degrees where students have little or no face-to-face interaction with their instructors or other students. The need for additional forms of student monitoring are necessary to detect when a student is under or over-performing so that they can either be given remedial help or advanced material. Even for university courses that deliver educational material traditionally, absenteeism from lectures is more prevalent and has been shown to negatively affect learning (Romer, 1993; Woodfield et al., 2006). However, the use of online learning can actually be beneficial for dealing with not only this problem, but also the problems encountered with large class sizes and dispersed students by providing consistency and accessibility in delivered materials (Welsh et al., 2003).
Secondly, online eLearning extends teaching and learning from the classroom to a wide and varied audience that has different needs, backgrounds, and motivations. Yet eLearning for the most part is one-size-fits-all. For these reasons there is a growing importance in designing effective eLearning materials that take these differences into consideration. One way of achieving this is by developing personalised education that is adaptive to students’ individual needs. We focus on analysis of text materials and the comparison of first (L1) and second (L2) English language readers, as students with different language backgrounds are an increasing diversity in audiences of online learning materials.
We discussed in the literature review (Chapter 2) that adaption can be provided through various methods. The use of physiological and physical responses allows for real time adaption based upon cognitive load (Rosch & Vogel-Walcutt, 2013). Eye tracking in particular is becoming more precise, less expensive, and is not invasive or obtrusive for the student. This offers the possibility of using eye tracking as a common input to computer systems, and thus a potentially effective way of providing adaptive eLearning. Indeed, the use of eye tracking in adaptive eLearning is not new and has been shown to provide benefit in learning (Barrios et al., 2004; Calvi et al., 2008; D'Mello et al., 2012; Gütl et al., 2005; Mehigan et al., 2011; Porta, 2008). In particular, eye tracking has a long history of being used to analyse reading behaviour (Rayner, 1998). Furthermore, eye tracking is especially useful at analysing the implicit differences between different types of readers, such as linguistic background (Dednam et al., 2014; Kang, 2014).
The user studies presented in this thesis utilised eye-tracking technology to investigate how participants interact with an online eLearning environment, Wattle17 (a Moodle18 variant). L1 and L2 English language participants were sourced in order to investigate the differences between groups under different scenarios. Whilst it is known that L1 and L2 readers have different eye movements and
17 https://wattle.anu.edu.au/ Last accessed: 22nd January 2016 18 https://moodle.com/hq/ Last accessed: 22nd January 2016
 162
Discussion and Implications
 reading behaviours (Dednam et al., 2014; Kang, 2014), there are several areas that had not been investigated. One of these is whether the two groups interact with eLearning environments in the same way, such as, how they answer questions in a tutorial, as investigated in Chapters 3 and 4. L2 readers take longer to read but perform at the same level when the materials are targeted to suit the right level of education. This result is expected given related research (Kang, 2014).
However, in Chapters 6 and 7 we see that once text is made more difficult, L2 readers perform worse than L1 readers in comprehension. However, the differences in eye movements between the different texts, and between the reader groups, were not as we expected. This warranted further investigation of how the eye movements were affected. In Chapter 7 the perceptions of text difficulty were investigated. L1 and L2 readers have different perceptions of text difficulty where L2 readers tend to overestimate the difficulty of a text and L1 readers underestimate difficulty. Neither group were good at perceiving difficulty. Using eye tracking data, better predictions of both the readability and conceptual difficulty of the text were achieved compared to participants’ perceptions. Further analysis of participants perceptions indicates that both groups tend to conflate the levels of readability and conceptual difficulty. This results in both groups overestimating texts with the same levels of readability and conceptual difficulty and underestimating the other texts, especially when the readability is notably higher than the conceptual level and vice versa.
This raises the question of whether the predefined measure of text difficulty is adequate. Throughout this thesis we have rated readability using the Flesch-Kincaid grade level (Kincaid et al., 1975). However, this measure only deals with the surface properties of the text, not accounting for the content problems and is generally aimed at English text for L1 readers, not L2 readers (Zhang et al., 2013). Yet it is important that text features be considered differently for L1 and L2 readers since they have differential effects on reader type (Zhang et al., 2013). The analysis in Chapter 7 showed that the intended text difficulty might not be perceived this way. Perceptions are powerful predictors of learning outcomes (Lizzio et al., 2002) and alleviate anxieties about learning (Chang, 2005). However, it has been shown that people are poor at assessing their own skills (Kruger & Dunning, 1999) so the perceptions of students cannot be relied upon to measure the implicit difficulty of a text.
Instead, the text difficulty could be predicting by the cognitive load of the reader when reading that text, which can be determined using eye gaze (Rosch & Vogel- Walcutt, 2013). Eye gaze measures have been correlated with task complexity (Crosby et al., 2001; Waniek & Ewald, 2008). In Chapter 8, clustering of average eye movement measures per text showed that texts have significantly different average reading behaviours. Some texts are associated with low levels of reading behaviour (skimming) whereas others require higher levels of reading. Importantly though, the predefined text difficulty did not guarantee the amount of reading, so the results may surprise the author of the text and be helpful in creating and classifying text for eLearning. Additionally, the clustering of average eye movements from the text is different for L1 and L2 readers.
163
Discussion and Implications
 Whilst investigating eye movements for this use we also examined further the differences between L1 and L2 readers, exploring whether there are discernible differences between the two groups. Whilst the analyses in this thesis have shown that there are differences between L1 and L2 readers in eye movements, there are some notable similarities. The comprehension and eye movements from the two groups are affected equally by the presentation of text and comprehension questions (see Chapter 3). Furthermore, there is no difference between L1 and L2 readers in their answer-seeking behaviour (Chapter 4). Given these results, it is perhaps unsurprising that the results from Chapter 8 show that there is no clear distinction in eye movements between L1 and L2 readers. What we observe is instead a spectrum of eye movements that range from thorough reading to skimming reading. L2 readers are more likely to be amongst the readers who read thoroughly when the text is easy to moderate in complexity. However, there are many L1 readers who read thoroughly so this is not a discriminating factor. When the text gets really difficult to read and understand, the differences between the L1 and L2 readers became less clear and L2 readers tended to revert to normal reading rather than thorough reading. The L2 readers tend to not deal with the difficulty as well as the L1 readers who instead switch to more thorough reading when the difficulty is notably increased.
Differences between the reader groups such as this are important to take into consideration when designing learning materials for students. It is also important when deciding what texts should be given to students in an adaptive eLearning environment. Indeed, what is appropriate for an L1 reader may not be appropriate for an L2 reader and vice versa. This leads back to the proposition that eLearning environments can be adapted to the learner. It has been shown that adaptive learning environments result in significant improvement in learning outcomes compared to no adaption (Dingli & Cachia, 2014; Lach, 2013; Paramythis & Loidl- Reisinger, 2003).
Learning environment adaption can be based on different qualities of the learner such as the current understanding, emotional state such as stress (Calvi et al., 2008; Porta, 2008), learner style (Mehigan et al., 2011; Spada et al., 2008; Surjono, 2014), cognitive load (Coyne et al., 2009), and skill level (Chen, 2008). Methods of determining adaption, i.e. learner style or emotional state, also vary from using questionnaires (Surjono, 2011) to the use of biometric technology (Mehigan et al., 2011; Spada et al., 2008) and physical and physiological response data (Rosch & Vogel-Walcutt, 2013), especially eye tracking (Barrios et al., 2004; Calvi et al., 2008; D'Mello et al., 2012; Gütl et al., 2005; Mehigan et al., 2011; Porta, 2008). There are a broad range of scenarios that these adaptive technologies are directed at helping students, such as plugging into traditional online learning environments (Barrios et al., 2004; De Bra et al., 2013), or providing adaption in mobile environments (Mehigan & Pitt, 2013), or accounting for dyslexia (Alsobhi et al., 2015) and foreign language reading (Hyrskykari et al., 2000).
Building on all of this past research we are able to take the results from the studies presented in this thesis and add to the current knowledge base of adaptive eLearning. The contribution is solely in the domain of text-based learning materials.
164
Discussion and Implications
 Eye tracking can certainly be used to make learning via reading more effective in the context of eLearning.
In Chapter 3 we made the observation that different presentation sequences of text and comprehension questions affect performance outcomes and eye movements of participants. The order in which text and assessment questions are presented to students can therefore be manipulated to optimize performance outcomes and / or reading behaviour. That is, the sequence in which you present information and then assess it can have a large bearing on students’ reading behaviour, learning performance, and perceived performance. In particular, making students rely on memory to answer comprehension questions promotes more accurate subjective ratings of understanding.
One of the major questions we investigate in this thesis is whether eye gaze can be used to predict reading comprehension measures in eLearning environments. The outcome of this is far more tangible than the previous question and needs far less explanation as to the benefits of such predictions. Being able to predict how well a reader understands text provides the benefits of removal of some comprehension assessment in place of using the implicit measure of eye tracking. It has been established that whilst eye movements are useful for investigating reading comprehension (Okoso et al., 2015; Rayner et al., 2006; Underwood et al., 1990), it is indeed difficult to predict quantified measures of reading comprehension (Martínez-Gómez & Aizawa, 2014). We have contributed to this research by investigating different methods for predicting reading comprehension (Chapter 5) and investigating how text difficulty affects eye gaze in such a way that reading comprehension prediction is improved (Chapter 6). Whilst the problem has not been solved, significant headway has been made. The lessons learnt from this investigation contribute to the production of effective eLearning materials. Most significantly, text difficulty should be considered from the students’ perspective and that this differs for L1 and L2 readers (Chapter 7).
The results from this thesis are intended for application in eLearning environments. Consistent with past research these applications are intended to ultimately be incorporated in existing learning management systems (Barrios et al., 2004; De Bra et al., 2013). We will now discuss these uses in the next section.
9.2 Framework for dynamic text selection and presentation based on eye gaze
9.2.1 FrameworkDescription
This thesis has investigated how eye gaze can be used, 1) to find optimal layouts of text and comprehension questions; 2) predict reading comprehension; and 3) predict implicit text difficulty. We now present how these conclusions are tied together by presenting their application in a framework for an eLearning system that dynamically presents text-based learning materials. The system utilises a commercial eye tracker. The framework for such a system is described in Figure 9.1.
165
Discussion and Implications
          Monitors Gaze
Reads Presents to
Informs
Feedback Reports
        166
Student
History
Instructor/ Author
Eye Trackers
Calibration Mechanism
Pre- processor
Reporter
    Predictive Agent
Uses past information & stores
     Informs
Selects
Develops new content Based on feedback
      Dynamic Reading Material
Presents in web browser
  Presenter
Dynamic Text Selector
  Figure 9.1. Framework for Dynamic presentation of reading material in an online learning environment (Copeland, Gedeon, & Caldwell, 2014).
Many of the components shown in Figure 9.1 are based on prior research, such as the calibration mechanism. These components will not be discussed in great detail. However, the components that showcase the use of the findings from this thesis will be discussed in more detail. These components are the Predictive Agent and the Reporter. The components of the framework are described in the rest of this subsection.
9.2.1.1 Calibration Mechanism
There is a need to account for error in recorded gaze location as it has been documented that eye trackers can lose precision during periods of use (Hyrskykari, 2006). A calibration mechanism will detect when the tracking data is out and prompt for a quick recalibration routine. It will do this by getting information from the content displayed. We propose using the calibration techniques described by (Hyrskykari, 2006); also see the auto-calibration we use described in Appendix C.
9.2.1.2 Pre-processor Mechanism
The output from eye trackers is x-y coordinate time series data, which is sent to the pre-processor mechanism to convert into eye movement measures. Pre-processing is necessary as it is the eye movements that can be used to make inferences about reading behaviour. The output from the eye tracker (eye gaze time series data) is sent to a pre-processor mechanism to turn the gaze points into fixation points and saccades using a fixation identification algorithm (Salvucci & Goldberg, 2000). A number of eye movement measures are calculated based on the content. Examples of these measures are answer-seeking behaviour as defined in this thesis in Chapter 4 and normalised number of fixations per paragraph. The output from the pre- processor mechanism is sent to the predictive agent.
9.2.1.3 Predictive Agent
This is one of the key parts of the system that highlights the use of the findings from this thesis. The eye movement measures used as the inputs for the predictive agent can be used to predict comprehension and implicit text difficulty. These predictions will be based on the presentation of the text. In Chapter 3 we identified that different presentation formats allowed for different deductions to be made
Discussion and Implications
 regarding comprehension and perceptions based on the sequence in which text and questions are shown to the student. Therefore, it is crucial to take this presentation method into consideration when drawing conclusions about a reader’s current state. Additionally, the presentation method will determine which eye movement measures will be sent to the predictive agent. For example, the case where questions and text are shown together, data focussed on answer-seeking behaviour will be generated.
It is important to make it clear that the predictive agent has two functions, first to detect the reader’s state in terms of comprehension and implicit text difficulty. Secondly, the predictive agent detects difficulty of the educational text and comprehension questions. Throughout the thesis we have highlighted two prospective uses of eye tracking in eLearning. The first is in regard to removing comprehension questions and implicitly predicting comprehension instead. The second is in regard to providing more information to the author of the eLearning materials so that materials can be optimised to facilitate learning. The results from the predictive agent from the latter function are passed to the Reporter, which is the second key part of the system, to achieve the latter goal.
Further, the student’s previous learning behaviour is accessed to make an overall calculation of the student’s current learning state. The current learning state is output to a content selector. Note from our analysis the predictive agent would be a combination of different machine learning techniques that are optimal for different situations. In the case where questions are shown with text, artificial neural networks using fuzzy output error (FOE-ANN) would be utilised as this provides optimal results (Chapter 5). However, when the questions are not shown with the text, then feature selection using genetic algorithms with a k-nearest neighbour classifier (GA-kNN) would be employed (Chapter 6). The output of the predictive agent is sent to the content selector.
9.2.1.3.1 Prediction of comprehension
The prediction of reading comprehension was covered in Chapters 4, 5 and 6. Chapter 4 highlights the use of eye movements to predict implicit comprehension when questions are presented with text. More specifically, when comprehension questions are presented with text, we showed in Chapter 3 that they are more likely to get the questions right. This is obvious; with the text there, the reader can search through the text and essentially do pattern matching with the words in the question. Do the questions then function to elicit true comprehension? This is not investigated here; instead we investigated answering behaviour, which can reveal the underlying state of the reader. More answer seeking indicates less confidence in answering a question. Whether this is due to not understanding, perception of not understanding, or simply that the reader did not read the text, this measure provides the system with key information that can be used with the reader’s answers and their reading behaviour measures. The information in particular can be used as an implicit measure of how difficult a participant finds text and the corresponding questions. Of course, the use of the reading behaviour measures is crucial, as not reading the text just shows that the reader has not previously seen the text and therefore high amounts of answer seeking would be expected. In this case
167
Discussion and Implications
 the predictive agent can make recommendations to change the presentation format so that the questions are not visible with the text to ensure that reading occurs. Alternatively, no reading measured along with low or no answer seeking most likely indicates that the reader has prior familiarity with the content being assessed and so the predictive agent can recommend increasing the difficulty of the content or a change to the next subject matter.
Chapters 5 and 6 looked directly at the prediction of reading comprehension scores from eye movements. The results in Chapter 5 highlight that the prediction of reading comprehension scores when the questions are shown with the text is achievable with great accuracy using FOE-ANN. From Chapter 6 we found that the use of GA-kNN to predict reading comprehension was best when the questions are not shown with the text. Additionally, when the text is more difficult, the prediction results are better. The predictive agent can then inform the content selector on the level of understanding and the suitable next text can be shown. For example, if a student does not understand the content, simpler text can be shown. Or if a student has high understanding, then advanced level text can be shown, possibly skipping further basic and intermediate steps in the learning path.
Finally, in Chapter 7 we investigated the use of eye tracking to predict the implicit difficulty of the text. Whilst we mention that texts with differing degrees of difficulty should be shown to participants based on their measured comprehension, is it crucial to actually measure how difficult an individual student perceives the text. We have shown that this varies between students and is quite different between L1 and L2 readers. These results tie back to those found in Chapter 4 where we used answer-seeking behaviour higher as a measure of implicit comprehension and therefore difficulty in answering the comprehension questions. Therefore, the predictive agent can predict how difficult the student finds text / comprehension questions, depending of the sequence of presentation, which is crucial for successful selection of the next text to be shown to that student.
9.2.1.3.2 Prediction for feedback
The second function of the predictive agent is in predicting properties about the text and questions presented to students. More specifically, this involves analysing students’ reading behaviours, answering behaviours, and understanding levels for each text. The predictive agent will predict how difficult text / comprehension questions are. Chapter 4 highlights the use of answer-seeking behaviour to measure question difficulty, which can be used as a feedback system to an instructor. This difficulty could be due to factors such as the technical nature of the material, and ambiguity in the material. Conversely, the instructor could see that the question is too easy and change it to be more challenging. This information could also be used to weight questions so that more difficult questions are weighted higher than those that are less difficult.
In Chapter 8 the clustering of eye movements revealed that the texts have different average reading behaviour that can be used to rate the texts’ implicit difficulty. This provides the author of the text with a measure of the amount of reading that the text elicits and thus the implicit difficulty of the text, which may
168
Discussion and Implications
 indeed be different from the predefined text difficulty. This can be used to improve the quality of texts as well as to ensure that the appropriate level of text difficulty is shown to students.
9.2.1.4 Content Selector
The author of the learning material prefills the content selector with different texts. These texts will include different versions of the same content. The different versions will include different levels of text readability, concept difficulty as well as remedial and advanced level supplementary material. Based on the student’s current state, as calculated by the predictive agent, a choice of version of the material is made by the content selector. This will also include generation of parameters that will change the rate at which the content is delivered to the student and change presentation format. The output of the selector is to the presenter.
9.2.1.5 Presenter
The presenter formats the selected content for the learning environment being used, such as Moodle. This is essentially the plug-in point to the existing learning environment.
9.2.1.6 Reporter
The reporting component is the second key component of the system is used by the author of the texts and questions to gain information about the difficulty of the questions and text, student performance and progression of learning data, in addition to reading behaviours. We have established that eye movements can be used in multiple ways to quantify the difficulty of text and questions. In Chapter 4 we showed how the average amount of answer-seeking behaviour that is observed could be used as an indicator for how difficulty on average students are finding particular questions to answer. This can then be used to check if particular students are performing above or below this average. The advantage of using this measure lies in the fact that it is a measure of the students’ implicit behaviour. More specifically, just getting the average scores of students on questions will not give a true representation of the difficulty or ease of the questions. This was shown in Chapter 4 where for questions with similar average scores there were quite different ranges of answer seeking behaviour. We were therefore able to more accurately rank the questions on difficulty. The same argument applies for the rating of students’ understanding. Below average answer seeking behaviour represents higher levels of understanding and high amounts of answer seeking behaviour indicates low levels of understanding, which may not be as accurately shown through the comprehension tests alone. The degree of answer seeking reflects how much they are learning now, while the comprehension score reflects the sum of prior and just learnt knowledge.
This line of inquiry was extended in Chapters 6, 7 and 8 where we investigated the effect of text difficulty on participants’ eye movements and perceptions. Not all students have the same conception of difficulty so the predefined difficulty may not be how difficult the student finds the text. Calculating the implicit reading difficulty of texts would be performed at both the student and the cohort level. We have already discussed the purpose of measuring the text difficult. However on the
169
Discussion and Implications
 student level, as with the answer-seeking behaviour, the implicit difficulty for each student can be calculated to differentiate the students’ abilities. The text complexity can be dynamically measured for the student rather than as a static measure. The long-term trends of these measured data can be used to assess how a student is progressing or if they are consistently underperforming and more assistance should be given to them. In an adaptive environment this provides a wealth of information and true power in giving learning material of the most appropriate level of difficulty to the student.
9.2.1.7 Student learning history
The student learning information is stored so that this information can be use in subsequent tutorials, and to track the learning progress. The information includes the basics such as what the student has learnt so far and their grades, but also includes their reading behaviour, how difficult they tend to find texts and questions, and the optimal way of presenting materials to them. This also allows the system to track how their perceptions change over time. This in itself is a measure for how the student is learning, as more accurate perceptions of text difficulty indicate increases in overall learning and comprehension, in addition to measuring levels of anxiety regarding the learning materials.
9.2.2 ReplacementofQuestionandAnswerAssessment
Since the predictive agent is designed to predict comprehension, the concept of removing question and answer-based assessment is plausible. In this case, a student’s reading and eye movement behaviour could be used to assess the student’s comprehension level. Prediction of reading comprehension from eye movement would allow for the removal of formative assessment of comprehension which could reduce learning time, workload, and potentially stress or anxiety of the students. Following on from this, predicting students’ comprehension using eye tracking would allow the learning environment to 1) adapt the questions asked of students about the content and 2) alter the learning path to reflect the students’ current understanding levels. This is similar to the traditional and summative interviews where answers to previous questions lead to easier or harder questions being asked.
The latter point is the main advantage of predicting comprehension from eye gaze. In the case where a student has read some learning materials and does not understand it, the student is then asked the same comprehension questions as all other students. Not understanding the questions makes it difficult and possibly increases the student’s anxiety about the learning material. Two solutions arise from this, first is that the questions themselves are modified to be easier, perhaps covering more superficial understanding of the content, or text with more explanation could be provided to the student. Previous studies have shown that simplifying text can improve reading performance (Dingli & Cachia, 2014). Text with more explanation of the content that was not understood could then be given to the student, after which the student is assessed on the original comprehension questions. Secondly, instead of asking comprehension questions at all, the text with more explanation could be provided.
170
Discussion and Implications
 If we now consider the converse case where a student has an extremely high level of understanding, as is the case when the student has prior knowledge on a certain topic, this student may become frustrated or bored by being presented with easy content and unchallenging questions. Again, either the questions or the content could be altered to present these students with more difficult subject matter and questions that require much more thought and insight then the student with a lower level of understanding.
9.3 Summary
This chapter discussed the results from the chapters of this thesis. Each chapter addressed a sub-question to the question of whether eye tracking can be used to make learning more effective in eLearning environments. This overall question is approached in two ways. The first is that making eLearning environments better suited to the individual learner. The demonstration of these results is through the use of adaptive eLearning whereby the system adapts to the student’s understanding levels and implicit difficulty. The second is the latent effects that eye tracking can have on making eLearning more effective. This is through the use of eye tracking to provide information to the author of the text and comprehension questions regarding their difficulty. This information can in turn be used to make better quality learning texts that are more accurately defined in difficulty. To show this we have tied the results from each chapter together in the presentation of a dynamic text selection method to make eLearning environments adaptive. The final chapter of this thesis is the conclusion and further work section.
171
Discussion and Implications
 172
Chapter 10
hapter 10. Conclusion
This thesis investigated the question of whether eye tracking can be used to make learning via reading more effective in the context of eLearning. Each chapter addressed a sub-question related to this research question. The investigation was approached from two directions: firstly, we investigated the use of eye tracking to adapt immediately to a student’s understanding and implicit text difficulty. This involved investigating method for improving prediction of reading comprehension from eye gaze. The second approach was using eye tracking to analyse how a cohort of readers perceived, interacted with, and read text and comprehension questions within an eLearning environment. This information about reading behaviour of texts can, in turn, be used to make better quality materials that are more accurately defined in difficulty.
Throughout the investigation we explored the differences between L1 and L2 English readers, as this is an area of growing diversity in audiences of eLearning materials. This was accomplished by performing two large user studies in which readers’ eye gaze was recorded. Chapters 3, 4 and 5 cover analysis of data collected from the first user study. In this study, different presentation sequences of text and comprehension questions were shown to readers as their eye gaze was recorded. In Chapter 3 we analysed how the sequence affected not only comprehension performance but also reading behaviour and student perceptions of performance. Chapter 4 covered analysis of reading and answering behaviour from a subset of the presentation sequences. From this analysis we proposed a new measure for reading
173
C
Conclusion
 comprehension called answer-seeking behaviour, which can also be used to provide feedback to authors of the learning materials about the implicit difficulty of text and comprehension questions. Finally, in Chapter 5 investigated predicting reading comprehension from the eye gaze data collected. The purpose of this prediction is to make eLearning environments adaptive to students based on their implicit understanding. We found that good predictions can be made for a subset of the presentation sequences. However, there is still much improvement needed to predict reading comprehension scores when no questions were shown with text.
The second user study picked up from where the first left off. In Chapter 6 we investigated the effect of text readability and conceptual difficulty on eye movements and prediction outcomes of reading comprehension. This was in an attempt to improve classification results of reading comprehension prediction. Whilst we did not observe significant differences in eye movements and prediction accuracies between the levels of text difficulty that we expected, we were able to achieve higher prediction accuracies. This led us to investigation of predicting text difficulty from eye movements, which was explored in Chapter 7.
Chapter 7 also further examined the participants’ perceptions of text difficulty, which indicated that text readability and conceptual difficulty interact to cause deviations from the predefined difficulty. Finally, in Chapter 8, cluster analysis of eye movements showed that average eye movements per text can be used to derive reading difficulty of the text. This can be used as feedback to the author of the text to assign derived text difficulty levels, as well as improve the quality of learning materials.
We conclude by tying these findings together in the discussion of how the research in this study can be applied to improve reading within eLearning environments. We propose an adaptive eLearning architecture that dynamically presents text to students and provides information to authors to improve the quality of texts and questions. However, much is left to investigate in this area so the following section of this chapter outlines some keys points that require further investigation as well as possible areas of interest for improving reading in eLearning environments.
10.1 Limitations
There are a number of limitations to the design of both studies and therefore the conclusions that can be drawn from them. In the first user study that is discussed in Chapters 3 through 5, a between-subjects design is used. However, this inherently introduces a lot of noise due to the differences in how people read. It would be beneficial to use a within-subjects design to reduce this noise and analyse in more depth the effects of layout on reading behaviour.
This leads to the next limitation, of both studies, which is that nature of the studies was highly artificial as they are conducted in a laboratory setting even though the tasks mimicked real life situations. The result of this highly artificial setting may, and probably did, influence their behaviour from a situation where
174
Conclusion
 they are not being observed. Performing the studies in the laboratory setting likely altered the goals the readers purely because participants knew they were being observed. The most probable changes to goals and behaviour are 1) participants don’t want appear lazy or unintelligent to the experimenter and so read the text more thoroughly than they would if unobserved, and 2) participants think they are being helpful by reading the text more thoroughly. In both cases, the end result is that the texts in the studies performed in this thesis are read more than they would be in-the-wild.
The results from in-the-wild studies would be highly beneficial in this area as they would capture more true to life behaviours. The focus of this thesis was to set ground work for the use of eye tracking to analyse reading and learning behaviour from text in eLearning environments. Additionally, the physical limitations of eye tracking hardware constrained our studies to be in a laboratory setting. For both reasons we limited the scope of studies performed so that they were in laboratory settings. However, recently small and portable eye tracking devices have become available. This does introduce the possibility of moving such studies into classrooms or in-the-wild settings and should most certainly be considered for future work.
Adding to these limitations are the highly restricted set of teaching materials that where considered in the studies. These limitations were briefly discussed in Chapter 3 but deserve more thorough discussion. As we concluded that studies should be extended to be in-the-wild, we so too conclude that the diversity and the nature of the content should be closer to a real world scenario. This means including different types of texts with different lengths, different topics, and especially those that are taken directly from course materials. Although the materials used in both studies were taken from a first year course at the university, they are a quite small subset of that course and they were chosen specifically because they were non- technical topics. This also resulted in a very particular subset of participants being used, mainly being selected from the course in which the materials were taken from. Whilst this meant we were testing on a realistic group of eLearning environments, and a realistic group who might access materials on these topics, it is still a subgroup of the population. Both of these factors are clear limitations on the studies and the results obtained.
Expanding the materials to a broader set of teaching materials, covering a much broader range of topics, and then testing these on a broader population would be necessary for future research. Firstly, because the current study only looked at a small subset and the results for different topics and different population subsets could differ from those reported here, and secondly, could increase the prediction results from the machine learning techniques.
The comprehension questions used in both studies were also limitations, as they represent significant subsets of the types of comprehension questions that could be asked. Additionally, the questions were designed in a way that made marking almost completely automated, as they were taken from a weekly tutorial quicz given to the students in the course. The questions where taken straight from the existing course materials, so they had been designed by the course convenor with
175
Conclusion
 significant experience in teaching and developing the course materials. However, the questions themselves may be flawed in assessing comprehension for the purposes of the research being conducted. That is, the questions may not have spanned enough of the realms of comprehension assessment to fully and widely test the comprehension of students.
Extending the questions set to include short answer and essay questions would not only be valuable, but could also yield interesting results in more fully assessing comprehension. This limits the results from the second user study where the comprehension questions were specifically designed for each level of conceptual difficulty but not for readability. This was because different levels of concepts were being taught in the texts and so the questions had to reflect this. Although aligning comprehension questions to texts makes sense, there was no control for the quality control of questions. This means that some questions could have been inadvertently easier or harder than others, inherently skewing answering behaviour. This problem is prevalent in the first user study as well, where in Chapter 4 answer-seeking behaviour was used to assess question quality and consistency. In future research, there should be better quality control of comprehension questions and a larger set of different types of comprehension questions to more wholly assess comprehension.
In adapting the materials for the study there are many lessons learnt. This comes primarily in the length of texts given to students; in the two studies conducted as part of this thesis, the texts were likely too short. The reason for keeping the texts short was to keep down the experiment duration; however, the short nature of the texts most likely did not help in gathering more natural looking reading patterns. As the texts were short it was not difficult for the participants to read them and therefore not get bored and result in reduced reading behaviour. However, this is in itself an interesting research question to be investigated, does keeping text short increase reading?
The next lesson learnt is in the text construction. For both experiments, there were clear constraints on the readability level of the text and the conceptual difficulty. They were altered to keep them within limits so that the studies could test these effects on the reader. As we saw from Chapter 8, this is perhaps not the optimal or true way of finding the derived text difficulty. Previous studies have surveyed readers on the text difficulty and used that as the measure of text difficulty (Rayner et. al., 2006). Whilst this is more laborious than running readability formulae over the texts, it would provide more appropriate text difficulty results. Leading on from this, and in the spirit or testing in-the-wild, simply taking course material in its current state would be beneficial to do, rather than manipulating the materials to fit experimental conditions.
Perhaps the biggest limitation is that we did not formally test both prior knowledge and language skill. For both studies participants were only asked to rate their familiarity with the topics being examined and what language they first learnt to read in. This meant that they categorisation of participants into the L1 / L2 groups has limitations. This is observed in Chapter 8, where some L2 participants read like L1 participants and vice versa. There are clear benefits of accounting for factors like
176
Conclusion
 prior-knowledge and language skill when constructing machine learning models of reading and learning behaviour, consequently, not formally accounting for either, limits the accuracy of the models and the potential of the models. Further work should be to take both into account.
This leads to the next point, which is, how would one deal with the situation where participants know the topic area sufficiently already that they answer the questions without reading the learning materials? Indeed, this is a limitation of the current studies given that prior-knowledge was not accurately assessed. It would be advantageous to detect that this situation, and would be an interesting future user study. A potential solution for this is to perform a pre-assessment similar to format D from Chapter 3, where participants were shown the questions before given the reading materials. Participants could be asked to complete the pre-assessment to the best of their ability and rate their knowledge on the subject matter and confidence in answering the questions. Then given the reading materials and observation of their eye gaze could take place. Such a scenario would set up testing for prior knowledge and therefore detection of eye gaze patterns of those who have (differing degrees of) prior-knowledge. This would allow for much more accurate personalisation of adaptive content. For example, detecting that a student has significant prior- knowledge of a subject allows the adaptive system to completely bypass the subject for that student. Moreover, if a student is detected to have partial prior-knowledge, then that student could be provided only with the materials that cover their knowledge gap. This also draws to light the potential benefits of combining pre- assessment with the use of eye tracking as complimentary drivers of adaptive and personalised eLearning.
Whilst the latter part of the analysis in the thesis considered the individual differences between readers, as we have discussed so far the clear limitations of not testing prior-knowledge and language and reading skills meant that individual models of readers were not considered more thoroughly. The machine learning prediction performance might be increased if there were more detailed models of each individual, particularly in terms of their actual reading skill, their prior- knowledge of the topic, and their demonstrated learning and comprehension performance.
10.2 Future work
There were several limitations of the first user study presented in the thesis. Some of these limitations were addressed in the second user study, such as variance of text complexity and analysis of perceptions of text complexity. Firstly, only two types of questions were investigated. Whilst these were chosen to test different parts of the comprehension spectrum, different types of questions and texts should be investigated. Further, given that inclusion of appropriate images and / or animations can enhance learning outcomes (Clark & Mayer, 2011; Harp & Mayer, 1998; Mayer et al., 2001; Sanchez & Wiley, 2006; Sung & Mayer, 2012) this should also be included in the different presentation sequence. The effects of inclusion of appropriate images and / or animations to text on L2 reading comprehension
177
Conclusion
 performance as well as comparison of L1 and L2 perceptions of difficulty should also be investigated.
The second user study involved reading educational text about digital images. After reading the text, participants were asked to identify within digital images examples they had learnt from the text, such as resolution, manipulations, and bit depth. An example of this was that participants were asked to identify manipulated images and the manipulations (Caldwell et al., 2015). Accuracy of identifying the other factors related to what the students learnt should be investigated to assess the applied knowledge as well as the reading comprehension.
Additionally, eye gaze and pupil dilation data are the only biometric data used in this thesis. Inclusion of galvanic skin response (GSR), electrocardiogram (ECG), and electroencephalogram (EEG) data should be investigated. Such inclusion could lead to improve reading comprehension prediction results. These biometric measures have been used with great success to predict stress during reading (Sharma & Gedeon, 2012) as well as predicting differences in stress between males and females during reading (Sharma & Gedeon, 2011, 2013a, 2013b). These biometric data have also been used to predict the nature of document content in relation to national security (Chow & Gedeon, 2015).
The extension of these findings to mobile devices, such as smart phones and tablets should be investigated to see if the results are generalizable to these devices. The use of mobile devices, and hence mobile learning (mLearning), is becoming more widespread and therefore increasing the need for making learning materials effective on these devices. Indeed, there are differences in behaviours when using small screen devices compared to large screen devices, such as different search behaviour and that fact that users have trouble extracting information from search results on smaller screens (Kim et al., 2012; 2015). This implies that care should be taken when designing learning materials for different devices.
Whilst studies have shown that adaptive eLearning is beneficial in learning (Dingli & Cachia, 2014; Paramythis & Loidl-Reisinger, 2003) the effects of altering the difficulty of text shown to students based upon their understanding should be investigated further to see if, and to what extent, this provides learning benefits. Both short and longitudinal studies on these effects would be beneficial in determining any short and long terms benefits of such adaptions.
Throughout the thesis we have highlighted the use of eye gaze to predict reading comprehension. In Chapter 8 the idea of using eye tracking to calculate cognitive load was introduced, which is the strain being placed on the learner’s working memory (Rosch & Vogel-Walcutt, 2013). The idea behind using cognitive load to adapt education materials is that there a limitations of working memory, where inducing too much load via an overly complex learning task is detrimental to learning, however so too is underload caused by a too simplistic task (Paas et al., 2004). Therefore if a learner were being too challenged according to their cognitive load then in an adaptive eLearning environment the material would be made simpler for the learner or more challenging in the opposite case. This is the same
178
Conclusion
 preposition we use except that we have highlighted changes be made based on reading comprehension. However, the inclusion of cognitive load measures along with comprehension could be highly beneficial, especially in analysing the relationship between cognitive load and reading behaviour.
Attention guiding is another way in which learning environments can be made more optimal for learners. It can be used to both minimize distraction of the learner as well as draw the learner’s attention to the important or relevant parts of the learning material. Attention guiding has been shown to improve problem solving by conveying task-relevant information (Groen & Noyes, 2010). Attention guiding can provide visual cues by using colours to emphasise relevant parts of animations (Boucheix & Lowe, 2010), or by zooming in on parts of animations (Amadieu et al., 2011), and signalling parts relevant parts of diagrams by adding temporary colour changes (Ozcelik et al., 2010). The addition of eye tracking data to the paradigms has been found to enhance their effectiveness (Boucheix & Lowe, 2010; Ozcelik et al., 2010). This leads to a similar concept which is the use of eye tracking to guide student learning using eye movement modelling examples (EMME) (Jarodzka et al., 2010). EMME is a technique where the eye movements of experts are superimposed onto a task to show how that expert performed a task. This is easily visualised when considering a visual task such as watching a video to learn how to classify fish locomotion (Jarodzka et al., 2013) or to diagnose seizures (Jarodzka et al., 2010). Importantly, this is achieved by blurring out areas where the expert was not looking at (Jarodzka et al., 2010) or using a dot or highlighting effect to focus on the parts that the expert was looking (Jarodzka et al., 2013). A similar approach has been used to in the context of reading and viewing an associated diagram (Mason et al., 2015). The use of EMME in reading could be explored.
Alternatively, investigation of methods of using rapid serial visual presentation (RSVP) to ensure reading of all text could be explored. RSVP of text has been shown to keep reading comprehension constant during speed-reading (Dingler et al., 2015). This poses the question of whether integrating the EMME and RSVP would be beneficial for eLearning. The idea being that using RSVP techniques to guide learning through text could possibly help reduce distractions, as it motivates the reader to keep up with the text, and secondly, could promote more thorough reading and comprehension of the text. In this way there would actually be no “rapid” presentation in the real sense of the technique, the goal would not be to promote speed-reading, or rapid reading, but rather to force reading (so the presentation of words or text would not be as fast). Examples of RSVP include the open source framework Squirt19 where one word is presented to a reader at a time. Another example is dynamic underlining of text to mark (Dingler et al., 2015).
Both of these concepts underpin the idea of streamlining the reading process as well as mitigating distraction. Distractions affect reading comprehension and behaviour (Halin et al., 2014a; 2014b; Sörqvist et al., 2010). Digital environments present many distractions, often bombarding users with information that disrupts processing (Maglio & Campbell, 2003). Reduction of visual distractions is pertinent
 19 https://www.squirt.io/ Last Accessed: 25th August 2015
179
Conclusion
 for avoid irrelevant objects increasing cognitive load (Sweller et al., 1998). When cognitive load induced by the primary task is made higher, then distractors are attended to far less (DeLeeuw et al., 2010). However, an interesting question for further research is whether eye tracking can be used to reduce the effect of distractions on reading. The idea of mitigating distractions during reading using eye-tracking technology plays on grabbing the attention back from students.
In Appendix D we present a preliminary study that looked at mitigating visual distractions during reading in a distracting environment. The results from the study first indicate that participants have high levels of distractions whilst studying, setting precedent for the need for distraction mitigation. The results show that the distraction mitigation signals helped L2 readers to restart reading and hence to read the hard text more effectively than in their absence. Additionally, the questionnaire data demonstrated that for both the L1 and L2 groups the mitigation signals helped to recover from a distraction by drawing participants’ attention back to the text as well as indicating where to start reading from. While the study had limitations and was preliminary, the results indeed show that there is no potential for such technology. One of the main problems with the experiment that could have led to inconclusive results was inaccuracy of the eye tracker. Participants noted that the text effect did not always appear where they had last read and when it was not working at all. Instead the effect would appear sporadically around the page causing the process to be more distracting than the planned experimental distractions themselves. In the busy environments in which we now work, the concept of mitigating distraction is highly important especially when it is known that distractions affect reading outcomes (Halin et al., 2014a; 2014b; Sörqvist et al., 2010). This is an area of active research that needs further investigation.
The discussion of distraction mitigation leads to the integration of attention managers that use eye tracking to manage alerts to the user into eLearning environments. The idea behind attention managers is that information, namely alerts, is controlled by a managing service to minimise the effect of distractions by scheduling them during skimming rather than thorough reading. Interruptions not only have negative effects on users task performance and emotional state, but these effects are more intense if the user is under high mental load (Adamczyk & Bailey, 2004; Bailey et al., 2001).
180
References
Adamczyk, P. D., & Bailey, B. P. (2004). If not now, when?: the effects of interruption at different moments within task execution. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (pp. 271-278). ACM.
Alsobhi, A. Y., Khan, N., & Rahanu, H. (2015). DAEL Framework: A New Adaptive E-learning Framework for Students with Dyslexia. Procedia Computer Science, 51, 1947-1956.
Amadieu, F., Mariné, C., & Laimay, C. (2011). The attention-guiding effect and cognitive load in the comprehension of animations. Computers in human behavior, 27(1), 36-40.
Amaratunga, D., Cabrera, J., & Lee, Y.-S. (2008). Enriched random forests. Bioinformatics, 24(18), 2010-2014.
Anderson-Inman, M. A. H., Lynne. (1999). Supported text in electronic reading environments. Reading & Writing Quarterly, 15(2), 127-168.
Anderson-Inman, L., & Horney, M. A. (2007). Supported eText: Assistive technology through text transformations. Reading Research Quarterly, 42(1), 153-160.
Atkins, M. S., Moise, A., & Rohling, R. (2006). An application of eyegaze tracking for designing radiologists' workstations: Insights for comparative visual search tasks. ACM Transactions on Applied Perception, 3(2), 136-151.
Bailey, B. P., Konstan, J. A., & Carlis, J. V. (2001). The effects of interruptions on task performance, annoyance, and anxiety in the user interface. In Proceedings of INTERACT (Vol. 1, pp. 593-601).
Barrios, V. M. G., Gütl, C., Preis, A. M., Andrews, K., Pivec, M., Mödritscher, F., & Trummer, C. (2004). AdELE: A framework for adaptive e-learning through eye tracking. In Proceedings of IKNOW (pp. 609-616).
Barton, D. (2007). Literacy: an introduction to the ecology of written language. Wiley- Blackwell.
Beatty, J. (1982). Task-evoked pupillary responses, processing load, and the structure of processing resources. Psychological Bulletin, 91(2), 276.
Bernard, M., & Mills, M. (2000). So, what size and type of font should I use on my website. Usability news, 2(2), 1-5.
Beymer, D., & Flickner, M. (2003). Eye gaze tracking using an active stereo head. In
Proceedings of 2003 IEEE Computer Society Conference on Computer Vision and
Pattern Recognition (Vol. 2, pp. II-451). IEEE.
Beymer, D., Russell, D., & Orton, P. (2008). An eye tracking study of how font size
and type influence online reading. In Proceedings of the 22nd British HCI Group Annual Conference on People and Computers: Culture, Creativity, Interaction (pp. 15- 18). British Computer Society.
 181
References
 Beymer, D., & Russell, D. M. (2005). WebGazeAnalyzer: a system for capturing and analyzing web reading behavior using eye gaze. In CHI'05 extended abstracts on Human factors in computing systems (pp. 1913-1916). ACM.
Biedert, R., Buscher, G., Lottermann, T., Schwarz, S., Möller, M., & Dengel, A. (2010). The Text 2.0 Framework: writing web-based gaze-controlled realtime applications quickly and easily. In Proceedings of the 2010 workshop on Eye gaze in intelligent human machine interaction (pp. 114-117). ACM.
Biedert, R., Buscher, G., Schwarz, S., Hees, J., & Dengel, A. (2010). Text 2.0. In
Proceedings of the 28th of the international conference extended abstracts on Human
factors in computing systems. (pp. 4003-4008): ACM.
Bohn, R. E., & Short, J. E. (2009). How Much Information?: 2009 Report on American
Consumers: University of California, San Diego, Global Information Industry
Center.
Bondareva, D., Conati, C., Feyzi-Behnagh, R., Harley, J. M., Azevedo, R., & Bouchet,
F. (2013). Inferring learning from gaze data during interaction with an environment to support self-regulated learning. In Proceedings of International Conference on Artificial Intelligence in Education. (pp. 229-238). Springer Berlin Heidelberg.
Boucheix, J.-M., & Lowe, R. K. (2010). An eye tracking comparison of external pointing cues and internal continuous cues in learning with complex animations. Learning and Instruction, 20(2), 123-135.
Bowman, L. L., Levine, L. E., Waite, B. M., & Gendron, M. (2010). Can students really multitask? An experimental study of instant messaging while reading. Computers & Education, 54(4), 927-931.
Bransford, J. D., & Franks, J. J. (1971). The abstraction of linguistic ideas. Cognitive Psychology, 2(4), 331-350.
Brasel, S. A., & Gips, J. (2011). Media multitasking behavior: Concurrent television and computer usage. Cyberpsychology, Behavior, and Social Networking, 14(9), 527- 534.
Breiman, L. (1996). Bagging predictors. Machine Learning, 24(2), 123-140.
Breiman, L. (2001). Random forests. Machine Learning, 45(1), 5-32.
Breiman, L., Friedman, J., Stone, C. J., & Olshen, R. A. (1984). Classification and
regression trees: CRC press.
Breslow, L., Pritchard, D. E., DeBoer, J., Stump, G. S., Ho, A. D., & Seaton, D. (2013).
Studying learning in the worldwide classroom: Research into edX’s first MOOC.
Research & Practice in Assessment, 8(1), 13-25.
Bunch, G. C., Walqui, A., & Pearson, P. D. (2014). Complex text and new common
standards in the United States: Pedagogical implications for English learners.
Tesol Quarterly, 48(3), 533-559.
Burton, L., Westen, D., & Kowalski, R. (2009). Psychology 2nd Edition.: Wiley. Buscher, G., Dengel, A., Biedert, R., & Van Elst, L. (2012). Attentive Documents: Eye
Tracking as Implicit Feedback for Information Retrieval and Beyond. ACM
Transactions on Interactive Intelligent Systems, 1(2), Article 9.
Buscher, G., Dengel, A., & Elst, L. v. (2008). Eye movements as implicit relevance
feedback. In CHI '08 Extended Abstracts on Human Factors in Computing Systems, Florence, Italy. (pp. 2991-2996). ACM.
182
References
 Buscher, G., Dumais, S. T., & Cutrell, E. (2010). The good, the bad, and the random: an eye-tracking study of ad quality in web search. If Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval. (pp. 42-49): ACM
Caldwell, S., Gedeon, T., Jones, R., & Copeland, L. (2015). Imperfect Understandings: A Grounded Theory And Eye Gaze Investigation Of Human Perceptions Of Manipulated And Unmanipulated Digital Images. In Proceedings of 3rd International Conference on Multimedia and Human-Computer Interaction (MHCI’15).
Calvi, C., Porta, M., & Sacchi, D. (2008). e5Learning, an e-learning environment based on eye tracking. In 2008 Eighth IEEE International Conference on Advanced Learning Technologies (pp. 376-380). IEEE.
Campbell, C. S., & Maglio, P. P. (2001). A robust algorithm for reading detection. In Proceedings of the 2001 workshop on Perceptive user interfaces (pp. 1-7). ACM.
Carenini, G., Conati, C., Hoque, E., Steichen, B., Toker, D., & Enns, J. (2014). Highlighting interventions and user differences: informing adaptive information visualization support. In Proceedings of the 32nd annual ACM conference on Human factors in computing systems (pp. 1835-1844). ACM.
Carpenter, P. A., & Just, M. A. (1983). What your eyes do while your mind is reading. Eye movements in reading: Perceptual and language processes, 275-307. Chang, S. E. (2005). Computer anxiety and perception of task complexity in learning
programming-related skills. Computers in human behavior, 21(5), 713-728.
Chen, C.-M. (2008). Intelligent web-based learning system with personalized
learning path guidance. Computers & Education, 51(2), 787-814.
Chen, S.-C., She, H.-C., Chuang, M.-H., Wu, J.-Y., Tsai, J.-L., & Jung, T.-P. (2014). Eye movements predict students' computer-based assessment performance of physics concepts in different presentation modalities. Computers & Education, 74,
61-72.
Chow, C., & Gedeon, T. (2015). Classifying Document Categories based on
Physiological Measures of Analyst Responses. In Proceedings of Cognitive Infocommunications (CogInfoCom), 2015 6th IEEE International Conference on (pp. 421-425). IEEE.
Clark, R. C., & Mayer, R. E. (2011). E-learning and the science of instruction: Proven guidelines for consumers and designers of multimedia learning: John Wiley & Sons. Cohen, L. G., Celnik, P., Pascual-Leone, A., Corwell, B., Faiz, L., Dambrosia, J.,
Catala, M. D. (1997). Functional relevance of cross-modal plasticity in blind
humans. Nature, 389(6647), 180-183.
Conati, C., Jaques, N., & Muir, M. (2013). Understanding Attention to Adaptive
Hints in Educational Games: An Eye-Tracking Study. International Journal of
Artificial Intelligence in Education, 23(1), 136-161.
Conati, C., & Merten, C. (2007). Eye-tracking for user modeling in exploratory
learning environments: An empirical evaluation. Knowledge-Based Systems, 20(6),
557-574.
Copeland, L. (2011). Extraction of information from Eye Gaze Data (Honours Thesis).
Research School of Computer Science. Australian National University, ACT, Australia.
 183
References
 Copeland, L., & Gedeon, T. (2013a). The effect of subject familiarity on comprehension and eye movements during reading. In Proceedings of the 25th Australian Computer-Human Interaction Conference: Augmentation, Application, Innovation, Collaboration. (pp. 285-288): ACM.
Copeland, L., & Gedeon, T. (2013b). Measuring reading comprehension using eye movements. In Proceedings of Cognitive Infocommunications (CogInfoCom), 2013 IEEE 4th International Conference on. (pp. 791-796): IEEE.
Copeland, L., & Gedeon, T. (2014a). Effect of presentation on reading behaviour. In
Proceedings of the 26th Australian Computer-Human Interaction Conference on
Designing Futures: the Future of Design. (pp. 230-239). ACM.
Copeland, L., & Gedeon, T. (2014b). What are You Reading Most: Attention in
eLearning. Procedia Computer Science, 39, 67-74.
Copeland, L., & Gedeon, T. (2015). Visual Distractions Effects on Reading in Digital
Environments: A Comparison of First and Second English Language Readers. In
Proceedings of the Annual Meeting of the Australian Special Interest Group for
Computer Human Interaction. (pp. 506-516). ACM.
Copeland, L., Gedeon, T., & Caldwell, S. (2014). Framework for Dynamic Text
Presentation in eLearning. Procedia Computer Science, 39, 150-153.
Copeland, L., Gedeon, T., & Caldwell, S. (2015). Effects of Text Difficulty and Readers on Predicting Reading Comprehension from Eye Movements. In Proceedings of the IEEE 6th International Conference on Cognitive Infocommunications
(CogInfoCom) 2015, Győr, Hungary. (pp. 407-412). IEEE.
Copeland, L., Gedeon, T., & Mendis, S. (2014a). Fuzzy Output Error as the
Performance Function for Training Artificial Neural Networks to Predict Reading Comprehension from Eye Gaze. In Proceedings of The 21st International Conference on Neural Information Processing 2014.(pp. 586-593). Springer International Publishing.
Copeland, L., Gedeon, T., & Mendis, S. (2014b). Predicting reading comprehension scores from eye movements using artificial neural networks and fuzzy output error. Artificial Intelligence Research, 3(3), p35.
Copeland, L. D., & Gedeon, T. D. (2015). Tutorials in eLearning; How Presentation Affects Outcomes. Emerging Topics in Computing, IEEE Transactions on, PP(99), 1- 1.
Coyne, J. T., Baldwin, C., Cole, A., Sibley, C., & Roberts, D. M. (2009). Applying real time physiological measures of cognitive load to improve training Foundations of augmented cognition. Neuroergonomics and operational neuroscience (pp. 469- 478): Springer.
Crosby, M. E., Iding, M. K., & Chin, D. N. (2001). Visual search and background complexity: Does the forest hide the trees? User Modeling 2001 (pp. 225-227): Springer.
Crossley, S. A., Greenfield, J., & McNamara, D. S. (2008). Assessing text readability using cognitively based indices. Tesol Quarterly, 42(3), 475-493.
Crowder, R. G., & Wagner, R. K. (1992). The Psychology of Reading: An Introduction (Second Edition ed.): Oxford University Press.
Cutting, L. E., & Scarborough, H. S. (2006). Prediction of reading comprehension: Relative contributions of word recognition, language proficiency, and other
184
References
 cognitive skills can depend on how comprehension is measured. Scientific Studies
of Reading, 10(3), 277-299.
D'Mello, S., Olney, A., Williams, C., & Hays, P. (2012). Gaze tutor: A gaze-reactive
intelligent tutoring system. International Journal of Human-Computer Studies, 70(5),
377-398.
De Bra, P., Smits, D., van der Sluijs, K., Cristea, A. I., Foss, J., Glahn, C., & Steiner, C.
M. (2013). GRAPPLE: Learning management systems meet adaptive learning environments. In Intelligent and Adaptive Educational-Learning Systems (pp. 133- 160): Springer.
De Jong, K. A. (1993). Genetic algorithms are NOT function optimizers. Foundations of genetic algorithms, 2, 5-17.
DeBoer, J., Stump, G. S., Seaton, D., & Breslow, L. (2013). Diversity in MOOC students’ backgrounds and behaviors in relationship to performance in 6.002 x. In Proceedings of the Sixth Learning International Networks Consortium Conference. (Vol. 4).
Dednam, E., Brown, R., Dani, #235, Wium, l., & Blignaut, P. (2014). The Effects of Mother Tongue and Text Difficulty on Gaze Behaviour while Reading Afrikaans Text. In Proceedings of the Southern African Institute for Computer Scientist and Information Technologists Annual Conference 2014 on SAICSIT 2014 Empowered by Technology, Centurion, South Africa. (p. 334). ACM.
Dehaene, S. (2009). Reading in the brain: the new science of how we read: Penguin. DeLeeuw, K. E., Mayer, R. E., & Giesbrecht, B. (2010). How does text affect the processing of diagrams in multimedia learning? Diagrammatic Representation and
Inference (pp. 304-306): Springer.
DeStefano, D., & LeFevre, J.-A. (2007). Cognitive load in hypertext reading: A
review. Computers in human behavior, 23(3), 1616-1641.
Dillon, A. (1992). Reading from paper versus screens: A critical review of the
empirical literature. Ergonomics, 35(10), 1297-1326.
Dillon, A. (2004). Designing usable electronic text: Ergonomic aspects of human
information usage: CRC Press.
Dillon, A., & Gabbard, R. (1998). Hypermedia as an educational technology: A
review of the quantitative research literature on learner comprehension, control,
and style. Review of educational research, 68(3), 322-349.
Dingler, T., Shirazi, A. S., Kunze, K., & Schmidt, A. (2015). Assessment of stimuli for
supporting speed reading on electronic devices. In Proceedings of the 6th
Augmented Human International Conference, Singapore. (pp. 117-124). ACM.
Dingli, A., & Cachia, C. (2014). Adaptive eBook. In Proceedings of the Interactive Mobile Communication Technologies and Learning (IMCL), 2014 International
Conference on. (pp. 14-19). IEEE.
Dombi, J. (1990). Membership function as an evaluation. Fuzzy sets and Systems,
35(1), 1-21.
Dombi, J., & Gera, Z. (2005). The approximation of piecewise linear membership
functions and Łukasiewicz operators. Fuzzy sets and Systems, 154(2), 275-286. Dombi, J., & Gera, Z. (2008). Rule based fuzzy classification using squashing
functions. Journal of Intelligent and Fuzzy Systems, 19(1), 3-8.
Dunlosky, J., & Lipko, A. R. (2007). Metacomprehension A Brief History and How to
Improve Its Accuracy. Current Directions in Psychological Science, 16(4), 228-232. 185
References
 Dunning, D., Johnson, K., Ehrlinger, J., & Kruger, J. (2003). Why people fail to recognize their own incompetence. Current Directions in Psychological Science, 12(3), 83-87.
Eagleman, D. (2011). Incognito: The secret lives of the brain. New York: Vintage Books. Ehrlinger, J., Johnson, K., Banner, M., Dunning, D., & Kruger, J. (2008). Why the unskilled are unaware: Further explorations of (absent) self-insight among the
incompetent. Organizational behavior and human decision processes, 105(1), 98-121. Eklund, J., & Brusilovsky, P. (1999). Interbook: an adaptive tutoring system.
UniServe Science News, 12(3), 8-13.
Engbert, R., & Kliegl, R. (2001). Mathematical models of eye movements in reading:
a possible role for autonomous saccades. Biological Cybernetics, 85, 77-87.
Engbert, R., Longtin, A., & Kliegl, R. (2002). A dynamical model of saccade generation in reading based on spatially distributed lexical processing. Vision
Research, 42(5), 621-636.
Engbert, R., Nuthmann, A., Richter, E. M., & Kliegl, R. (2005). Swift: A dynamical
model of saccade generation during reading. Psychological Review, 112(4), 777-
813.
Engelhardt, P. E., Ferreira, F., & Patsenko, E. G. (2010). Pupillometry reveals
processing load during spoken language comprehension. The quarterly journal of
experimental psychology, 63(4), 639-645.
Fahey, D. (2009). A Preliminary Investigation into using eye-tracking to analyse a person's
reading behaviour (Honours Thesis). Research School of Computer Science.
Australian National University.
Fletcher, J. M. (2006). Measuring Reading Comprehension. Scientific Studies of
Reading, 10(3), 323-330.
Fox, A. B., Rosen, J., & Crawford, M. (2009). Distractions, distractions: does instant
messaging affect college students' performance on a concurrent reading
comprehension task? CyberPsychology & Behavior, 12(1), 51-53.
Francis, D. J., Snow, C. E., August, D., Carlson, C. D., Miller, J., & Iglesias, A. (2006). Measures of reading comprehension: A latent variable analysis of the diagnostic
assessment of reading comprehension. Scientific Studies of Reading, 10(3), 301-322. Frazier, L., & Rayner, K. (1982). Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous
sentences. Cognitive Psychology, 14(2), 178-210.
Freund, Y., Schapire, R., & Abe, N. (1999). A short introduction to boosting. Journal-
Japanese Society For Artificial Intelligence, 14(771-780), 1612.
Fritz, T., Begel, A., Müller, S. C., Yigit-Elliott, S., & Züger, M. (2014). Using psycho-
physiological measures to assess task difficulty in software development. In Proceedings of the 36th International Conference on Software Engineering. (pp. 402- 413). ACM.
Fuchs, L. S., Fuchs, D., & Maxwell, L. (1988). The Validity of Informal Reading Comprehension Measures. Remedial and Special Education, 9(2), 20-28.
Garrett, D., Peterson, D. A., Anderson, C. W., & Thaut, M. H. (2003). Comparison of linear, nonlinear, and feature selection methods for EEG signal classification. Neural Systems and Rehabilitation Engineering, IEEE Transactions on, 11(2), 141-144.
Gedeon, T., Copeland, L., & Mendis, B. S. (2012). Fuzzy Output Error. Australian Journal of Intelligent Information Processing Systems, 13(2), 37-43.
186
References
 Gedeon, T. D., Zhu, D., & Mendis, B. S. U. (2008). Eye gaze assistance for a game- like interactive task. International Journal of Computer Games Technology, 3.
Gedeon, T., Zhu, X., Copeland, L., & Sharma, N. (2015). Feature selection and interpretation of GSR and ECG sensor data in Biofeedback Stress Monitoring. In Proceedings of the Ninth International Conference on Sensor Technologies and Applications (SENSORCOMM 2015), Venice, Italy.
Gehring, W. J. (2005). New Perspectives on Eye Development and the Evolution of Eyes and Photoreceptors. Journal of Heredity, 96(3), 171-184.
Gera, Z., & Dombi, J. (2005). Genetic Algorithm with Gradient Based Tuning for Constructing Fuzzy Rules. Publications of International Symopsium of Hungarian Researchers of Computational Intelligence, 86-95.
Goldberg, J. H., & Wichansky, A. M. (2003). Eye tracking in usability evaluation: A practitioner’s guide. 493-516.
Graesser, A. C., McNamara, D. S., & Kulikowich, J. M. (2011). Coh-Metrix providing multilevel analyses of text characteristics. Educational Researcher, 40(5), 223-234.
Groen, M., & Noyes, J. (2010). Solving problems: How can guidance concerning task-relevancy be provided? Computers in human behavior, 26(6), 1318-1326.
Gustavsson, C. J. (2010). Real Time Classification of Reading in Gaze Data (Masters Thesis). School of Computer Science and Engineering. Royal Institute of Technology. Stockholm, Sweden.
Gütl, C., Pivec, M., Trummer, C., García-Barrios, V. M., Mödritscher, F., Pripfl, J., & Umgeher, M. (2005). Adele (adaptive e-learning with eye-tracking): Theoretical background, system architecture and application scenarios. European Journal of open, Distance and E-learning (EURODL), 2.
Guyon, I., & Elisseeff, A. (2003). An introduction to variable and feature selection. The Journal of Machine Learning Research, 3, 1157-1182.
Hagan, M. T., & Menhaj, M. B. (1994). Training feedforward networks with the Marquardt algorithm. Neural Networks, IEEE Transactions on, 5(6), 989-993.
Halin, N., Marsh, J. E., Haga, A., Holmgren, M., & Sörqvist, P. (2014a). Effects of speech on proofreading: can task-engagement manipulations shield against distraction? Journal of Experimental Psychology: Applied, 20(1), 69.
Halin, N., Marsh, J. E., Hellman, A., Hellström, I., & Sörqvist, P. (2014b). A shield against distraction. Journal of Applied Research in Memory and Cognition, 3(1), 31- 36.
Hansen, L. K., & Salamon, P. (1990). Neural network ensembles. IEEE transactions on pattern analysis and machine intelligence, 12(10), 993-1001.
Harp, S. F., & Mayer, R. E. (1998). How seductive details do their damage: A theory of cognitive interest in science learning. Journal of Educational Psychology, 90(3), 414.
Henderson, J. M. (2003). Human gaze control during real-world scene perception. Trends in cognitive sciences, 7(11), 498 - 504.
Hess, E. H., & Polt, J. M. (1964). Pupil size in relation to mental activity during simple problem-solving. Science, 143(3611), 1190-1192.
Hornof, A. J., & Halverson, T. (2002). Cleaning up systematic error in eye-tracking data by using required fixation locations. Behavior Research Methods, Instruments, & Computers, 34(4), 592-604.
187
References
 Houts, P. S., Doak, C. C., Doak, L. G., & Loscalzo, M. J. (2006). The role of pictures in improving health communication: a review of research on attention, comprehension, recall, and adherence. Patient education and counseling, 61(2), 173- 190.
Howland, J. L., & Moore, J. L. (2002). Student Perceptions as Distance Learners in Internet-Based Courses. Distance Education, 23(2), 183-195.
Huey, E. B. (1968). The Psychology & Pedagogy of Reading. Cambridge: MIT Press. Hyona, J., Lorch Jr, R. F., & Rinck, M. (2003). Chapter 16 - Eye Movement Measures to Study Global Text Processing. In J. Hyona, R. Radach, R. R. H. DeubelA2 - J. Hyona & H. Deubel (Eds.), The Mind's Eye (pp. 313-334). Amsterdam: North-
Holland.
Hyrskykari, A. (2006). Utilizing eye movements: Overcoming inaccuracy while
tracking the focus of attention during reading. Computers in human behavior,
22(4), 657-671.
Hyrskykari, A., Majaranta, P., Aaltonen, A., & Räihä, K.-J. (2000). Design issues of
iDICT: a gaze-assisted translation aid. In Proceedings of the 2000 symposium on Eye
tracking research & applications. (pp. 9-14). ACM.
Initiative, C. C. S. S. (2010). Appendix A: Research supporting key elements of the
standards. Common Core State Standards for English language arts & literacy in
history/social studies, science, and technical subjects.
Initiative, C. C. S. S. (2012). Common core state standards for English language arts &
literacy in history/social studies, science, and technical subjects: Common Core
Standards Initiative.
Iqbal, S. T., Adamczyk, P. D., Zheng, X. S., & Bailey, B. P. (2005). Towards an index
of opportunity: understanding changes in mental workload during task execution. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (pp. 311-320). ACM.
Iqbal, S. T., & Bailey, B. P. (2004, October 6–9). Using Eye Gaze Patterns to Identify User Tasks. In Grace Hopper Celebration of Women in Computing 2004, (pp. 5-10). Iqbal, S. T., Zheng, X. S., & Bailey, B. P. (2004). Task-Evoked Pupillary Response to
Mental Workload in Human-Computer Interaction. In CHI '04 extended abstracts
on Human factors in computing systems (pp 1477-1480). ACM.
Isokoski, P., Joos, M., Spakov, O., & Martin, B. (2009). Gaze controlled games.
Universal Access in the Information Society, 8(4), 323-337.
Itti, L., & Baldi, P. (2009). Bayesian surprise attracts human attention. Vision Research,
49(10), 1295 - 1306.
Jacob, R. J., & Karn, K. S. (2003). Eye tracking in human-computer interaction and
usability research: Ready to deliver the promises. Mind, 2(3), 4.
Jacobsen, W. C., & Forste, R. (2011). The wired generation: Academic and social outcomes of electronic media use among university students. Cyberpsychology,
Behavior, and Social Networking, 14(5), 275-280.
Jain, A. K., Mao, J., & Mohiuddin, K. (1996). Artificial neural networks: A tutorial.
Computer, 29(3), 31-44.
Jaques, N., Conati, C., Harley, J. M., & Azevedo, R. (2014). Predicting affect from
gaze data during interaction with an intelligent tutoring system. In Proceedings of the International Conference on Intelligent Tutoring Systems. (pp. 29-38). Springer International Publishing.
 188
References
 Jarodzka, H., Balslev, T., Holmqvist, K., Nyström, M., Scheiter, K., Gerjets, P., & Eika, B. (2010). Learning perceptual aspects of diagnosis in medicine via eye movement modeling examples on patient video cases. In S. Ohlsson & R. Catrambone (Eds.), Cognition in flux: Proceedings of the 32nd Annual Meeting of the Cognitive Science Society (pp. 1703–1708). Austin: Cognitive Science Society.
Jarodzka, H., van Gog, T., Dorr, M., Scheiter, K., & Gerjets, P. (2013). Learning to see: Guiding students' attention via a model's eye movements fosters learning. Learning and Instruction, 25, 62-70.
Kahneman, D. (1973). Attention and Effort: Prentice-Hall.
Kahneman, D., & Beatty, J. (1966). Pupil Diameter and Load on Memory. Science,
154(3756), 1583-1585.
Kahneman, D., Tursky, B., Shapiro, D., & Crider, A. (1969). Pupillary, heart rate, and
skin resistance changes during a mental task. Journal of Experimental Psychology,
79(1, pt. 1), 164.
Kang, H. (2014). Understanding online reading through the eyes of first and second
language readers: An exploratory study. Computers & Education, 73, 1-8.
Kardan, S., & Conati, C. (2012). Exploring gaze data for determining user learning with an interactive simulation. In Proceedings of the International Conference on User Modeling, Adaptation, and Personalization (pp. 126-138). Springer Berlin
Heidelberg.
Kardan, S., & Conati, C. (2013). Comparing and combining eye gaze and interface
actions for determining user learning with an interactive simulation. In
Proceedings of the International Conference on User Modeling, Adaptation, and
Personalization (pp. 215-227). Springer Berlin Heidelberg.
Kardan, S., & Conati, C. (2015). Providing Adaptive Support in an Interactive
Simulation for Learning: An Experimental Evaluation. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. (pp. 3671-3680). ACM.
Kareal, F., & Klema, J. (2006). Adaptivity in e-learning. In A. Méndez-Vilas, A. Solano, J. Mesa and JA Mesa: Current Developments in Technology-Assisted Education, 1, 260- 264.
Katidioti, I., Borst, J. P., & Taatgen, N. A. (2014). What happens when we switch tasks: Pupil dilation in multitasking. Journal of Experimental Psychology: Applied, 20(4), 380.
Keenan, J. M., Betjemann, R. S., & Olson, R. K. (2008). Reading comprehension tests vary in the skills they assess: Differential dependence on decoding and oral comprehension. Scientific Studies of Reading, 12(3), 281-300.
Kim, J., Thomas, P., Sankaranarayana, R., & Gedeon, T. (2012). Comparing scanning behaviour in web search on small and large screens. In Proceedings of the Seventeenth Australasian Document Computing Symposium (pp. 25-30). ACM.
Kim, J., Thomas, P., Sankaranarayana, R., Gedeon, T., & Yoon, H. J. (2015). Eye- tracking analysis of user behavior and performance in web search on large and small screens. Journal of the Association for Information Science and Technology, 66(3), 526-544.
Kincaid, J. P., Fishburne Jr, R. P., Rogers, R. L., & Chissom, B. S. (1975). Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel: DTIC Document.
 189
References
 Kintsch, W., & Rawson, K. A. (2005). Comprehension. In M. J. Snowling & C. Hulme (Eds.), The Science of Reading: A Handbook: Blackwell Publishing.
Kirschner, P. A., & Karpinski, A. C. (2010). Facebook® and academic performance. Computers in human behavior, 26(6), 1237-1245.
Klingner, J., Kumar, R., & Hanrahan, P. (2008). Measuring the task-evoked pupillary response with a remote eye tracker. In Proceedings of the 2008 symposium onEye tracking research & applications. (pp. 69-72). ACM.
Kozek, K. K. (1997). Classification of eye tracking data using hidden markov models (Honours thesis). University of New South Wales, NSW, Australia.
Kruger, J., & Dunning, D. (1999). Unskilled and unaware of it: how difficulties in recognizing one's own incompetence lead to inflated self-assessments. Journal of personality and social psychology, 77(6), 1121.
Lach, P. (2013). Intelligent Tutoring Systems Measuring Student’s Effort During Assessment. InCanadian Conference on Artificial Intelligence (pp. 346-351): Springer.
Lallé, S., Conati, C., & Carenini, G. (2016). Predicting Confusion in Information Visualization from Eye Tracking and Interaction Data. In Proceedings of the 25th International Joint Conference in Artificial Intelligence.
Lallé, S., Toker, D., Conati, C., & Carenini, G. (2015). Prediction of Users' Learning Curves for Adaptation while Using an Information Visualization. In Proceedings of the 20th International Conference on Intelligent User Interfaces. (pp. 357-368). ACM.
Lankford, C. (2000). Effective eye-gaze input into windows. In Proceedings of the 2000 symposium on Eye tracking research & applications. (pp. 23-27). ACM.
Little, J. L., & Bjork, E. L. (2012). Pretesting with multiple-choice questions facilitates learning. In Proceedings of the Annual Meeting of the Cognitive Science Society. (pp. 23-27). ACM.
Liu, Z. (2005). Reading behavior in the digital environment: Changes in reading behavior over the past ten years. Journal of Documentation, 61(6), 700-712.
Liversedge, S. P., & Findlay, J. M. (2000). Saccadic eye movements and cognition. Trends in cognitive sciences, 4(1), 6-14.
Lizzio, A., Wilson, K., & Simons, R. (2002). University students' perceptions of the learning environment and academic outcomes: implications for theory and practice. Studies in Higher Education, 27(1), 27-52.
Loboda, T. D., Brusilovsky, P., & Brunstein, J. (2011). Inferring word relevance from eye-movements of readers. In Proceedings of the 16th international conference on Intelligent user interfaces. (pp. 175-184). ACM.
Longmore, M. A., Dunn, D., & Jarboe, G. R. (1996). Learning by doing: Group projects in research methods classes. Teaching Sociology, 84-91.
Maglio, P. P., & Campbell, C. S. (2003). Attentive agents. Communications of the ACM, 46(3), 47-51.
Mangen, A., Walgermo, B. R., & Brønnick, K. (2013). Reading linear texts on paper versus computer screen: Effects on reading comprehension. International Journal of Educational Research, 58(0), 61-68.
Mansfield, J. S., Legge, G. E., & Bane, M. C. (1996). Psychophysics of reading. XV: Font effects in normal and low vision. Investigative Ophthalmology & Visual Science, 37(8), 1492-1501.
 190
References
 Marshall, C. C., & Bly, S. (2005). Turning the page on navigation. In Proceedings of the 5th ACM/IEEE-CS Joint Conference on Digital Libraries, (JCDL'05). (pp. 225-234). IEEE.
Martinez-Conde, S. (2006). Fixational eye movements in normal and pathological vision. Progress in brain research, 154, 151-176.
Martínez-Gómez, P., & Aizawa, A. (2014). Recognition of understanding level and language skill using measurements of reading behavior. In Proceedings of the 19th international conference on Intelligent User Interfaces. (pp. 95-104). ACM.
Mason, C., & Kandel, E. R. (1991). Central visual pathways. Principles of neural science, 3, 420-439.
Mason, L., Pluchino, P., & Tornatora, M. C. (2015). Eye-movement modeling of integrative reading of an illustrated text: Effects on processing and learning. Contemporary Educational Psychology, 41, 172-187.
Mayer, R. E. (1999). Research-based principles for the design of instructional messages: The case of multimedia explanations. Document design, 1(1), 7-19.
Mayer, R. E., Heiser, J., & Lonn, S. (2001). Cognitive constraints on multimedia learning: When presenting more material results in less understanding. Journal of Educational Psychology, 93(1), 187.
McConkie, G. W., Kerr, P. W., Reddix, M. D., & Zola, D. (1988). Eye movement control during reading: I. The location of initial fixations on words. Vision Research, 28(10), 1107-1118.
McConkie, G. W., Kerr, P. W., Reddix, M. D., & Zola, D. (1989). Eye movement control during reading: II. Frequency of refixating a word. Perception & Psychophysics, 46(3), 245-253.
McConkie, G. W., & Rayner, K. (1975). The span of the effective stimulus during a fixation in reading. Attention, Perception, & Psychophysics, 17(6), 578-586.
McKay, D. (2011). A jump to the left (and then a step to the right): reading practices within academic ebooks. In Proceedings of the 23rd Australian Computer-Human Interaction Conference, Canberra, Australia. (pp. 202-210). ACM.
McNamara, D. S., Graesser, A. C., McCarthy, P. M., & Cai, Z. (2014). Automated evaluation of text and discourse with Coh-Metrix: Cambridge University Press.
McNamara, D. S., Louwerse, M. M., Cai, Z., & Graesser, A. (2013). Coh-Metrix version 3.0. Retrieved 30th July 2015, from http://cohmetrix.com
Mehigan, T. (2014). Chapter Four Assessing Eye-Tracking Technology For Learning- Style Detection. In Adaptive Game-Based Learning Tracey Mehigan And Ian Pitt. Game-Based Learning: Challenges and Opportunities, 77.
Mehigan, T., & Pitt, I. (2013). Intelligent mobile learning systems for learners with style. In Tools for Mobile Multimedia Programming and Development (May 2013), D. Tjondronegoro, Ed., IGI-Global, 131-149.
Mehigan, T. J. (2013). Automatic detection of learner-style for adaptive eLearning. Mehigan, T. J., Barry, M., Kehoe, A., & Pitt, I. (2011). Using eye tracking technology to identify visual and verbal learners. In Proceedings of the 2011 IEEE International
Conference on Multimedia and Expo (ICME). (pp. 1-6). IEEE.
Memmert, D. (2006). The effects of eye movements, age, and expertise on
inattentional blindness. Consciousness and Cognition, 15(3), 620-627.
 191
References
 Mendis, B. S. U., & Gedeon, T. D. (2008, 1-6 June 2008). A comparison: Fuzzy signatures and Choquet Integral. In Proceedings of the IEEE International Conference on Fuzzy Systems, 2008 (FUZZ-IEEE 2008). (pp. 1464-1471). IEEE.
Merten, C., & Conati, C. (2006). Eye-tracking to model and adapt to user meta- cognition in intelligent learning environments. In Proceedings of the 11th international conference on Intelligent user interfaces. (pp. 39-46). ACM.
Miller, G. A. (1956). The magical number seven, plus or minus two: Some limits on our capacity for processing information. Psychological Review, 63(2), 81.
Møller, M. F. (1993). A scaled conjugate gradient algorithm for fast supervised learning. Neural networks, 6(4), 525-533.
Moresi, S., Adam, J. J., Rijcken, J., Van Gerven, P. W. M., Kuipers, H., & Jolles, J. (2008). Pupil dilation in response preparation. International Journal of Psychophysiology, 67(2), 124-130.
Morimoto, C. H., & Mimica, M. R. M. (2005). Eye gaze tracking techniques for interactive applications. Computer Vision and Image Understanding, 98(1), 4-24.
Murata, A. (2006). Eye-gaze input versus mouse: Cursor control as a function of age. International Journal of Human-Computer Interaction, 21(1), 1-14.
Nugrahaningsih, N., Porta, M., & Ricotti, S. (2013). Gaze behavior analysis in multiple-answer tests: An Eye tracking investigation. In Proceedings of the International Conference on Information Technology Based Higher Education and Training (ITHET), 2013. (pp. 1-6). IEEE.
O'Hara, K., & Sellen, A. (1997). A comparison of reading paper and on-line documents. In Proceedings of the ACM SIGCHI Conference on Human factors in computing systems (pp. 335-342). ACM.
O'Regan, J. K. (1981). The convenient viewing position hypothesis Eye movements: Cognition and visual perception (pp. 289-298): Erlbaum.
O'Regan, J. K. (1984). How the Eye Scans Isolated Words. In A. G. Gale & F. Johnson (Eds.), Theoretical and Applied Aspects of Eye Movement Research Selected/Edited Proceedings of The Second European Conference on Eye Movements (Vol. 22, pp. 159 - 168): North-Holland.
Okoso, A., Toyama, T., Kunze, K., Folz, J., Liwicki, M., & Kise, K. (2015). Towards Extraction of Subjective Reading Incomprehension: Analysis of Eye Gaze Features. In Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems, Seoul, Republic of Korea. (pp. 1325-1330). ACM.
Oluleye, B., Armstrong, L., Leng, J., & Diepeveen, D. (2014). A genetic algorithm- based feature selection. British Journal of Mathematics & Computer Science, 4(21), pp. 889-905.
Ozcelik, E., Arslan-Ari, I., & Cagiltay, K. (2010). Why does signaling enhance multimedia learning? Evidence from eye movements. Computers in human behavior, 26(1), 110-117.
Paas, F., Renkl, A., & Sweller, J. (2004). Cognitive load theory: Instructional implications of the interaction between information structures and cognitive architecture. Instructional science, 32(1), 1-8.
Paramythis, A., & Loidl-Reisinger, S. (2003). Adaptive learning environments and e- learning standards. In Proceedings of the Second European Conference on e-Learning. (Vol. 1, pp. 369-379).
192
References
 Peterson, L. E. (2009). K-nearest neighbor. Scholarpedia, 4(2), 1883.
Pollatsek, A., & Rayner, K. (2009). Reading. In L. R. Squire (Ed.), Encyclopedia of
Neuroscience. Oxford: Academic Press.
Pomplun, M., & Sunkara, S. (2003). Pupil dilation as an indicator of cognitive
workload in human-computer interaction. In Proceedings of the International
Conference on HCI. (pp. 542-546).
Poole, A., & Ball, L. (2005). Eye Tracking in Human-Computer Interaction and
Usability Research: Current Status and Future Prospects. In C. Ghaoui (Ed.),
Encyclopedia of Human-Computer Interaction. Pennsylvania: Idea Group, Inc.
Porta, M. (2008). Implementing eye-based user-aware e-learning. In Proceedings of the CHI'08 Extended Abstracts on Human Factors in Computing Systems. (pp. 3087-
3092). ACM.
Prusty, B. G., & Russell, C. (2011, 21 - 26 August 2011). Engaging students in
learning threshold concepts in engineering mechanics: adaptive eLearning tutorials. In Proceedings of the International Conference on Engineering Education (ICEE2011), University of Ulster, Belfast, Northern Ireland, UK.
Purves, D., Augustine, G. J., Fitzpatrick, D., Katz, L. C., LaMantia, A.-S., McNamara, J. O., & Williams, S. M. (2001). Types of eye movements and their functions. Neuroscience. 2nd edition: Sunderland (MA): Sinauer Associates.
Quinlan, J. R. (1986). Induction of decision trees. Machine Learning, 1(1), 81-106. Quinlan, J. R. (2014). C4. 5: programs for machine learning: Elsevier.
Ramakrisnan, P., Jaafar, A., Razak, F. H. A., & Ramba, D. A. (2012). Evaluation of
user Interface Design for Leaning Management System (LMS): Investigating Student's Eye Tracking Pattern and Experiences. Procedia-Social and Behavioral Sciences, 67, 527-537.
Rayner, K. (1998). Eye movements in reading and information processing: 20 years of research. Psychological Bulletin, 372-422.
Rayner, K. (2009). Eye movements and attention in reading, scene perception, and visual search. The quarterly journal of experimental psychology, 62(8), 1457-1506.
Rayner, K., & Bertera, J. H. (1979). Reading without a fovea. Science, 206(4417), 468- 469.
Rayner, K., Chace, K. H., Slattery, T. J., & Ashby, J. (2006). Eye movements as reflections of comprehension processes in reading. Scientific Studies of Reading, 10(3), 241-255.
Rayner, K., & McConkie, G. W. (1976). What guides a reader's eye movements? Vision Research, 16(8), 829 - 837.
Reichle, E. D., Pollatsek, A., Fisher, D. L., & Rayner, K. (1998). Toward a model of eye movement control in reading. Psychological Review, 105(1), 125-157.
Reichle, E. D., Pollatsek, A., & Rayner, K. (2006). E-Z Reader: A cognitive-control, serial-attention model of eye-movement behavior during reading. Cognitive Systems Research, 7(1), 4-22.
Reichle, E. D., Rayner, K., & Pollatsek, A. (1999). Eye movement control in reading: Accounting for initial fixation locations and refixations within the EZ Reader model. Vision Research, 39(26), 4403-4411.
Reichle, E. D., Rayner, K., & Pollatsek, A. (2003). The EZ Reader model of eye- movement control in reading: Comparisons to other models. Behavioral and brain sciences, 26(4), 445-476.
193
References
 Reichle, E. D., Rayner, K., & Pollatsek, A. (2012). Eye movements in reading versus nonreading tasks: Using EZ Reader to understand the role of word/stimulus familiarity. Visual cognition, 20(4-5), 360-390.
Reichle, E. D., Warren, T., & McConnell, K. (2009). Using EZ Reader to model the effects of higher level language processing on eye movements during reading. Psychonomic Bulletin & Review, 16(1), 1-21.
Rho, Y. J., & Gedeon, T. D. (2000). Academic articles on the web: reading patterns and formats. International Journal of Human-Computer Interaction, 12(2), 219-240. Robinson, P. (2007). Task complexity, theory of mind, and intentional reasoning:
Effects on L2 speech production, interaction, uptake and perceptions of task difficulty. IRAL-International Review of Applied Linguistics in Language Teaching, 45(3), 193-213.
Rockinson- Szapkiw, A. J., Courduff, J., Carter, K., & Bennett, D. (2013). Electronic versus traditional print textbooks: A comparison study on the influence of university students' learning. Computers & Education, 63(0), 259-266.
Rokach, L. (2010). Ensemble-based classifiers. Artificial Intelligence Review, 33(1), 1- 39.
Rokach, L., & Maimon, O. (2005). Clustering methods Data mining and knowledge discovery handbook (pp. 321-352): Springer.
Romer, D. (1993). Do Students Go to Class? Should They? The Journal of Economic Perspectives, 7(3), 167-174.
Rosch, J. L., & Vogel-Walcutt, J. J. (2013). A review of eye-tracking applications as tools for training. Cognition, technology & work, 15(3), 313-327.
Rousseeuw, P. J. (1987). Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. Journal of computational and applied mathematics, 20, 53-65.
Russell, S. J., Norvig, P., Canny, J. F., Malik, J. M., & Edwards, D. D. (2003). Artificial intelligence: a modern approach (Vol. 2): Prentice hall Upper Saddle River.
Salojarvi, J., Puolamaki, K., Simola, J., Kovanen, L., Kojo, I., & Kaski, S. (2005). Inferring relevance from eye movements: Feature extraction: In Workshop at NIPS 2005, in Whistler, BC, Canada, on December 10, 2005.(p. 45).
Salvucci, D. D. (1999). Inferring intent in eye-based interfaces: tracing eye movements with process models. In Proceedings of the SIGCHI conference on Human factors in computing systems: the CHI is the limit (pp. 254-261). ACM.
Salvucci, D. D., & Anderson, J. R. (1998). Tracing eye movement protocols with cognitive process models. In Proceedings of the Twentieth Annual Conference of the Cognitive Science Society (pp 923-928).
Salvucci, D. D., & Anderson, J. R. (2001). Automated Eye-Movement Protocol Analysis. Human–Computer Interaction, 16(1), 39-86.
Salvucci, D. D., & Goldberg, J. H. (2000). Identifying fixations and saccades in eye- tracking protocols. In Proceedings of the 2000 symposium on Eye tracking research & applications (pp. 71-78). ACM.
Sanchez, C. A., & Wiley, J. (2006). An examination of the seductive details effect in terms of working memory capacity. Memory & cognition, 34(2), 344-355.
Satterthwaite, T. D., Green, L., Myerson, J., Parker, J., Ramaratnam, M., & Buckner, R. L. (2007). Dissociable but inter-related systems of cognitive control and
194
References
 reward during decision making: Evidence from pupillometry and event-related
fMRI. Neuroimage, 37(3), 1017-1031.
Scherr, K. C., Agauas, S. J., & Ashby, J. (2015). The Text Matters: Eye Movements
Reflect the Cognitive Processing of Interrogation Rights. Applied Cognitive
Psychology, 30, 234–41.
Schroder, M., Bogdan, M., Hinterberger, T., & Birbaumer, N. (2003). Automated
EEG feature selection for brain computer interfaces. In Proceedings of the First International IEEE EMBS Conference on Neural Engineering, 2003. (pp. 626-629). IEEE.
Schwarz, U., & Schmückle, T. (2002). Cognitive Eyes. Schweizer Archiv Für Neurologie Und Psychiatrie, 153(4), 175-179.
Seiffert, C., Khoshgoftaar, T. M., Van Hulse, J., & Napolitano, A. (2010). RUSBoost: A hybrid approach to alleviating class imbalance. Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on, 40(1), 185-197.
Sharma, N., & Gedeon, T. (2011). Stress classification for gender bias in reading. In Proceedings of Neural Information Processing. (pp. 348-355). Springer Berlin Heidelberg.
Sharma, N., & Gedeon, T. (2012). Artificial neural network classification models for stress in reading. In Proceedings of Neural Information Processing. (pp. 388-395). Springer Berlin Heidelberg.
Sharma, N., & Gedeon, T. (2013a). Computational Models of Stress in Reading Using Physiological and Physical Sensor Data. InPacific-Asia Conference on Knowledge Discovery and Data Mining (pp. 111-122). Springer.
Sharma, N., & Gedeon, T. (2013b). Hybrid genetic algorithms for stress recognition in reading. In European Conference on Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics. (pp. 117-128). Springer Berlin Heidelberg.
Sharmin, S., Spakov, O., & Raiha, K.-J. (2012). The effect of different text presentation formats on eye movement metrics in reading. Journal of Eye Movement Research, 5(3), 1-9.
Shibata, H., Takano, K., Omura, K., & Tano, S. i. (2015). Page Navigation on Paper Books and Electronic Media in Reading to Answer Questions. In Proceedings of the Annual Meeting of the Australian Special Interest Group for Computer Human Interaction, Parkville, VIC, Australia. (pp. 526-534). ACM.
Sibert, J. L., Gokturk, M., & Lavine, R. A. (2000). The reading assistant: eye gaze triggered auditory prompting for reading remediation. In Proceedings of the 13th annual ACM symposium on User interface software and technology, San Diego, California, United States. (pp. 101-107). ACM.
Sibley, C., Coyne, J., & Baldwin, C. (2011). Pupil Dilation as an Index of Learning. In Proceedings of the human factors and ergonomics society 55th Annual meeting, (Vol. 55, No. 1, pp. 237-241). SAGE Publications.
Siedlecki, W., & Sklansky, J. (1989). A note on genetic algorithms for large-scale feature selection. Pattern Recognition Letters, 10(5), 335-347.
Simola, J., Salojärvi, J., & Kojo, I. (2008). Using hidden Markov model to uncover processing states from eye movements in information search tasks. Cognitive Systems Research, 9(4), 237 - 251.
Simons, D. J., & Chabris, C. F. (1999). Gorillas in our midst: Sustained inattentional blindness for dynamic events. Perception, 28(9), 1059-1074.
 195
References
 Simons, D. J., & Levin, D. T. (1998). Failure to detect changes to people during a real- world interaction. Psychonomic Bulletin & Review, 5(4), 644-649.
Snow, C. E. (2002). Reading for understanding: Toward an R&D program in reading comprehension: Rand Corporation.
Sörqvist, P., Halin, N., & Hygge, S. (2010). Individual differences in susceptibility to the effects of speech on reading comprehension. Applied Cognitive Psychology, 24(1), 67-76.
Spada, D., Sánchez-Montañés, M., Paredes, P., & Carro, R. M. (2008). Towards inferring sequential-global dimension of learning styles from mouse movement patterns. In Proceedings of the Adaptive Hypermedia and Adaptive Web-Based Systems. (pp. 337-340). Springer Berlin Heidelberg.
Sparrow, B., Liu, J., & Wegner, D. M. (2011). Google Effects on Memory: Cognitive Consequences of Having Information at Our Fingertips. Science, 333(6043), 776- 778.
Specht, M., Kravcik, M., Klemke, R., Pesin, L., & Hüttenhain, R. (2006). Adaptive learning environment for teaching and learning in WINDS. In Proceedings of the Adaptive Hypermedia and Adaptive Web-Based Systems (pp. 572-575). Springer Berlin Heidelberg.
Staub, A., & Rayner, K. (2007). Eye movements and on-line comprehension processes. The Oxford handbook of psycholinguistics, 327, 342.
Steichen, B., Conati, C., & Carenini, G. (2014). Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities from Eye Gaze Data. ACM Transactions on Interactive Intelligent Systems, 4(2), 1-29.
Sung, E., & Mayer, R. E. (2012). When graphics improve liking but not learning from online lessons. Computers in human behavior, 28(5), 1618-1625.
Surjono, H. D. (2011). The design of adaptive e-Learning system based on student’s learning styles. International Journal of Computer Science and Information Technology (IJCSIT), 2(5), 2350-2353.
Surjono, H. D. (2014). The Evaluation of a Moodle Based Adaptive e-Learning System. International Journal of Information and Education Technology, 4(1): 89.
Sweller, J., Van Merrienboer, J. J., & Paas, F. G. (1998). Cognitive architecture and instructional design. Educational psychology review, 10(3), 251-296.
Toker, D., & Conati, C. (2014). Eye Tracking to Understand User Differences in Visualization Processing with Highlighting Interventions. In V. Dimitrova, T. Kuflik, D. Chin, F. Ricci, P. Dolog, & G.-J. Houben (Eds.), User Modeling, Adaptation, and Personalization: 22nd International Conference, UMAP 2014, Aalborg, Denmark, July 7-11, 2014. Proceedings (pp. 219-230). Cham: Springer International Publishing.
Traphagan, T., Kucsera, J., & Kishi, K. (2010). Impact of class lecture webcasting on attendance and learning. Educational Technology Research and Development, 58(1), 19-37.
Tsai, M.-J., Hou, H.-T., Lai, M.-L., Liu, W.-Y., & Yang, F.-Y. (2012). Visual attention for solving multiple-choice science problem: An eye-tracking analysis. Computers & Education, 58(1), 375-385.
Underwood, G., & Batt, V. (1996). Reading and Understanding. Massachusetts, USA: Blackwell Publishers.
196
References
 Underwood, G., Hubbard, A., & Wilkinson, H. (1990). Eye fixations predict reading comprehension: The relationships between reading skill, reading speed, and visual inspection. Language and speech, 33(1), 69-81.
Victor, T. W., Harbluk, J. L., & Engström, J. A. (2005). Sensitivity of eye-movement measures to in-vehicle task difficulty. Transportation Research Part F: Traffic Psychology and Behaviour, 8(2), 167-190.
Vo, T., Mendis, B. S. U., & Gedeon, T. D. (2010). Gaze Patterns and Reading Comprehension. In International Conference on Neural Information Processing (pp. 124-131). Springer Berlin Heidelberg.
Waniek, J., & Ewald, K. (2008). Cognitive costs of navigation aids in hypermedia learning. Journal of Educational Computing Research, 39(2), 185-204.
Welsh, E. T., Wanberg, C. R., Brown, K. G., & Simmering, M. J. (2003). E-learning: emerging uses, empirical results and future directions. International Journal of Training and Development, 7(4), 245-258.
Whitley, D. (1994). A genetic algorithm tutorial. Statistics and Computing, 4(2), 65-85. Whitley, D. (2001). An overview of evolutionary algorithms: practical issues and
common pitfalls. Information and software technology, 43(14), 817-831.
Woodfield, R., Jessop, D., & McMillan, L. (2006). Gender differences in
undergraduate attendance rates. Studies in Higher Education, 31(1), 1-22.
Xu, R., & Wunsch, D. (2008). Clustering (Vol. 10): John Wiley & Sons.
Yang, J., & Honavar, V. (1998). Feature subset selection using a genetic algorithm. In
Feature extraction, construction and selection (pp. 117-136): Springer.
Yarbus, A. (1967). Eye Movements and Vision. New York: Plenum Press.
Yatabe, K., Pickering, M. J., & McDonald, S. A. (2009). Lexical processing during
saccades in text comprehension. Psychonomic Bulletin and Review, 16, 62-66. Zekveld, A. A., Heslenfeld, D. J., Johnsrude, I. S., Versfeld, N. J., & Kramer, S. E. (2014). The eye as a window to the listening brain: neural correlates of pupil size
as a measure of cognitive listening load. Neuroimage, 101, 76-86.
Zhang, L., Liu, Z., & Ni, J. (2013). Feature-Based Assessment of Text Readability. In 2013 Seventh International Conference on Internet Computing for Engineering and
Science (pp. 51-54). IEEE.
Zhu, J., Zou, H., Rosset, S., & Hastie, T. (2009). Multi-class adaboost. Statistics and its
Interface, 2(3), 349-360.
197
References
 198
 pendix A. Materials for Experiment 1 - Eye Gaze in eLearning Environments
This appendix includes the supporting documentation and resources that were used for the first experiment explained in this thesis – Eye Gaze in eLearning Environments. The resources included the participant information sheet, the consent form, the texts and questions used for the experiment, and the pre- and post-experiment questionnaires.
Ethics approval was sought from the Australian National University Research Ethics Committee before the experiment was conducted. The experiment was conducted under Human Ethics Protocol 2012/006. The participant information sheet and consent form was designed according to the requirements of the ethics approval.
199
p
Materials for Experiment 1 - Eye Gaze in eLearning Environments
 A.1 Participant Information Sheet
 200
Materials for Experiment 1 - Eye Gaze in eLearning Environments
 A.2 Participant Consent Form
 201
Materials for Experiment 1 - Eye Gaze in eLearning Environments
 A.3 Experiment texts
Original lecture notes written by: Nandita Sharma Tutorial texts written by: Leana Copeland
A.3.1 The World Wide Web
The World Wide Web (WWW), or colloquially the Web, is a widely used information system that enables locating and viewing of a variety of multimedia based files including text documents, audio, visual and graphic files.
Sir Tim Berners-Lee wrote a proposal in 1989 based on earlier concepts of hypertext systems for what eventually became the Web. It was Berners-Lee that built the first web browser, web server and web pages, which are the main components of the Web, and he is now the Director of theWeb Consortium (W3C), which is the main international standards organization for the Web.
The Web is essentially a big graph made up of billions of web pages and hyperlinks. A Web page is a document or information that can be viewed using a web browser. Web pages can contain content such as text, images, videos, audio, as well as hyperlinks, which enable navigation to other Web pages. Web pages are generally formatted in HyperText Markup Language (HTML). HTML provides the ability to embed images, create interactive forms, and a means of structuring documents into headings, paragraphs, lists, links, and so on. Although some formatting and presentation of information can be handled by HTML, it is generally the Cascading Style Sheets (CSS) that are used to define the appearance and layout of the web pages.
Scripts can be embedded into HTML that affect the behaviour of a Web page. This allows the content of Web pages to be dynamically generated. These are termed dynamic Web pages and refer to Web content that is based on user input. Examples of these types of Web pages are on websites for flight status or stock exchange rates. Usually dynamic Web pages are assembled at the time of a request from a browser and typically their URL has a "?" character in it. Scripts to create dynamic Web pages can be written in languages such as Javascript and Ajax.
Web pages are requested and served from Web servers using the Hypertext Transfer Protocol (HTTP). For example, when you enter a Uniform Resource Locator (URL) in your browser, this actually sends an HTTP request command to a Web server directing it to fetch and transmit the requested Web page. HTTP is an application layer protocol designed within the framework of the Internet protocol suite. This means that it presumes there is an underlying transport layer protocol such as the Transmission Control Protocol (TCP).
A.3.2 The Importance of Search Engines
The Web is popular. Every day the number of Web pages on the Web increases. The Indexed Web contained at least 15.2 billion pages as of Wednesday, 13 February, 2013. Similarly, the number of Internet hosts connected to the Internet increases, with close to 1 billion hosts as of July, 2012. The content on the Web is rapidly changing and expanding and there are users of the Web all over the world. This also means that the Web is full of information in different languages. There is no central coordination over content, presentation or location of Web pages. Most web pages are titled by their author and are located on servers with cryptic names. With the vast number of resources that are scattered in an ad hoc way, located in different locations, and in no order, how does anything get found? This is where search engines come in. Web search engines are designed to search the information on the Web based upon keywords that the user enters into their interface and return a list of results referred to as search engine results pages (SERP's). Search engines essentially make the content of the Web accessible and they make the web seem organised to the user.
There are numerous search engines and they are often specialised to perform certain searches. The major search engines are Google, Yahoo!, Bing and Ask. There are also different types of search,
202
Materials for Experiment 1 - Eye Gaze in eLearning Environments
 including text search, audio search, location-based search, image recognition search, barcode search, and many more. Of the uses of the Web, searching the Web is very popular. Other uses of the web include social networking, accessing news, sending and receiving email, online shopping, and many more. However, the fact remains that people often use search engines to first find their way to one or more of these other uses. Furthermore, there is no incentive in creating content on the Web unless it can be easily found.
Other methods of finding Web pages exist, such as web directories, taxonomies and bookmarks, but have not kept up the pace of search engines to perform large and very fast searches of the Web. There are also answer engines that are a type of search engine that answers natural language queries directly by computing an answer from structured data as opposed to returning a list of the most suited web pages for queries.
A.3.3 Brief non-technical History of Search Engines
During the early development of the Web a manual list was kept of the Web servers but as the number of Web servers grew, the central list could not keep up. The first search engine on the Internet was called Archie, which was created in 1990. The name stands for "archive" without the "v" and was a database of file names that could be searched manually rather than be indexed.
The Gopher protocol was created in 1991 by Mark McCahill, which led to two new search programs, Veronica and Jughead. These programs searched the file names and titles stored in Gopher index systems. The Gopher protocol is a TCP/IP application layer protocol designed for distributing, searching, and retrieving documents over the Internet. Gopher was eventually superseded by HTTP.
The W3Catalog was the first primitive search engine for the Web, which was released in 1993. One of the first publicly available crawler-based search engines called WebCrawler was introduced to the Web In 1994. The difference between this search engine and its predecessors was that it allowed users to search for any word in any Web page, which has become the standard for all major search engines today.
There was a rapid emergence of search engines in the 1990's with search engines such as, Yahoo!, Lycos, Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. By the end of this period, search engines had begun to adopt the use of paid placement rankings and the selling of search terms. This move made search engine companies one of the most profitable businesses on the Internet at the time.
The Google search engine rose to prominence around 2000 and has remained the most popular search engine. Up until Google's search engine, the conventional method of ranking search results was by counting the number of times a search term appeared on a web page. However, Google's search engine employed the use of the PageRank algorithm to rank its search results. PageRank is a ranking system where the number of pages and the importance of those pages that linked back to the original site determine a website’s relevance.
In 2012, Google released Open Drive, which is a file search engine that enables files stored in cloud storage that are publically available. Open Drive will return search results from cloud storage content services including Google Drive, Dropbox, SkyDrive, Evernote and Box.
A.3.4 Web Search Basics
A Web search engine is a program that is designed to search for information on the Web for a user query and return a set of results to the user. In short, a Web search engine performs the following tasks: Web crawling, indexing, calculating relevancy and rankings, and serving results back to the user. Web search engines need to store information about a lot of Web pages for effective and efficient search. They get this information by Web crawlers that record information from the HTML of a Web page and follow every link from the Web page. The data collected by the Web crawlers about Web pages they visit get stored in an index database so that it can be used when users make queries.
203
Materials for Experiment 1 - Eye Gaze in eLearning Environments
 To serve its users, Web search engines must take user input in the form of a query, which are usually keywords that they wish to find information or Web sites on. The search engine examines its index and then provides a listing of the most suitable Web pages given the search criteria the user has given as input. The results of a text-based search are the document’s title as well as a short excerpt of text from the page or document, or an image in the case of image search, or a location in the case of location based search.
Of course, just because the keywords that a user has queried appear on a Web page does not mean that it is an appropriate result to return to the user because some other pages may be more relevant or reliable than others. Search engines rank results using different ranking methods before they are returned to the user so that the most relevant results are returned towards the beginning of the list or ranked higher in the search results.
Advertising revenue to some extent supports most commercial search engines and is what made a lot of search engine companies quite profitable. Search engines allow advertisers to pay to have their listings ranked higher in search results. Also, search engines feature related ads next to the search engine results for a query. Every time a user clicks on one of these ads the search engine is paid. This way search engines can maintain their credentials with their users and the advertisers – users get their search results and advertisers have their ads placed towards the best search results.
A.3.5 Web Crawling
Web crawling is the first step that a search engine takes to return results of a search query to a user. This step is invisible and most people do not know that it exists. This is the step in which a search engine identifies that a file or document exists. Simple automated programs or scripts, colloquially called Web spiders and crawlers, perform Web crawling whereby a list of words and notes about where they were found is generated. These Web spiders build lists of words found on Web pages by methodically scanning through web pages and creating an index from the information they scanned. Web crawlers are not only used by search engines, but are used by linguists and market researchers or anyone trying to find information from the Internet in an organised manner.
Web crawlers usually start at popular sites and servers where they index the words on the pages and follow every link within the site. The crawler eventually builds an index based on its own system of weighting. For example, words in titles or headings may be deemed more important. This data is encoded to save space and stored for users to access through search queries.
There are limits to how much a web crawler can download at any one time and given that there is a large amount of rapidly changing data web crawlers have access to, the behaviour of a web crawler can determine how efficient and how up-to-date the information that is collected and stored. There are several policies that contribute to the behaviour of a web crawler. The selection policy of a web crawler determines which pages to download and the re-visit policy determines when the web crawler checks for changes to the pages. The politeness policy determines how the Web crawlers avoid overloading Web sites, and the parallelization policy coordinates distributed Web crawlers.
Different search engines employ Web crawlers that record different types of words on Web pages. Different approaches are usually an attempt to make the spider operate faster, allow users to search more efficiently, or both. For example, some Web crawlers will keep track of the words in the title, sub- headings and links while others will keep track of the 100 most frequently used words on the page. The early Google search engine was built with only a few crawlers that could keep around 300 connections to Web pages open at any one time.
A.3.6 Building an Index
A Web search engine must store the information that is constantly being collected by web crawlers so that it is accessible to users when they make queries. It would be neither computationally efficient nor fast for a search engine to scan every page in its collection of crawled pages. Instead a search engines
204
Materials for Experiment 1 - Eye Gaze in eLearning Environments
 index is a compact storage of Web information that is designed to optimize the speed and performance in finding results for a search query.
There are many challenges in search engine indexing which centre on the fact that the Web has an enormous amount of data that is constantly changing. So indexes must be designed to maintain efficient indexing, fast retrieval and compact storage. This motivates the index policy to consider which pages should be indexed and the extent to which these pages are indexed. Often information such as how many times the word appears on the page, whether the word was just used in a trivial way, and whether there are links from that page to other pages containing the word are stored in the index along with the words and URLs. This additional information is used to assign a weight to each entry in the index that is later used in ranking of the search results. Search engines have different methods of assigning weight to entries but an example is that higher weights are assigned when the word appears in the title, sub-headings or in links of the document. Some search engines store all or part of the source page as well as information about the page whilst others store every single word on the pages they crawl.
There are many factors that affect the design of a search engine's index such as how information is entered into the index and how and if information is compressed or filtering to reduce the storage size. Lookup speed of finding an entry in the index, as well as update and removal speeds are another factor that affect the design of search engine indexes. Furthermore, maintenance and fault tolerance are also considered in designing the index. The method of index storage also plays an important role in how search engines perform indexing and although there are many types of data structures that a search engine could be built from, a common web search engine index structure is the inverted index i.e. a hash table.
A.3.7 User Queries
Once the search engine has built an index of the parts of the web that its web crawlers have explored, users can submit queries to find information from that part of the web. The query submitted to the search engine actually queries the index that was built by the web crawlers. Queries can be quite simple, such as one word, or quite complex to make a query more specific. Boolean operators such as AND, OR and NOT can be used to make a query more specific. The AND operator allows the user to specify that they want all the words joined by the AND to be present in the results. The OR operator allows the user to specify that they want at least one of the words joined by the OR to appear in the results. The NOT operator allows the user to define terms that they do not want to appear in the results. Searches of this kind are termed literal searches because the search engine looks for the words or phrases exactly as they are queried.
There are additional advanced queries that can be made such as searching for an exact word or phrase, which in Google search is denoted by "search query", or finding words similar to a query term, again denoted in Google search as ~query term. There are also wildcard characters that are used as placeholders for unknown terms, i.e. fill in the blank, and are denoted as * in Google search. Furthermore, searching directly within a site or domain directly, which in Google search is denoted query term site: site or domain.
There are searches that are concept-based which involve using statistical analysis on the pages that contain the words or phrases that were queried in order to find the pages that the user would be more interested in. The problem with this approach is that it requires more information to be stored about the crawled pages and the processing time for a search of this type would be longer.
Furthermore, there are natural language based searches that are based on the premise that the user types a question as a query. The question is structured the same as if you were asking another human and hence a natural approach. An example of this is Wolfram Alpha, which takes questions of many different forms, such as mathematical equations, and returns a computed answer rather than a list of results.
205
Materials for Experiment 1 - Eye Gaze in eLearning Environments
 A.3.8 Calculating Relevancy and Rankings
When the user inputs a query to a search engine they expect the most relevant web pages to be returned in the list of search results. However, relevance means more than simply finding a page that contains some keywords, as some pages may be more appropriate, popular, or authoritative than others. Often how useful a search engine is considered is dependent upon the relevance of the search result set it gives back. Search engines risk losing users, to other providers and to offline methods if they cannot provide relevant results, this is why search engines rank search results.
The search results with the highest rankings are deemed most relevant and are presented at the top of the page to the user. There are many factors that affect how the search engine calculates relevance and hence ranking of pages. Some of these factors include: page content, frequency and location of keywords within a page, age of the page, number of pages linking to the page, discovery of additional sites, updates made to indices, changes to the search algorithm, and many more. The rankings each search engines uses are different which is why submitting the same query into several different search engines will return different results. However most search engines have a few things in common such as the more popular a site is the more important it must be, as well as, the location and frequency of keywords on the web pages. So the more popular a site, page or document is, the more valuable the information must be.
Furthermore, adverts are included in the search results, which also must be appropriately matched to what was searched by the user. This means that if a user searches for cars they will be presented with adverts that are about products related to cars and not products relating to boats.
It has long been established that users generally tend to look at the first page of search results and generally gravitate to the top of the first page of search results. A sample of over 8 million clicks showed that over 94% of users clicked on a first page result with the first spot being the clicked the most. This fact motivates search engines to order search results with the “better” pages at the top. However, it also motivates designers of web pages to optimise how search engines can find their web pages.
A.3.9 Search Engine Optimisation
Search engines allow companies or individuals to pay to have their Web pages placed at the top of search results for certain queries. This is not the only way to ensure a web page tops the search results for a given query. The alternative is to use search engine optimisation (SEO) which is the process of tuning a website or web page to rank higher in the search results for certain queries and hence increase visibility and visitors to the site.
SEO considers how search engines work, what people search for, and which search terms are used. The first step of SEO is to get indexed by a leading search engine. These search engines use web crawlers to find pages and offer either free or paid submission of pages. Web crawlers look at a number of different factors when crawling a site so the search engines index not every page. However, web crawlers intentionally avoid some content because the owner has specified for it not to be indexed. This is done through the robots.txt file in the root directory of the domain. Typically pages such as shopping carts and user-specific content are prevented from being indexed because search engines such as Google consider those pages as search spam. Finally, there are a number of ways to increase the visibility of a webpage within the search results, such as by cross-linking web pages on a website to provide more links to most important pages. Other methods include writing content that includes frequently searched keyword phrase because that will make the page or site relevant to a wider variety of search queries. Also, updating content to give additional weight to a site because web crawlers will have to visit the site or page more frequently.
There are two categories of SEO techniques, which are term white hat and black hat SEO. The white hat SEO techniques are ones that are approved and recommended by a search engines guidelines, and involves no deception. Using white hat techniques tends to produce results that will last longer. Black hat SEO techniques are techniques of which search engines do not approve and involve deception. An
206
Materials for Experiment 1 - Eye Gaze in eLearning Environments
 example of black hat SEO is hidden text, which is either text coloured similar to the background or positioned off screen. Once the search engine realises that a site is using black hat techniques it may be banned either temporarily or permanently.
A.4 Web Search Tutorial Quiz
The quiz for the web search tutorial to test understanding consists of 18 questions. There are 2 questions for each tutorial heading/slide, one of which is a multiple-choice question and the second, which is a cloze (fill-in-the-blanks) question.
Questions for “The World Wide Web” Slide:
1. What are Web pages formatted in and what protocol are they transmitted in?
a) Cascading Style Sheet (CSS) and Hypertext Transfer Protocol (HTTP), respectively.
b) HyperText Markup Language (HTML) and Hypertext Transfer Protocol (HTTP),
respectively.
c) JavaScript and Transmission Control Protocol (TCP), respectively.
d) HyperText Markup Language (HTML) and Transmission Control Protocol (TCP),
respectively.
2. Web pages that are generated based upon user input are called
_____________(dynamic/interactive) web pages. These types of web pages have _____________(scripts/forms) embedded into the HTML.
Questions for “The Importance of Search Engines” Slide:
3. Why are Web search engines important?
a) They are the only way to find web pages on the Web.
b) They control the location and presentation of the content on the Web
c) They control the information of the Web
d) They make the content of the Web accessible and seem organised to the user
4. Web search engines are designed to search the information on the Web based upon _____________(keywords/queries/search terms) that the user enters into their interface and return a list of results.
Questions for “Brief non-technical history of Search Engines” Slide:
5. Open Drive is a file search engine that enables files stored in _____________(cloud) storage that are publically available to be searched.
6. The conventional method of ranking results was by counting the number of times a search term appeared on a web page. What changed this?
a) PageRank
b) Web crawlers
c) Archie
d) Gopher Protocol
Questions for “Web Search Basics” Slide:
7. What is the correct order that Web search engines perform their four main tasks:
a) Web crawling; Indexing; serving results; calculating relevancy and rankings
b) Indexing; Web crawling; Serving results; calculating relevancy and rankings
c) Web crawling; Indexing; calculating relevancy and rankings; serving results
d) Indexing; Web crawling; Calculating relevancy and rankings; serving results
207
208
Materials for Experiment 1 - Eye Gaze in eLearning Environments
 8. Web search engines gain _____________(revenue/money/profit) by allowing advertisers to pay to rank their websites higher in search results and by running related ads _____________(next) to the search results.
Questions for “Web Crawling” Slide:
9. Web crawlers are automated programs responsible for methodically scanning through _____________(Web pages/Web sites) and creating an _____________(index) of information so that users can make queries on it later.
10. Which is false:
a) Web crawlers are also called spiders.
b) Web crawlers start at the home page of the search engine. c) Web crawlers build lists of words found on a web page. d) Web crawler behaviour is dictated by a set of policies.
Questions for “Building an Index” Slide:
11. Which of these is not a factor that affects how a search engine's index is designed? a) Relevancy ranking within the index
b) How data is entered into the index
c) The data structure used to build the index
d) Fault tolerance of the index
12. There are many ways to build an index but the main purpose for search engines is to provide
_____________(compact/efficient) storage and _____________(quick/fast/efficient/rapid/optimised) retrieval of the information.
Questions for “User Queries” Slide:
13. When a search engine looks for the words or phrases exactly as they are queried this is an example of:
a) a concept based search
b) a literal search
c) a natural language based search
14. Complex queries to search engines can include _____________(Boolean) operators to make the search more specific.
Questions for “Calculating Relevancy and Rankings” Slide:
15. The search results with the highest rankings are judged most _____________(relevant) to the search query and are presented at the _____________(top) of the page to the user.
16. Why do web search engines rank search results?
a) Search engines rank results based on how much they are paid by website owners, so
they rank the highest paying sites highest.
b) Users want results with a high frequency of query keywords within the web page.
c) Users tend to look more frequently at the top of the search results list.
d) Users don't care how the search results list is ordered, so search engines do not rank
results.
Questions for “Search Engine Optimisation” Slide:
17. Search engine optimisation (SEO) is:
a) the process of making search engines faster.
b) the process of making a web site or page more highly ranked.
Materials for Experiment 1 - Eye Gaze in eLearning Environments
 c) the process of making search engines return more relevant results.
d) The process of making search engines more profitable.
18. There are two types of SEO, white hat SEO are techniques that are _____________(approved
and recommended) by search engines and black hat SEO are techniques that involve _____________(deception).
A.5 Questionnaires
A.5.1 Pre-experiment questionnaire
The pre-study questions that participants were asked:
1. Gender: (Female / Male)
2. Age:
3. Is English your native language? (Yes / No )
4. Do you have any form of dyslexia or difficulties reading? (Yes / No)
5. Do you have normal or corrected to normal vision? (Yes / No )
6. Highest education level: ___________________________________
7. What area did major in or are currently majoring in?
8. Are you studying COMP1710? (Yes / No )
A.5.2 Post-experiment questionnaire
The post-study questions that participants were asked to complete:
1. Were you already familiar with the content you have just read and been quizzed on? (Very Familiar / Familiar / Somewhat Familiar / Not Familiar )
2. Would you find it useful/helpful to be given feedback about the parts of the text that you should re-read before attempting the quiz? (Yes / No / I don’t know )
3. Would you find it useful/helpful to have electronic documents annotated with the areas of the text that you: (tick any relevant) Skimmed / Did not read properly (e.g. mindless reading) / Seemed to have trouble reading (may have not understood the concepts or language used / Read thoroughly / Read normally /Annotate nothing; I don’t think this would be useful/helpful feedback.
4. Do you have any other comments?
209
Materials for Experiment 1 - Eye Gaze in eLearning Environments
 210
 pendix B.
Materials for Experiment 2 - Adaptive eLearning and Digital Images
This appendix includes of the supporting documentation and resources that were used for the Second experiment explained in this thesis – Adaptive eLearning and Digital Images Experiment. The resources included the participant information sheet, the consent form, the run sheet for the experiment, the texts used in the experiment, and the pre-experiment questionnaires.
Ethics approval was sought from the Australian National University Research Ethics Committee before the experiment was conducted. The experiment was conducted under Human Ethics Protocol 2012/006. The participant information sheet and consent form was designed according to the requirements of the ethics approval.
211
p
Materials for Experiment 2 - Adaptive eLearning and Digital Images
 B.1 Participant Information Sheet
 212
Materials for Experiment 2 - Adaptive eLearning and Digital Images
  213
Materials for Experiment 2 - Adaptive eLearning and Digital Images
 B.2 Participant Consent Form
 214
Materials for Experiment 2 - Adaptive eLearning and Digital Images
 B.3 Run sheet for user study
The following document outlines the run sheet for the user study so that the study is consistent no matter who is there.
1. Participant ID’s are assigned as follows, concatenate the following:
a. The first 3 letters of the day, e.g. mon, tue, wed, etc.
b. The first 2 digits of the sign up time (in 12 hour time), e.g. 9am is 09, 12pm is 12, 3pm
is 03
c. Whether it is am or pm
d. The date of the experiment in the form: DDMM
For example, an experiment run on Monday 14th April at 9am is mon09am1404
2. Open FaceLab and use the stereo-head “jointExpSH2”
3. Open EyeWorks Record and set the following:
a. The script file: C:\Users\faceLAB\Documents\My EyeWorks Projects\Leana\Joint
Experiment\HCIExperiment.egs
b. The Output file:
i. location of where to store the data: C:\Users\faceLAB\Documents\My EyeWorks Projects\Leana\Joint Experiment\Data
ii. filename: the above participant ID
4. Log into Wattle and got to the course: Tom Gedeon’s Sandpit; go to the Administration panel and select Users>Enrolled Users. Select Enrol User. Get the participants Uni Id and search for it in the Not Enrolled Users section and enrol the student.
5. Go back to the course’s main page, Turn editing on, and make visible one quiz under each of the sections Part 1, Part 2 and Part 3 in the Topic area Leana and Sabrina’s eLearning User Study. Make visible the quizzes based on the combinations outlined in \Dropbox\JointExperiment\Administration_of_Experiment\Combinations.xlsx and update the spreadsheet to include the participant’s ID in the attendance column.
6. Ask participant to read the Participant Information sheet (\Dropbox\JointExperiment\Administration_of_Experiment\Consent Form.docx) and sign the consent form (\Dropbox\JointExperiment\Administration_of_Experiment\Consent Form.docx)
7. Explain the following to the student:
a. “Have you ever been in front of a Gaze Tracker before?” If No, show the participant
what the tracker is and the video feed.
b. “The gaze tracker has a narrow field of view so throughout the study you must
remain relatively still as your face must remain inside these boxes at all times so that you eye gaze can be constantly monitored throughout the task. Please not get into a position that you will be comfortable to remain in for the next hour and make sure you can reach and keyboard and mouse. Try not to have a really straight back as people always slump their shoulders when they start to relax.” Change the height of the desk to ensure that their face is within the video box.
c. “When we start the experiment the gaze tracker will initialise by locating your pupils, once this has occurred a calibration sequence will begin. This involves a series of 9 red dots appearing on the screen; please look at each dot as it appears.
215
216
Materials for Experiment 2 - Adaptive eLearning and Digital Images
 Once the calibration sequence has completed the experiment script will load. This can take a few moments.
Once the script loads you will be asked a few pre-experiment questions such as your age and gender. Once you have completed the pre-experiment questionnaire the experiment will begin. The experiment is divided into three sections in which you will complete a quiz and then view a set of images. Please keep in mind that you cannot click on the ‘back’ button in the browser to view earlier pages due to experimental constraints.
When you view the images a series of questions will be asked verbally. There are no right or wrong answers to these questions and they are not graded. You will begin by completing the first quiz and then you will view the first set of images. Then you will move onto the second quiz and then view the second set of images. Finally you will complete the last quiz and view the final set of images. You will not be shown the grade that you get for each quiz that you complete, as we do not want to affect your confidence. Note that you cannot go back within the quiz so make sure that you feel confident that you understand the content before pressing the next button. Do you have any questions before we begin?” If yes, answer the questions.
8. Ask the participant to look straight ahead and then press the Start Button in EyeWorks Record.
9. Once the calibration sequence is complete press Enter on the screen with 5 dots on it and then wait for the script to load. Since an IE window will open press the EyeWorks Presenter Icon in the system tray to bring up the start of the script. Hand over control to the participant from this point.
10. Allow the participant participant to complete the questionnaire and then the first quiz. Once they have completed the quiz make sure you click on the EyeWorks Presenter icon in the system tray to bring them back to the script so that they can view the images. Ask the following questions, record answers with iPhone or recording pen and take notes of answers:
a. Two images: vector graphic frogs and raster graphic chameleon
• Interest question “what are the interesting bits in these images” and/or “what in
these images draws your eye?”
• “Of these two images, which is a vector graphic and which is a raster graphic?”
• “Do you believe that the raster graphic has been manipulated?“
b. Two images: one 8-bit b&w and one 24-bit colour image of boy
• Interest question “what are the interesting bits in these images” and/or “what in
these images draws your eye?”
• “Of these two images, which do you think has the highest bit depth?”
• “Do you believe that either of these images has been manipulated?” If
participant answers yes, ask “How do you think it was manipulated?”
c. Two images: one low-res cat and one high-res cat
• Interest question “what are the interesting bits in these images” and/or “what in
these images draws your eye?”
• “Of these two images, which do you think has the highest resolution?”
• “Do you believe that either of these images has been manipulated?” If
participant answers yes, ask “How do you think it was manipulated?”
d. Two images: one low-res cat eyes and one high-res cat eyes.
Materials for Experiment 2 - Adaptive eLearning and Digital Images
 • “This is just a back up image to assist in identifying higher vs lower resolution. Which of these do you think has the highest resolution?
e. One image, coins
• Interest question “what are the interesting bits in these images” and/or “what in
these images draws your eye?”
• “Do you believe that this image has been manipulated?” If participant answers
yes, ask “How do you think it was manipulated?”
11. Once they have finished with the images the participant will return to Wattle and complete the next quiz. Again once they are finished ensure that you click the EyeWorks Presenter icon in the system tray to bring them back to script so that they can view the images. Ask the following questions:
a. One image, watermarked keypad
• Interest question “what are the interesting bits in these images” and/or “what in
these images draws your eye?”
• If they mention the text / water mark, ask “What do you think the text /
watermark is for?
b. One image, James Blundt photo
• Say “this is a screen capture from Facebook. Looking at it, how do you think
Facebook as a company benefits from uploads and discussions like this?”
c. One image, Creative Commons logo
• Question “this is the logo of an organisation, can you tell me who it is and what
they do?”
d. One image, John Howard and image of Queen in media scrum
• Interest question “what are the interesting bits in these images” and/or “what in
these images draws your eye?”
• “Do you believe that this image has been manipulated?” If participant answers
yes, ask “How do you think it was manipulated?”
e. One image, anemone in pond
• Interest question “what are the interesting bits in these images” and/or “what in
these images draws your eye?”
• “Do you believe that this image has been manipulated?” If participant answers
yes, ask “How do you think it was manipulated?”
12. Once they have finished with the images the participant will return to Wattle and complete the next quiz. Again once they are finished ensure that you click the EyeWorks Presenter icon in the system tray to bring them back to script so that they can view the images. Ask the following questions:
a. Two images, ‘Fading Away’ and zebra in clothes shop
• Interest question “what are the interesting bits in these images” and/or “what in
these images draws your eye?”
• “Do you believe that either of these images has been manipulated?” If
participant answers yes, ask “How do you think it was manipulated?”
b. One image, group of girls
• Interest question “what are the interesting bits in these images” and/or “what in
these images draws your eye?”
217
Materials for Experiment 2 - Adaptive eLearning and Digital Images
 • “Do you believe that this image has been manipulated?” If participant answers yes, ask “How do you think it was manipulated?” If participant answers no, say, “Actually, this image has been manipulated; one girl has been spliced in. Can you guess who it is?”
c. One image, missiles
• •
Interest question “what are the interesting bits in these images” and/or “what in these images draws your eye?”
“Do you believe that this image has been manipulated?” If participant answers yes, ask “How do you think it was manipulated?”
If the participant answers no, say “Actually, this image is manipulated; one missile has been added. Can you tell which one?”
d. One image, car cow
• Interest question “what are the interesting bits in these images” and/or “what in
these images draws your eye?”
• “Do you believe that this image has been manipulated?” If participant answers
yes, ask “How do you think it was manipulated?” If the participant answers no, move on.
e. One image, people on pier with ‘jumping from pier’ sign
• Interest question “what are the interesting bits in these images” and/or “what in
these images draws your eye?”
• “Do you believe that this image has been manipulated?” If participant answers
yes, ask “How do you think it was manipulated?” If the participant answers no, move on.”
13. The participant is now complete. Stop the eye tracker and ask if they have any questions.
14. Request the student not discuss the experiment with other students since they may participate in the experiment and should not have knowledge of its contents beforehand.
15. Once the participant has left un-enrol the student from the course and disable all three quizzes.
B.4 Pre-experiment Questionnaire
1. What is your age?
2. Are you enrolled in COMP1710?
3. What are you currently studying and how many years have you been studying in this degree?
4. What is you gender? (Male / Female / I would prefer not to say)
5. What Language did you first learn to read in?
6. Do you have any form of dyslexia or difficulties reading? (Yes / No)
7. Do you have normal or corrected to normal vision? (Yes / No)
8. How fast do you think you read in comparison to others? (Very slow / slow / average / fast /
very fast)
B.5 Experimental Content
The following text was written by Dr Sabrina Caldwell.
218
B.5.1 Topic 1: Working with Digital Images
   Conceptual Difficulty
           Readability
   Easy
     Medium
     Difficult
   Basic
  Digital images come in many forms: photographs, icons, clipart, graphs, diagrams and sketches to name a few. They have many sources including scanning, photography, ‘born digital’ art and video stills.
Digital images can be either vector or raster graphics. Vector graphics are created using mathematic descriptions such as lines and curves. The vector graphics we know best are fonts, but they are also used for clipart and icons. Raster graphics are better known as bitmaps. Bitmaps include the digital photographs we know as jpgs, tiffs and pngs.
Digital cameras arrived in Australia in 1998, and rapidly overtook conventional photography. Today digital photographs are the most prevalent type of digital image. Over the years cameras have been included in many devices including mobile phones and tablets. Millions of digital photos find their way onto websites every day as media content, where they provide communication, information and entertainment.
Digital cameras work by registering the light that falls on the camera sensor when the shutter button is pressed. Camera sensors are normally CCDs (Charge Coupled Devices) or CMOSs (Complementary Metal-Oxide Semiconductors). Together with other hardware and software within the camera, sensors record a series of bits known as pixels. Pixels (short for picture elements) store information about the light that fell on the sensor. The camera or your computer then assembles the pixels into an image that you can see.
Words: 227
FK Reading ease: 39.6, FK Grade level 11.2
 Digital images derive from numeric representations of two-dimensional areas of two types: vector and raster. They include photographs, clipart, video still captures and digital art.
Vector graphics use scalable mathematical expressions to store and represent images. Geometrical primitives (lines, curves, polygons) are mapped onto the x,y axes of a plane and the resulting outline graphic is ‘painted’ with textures, shading and colours. Font types are simple vector graphics we use every day, and in addition, their high quality and economical space requirements make vector graphics useful for icons and clipart, and they are popular in ‘born digital' graphical art. Raster graphics are dot matrices using rectangular squares known as pixels (contraction of the term picture elements) painted onscreen or printed one horizontal line at a time. We use these rectangular grids of pixels every day in digital photography as bmps, jpgs, tiffs, and pngs.
Since their introduction in Australia in 1998, digital cameras have become ubiquitous in computers, mobile devices and tablets, however all such cameras work similarly, in that they use sensors (normally Charge Coupled Devices [CCDs] or Complementary Metal- Oxide Semiconductors [CMOSs]) to evaluate light and convert it into stored bits for later assemblage into viewable images.
These images are uploaded to and shared across the Internet at a rate of millions per day.
Words: 216
FK Reading ease: 25.0, FK Grade level 15.7
 Digital images are electronic renderings using either vector or raster graphics and depicting representations of existing real world scenes, scanned texts and art.
Vector graphics is a mathematical language modelling geometrical primitives on a working plane in two dimensions. Unlike resolution-dependent raster images, vector graphics render independent of device resolution, and scale seamlessly according to the screen device used for display or printing, although ultimately the graphic is displayed/printed as a raster image due to the constraints of current hardware and software infrastructure. Raster graphics such as GIF, PNG and BMPs store a dot matrix of individual pixels to compose an image. While not as compact as and more difficult to process and analyse than vector graphics, and unable to scale as well as vector graphics to arbitrary resolutions and sizes, raster graphics are more common, particularly in their use in digital photography.
Raster graphic images, or digital photographs, obtain from the light incidence upon the multi-element camera sensors, which is predominately either a Charge Coupled Devices [CCD] sensor that exposes to the light all elements simultaneously, or a Complementary Metal- Oxide Semiconductors [CMOSs]) which has a rapidly rolling shutter that sequentially exposes the elements of the sensor. In either model the associated software computes the values of the light incidence for each element and converts it into stored pixel (picture element) information.
Since 1998 when digital cameras were introduced in Australia, digital photo upload to the web has risen to millions of images daily.
Words: 245
FK Reading ease: 13.1, FK Grade level 19.0
    Intermediate
       Digital photographs are an assemblage of information your camera stores about the pixels that make up the image. The quality and quantity of the information is
     Digital photographs are constructed from pixel information recorded and stored by your camera. The colour information and size of your image file are based
     Bit depth and resolution determine the extent of the palette of colours available to digital images (bits) and the level of detail defining their resolved vs pixelated
   219
Materials for Experiment 2 - Adaptive eLearning and Digital Images
        dependent upon the bit depth and resolution respectively, which construct the pixels that form your image.
Pixel information for digital images is contained in bits. The higher the bit depth, the more colours can be used for the pixel. Many standard colour image formats use 24 bits, comprising 8 bits for each red, green and blue channel of a pixel; combined, they define the overall colour of the pixel. This means that in its uncompressed state, each pixel requires a total of 3 bytes to record. This large amount of information provides a palette of 16 million colours and is known as 'true color.' Bit depth can be used in special ways for managing pixel colour. Using 32 bit colour offers transparency for formats such as png, and 32 or even 48 bit colour provides extra-large colour palette.
The more pixels a camera can record, the better the resolution of the photo. A digital photo has high resolution when you can see a lot of detail in the picture, even when you zoom in. Low resolution photos quickly ‘pixelate,’ that is they degrade into blocks of pixels when you zoom in. This is because of the amount of information that the camera records when it takes the photo. The higher the megapixels, the more information. So for example a camera that takes photos using 3 megapixels records only a quarter of the information about a scene that a 12 megapixel camera captures. Words: 268
FK Reading ease: 52.5, FK Grade level 10.0
   on the bit depth of your image and the resolution of your camera sensor.
Digital image data is stored in variable numbers of bits (from 1 to 48) that dictate the pixel colour value; as more bits are employed to store data about the pixel colour, the range of colour choices available for pixels increases. The majority of image formats commonly used today use 'true color.' True color requires each pixel to be defined utilising 24 bits, comprising 8 bits each of red green and blue, or 24 bits (3 bytes) per pixel, which are blended to provide any one of 16 million distinct colours. Bit depth strategies can be employed to achieve specific colour management outcomes, for instance additional bits enable transparency in formats such as png, and 32-48 bits are used for ultra high colour resolution.
Quality images result from a smooth gradation of pixels at sizes too small for the human eye to resolve. The lower the resolution, the more likely the eye will see the individual pixels, a phenomenon called ‘pixelation’ in which the image appears jagged and choppy, distracting from the image viewing experience. High resolution images result from cameras with 8-12 megapixel sensors that record high volumes of data about the image, meaning that the image will not pixelate under reasonable zoomed conditions. Conversely, low resolution images pixelate quickly and are best used as small images to avoid apparent pixelation.
Words: 260
FK Reading ease: 35.4, FK Grade level 14.1
   range (resolution).
The array of colours available for the pixel value bit determinants increases exponentially as the bit depth increases linearly with minimum bit depth being 1 bit (enabling only 2 colours) and ranging upwards to as high as 48 bits (enabling 281 trillion colours). Bits can be deployed in alternative models such as 32 bit png files that facilitate fine-grained colour image resolution, calibrate germane colour indexes, and/or instill transparent regions, however the prevailing standard for bit depth is 24 bit 'true color.' This paradigm employs an 8 bit per colour arrangement in which component colours red, green and blue are sequentially represented by 8 bits per channel resulting in 3 bytes of information per pixel. This configuration enables pixel colour determinations derived from a colour palette encompassing 16 million hues.
Image resolution is dictated by the number of sensing elements in the camera sensor behind the lens and iris of the camera. Low end cameras record little more than 1 megapixel of information and yield images that pixelate (become jagged and sharp-edged with visible squares of individual pixels) while high end cameras can record 8- 12 megapixels or more of image data, and images of these megapixel magnitudes can be printed in large format or zoomed in without pixelation artifacts visible in the print or screen display.
Words: 246
FK Reading ease: 15.3, FK Grade level 19.8
    Advanced
       More megapixels and high bit depths mean larger file sizes, but there are bit depth strategies for reducing files sizes, and many image formats have evolved that compress these large files into sizes that are easier to use. In their ‘raw’ state, digital images are quite large. With a 24-bit true color image and an 8 megapixel camera, each raw image file would require 24 megabytes to store.
To manage this problem, especially when using digital images online, a number of image formats are available, often in camera. One of the most common formats is the jpg. Jpgs work by discarding information about colours our eyes are less sensitive to and encoding similarly
     Memory costs are incurred when recording high resolution ‘true color’ images, however image formats have been developed to address this problem by compressing the information into manageable sizes with varying results.
To calculate the uncompressed (raw file) size of a digital image, you multiply the number of horizontal pixels times the number of vertical pixels times the bit depth. Consequently, when taking a photo with a 12 megapixel camera set at the common setting of 24 bits, each photo will require 288 million or 36 megabytes.
A common format to reduce these image sizes is the jpg.
     As the quantity of pixel information recorded increases at higher bit depths and resolutions, so do computer storage requirements; in their raw form images of 48 bits and 12 megapixels would necessitate 78 megabytes storage space. To address this prohibitively space- expensive exigency, several industry-standard image formats have been developed that use compression/expansion algorithms to reduce file sizes on compression and restore images to full or partial original resolution upon viewing time.
Jpgs are amongst the most common compression formats; using downsampling of chrominance channels,
   220
B.5.2 Topic 2: Copyright and Intellectual Property
Materials for Experiment 2 - Adaptive eLearning and Digital Images
           weighted blocks of pixels. This is a ‘lossy’ method, which means that once compressed, jpgs will not expand back to the same quality.
An example of lossless compression is the png file. Pngs deflate the data into smaller pieces. The process is completely reversible.
You can use bit depth settings to reduce file size as well. Grayscale images can be effectively rendered using only 8 bits (256 colours), which allows for small image sizes. Bits can also be assigned an index of colours from your image, thus reducing the file size without losing colour quality.
Other techniques for reducing image size include resizing and cropping.
Words: 220
FK Reading ease: 54.1, FK Grade level 9.62
     Jpgs are a lossy compression method with a three pass methodology of a) reducing colour information in the less visible spectra, b) assigning relevance to blocks in the image, and c) identifying repeating patterns to reduce encoding.
The png format is more recent and is a lossless compression method. As a raw image is reduced to a png, the png ‘sliding-window’ algorithm identifies data that can be transformed into reconstructable mathematical expression and stores it.
In addition to other techniques such as image resizing and cropping, the use of bits can be varied so that they also act to reduce image size. They can be set to hold only a subset of colours, thereby creating an indexed colour image. Also, grayscale images require fewer shades and render well in as few as 8 bits.
Words: 231
FK Reading ease: 39.0, FK Grade level 13.
     quantization and entropy coding heuristics to reduce image sizes. Jpg is a lossy format, and information discarded through jpg compression cannot be recovered, leading to often substandard quality reconstructions when upsampling including block artifacts, jagged edges and pixelation.
Pngs, a more recent compression format, use a deflate algorithm based on a sliding-window concept that capitalizes on the fact that data in images contains identical pixel repetitions, fragment repetitions, and gradients. As a raw image is reduced to a png, the compression engine algorithmically identifies data that can be transformed into and stored as one of a suite of mathematical expressions in a reversible framework. Resizing and cropping are additional image size reduction options as is alternative bit depth strategies: bit depth can be deployed in a selective colour model which calibrates germane colour indexes thereby removing unused ‘placeholder’ values, and grayscale images that render well at 256 shades of gray can be set to 8 bits for certain applications, greatly reducing images size.
Words: 249
FK Reading ease: 11.1, FK Grade level 18.7
      Conceptual Difficulty
           Readability
   Easy
     Medium
     Difficult
   Basic
     To protect people who create original work, copyright law provides a way of deciding who pays and who gets paid for the use of original work. Copyright is one of a range of intellectual property rights, which also includes patents and trademarks. A photographer has copyright in his or her photos from the moment they are created, which lasts for 70 years after the photographer’s lifetime. However, it is easy for others to acquire and use your work without permission. This is called copyright infringement and it is something others should not do to
   Copyright is an automatic right afforded to creators of original works giving these creators exclusive economic rights to control copying, adaptation, issuance of copies to the public, performance and broadcasting of the work that they create. In return for licensing their materials the creators are entitled to receive royalties. Infringing copyright by using images without permission is ethically and legally wrong.
Copyright, together with patents, trademarks, database rights, design rights, and performers' rights form part of
   Photographs fall under the auspices of Intellectual Property and are afforded protection through the concept of copyright, a protection that persists until 70 years after your lifetime and gives photographers exclusive rights to license their image to others in respect of copying, performing, broadcasting and publishing. Should others use your images without permission, they are committing copyright infringement and may be liable to remunerate and/or make reparations for such infringement should you decide to take civil legal action
   221
Materials for Experiment 2 - Adaptive eLearning and Digital Images
        you and you should not do to others.
Your photos and your image art belong to you, but if
you don’t protect your copyright, who will? Digital images should be associated through metadata with the name of their creator. However this is often not the case, and a photograph can easily become 'orphaned' (an image without an author); as an orphaned image your photograph is more susceptible to infringement.
Once you’ve created your work and associated your name to it through metadata, how can you empower your image to go out into the world and work for you? Licensing.
There are many approaches to licensing your work. Images can be licensed to others via a stock image organization like Shutterstock or Getty Images. They can carry a bespoke license you create. A popular approach is to offer your image through a Creative Commons license. Creative Commons is an international non-profit organization that offers six standard licenses that brand your image as available for uses ranging from simple attribution to fully commercial, modifiable, and able to be on-licensed.
Words: 271
FK Reading ease: 49.7, FK Grade level 10.5
   the family of Intellectual Property Rights, which is the name of the broad range of rights that protect the fruits of human innovation, creation and invention.
To ensure these rights are attributable to the correct creator, adequate author identification is required to ensure the work does not become ‘orphaned,’ or disassociated from the author; orphaned works are difficult to police and can easily be reused without recompense. This can be accomplished with metadata outlining authorship and licensing requirements.
When you want to license and/or commercialise your photographs, there are a range of options from licensing your images through a commercial service like Shutterstock or Getty Images through to offering your images to the public under a Creative Commons license. By attaching a Creative Commons license to your image you specify who is able to use your photo and in what manner.
Words: 223
FK Reading ease: 31.1, FK Grade level 14.8
   against them.
A particular risk with digital photographs is that an
image can quickly become disassociated with its author by virtue of being transmitted and retransmitted without any attendant information identifying the copyright owner. When this occurs, it is said that the photograph has become an ‘orphaned’ work. An important precaution against this eventuality is ensuring adequate author identification and permissions identification in the metadata fields of the image, which can significantly reduce this risk.
Photographers do not usually wish to sequester their photographs from the world; quite the opposite. To deal with copyrighted works legally and ethically requires licensing arrangements to be executed. Licenses can be adhered to a work by proffering it through a commercial stock photo company such as Getty Images or Shutterstock, licensing using a bespoke license or utilizing an open source copyright such as Creative Commons. Creative offers internationally recognised licensing that is embedded in or attached to copyright protected material; the open source non-profit organisation offers a range of standard licenses that specify what licensees are allowed to do with your work, from attribution only through to modification, distribution, commercialization and licensing derivative works to others.
Words: 271
FK Reading ease: 16.3, FK Grade level 17.0
    Intermediate
       Online environments are complicated things. In addition to protecting your intellectual property, you need to consider how you use other people’s information and intellectual property, and how other people are affected by the content you upload. For example, commonsense should tell you that unflattering images of friends and family should not be uploaded lest it create a future problem for them.
Less obviously, you may need to consider how to content you receive from others. Web site owners soon find themselves on the receiving end of a range of information about individuals. Over the years two key
     Posting images to websites entails responsibility that extends beyond copyright infringement and encompasses ethics, privacy and security. The photos you choose to upload may have consequences for you or people you know if they are ill-advised; images of friends may linger on the Internet and ultimately influence perspectives of others, including prospective employers, in future.
Furthermore, in considering how to safeguard your intellectual property, it is wise to consider where you distribute your photos: does the site to which you intend to upload your work have privacy and security policies
     There are ethical considerations in distributing/publishing your images. Sharing too much information within the public domain, particularly in image form, can be problematic; first it can reveal things to future employers, colleagues and social contacts about the person or people involved that may be detrimental, and second it may create personal safety issues. Understanding the privacy policies (methodologies and procedures by which your information is retained and shared with third parties) and security policies (safeguards in place to protect your information from unauthorized access) of the websites to which you
   222
Materials for Experiment 2 - Adaptive eLearning and Digital Images
        documents have evolved to manage user expectations: privacy policies and security policies. A privacy policy describes what information the website keeps and how it is shared. Security policies describe how that information is secured against accidental or fraudulent access.
If you decide to try to make money from your site, exploiting user information is an obvious but controversial choice. Many high profile sites have chosen this path with mixed results. Facebook for example does not charge fees. The CEO, Mark Zuckerburg and the Facebook shareholders benefit financially from users uploading images, personal information and other information such as 'likes’ which enable them to target users with relevant ads. And they safeguard that information, which becomes an asset of the company. The data you upload may follow you on the Internet for years.
Words: 229
FK Reading ease: 38.7, FK Grade level 11.9
   (a privacy policy describes what information is retained by site owners and how it is shared; a security policy describes how it is protected)? Keep in mind that privacy and security are also the responsibility of anyone managing a website of their own.
This becomes particularly important and potentially problematic if you decided to monetize your site. User data is in fact an asset prone to exploitation. Facebook is a good example of a high profile site that allows unlimited free uploads of images and other data. However, Facebook is not a beneficial society, it is a commercial, publicly-traded company with shareholders seeking profits; while Facebook does not charge users money the CEO, Mark Zuckerburg and the company derive $3 billion dollars per year in advertising revenues. However as users continue to find their user experience tailored ever more tightly to their preferences and content and ads customized around them in accordance with their needs, they may become increasingly wary of how the company uses their information.
Words: 257
FK Reading ease: 28.7, FK Grade level 14.5
   upload your content is important.
While issues relating to inappropriate content sharing
may be generally understood, many website owners do not realize the complexity of receiving and managing user information themselves. Website owners should develop privacy and security policies appropriate to the specific arena the website occupies (for example a counselling site may have higher privacy and security obligations than a movie review site), and special care needs to be taken when seeking to obtain revenue from the activities of and information inherent in the website. Facebook is a good example of how user information can be monetized in a manner that has implications for users’ data.
Mark Zuckerburg’s business model is not predicated on a user pays approach but rather a ‘data mining’ model in which personal information gleaned from ‘likes’ and personal profiles is aggregated as marketing demographic data or used for targeted advertising known as ‘relevance ads.’ Depending on perceptions of this tactic, it may be viewed as beneficial or invasive by users.
Words: 257
FK Reading ease: 16.5, FK Grade level 18.0
    Advanced
       Once intellectual property has been created it is tempting to believe that it will remain in existence. However, with digital images, this is not always the case. With conventional photography the output was almost always prints, physical copies of the image. These prints were shared and stored, where, depending on storage conditions, they could be expected to remain viable for many decades while aging gracefully. Digital images however are vulnerable to ‘the digital cliff,’ a phenomenon in which these photographs can become completely non-existent overnight due to technology obsolescence or failure of storage media. To protect your digital images it is important to upgrade them to current technologies and ensure backups are taken.
Once these basic precautions have been taken, other forms of intellectual property protection can be considered. To preclude others from using your images
     The ephemerality of electronic constructs implies additional vulnerability to destruction in comparison with their analog counterparts. The ‘digital cliff’ effect is a symptom of this phenomenon; rather than degrading over time and space as does an analog signal, digital signals are normally either received in their entirety or else not received at all. Archivists have compared conventional photographic prints to an analog signal (the print fades in storage over time) and digital images to a digital signal (the file persists until outdated or destroyed). Protecting digital assets from the digital cliff is the first and most basic step in intellectual property protection.
But there are other electronic rights management strategies to apply on top of this step.
One such strategy is using watermarking, is the application of faint or even invisible images and patterns
     In contrast to conventional photography in which one or more examples of physical prints could be counted upon to exist, the intellectual property of digital photography must be safeguarded by electronic means. Without regular backups and technology upgrades including updating storage media, digital images may become obsolete or unavailable and fall over the ‘digital cliff’ into non-existence.
Assuming that the existence of the digital image is protected, additional electronic rights management features can be employed to protect your intellectual property. One prevalent strategy is the use of watermarking, which is the application either visibly or invisibly of an auxiliary image or pattern to the surface of the image that identifies ownership of the image.
A further useful modification intellectual property holders can make to their images is to identify their
   223
B.5.3 Topic 3: Photo Credibility
Materials for Experiment 2 - Adaptive eLearning and Digital Images
           without permission, it may be desirable to use a watermark. Watermarks are a visible form of electronic right management information embedded in your image that make your photograph an undesirable target for infringement.
Lastly, it is important to understand what metadata is and how it can work to protect your copyright. Some metadata schemes are EXIF, IPTC and XMP. Of these, XMP metadata is the most flexible for photographers. EXIF data is supplied by the camera when writing the image file to record camera settings. IPTC metadata is a particular system developed for news services who need verification details. But XMP, or Extensible Metadata Platform allows users to define and edit metadata tags to provide information about your image. You can use this for copyright information to safeguard your claims to your own original work.
Words: 271
FK Reading ease: 35.0, FK Grade level 12.5
     demarcating copyright.
Most readily apparent of all is metadata tagging. This is
information stored within the image file in one of a range of metadata systems. For example, IPTC metadata is an important tool for news investigation and broadcasting to clarify reporter and photographer identities and specific details about the evidence presented in the image. EXIF metadata is an automatically written set of metatags denoting camera settings. And XMP (Extensible Metadata Platform) is a flexible user customizable metadata system that photographers can use to identify their work and attendant information about that work, potentially warding against copyright infringement.
Words: 232
FK Reading ease: 18.7, FK Grade level 15.4
     ownership using image metadata, in particular XMP fields. While EXIF metatags are written automatically in camera and contain details of camera setting such as shutter speed and white balance, and IPTC metadata is used by news reporters and agencies in industry-specific ways, XMP can be customized by users. This means that of EXIF, IPTC and XMP metadata, XMP is most useful for photographers. In XMP, or Extensible Metadata Platform, users can define and edit metadata tags to provide information about images and ownership to safeguard against infringements.
Words: 217
FK Reading ease: 10.1, FK Grade level 18.5
      Conceptual Difficulty
           Readability
   Easy
     Medium
     Difficult
   Basic
     Altering photos is as old as photography itself. Back in the 1850’s, only about ten years after the invention of photography by Henry Fox Talbot and Louis Jacques Mande Daguerre, photographers were using many negatives to create ghostly apparitions and photoart.
The difference between then and now is that with conventional photography altering photos was an expensive and time consuming process only skilled photographers could do. Today anyone can modify a digital image using Photoshop or Instagram. They can then share it with the world in moments.
People usually make these changes for fun or art. However, sometimes people change photographs to create false images to criticize others or to make money. While there is no process at present to authenticate a
   Image tampering has been around since the advent of the photographic process. In the 150 years commencing with the invention of conventional photography by Henry Fox Talbot and Louis Jacques Mande Daguerre, and before digital photography was introduced, photographers were staging images and/or creating seemingly real but actually false photographic prints or photoart pieces crafted from disparate negatives in photographic darkrooms.
Digital photography and the long arm of the Internet increased the problem of photo manipulation. Image manipulation software has become inculcated into photographer’s postprocessing of photographs, offering easy access to an extensive palette of image tampering tools. Such manipulated images are now common;
   In the1840s, photography was simultaneously invented by William Henry Fox Talbot and Louis Jacques Mande Daguerre. Initially a completely scientific discipline, within a decade photographers were superimposing negatives to manufacture manipulated photographs and offering specimens as photoart, or more problematically, factual representations of reality for publication. Once a vanishingly small cohort of practitioners, such image tampering is now ubiquitous, done for reasons of fun or art but sometimes, more insidiously, for profit or libel. “Owing to such sophisticated digital image/video editing software tools, the establishment of the authenticity of an image has become a challenging task, encompassing a variety of issues.” This is of particularly relevant social concern given the pervasiveness of manipulated
   224
Materials for Experiment 2 - Adaptive eLearning and Digital Images
        digital photograph from the moment it was taken, there are a number of tests that can indicate if an already existing image has been altered. Together, these tests are called digital image forensics.
Digital image forensics analysts compare elements within an image to identify changes. It is an important field because digital images are increasingly being used in areas such as intelligence gathering, court proceedings, news, medical imagery and sports. This can have a direct impact on people such as defendants, insurance claimants and ordinary citizens, as well as industries such as news publishing, betting, and medicine. For example, defendants can claim that digital photographs are unreliable, and insurance claimants can falsify photographs of damage.
Words: 237
FK Reading ease: 35.1, FK Grade level 12.8
   although usually manipulated for fun or art some photos are manipulated for political or commercial ends. Presently, photos are often illusive electronic constructs, globally distributed at the speed of light with little context or explanation.
Despite attempts by some camera manufacturers, no authentication process has yet been successfully implemented. However, a range of digital image forgery detection techniques have been developed in recent years. Collectively these techniques are known as digital image forensics, a field that analyses images to determine image veracity through identifying image manipulation artifacts. Interest in and development of digital image forensics techniques is increasing due to the potential of image manipulation to impact on medicine, justice, news reporting and the legal and accounting professions. Recently, defendants have been successful in rejecting photographic evidence based on the fact that they cannot be authenticated.
Words: 242
FK Reading ease: 12.9, FK Grade level 16.4
   photographs throughout most disciplines and social platforms, which has logarithmically exacerbated the problem.
In fact, positively authenticating an image is not currently possible due to the lack of an accepted proactive authentication software solution. However there are several technological approaches extant, which together comprise the fledgling discipline of digital image forensics.
“Digital image forensics is a field that analyses images of a particular scenario to establish (or otherwise) credibility and authenticity through a variety of means. It is fast becoming a popular field because of its potential applications in many domains, such as intelligence, sports, legal services, news reporting, medical imaging and insurance claim investigation.” In courts, defendants are beginning to challenge digital photographic evidence on the grounds that their veracity cannot be guaranteed. Words: 238
FK Reading ease: 1.3, FK Grade level 18.9
    Intermediate
       Let’s consider the three most popular kinds of photo manipulation: copy/move, splicing, and retouching. Copy/move is an approach in which an area is copied from one place in the image and moved to another place in the same or similar image. These types of manipulations can be found using a technique called approximate block matching. In this technique, a range of overlapping blocks are separated out and each is compared to its neighbour to identify similarities and differences.
Image splicing has more potential to create fictional images. In image splicing a false image is created by combining more than one image. Detecting spliced images mainly occurs by identifying adjoining regions and edges. For example sharp edges or abrupt changes between different regions suggest that an image has been created by splicing.
Image retouching is the third main type of image alteration. It includes airbrushing (which we are familiar with from photos of models in magazines) and using
     There are three main forms of image tampering (copy/move, splicing, and retouching), each with their own suite of forensics detection techniques.
Copy/move forgery is one of the most popular forms of tampering, in which a target region is copied from a particular location in an image and thereafter pasted at one or more locations within the same image or a different image of preferably the same scene.” These types of forgeries are detected using approximate block matching strategy. “A typical approximate block matching strategy splits the image into overlapping blocks and applies a suitable technique to extract features on the basis of which the blocks are compared to determine similarity.”
Image splicing techniques are used to compose one image from multiple images. “Splicing detection is a challenging problem whereby the joining regions are investigated by a variety of methods. The presence of sharp edges (or changes) between different regions and their surroundings constitute valuable clues to splicing in
     Copy/move, splicing, and retouching are three popular image tampering paradigms, for each of which forensics detection techniques have been developed.
Image forgery employing ‘copy/move,’ a technique in which regional image components are cloned and applied intra-image or more rarely inter-image, is amongst the most common tampering strategies. In this instance, approximate block matching detection is utilized to sequester areas of the image exhibiting repetitive pixels between overlapping blocks.
Spliced images involve multiple photographic sources from which salient features are extracted and combined to create new images; this technique affords greater potential for forgery and falsification of photographs. Detecting spliced images is a difficult problem wherein forensic investigators seek indicative artifacts such as sharp edges and changes suggesting combinatorial regions.
Retouching is historically the most prevalent form of image tampering (particularly as used in airbrushing
   225
Materials for Experiment 2 - Adaptive eLearning and Digital Images
           filters to soften or sharpen or adjust colour. Retouching enhances or diminishes individual features in an image or applies a global change to the whole image. Individual changes are usually made using a number of small copy/moves such as cloning skin pixels to cover a blemish.
Detecting the use of this technique involves finding one or more or enhancements, blurring, illumination and colour changes. If the source photo is available this may be easy. Otherwise, the task may be very difficult. Individual manipulations are investigated using copy/move forensics. To detect contrast and colour enhancements (if visible) investigators usually for global modifications.
Words: 262
FK Reading ease: 41.6, FK Grade level 11.2
     the image under investigation.”
“Image retouching is another class of forensic methods that pertains to a slight change in the image for various aesthetic and commercial purposes, not necessarily conforming to the standards of morality. The retouching is mostly used to enhance or reduce the image features.” “Forgery detection, in case of image retouching, involves finding the enhancements, blurring, illumination and colour changing.” Enhancements may be local (usually copy/move modifications) or global (contrast enhancements affecting the entire image) and forensic investigation requires the application of an extensive range of techniques. “Forgery detection may be an easy task, if the original version is available. Otherwise, with blind detection, the task may be very challenging.” Words: 274
FK Reading ease: 29.5, FK Grade level: 14.3
     photos of models). Retouching involves either global filtering of images for softening/sharpening effects or chromatic enhancement, or local changes, especially move/copy such as cloning pixels to erase imperfections. Where the source image is available detection of retouching is more facile, however in the absence of reference images the image under investigation must be ‘blind detected,’ and the task becomes significantly more difficult.
“For the manipulation of retouched images, two types of modifications are applied, namely local and global modifications Local modifications are usually used in the copy/move forgery or in the case of splicing. For the detection of contrast enhancements that perceptually impact the image, global modifications are usually investigated.”
Words: 249
FK Reading ease: 11.6, FK Grade level 17.2
   Advanced
     We communicate with each other in images far more frequently than once was the case. For example, we may take a photograph of our coffee and cake in a café and forward it on our iPhone to a friend instead of chatting on the phone and describing our trip to the café. What has also changed is how easy it is to change our photographs. It was once the case that few photographers could tinker with their photos, now almost anyone can. While the effects of airbrushed models and product image enhancement is commonly understood, there is little understanding of the effects of day to day manipulation of photographs in social media, family photos, and public images.
Why is all this image manipulation a problem? There are many reasons, but to take one significant issue, image manipulation is a problem because we are manipulating our personal stories one image at a time. According to Dr Ira Hyman, Professor of Psychology at Western Washington University, “our photographs can actually change and modify our memories over time.” He asks, “How many of your childhood memories resemble the pictures that your parents took? Is it your memory or their picture?” When we alter our photos, we also alter
   Increasingly, we encounter information about the world in visual form. At the same time, human capability to manipulate images is greater than at any previous point in history; it is an intuitive and rapid process within the reach of anyone with an iPhone and Instagram.
Copious research has been undertaken on the use of manipulated images in advertising and marketing, but there is inadequate understanding of the effects of casual photo manipulation such as is prevalent in news, social media and family photos.
However, some issues are coming to the fore. One such issue pertains to the relationship between human memory and photographs. Photographs are memory aids that represent events, people and places in our personal experiences. That they are effective in this role can be seen in the fact that photographs can act upon our memories to emphasise some aspects and minimize others; in effect, they influence our memories (Ira Hyman, Professor of Psychology, Western Washington University). Altering our photographs equates to altering our memories of the locations, participants and events within which the photograph transpired.
When considering the construction of history through
   An effect of the information technology revolution is that society is increasing its consumption of information in visual form (witness Facebook, Pinterest, Instagram). Simultaneously, the ability of humans to tamper with visual information through Photoshop and ‘on-the-fly’ image altering has dramatically increased.
Extensive research exists regarding advertising and marketing uses of image manipulation, however the effects of ubiquitous use of photo manipulation in news, social media and family photographs is as yet largely unquantified.
One initial issue gaining attention is the impact of image manipulation on the photograph-memory link. Photographs are mnemonics whose importance increases with chronological imperatives; memories fade over time and we rely upon photographs as reportage to prompt our memories of the events contemporaneous with the image acquisition. Tampered images create a flow-on effect of tampered memories: our memory acuity is influenced by the photographic representations appertaining to them (Ira Hyman, Prof. Psych, WWU). As unverifiable images continue to provide evidentiary reportage of the real world our understanding of those
   226
Materials for Experiment 2 - Adaptive eLearning and Digital Images
           our memories.
Further, in the big picture, we as a society are amassing a large, mobile body of images that cannot be relied upon as records of actual people, places and events as was the case in the era of conventional photography.
Words: 247
FK Reading ease: 47.7, FK Grade level 11.6
     personal narratives and the body of evidence including photographs, these alterations may affect our societal cognition of history, subverting our evidence of events, places and people over time.
Words: 212
FK Reading ease: 28.8, FK Grade level 14.0
     events becomes questionable; evidence of societal meta- narratives become polluted with unquantifiable and unqualifiable falsifications; future generations will be forced to consider whether the photographs extant relevant to their interests and investigations are reliable as reportage of real events, places and people.
Words: 204
FK Reading ease: .3, FK Grade level 18.2
   227
Materials for Experiment 2 - Adaptive eLearning and Digital Images
 228
 endix C.
Dealing with imperfect eye gaze data
Gaze data often needs to be cleaned up and inference must be made about where fixations actually occurred subsequent to data collection. Noise in eye gaze data can be due to inaccuracy of the equipment, and characteristics of a participant’s eye that make it hard to track them (Hyrskykari, 2006). Stereo camera eye tracking allows for head movement (Beymer & Flickner, 2003) and so are used, as the eye tracker in these experiments uses stereo cameras. The stereo cameras are first calibrated and for each participant the eye tracker begins with a 9-point calibration sequence. Even so, it is often noted that calibration must be done throughout experiments to ensure that it is correct throughout the experiment (Hornof & Halverson, 2002; Hyrskykari, 2006). There are methods for adjusting and recalibrating the eye tracker during use such as the use of implicit required fixation locations (RFLs) (Hornof & Halverson, 2002). Implicit RFLs are locations on a screen that a participant must look at as part of a task and therefore provide a location where the eye gaze data can be recalibrated from if deviation has been encountered. Other algorithms such as presented by Hyrskykari (2006) are highly related to reading tasks and involve using lines of text as the locations where fixations are reference points for mapping of the gaze data. This algorithm is used in real time as part of a reading aid called iDict and allows for manual corrections to be made if the fixations are not be mapped to the right words (Hyrskykari, 2006). This algorithm focuses highly on the vertical disposition of gaze points rather than the horizontal disposition.
For post-collection recalibration of data, inference about where the fixations should occur can use the same logic about the above examples of recalibration of eye gaze trackers during experimentation. To deal with the distortion of the data, one solution is to apply transformations to the data to move points to where they reasonably should be. Of course this begs the questions of how one defines where the data points should reasonably be. The next problem is applying such a transformation. The data points are distorted in different ways between and within participants. That is, even for the same participant, from text presentation to the next text presentation the distortion may be different. The simplest option is to manually apply the transformation for each set of fixations for each participant. The next step is to automate the process. Examples of misaligned fixations are shown in Figure C.1.
229
p
Dealing with imperfect eye gaze data
  Figure C.1. Example of misaligned fixation data.
The code used to shift the fixations is:
vert_box_ratio=y/(max(y)-min(y)); hor_box_ratio=x/(max(x)-min(x)); new_y=y+(vertical_shift*((1-vert_box_ratio)+(1-hor_box_ratio))); new_x=(x*hor_spread_factor)-horizontal_shift;
Note that x indicates the x coordinate of the fixation and y indicates the y coordinate of the fixation and the experimenter defines the values for variables.
The outcome of this shift is:
230
Dealing with imperfect eye gaze data
  Figure C.2. Example of re-aligned fixation data.
The experimenter would manually go through the fixation data and re-align the fixations using this shift. The process was not automatic.
231
Dealing with imperfect eye gaze data
 232
 ppendix D.Reading in distracting environments
 Reading in digital environments can be very distracting. In this appendix we present a preliminary user study in which participants’ eye gaze was recorded as they read text in a visually distracting environment. We explore two distraction mitigation signals using real-time eye gaze data to investigate whether the effects help reduce distraction rate as well as aid recovery from distractions. These signals involved adding a signal to the last word read before a distraction occurred to show the reader where they were up to. We compared these experimental conditions on both first (L1) and second (L2) English language readers and for easy and hard to read texts. The results demonstrate that the mitigation signals helped recovery from a distraction by drawing participants’ attention back to the text as well as indicating from where to recommence reading. We conclude with recommendations on implementing distraction mitigation signals in text and limitations of this study. This appendix is based on work presented at OzCHI 2015 (Copeland & Gedeon, 2015).
D.1 Introduction
Digital environments make vast amounts of information readily available. However, these environments are dynamic, distracting the user with alerts, advertising, social media, and other distractions. It has been shown that auditory distractions, such as background noise, impair reading comprehension (Sörqvist, Halin, & Hygge, 2010) and that visual distractions lead to disruptions in cognition (Atkins, Moise, & Rohling, 2006). In the case of educational material, irrelevant and attention grabbing images or animations alongside text material have negative effects on learning (Clark & Mayer, 2011; Harp & Mayer, 1998; Mayer et al., 2001; Sung & Mayer, 2012). However, distractions can be avoided by using attention
“Any distraction tends to get in the way of being
 an effective gangster.”
 ― Terence Winter, creator of Boardwalk Empire
233
Reading in distracting environments
 guiding to ensure that important information is seen (Rosch & Vogel-Walcutt, 2013). Our hypotheses therefore are that visual distractions have a negative impact on reading behaviour and comprehension, but can be mitigated using attention guiding, to help reduce the disruption of visual distractions during reading.
We explore these hypotheses by also investigating the effects of text readability on the extent to which the visual distractions impact comprehension and distraction rate. We know that auditory distractions impair proofreading performance and prose recall, but the impairments only occur when the reading task is easy (Halin, Marsh, Haga, Holmgren, & Sörqvist, 2014; Halin, Marsh, Hellman, Hellström, & Sörqvist, 2014). In digital environments many visual distractions are possible, such as the reader having dual screens open with Facebook showing on one screen, advertising on webpages, or simply the pop-up alerts used by many applications such as email.
The objective of this study is to investigate firstly, the effects of text readability on the rate at which participants are distracted and secondly, whether attention guiding can be used to mitigate distractions for test with different readabilities. Readability is determined by readability formulas such as the Flesch-Kincaid Grade level. We investigate the effects of text readability and distractions on first language English (L1) and second language English (L2) readers. Distractions are induced using images that change at constant rates in a side bar. An eye tracker was used to record and monitor eye gaze of participants. Using this live data, we implemented a signal to trigger on the last word read before a distraction.
We hypothesize that the easy readability text and the L1 readers will be associated with higher distractions rates, and that the mitigation signals will reduce distraction rates and will help the reader recover after distraction.
This appendix is organized into the following sections: background information; user study method; results and discussion; and further work.
D.2 Background
Much of the background has been covered in the literature review (Chapter 2) of the thesis. As follows, only the literature that has not been covered in that review will be addressed in this section.
D.2.1 Images and text
It is generally accepted that including images along with text is beneficial to the learning process, the basis of which lies in dual coding theory (Mayer, 1999). Put simply, the activation of two cognitive subsystems results in more effective learning. In this way Mayer (1999) proposed five design principles for multimedia education, amongst which using words and images is primary. Images have a large effect in real word scenarios, such as educating patients in health care. Images improve understanding of health care instructions and change adherence such instructions (Houts et al., 2006).
234
Reading in distracting environments
 However, it has been shown extensively that the images or animations must be relevant to the learning materials (Clark & Mayer, 2011; Harp & Mayer, 1998; Mayer et al., 2001; Sanchez & Wiley, 2006; Sung & Mayer, 2012). Use of seductive images, those that attract attention but are irrelevant to the learning materials have been shown to have a negative effect on learning because the images draw the reader’s attention away (Sanchez & Wiley, 2006; Sung & Mayer, 2012). The effects of seductive images explored using eye tracking suggest that readers with low working memory capacity are affected more as they spend longer looking at the seductive images than those with high working memory capacity (Sanchez & Wiley, 2006). Another image type that is used in learning materials is decorative images, which are irrelevant to the learning material but not attention grabbing. Whilst it has been shown that decorative images do not negatively impact learning, they do not improve learning (Sung & Mayer, 2012).
D.2.1.1 Distractions during reading
Irrelevant and attention grabbing images can be considered distractions from the text rather than helpful resources. Simplification and reduction of distractions is best when aiming to avoid unnecessary cognitive load (Sweller et al., 1998). Visual distractions from unnecessary elements have been shown to lead to disruptions to cognition (Atkins et al., 2006). Additionally, auditory distractions such as background noise have been found to impair reading comprehension (Sörqvist et al., 2010). The extent of the impact of these distractions is aligned with the complexity of the task, where impairments on prose recall and proofreading performance only occurred when the reading task was easy (Halin et al., 2014a; 2014b).
Distractions, such as television, provide both visual and auditory disturbance. Computer use in front of a television has shown that people switch between the two medias frequently and that they underestimate the extent of how frequently they are switching (Brasel & Gips, 2011). Whilst not directly related to reading, these results emphasise the importance of investigating how distractions affect readers in a digital environment.
As stated, digital environments provide many distractions within themselves. One such distraction is computer mediated communication technologies such as instant messaging (IM). Whilst using IM during reading does not appear to negatively impact reading comprehension, extensive used of IM is associated with lower reading comprehension scores as well as lower GPA scores (Fox et al., 2009). Whilst “IMing” during a reading task does not negatively impact reading comprehension scores (Bowman et al., 2010; Fox et al., 2009; Jacobsen & Forste, 2011), it negatively impacts the time taken to complete the reading task.
IM is not the only distraction ever-present in digital environments. Recently the use of social media has proliferated in use, especially amongst the young generations. These are the generations now studying so the effects of such technology on learning are especially important. It has been found that students who use Facebook spend less time studying and have lower GPAs (Kirschner & Karpinski, 2010).
235
Reading in distracting environments
 D.2.1.2 Mitigating distractions
Attention guiding can be used to minimise distractions by providing visual cues using colours to emphasise relevant parts of animations (Boucheix & Lowe, 2010), or by zooming in on parts of animations (Amadieu et al., 2011), and signalling parts relevant parts of diagrams by adding temporary colour changes (Ozcelik et al., 2010). The addition of eye tracking data to the paradigms has been found to enhance their effectiveness of attention guiding (Boucheix & Lowe, 2010; Ozcelik et al., 2010).
D.3 Method D.3.1 Participants
Data was collected from 66 (28 female) participants with an average age of 21.7 years (standard deviation of 3.9). All participants had normal or corrected to normal vision and were primarily (n=54) recruited from a first year Computer Science course on Web Development and Design offered at the Australian National University (ANU). The remaining participants were all students from ANU. Participants were divided into two groups; those that first learnt to read in English, denoted L1, and those that first learnt to read in another language, denoted L2. There were 42 L1 participants and 24 L2 participants.
D.3.2 Design
The study used a between-subjects design with 3 independent factors: 1) text difficulty; 2) distraction mitigation signal; and 3) whether English was their first reading language. There were two levels of text difficulty, three distraction mitigation signal conditions, and two language groups. All participants were exposed to the same distracting environment.
We experimented using two distraction mitigation signals and had a control condition, these conditions are denoted and described as:
Condition A: Cue is yellow highlighting and bolding the last word the reader fixated on.
ConditionB: Cue is the last word the reader fixated on coloured grey and italicized.
ConditionC: Nocueappliedtotext
The aims of these conditions are to explore the effects of bringing the readers’ eyes back to the text, in particular the point they were up to in the text. Secondly, rather than actively drawing their attention back to the text, just give the reader a signifier of where they are up to in the text to help when they do focus their attention back on the text. In both cases the reader will feel the presence of the system monitoring them. The question is whether the cues reduce the effect of distraction? The remainder of this section discusses the design of the texts used, the distracting environment and finally the mitigation techniques.
236
Reading in distracting environments
 D.3.2.1 Text Properties
The experiment involved two parts; firstly, the participant was asked to read a piece of text with either easy or hard readability. The readability was calculated using several readability formulae and the average of the tests was used. The readability formulae used were, Flesch-Kincaid Grade Level, Gunning-Fog Score, Coleman- Liau Index, SMOG Index, Automated Readability Index. The easy-to-read text has an average score of 10.6 (Table D.1); this equates to only a high school level of education needed to comfortably read this text. Given that participants are university students the text should be comfortable to read by participants. However, the hard-to-read text has an average score of 18.0 (Table D.1) indicates that a much higher level of education is needed to comfortably read the text. Participants should therefore find it difficult to read.
Table D.1. Readability scores for each text type.
Readability Formula       Easy Text     Hard Text Flesch-Kincaid Grade Level       9.5     17.8
     Gunning-Fog Score Coleman-Liau Index
SMOG Index
Automated Readability Index Average Grade Level
12.2 12.7 9 9.5 10.6
21.3 15.8 15.2 19.7 18
        The statistics of each text type are shown in Table D.2. Whilst the number of words is different by more than 100 words the number of characters is kept roughly the same, which in turn equates to the lengths of the text being approximately the same. We can see that the hard text has significantly longer words as well as longer sentences compared to the easy text.
Table D.2. Text statistics for each text type
Text Statistics       Easy Text     Hard Text
          Character Count Syllable Count Word Count Sentence Count Characters per Word Syllables per Word Words per Sentence
3,693 1,215 764 47 4.8 1.6 16.3
3,746 1,246 698 22 5.4 1.8 31.7
        The experiment used a between-subjects design so each participant was shown either an easy or a hard text to read. After the text was read, participants’ comprehension was tested using 10 comprehension questions that were the same for both texts.
237
Reading in distracting environments
 D.3.2.2 Making the environment distracting
Participants were required to read text in a distracting environment. This involved creating an environment with a controlled level of distraction so that each participant would be exposed to distraction to the same degree. To accomplish this, a sidebar on the right of the screen was added. In the sidebar a picture at the top is changed every 20 seconds. The pictures in this box are different animals, for example a meerkat. Below this in a rectangular box, names are changed at random every 5 seconds. Both are shown in Figure D.1. The right sidebar is designed to stay constantly in focus whilst the participant scrolls through the text. This mimics some properties of Facebook pages, while being consistent for each subject.
 Figure D.1. Example of distracting environment
Distraction mitigating signals are added to the text to show where the reader was up to in the text before they were distracted. This was to investigate whether adding text signals helps the reader recover after reading, and if the participants consider it helpful. Two signals were used in the study, the first is an overt signal and the second is a subtler signal. In both cases the signal is only applied to the last word the reader fixated on according to the eye tracker, before a distraction drew the reader’s eyes away from the text. Both signals were designed so that as soon as the reader looks at the affected word the signal would disappear.
SignalA: Highlighting (yellow) and bolding the last word read before a distraction, shown in Figure D.2.
Figure D.2. Example of signal A; highlighting and bolding of the last word read before a distraction.
 238
Reading in distracting environments
 Signal B: Italicizing and making the last word read before a distraction grey, shown in Figure D.3.
Figure D.3. Example of signal B; greying out and italicizing the last word read before a distraction.
The aim of the two text signals is to explore the effects of bringing the readers’ eyes back to the text, in particular, where they were up to in the text. Secondly, the signals are designed to only give readers a signifier of where they are up to in the text to help when they do focus their attention back on the text.
D.3.3 Materials and Procedure
The experiment duration was approximately 30 minutes. First, the experiment was explained to participants. Then participants were asked to read and sign a consent form. Participants were given a pre-experiment questionnaire. The questions were designed so that we could gauge participants’ use of potentially distracting technologies. The questions asked of the participants are:
1. Do you use social media? (If yes, how regularly?)
2. Do you use email? (If yes, how regularly?)
3. Do you use instant messaging? (If yes, how regularly?)
4. Do you often use social media, email and/or instant message while you are
reading course materials or work materials? (If yes, how regularly?)
5. Do you find that you are distracted by these technologies during study or
work time? (If yes, how regularly?)
Note that how regularly was restricted to the following options: Never; Once a month; Once a week; Once a day; 2-5 times per day; 5-10 times per day; and 10+ times per day.
Calibration of the EyeTribe eye tracker was performed until ‘perfect’ calibration was obtained according to the tracker. A 9-point calibration protocol was used, shown in Figure D.4. According to the EyeTribe software, perfect calibration is the optimal calibration result and equates to accuracy being < 0.5°. The eye tracker recorded eye gaze at 30Hz.
Figure D.4. Example of the 9-point calibration screen used in the experiment showing that perfect calibration was accomplished.
  239
Reading in distracting environments
 The experiment was run on a Macbook Pro 13” and participants were free to move their heads, however, they were asked to stay relatively still while the tracker was on. The setup of the experiment is shown in Figure D.5.
Figure D.5. Experiment setup
After the calibration routine, participants read the text whilst their eye gaze was being monitored and recorded. Finally, a post-experiment questionnaire was given to the participants. In the post questionnaire participants were asked if they were: 1) distracted whilst reading; and 2) whether they thought this had an impact on their understanding. In the conditions where a text signal was used, participants were also asked if they thought the text effect 1) reduced their distraction rate; and 2) helped them to start reading the text again.
D.3.4 Data pre-processing
The raw eye gaze data collected from the eye tracker consists of x,y-coordinates recorded at equal time samples. Fixation and saccade identification was performed on the eye gaze data. To detect fixations, the dispersion threshold identification algorithm (Salvucci & Goldberg, 2000) was used. The duration threshold was set to 150ms and the dispersion threshold was set to 30 pixels.
Once the fixations have been identified, eye movement measures were derived to characterise the reading behaviour. The measures used in this analysis are:
Number of fixations: From the fixation identification algorithm the number of fixations observed for the page is calculated. We report the total number of fixations.
Total fixation duration: The sum of the durations of all recorded fixations is calculated as well as the sum of fixation durations.
Number of distractions: The number of times a participant moves their eyes to the distractions from the text.
 240
Reading in distracting environments
 Percentage of fixations on distractions: Number of fixations recorded on the distractions divided by the total number of fixations. This provides information about the extent to which a participant was distracted rather than a raw count of distractions.
D.4 Results
There are a number of results from the study. First we look into the pre- questionnaire data to investigate their use of communications technologies, and then we investigate the effect each condition had on participants’ eye movements and reading comprehension. Finally, we look at the post-experiment questionnaire data to explore their perceptions of the distracting environment and the text signals.
D.4.1 Pre-experiment questionnaire data
Participants completed a pre-experiment questionnaire to reveal their use of communication technologies. 50 stated that they use social media, however all participants (n=66) stated that they use email and instant messaging technology. Additionally, 65 of the 66 participants stated that they use social media and / or emails and / or instant messaging while they are reading learning materials for university.
We can also analyse the self-rated frequency with which participants are distracted by social media, email, or instant messaging while studying (or working). Participants were asked to rate their use and distraction levels on a Likert scale as described in
Figure D.6. When asked how regularly they use these technologies whilst reading learning materials, 46% of these participants stated that they use these technologies more than 10 times per day. 56 stated that these technologies distract them while they are studying. As Brasel and Gips (2011) people underestimate the amount they are distracted so this level could in fact be a lot higher. This establishes that participants have quite a high level of usage of communicative technologies and on average are quite distracted by them while they are studying.
Participants who stated that they use communication technologies the most (more than 10 times a day) are also those who were distracted most (see Figure D.6). However, the figure also shows that there are certainly discrepancies in participants’ ratings of distractions versus their ratings of use of the technologies. This is seen most prominently in the case where the individual who stated they never get use communication technologies and yet is highly distracted by them. We can observe that for almost every bracket of the frequency of use
.
241
  Figure D.6. Pre-experiment questionnaire data on self-rated distraction levels from communication technologies, grouped by frequency use of technologies
242
Frequency of distraction whilst studying:
Figure D.7. Time taken to complete for each condition
Reading in distracting environments
 D.4.2 Performance outcomes
The times taken to complete the reading task for each condition are shown Figure D.7 and participants measured comprehension levels for each condition are shown in Figure D.8. L2 readers take longer to complete the reading task, for all conditions. Contrary to our predictions, there is no visible increase in time taken to read the hard text compared to reading the easy text. The distraction mitigation signals appear to only affect the L2 readers, however in the opposite way to what we expected – reading time increases for the signal conditions.
  7:12 6:29 5:46 5:02 4:19 3:36 2:53 2:10 1:26 0:43 0:00
                                     CABCAB Easy Hard
L1 L2
       10.0 9.0 8.0 7.0 6.0 5.0 4.0 3.0 2.0 1.0 0.0
                                CABCAB Easy Hard
L1 L2
     Figure D.8. Comprehension for each condition
Additionally, we can observe from Figure D.8 that in most cases L1 readers score higher on the comprehension tests compared to the L2 readers. The distraction mitigation signals do not appear to help the L1 readers, if anything there is an
243
Comprehension score (/10) Time taken (m:ss)
Reading in distracting environments
 observable decrease in reading comprehension when the signals are used. The opposite is seen for the L2 readers where an increase in comprehension score is seen when the signals are used.
To address the above hypotheses a MANOVA is used to determine if there are any statistical differences between the conditions. The correlations between the dependent variables are within the acceptable limits for MANOVA outcomes, i.e. the correlations lie between r=-0.4 and r=0.9. To test for normality in the dependent variables the Shapiro-Wilk Test is used, as it is more appropriate for small sample sizes. The quiz scores are normally distributed for all formats. The times taken are normally distributed for both of the L1 and L2 data sets. The comprehension scores are normally distributed for the L2 data set and not for the L1 data set. The Levene’s test for equality of variances shows that there is homogeneity for all dependent variables (significance>0.05). Finally, the homogeneity of variance-variance- covariance matrices is satisfied as the Box's M value of 36.92 (p=0.653).
The MANOVA shows there is a statistically significant difference between L1 and L2 participants, F(2,53)=10.94, p<0.0005; Wilk's λ=0.708, partial η2=0.292. However, there is no significant difference based on text difficulty, F(2,53)=1.82, p<0.172; Wilk's λ=0.936, partial η2=0.064, or text signal condition, F(4,106)=0.818, p<0.516; Wilk's λ=0.945, partial η2=0.030. Additionally, there is no significant effect of interaction between the format and reader type. Since statistically significant results have been found between-subjects ANOVAs are performed. L1 readers have significantly lower reading times (F(1,54)=13.25; p=0.001, partial η2=0.197) and higher comprehension scores compared to L2 readers (F(1,54)=6.36; p=0.015, partial η2=0.105). The difference in reading duration is not only consistent with similar research (Kang, 2014) but also with results from this thesis. The differences in comprehension score is consistent with the results from Chapter 7 of this thesis that showed that there is a difference between L1 and L2 readers when the difficulty of the text is increased. Whilst there is no statistically significant difference in comprehension scores based on text difficulty we can see that the difference between the L1 and L2 readers in comprehension scores largely comes from the hard text conditions.
D.4.3 Eye gaze and distractions
The comparison of percentages of fixations on the distractions and the distraction rates for each of the conditions are shown in Table D.3. MANOVA analysis of the eye gaze measures cannot be performed as the data violates the preconditions of the test. However, we can make observations about the recorded data. In all cases there are low distraction rates, as shown in Table D.3. On average L1 participants only look away from the text about 5 times and L2 participants only look away from the text about 4 times. Even when participants did get distracted they spent relatively no time looking at the distraction. For the L1 participants, only about 2.4% of the fixations were recorded on the distraction area and only 1.9% for the L2 participants. Our expectation was that there would be a higher level of distraction. However, two key points can be made from these results; firstly, the L2 participants tend to be distracted less than the L1 participants, and secondly, there is
244
Reading in distracting environments
 considerable variation in the distraction of participants, as seen in the large standard deviations. The latter point suggests that the amount to which an individual is distracted is largely based on the characteristics of that individual.
Table D.3. Distraction rates for each experimental condition
              Text Readabi lity
A Easy B C A Hard B C
Eye Gaze Measures
Mitigation Condition
Reader Group
Total
number of
fixations           distractions
       Total fix. dur. (m:ss)
% fixations on
Number of distractions
5.9 ± 3.2 3.8 ± 5.0 5.0 ± 2.2 2.3 ± 1.0 3.6 ± 2.3 5.8 ± 3.3 4.4 ± 5.3 5.3 ± 2.5 6.3 ± 3.8 2.5 ± 2.4 5.4 ± 4.9 3.5 ± 3.1
        L1 532 ± 270
L2 464 ± 566
L1 519 ± 231
L2 550 ± 208
L1 452 ± 129
L2 560±63
L1 613 ± 135
L2 692 ± 298
L1 477 ± 128
L2 512 ± 258
L1 564 ± 245
L2 400 ± 310
2:12 ±1:29 1:37 ± 2:07 1:53 ± 0:59 2:12 ± 1:07 1:33 ± 0:36 2:23 ± 0:25 2:22 ± 0:47 2:35 ± 1:11 1:44 ± 0:35 1:59 ± 1:08 2:08 ± 1:08 1:35 ± 1:13
2.4 ±3.0 1.8 ±2.5 2.3 ± 1.4 1.3 ± 3.7 1.9 ± 8.3 3.9 ± 23.0 1.8 ± 11.1 1.5 ± 3.1 3.8 ± 12.1 1.4 ± 3.0 2.1 ± 4.7 1.6 ± 1.1
                                       In most cases the L2 readers have higher numbers of fixations and longer fixation durations. Notably, this is not the case for the hard text condition C, where the L2 group has a considerably lower average number of fixations and fixation duration. This is an interesting point given the results from Chapter 6, 7 and 8 of this thesis that highlight that L2 readers tend to stop reading thoroughly when a text becomes too difficult for them. However, for the other two conditions, A and B, in the hard text condition, the number of fixations and fixation durations are certainly higher than for the L1 readers. In these cases, the distraction signals may not have worked in mitigating distractions but perhaps have helped the L2 readers to keep reading the text more thoroughly than if the signals were not there. Our conclusion therefore is that the distraction mitigation signals do not seem to reduce the amount of distractions but they may provide encouragement to read the text more thoroughly, especially for the L2 readers when reading difficult text.
Finally, the difference in eye gaze measures between the easy and hard texts appears to be minimal, contrary to what we would expect. Further analysis using more participants is required to investigate this further.
D.4.4 Participants’ perceptions
After the reading and comprehension tasks participants were asked if they were: 1) distracted whilst reading; and 2) whether they thought this had an impact on their
245
Reading in distracting environments
 understanding. Of participants, 82% stated that they were distracted whilst reading and 61% stated that it did affect their comprehension.
There is no difference found in the perceptions between L1 and L2 readers using Chi-square test for independence (χ2(1)=1.99, p=0.16) but there is a relationship between the language group and whether the participants thought the distractions affected their understanding (χ2(1)=4.99, p=0.03). Of the L1 participants, 50% thought the distractions affected their understanding, whereas 79% L2 participants thought the distractions affected their understanding.
Again using the Chi-square test for independence, the distraction mitigation signal conditions were found to have no relationship to whether participants thought they were distracted (χ2(2)=0.26, p=0.88) nor whether they thought the distractions affected their comprehension (χ2(2)=0.72, p=0.69). Finally, text difficulty was found to have no relationship to whether participants thought they were distracted using Chi-square test for independence (χ2(1)=0.88, p=0.35) nor whether they thought the distractions affected their comprehension (χ2(1)=0.74, p=0.39).
D.4.4.1 Perceptions of the distraction mitigation signals
For the conditions where the distraction mitigation signal are applied to the text participants were also asked, 1) did you find that the text effect reduced your distraction? And, 2) did you find that the text effect helped you to start reading the text again? The results from this in general point to three main findings; firstly, that the majority of participants did not even notice the distraction mitigation signal in condition B. Only 9% of participants thought that the signal in condition B helped reduce their distractions however, 14 of these participants did not even see that there was a signal. Unsurprisingly, only 14% of participants in the B condition stated that the signal helped them recover after reading.
The second point that can be made is that whilst the signal was meant to be applied with the last word read, this was seldom the case. That is, limitations in the eye tracking accuracy impacted the effectiveness of the signal. This was not picked up in condition B since a large majority of participants did not even notice the effect. However, in condition A the signal was more noticeable and hence the limitation was detected. For condition A, 32% of participants found that the signal reduced their distraction rate. Whilst this is a low percentage, for those that it worked for it did do the job it was supposed to do with participants noting: “Yes it showed me I was distracted” and “Yes as it went a bright colour and reminded me I should be reading”. But for the rest of the participants the signal was not working correctly with participants noting “It actually distracted me more than the pictures did because it went to something that I either hadn’t read yet or already read”, “No, it was the reason why I distracted.” and “Nope. Very random.”
Remarkably, even with the effect not working correctly, 55% of participants actually stated that they thought it helped to start reading again. So even for participants who stated that the effect was not working correctly, they still found it helped, mainly because it drew their attention back and made them re-read text. Participants stated: “It did bring me back to the text a bit.”, “Some help, it always drag my
246
Reading in distracting environments
 attention to the start point to read again.” “Yes - but it was a bit behind so I re-read the sentence I had previously read”, “Well it made me reread things”, and “Yes, I kind of forgot what I was reading after I saw the text effect, and then I just read from the highlighted text again”.
This brings us the third and final point that perhaps it is useful to consider the distraction mitigation signal not being on the word that was read before distraction occurred but to being slightly behind that point, therefore inducing re-reading of the text.
D.5 Discussion
In this study we investigated two methods for mitigating distractions during reading. The insights gained from this study come from several directions, the first of which is in regards to the pre-experiment questionnaire about usage of distracting technologies during study periods. All participants stated that they use emails and IM but the shock comes from the fact that a large majority (98%) of participants use social media and / or email and / or instant messaging while they are reading materials for university. And 85% of participants admit that these technologies distract them while studying. Almost half (46%) of the participants are using these technologies more than 10 times a day and 85% of them are using these technologies at least 2 times per day. Perhaps more interesting is that 65% of participants admit that they are distracted by these technologies during study at least 2 times per day. This indicates that people are getting distracted whilst reading and studying and therefore there is a need to mitigate these distractions.
We hypothesised that the L1 readers would be associated with higher distractions rates and hence the eye gaze would be more affected in this case. The eye gaze analysis in the study is not conclusive enough to provide evidence for or against this hypothesis, but what they do show is that L2 readers tended to be slightly less distracted than L1 readers. The L2 readers were seen to take longer to read the texts and scored lower than the L1 readers. In general, the L2 readers have higher numbers of fixations for longer durations, as we would expect from past research (Kang, 2014).
The hypothesis that the easy-to-read text would be associated with higher distraction rates was based on past research that auditory distractions impair proofreading and prose recall task performance when the task is easy and not when it is hard (Halin, Marsh, Haga, et al., 2014; 2014). However, there are several differences to these studies, mainly being, the distraction type and the way in which the text is made difficult to read. In our study the visual distractions we used may not have been distracting enough. Participants on average fixated about 2% of the time in the distractions area which is a very small percentage and raises the question of whether the environment is actually “highly” distracting or not.
The visual distractions were an experimental condition and not entirely a realistic situation. However, the images rapidly change, which is common for advertising on webpages as well as the rapid changes that occur in social media site
247
Reading in distracting environments
 such as Facebook. The choice was made to not use a real scenario, i.e. a webpage with changing adverts, because the objective of the experiment was to control the distraction rate to keep it constant for all participants. Changing the images at a random rate could perhaps increase the level of distraction.
Another explanation that is that whilst attention grabbing irrelevant images and animations alongside text material have negative effects on learning (Clark & Mayer, 2011; Harp & Mayer, 1998; Mayer et al., 2001; Sung & Mayer, 2012), decorative images have been found to have neither a negative nor positive effect on learning (Sung & Mayer, 2012). The images chosen have only a covert association with the topic in that primarily they are digital images and the topic of the text was on digital images. Given that the images have no overt association to the topic they are perhaps more similar to decorative images rather than seductive images. In either case it would be desirable to redesign the environment to be more overt in distracting participants.
The second difference from previous research on auditory distractions lies in the fact that task difficulty was altered using the readability of the text rather than by changing the font used. The reason for this is because we are interested in investigating reading behaviour and the effects of distractions on reading. This is different to previous studies where only the outcomes of reading, in terms of comprehension, recall, or time taken, and not the reading process itself. There is a large body of research on reading behaviour that we can compare against. For these reasons, we decided to change the readability instead of the font. In the study a sans serif font was used throughout the whole experiment, namely Verdana. However, the hard to read font used by Halin et al. (2014b) was the sans serif font Haettenschweiler and the easy to read font was serif font Times New Roman. In follow-up studies the use of Times New Roman as the font for text display could be tested to see if the font indeed has an effect.
Finally, we hypothesized that the signals would reduce the distraction rate and help the reader recover after being distracted. Neither signal used in the experiment was found to affect the distraction rate; however, the distraction rates themselves are quite low. Even though on average participants were distracted about 5 times during the reading task, the distractions were short with only about 2% of recorded fixations lying on the distractions. Therefore, it is not surprising that the mitigation signals had little overt effect. Additionally, the distraction rates are highly variable between participants indicating that some participants are much more easily distracted than others.
D.5.1 Implications for eLearning
The pre-experiment questionnaire shows that there is a problem with participants being distracted by communication technologies whilst studying. There is a need to mitigate these distractions and help students in their learning. Attention guiding could be used to both minimize distraction of the learner as well as draw the learner’s attention to the important or relevant parts of the learning material.
248
Reading in distracting environments
 Another use of adaptive eLearning is to overcome the effects of distractions. Detection of distractions of readers could be used to determine whether text should be reshown to students. Additionally, labelling parts of the text that the reader was highly distracted during reading could be used to either show the student where they were distracted or be used to control what content is re-shown to the student, where the parts of text that the student was highly distracted during reading could be re-shown.
D.6 Future work
The study showed interesting results about the presence of distractions during reading and the potential of distraction mitigation signals, especially for L2 readers. However, the results from the study are preliminary, primarily due to the fact that more participants are needed and that better eye tracking technology needs to be used to produce more accurate eye tracking and thus better implementation of the distraction mitigation signals. Follow-up experiments are suggested to address these limitations of the experiments.
Furthermore, given the relatively low distraction rate observed in this study, it is suggested that the environment be made more distracting and have more overt distractions. In this way we could see if an even more distracting environment causes more distractions and therefore has a more prominent effect on eye gaze and reading behaviour. The optimal setting for this would be the use of wearable eye trackers that monitor the student being distracted off the laptop screen as well. Thus, we can induce more distractions such as those that come from mobile phones or televisions, as well as the onscreen distractions that were proposed. Additionally, we observed that some participants are more easily distracted than others, trying a within-subjects design could control for this.
We never investigated the case where no distractions are given to the reader. Including this case would allow us to investigate how, or if, distractions alter reading behaviour of participants. Additionally, this would allow us to investigate further the effects of text readability on distraction rates.
249
    CHI 2008 Proceedings · Works In Progress April 5-10, 2008 · Florence, Italy
Georg Buscher
Florian Mittag
Abstract
Dept. for Knowledge-Based Systems University of Kaiserslautern Kaiserslautern, Germany georg.buscher@dfki.de
Knowledge Management Dept. DFKI GmbH
Kaiserslautern, Germany fmittag@dfki.uni-kl.de
In this paper we describe a prototypical system that is able to generate document annotations based on eye movement data. Document parts can be annotated as being read or skimmed. We further explain ideas how such gaze-based document annotations could enhance document-centered office work in the future.
Andreas Dengel
Dept. for Knowledge-Based Systems University of Kaiserslautern
And
Knowledge Management Dept.
Keywords
DFKI GmbH Kaiserslautern, Germany andreas.dengel@dfki.de
General Terms
Ludger van Elst
ACM Classification Keywords
Knowledge Management Dept. DFKI GmbH
Kaiserslautern, Germany ludger.van_elst@dfki.de
H.5.2 [User Interfaces]: Input devices and strategies
Copyright is held by the author/owner(s). CHI 2008, April 5–10, 2008, Florence, Italy. ACM 978-1-60558-012-8/08/04.
In the last few years much effort has been put into the further development of eye tracking devices. Nowadays they have reached such a state of development that they are unobtrusive and easy to use and that their accuracy is sufficient for many practical applications. Since their development is progressing further, there is a good chance that they will become more affordable and, as a consequence, become more widespread. This
Generating and Using Gaze-Based Document Annotations
Eye tracking, reading detection, line matching, OCR, document-centered office work
Design, Measurement
Introduction
3045
  CHI 2008 Proceedings · Works In Progress
April 5-10, 2008 · Florence, Italy
means that their main areas of application of today (e.g., usability studies) will be shifted to bigger areas like, e.g., applications in the office environment.
A related area of research focuses on gaze-based pro- active information delivery. The scenario of [5] is to provide the reader with translations of unknown words in a text of a foreign language. The work aimed at de- tecting unknown words automatically by analyzing eye movements. The application described in [7] used gaze data, among other behavioral data, to provide the user with relevant information for the currently viewed or worked-with document. Here, simply looking at an en- try on a search result page for a longer time is taken as positive relevance feedback from the user. Then the appropriate entire document is displayed automatically.
The office environment is the focused application area in this paper. Especially for knowledge workers, most work done in an office is document-centered work. To- day, many documents are viewed and edited with the computer and we think that such digital document processing will increase in the future (be it on large screens, on e-paper, or on normal paper that is tightly coupled with a digital representation). In the digital environment, meta-data such as annotations for a doc- ument can be useful for a variety of applications. E.g., highlightings are typical user-generated annotations. They often indicate interest but are rarely created. Since the eyes are (almost always) involved while working with documents, gaze-based document anno- tations are far more frequent and one can expect that they can be used to make office work more effective.
The work in [1, 2, 3, 4] focuses on interpreting eye movements to come to higher abstractions of the gaze data. They describe methods that detect whether a person is currently reading by analyzing the sequence of eye movements. Our method for creating gaze- based document annotations uses ideas of that work.
In this paper we describe a robust method for gaze- based document annotation and present ideas directing our future research, i.e., how such annotations should be used for more effective office work. But first, we give a rough overview of related work.
Roughly spoken, eye movements are composed of fixa- tions and saccades. During a fixation the eyes are stea- dily gazing at one point. A saccade is a quick move- ment from one fixation to the next. Since the sequence of fixations and saccades is very characteristic during reading behavior, it is possible to detect whether a per- son is currently reading or skimming a text.
Related Work
Some research has been conducted in the past related to eye tracking while working with documents. Gaze data has been used for estimating the relevance of a text in an information retrieval task [2, 9]. The studies aimed at finding and combining eye movement meas- ures in order to create a system that automatically pre- dicts relevance of a viewed text.
Reading and Skimming Detection
From Gaze Data to Document Annotations
As described in [3], we conceptualized and imple- mented an algorithm being capable of detecting and differentiating between reading and skimming behavior. For testing its functionality, we applied a Tobii 1750 desk-mounted eye tracker which has a data generation frequency of 50 Hz and an accuracy of around 40 pixel
3046
      CHI 2008 Proceedings · Works In Progress
April 5-10, 2008 · Florence, Italy
a) Sequences of sac- cades over text solid: read
dashed: skimmed
b) Horizontally aver- aged saccade se- quences
c) The third saccade sequence is matched with the third text row. Green and blue rectan- gles indicate detected nearby text rows.
figure 1. Three processing steps to match saccade sequences with actual text rows.
at a resolution of 1280x1024. In short, the algorithm works in several steps: First, the raw gaze data is ac- cumulated to fixations. Second, the saccades between fixations are classified according to their directions and distances. If there are enough successive saccades that might belong to reading (or skimming) behavior of one text row, then reading (or skimming) is detected along the path of the saccades on the fly. Figure 1a shows some paths of saccades belonging to reading (solid lines) and skimming (dashed lines) behavior. The dif- ferentiation between reading and skimming is simply done based on the saccade lengths, but this will not be discussed here any further.
lines horizontally: the averaged horizontal position is the average of all fixations’ horizontal positions. (Since the accuracy of the eye tracker varies spatially, we do not take a duration-weighted average.) See figure 1b for an example.
Mainly due to eye tracker inaccuracies the saccade paths are not very smooth (especially horizontally). Therefore, we apply a method that averages the zigzag
The eye tracker inaccuracies do not only consist of noise that can be managed with simply by averaging values. What is more, the inaccuracy is biased and var- ies from person to person. To handle this bias in a ge- neric way, we apply OCR (optical character recognition) techniques: If a sequence of saccades has been classi- fied as reading or skimming behavior, a small screen- shot around all the saccades is taken and analyzed by the OCR system OCRopus [8]. This system returns the positions of all text rows on the screenshot (see figure
Matching the Lines
3047
   CHI 2008 Proceedings · Works In Progress
April 5-10, 2008 · Florence, Italy
1c). By moving the horizontally averaged line of saccades to the nearest text row, we match the eye-tracked line with the actually read text on the screen.
To get increased preci-
sion in matching the
lines we apply two main heuristics: Since we always have two lines to match – the saccade line and the most plausible text row – we can use the spatial differ- ence between them to recalibrate the eye tracking de- vice on the fly. The second heuristic takes advantage of the mostly sequential nature of reading behavior: one normally reads line by line from top to bottom. If a sac- cade line is located between two text rows and the up- per text row has been read or skimmed immediately before, then the lower text row is more likely to be chosen as the matching row. A very preliminary case study indicated that the accuracy of the line matching is around 90%. In the remaining 10% a line directly above or below the currently read line was selected.
stored (i.e., reading or skimming) with the annotation but also, e.g., a timestamp and the duration needed for reading. Moreover, since wikis are typically used by many people, the name of the reader (if logged in) is also saved. Figure 2 gives an impression of an anno- tated wiki-document.
Annotating a Document
Recontextualization
Now that we matched each saccade line with the most plausible text row, we can annotate the document ac- cordingly. Since not all applications are capable of stor- ing annotations or do that differently, we prototypically extended the wiki system Kaukolu [6] to be able to handle annotations. Hence, when a person reads an article presented by that wiki, the read and skimmed text parts automatically get annotated appropriately. However, not only the type of reading behavior is
An obvious way is to use such document annotations for recontextualization purposes. It is often happening that one re-opens a document after some time in order to find exactly the same information as during the first time. Especially when the document has dozens or hundreds of pages (e.g., an e-book) the time needed to find the specific piece of information can be annoyingly long. Since one often knows from the first time viewing the document that the information has to be in there
figure 2. Prototypical visualization of a gaze-based annotation for a wiki document.
Usage of Gaze-Based Document Annotations
By keeping track of the user’s eye movements over a document, our implemented system can gather very valuable attention information being useful for a variety of applications. Some use-cases that we think are worth pursuing in the future are shortly presented in the following.
3048
  CHI 2008 Proceedings · Works In Progress
April 5-10, 2008 · Florence, Italy
somewhere, it would be helpful to see which document parts have been viewed before. So, the gaze-based annotations can be used as a document-internal filter.
be applied when he or she wants to obtain information (e.g., a new argument) that he or she is not aware of.
Such a filter would be especially useful for people like, e.g., lawyers who often have many ongoing cases in parallel. Such cases often include a number of long documents where only a few paragraphs really matter to the case. When a lawyer switches from one case to a previous one, it might be very helpful to get a quick overview of the text parts that mattered before, i.e., that had been viewed in the past.
Normally, after putting in a query, desktop search en- gines return a list of documents as the result. For most search engines this list contains abstracts for each doc- ument showing the most characteristic sentences con- taining the query terms. Gaze-based document annota- tions could influence the methods for generating those document abstracts. As a consequence, the user might be able to recognize the important contents of the doc- uments more quickly and might not have to open the whole document. This is might be especially useful for re-finding information.
Looking for something known or unknown?
In the last years, desktop search engines became more and more popular. They help to find documents stored on the own computer. Here, gaze-based document an- notations provide the possibility for a new kind of search filter: does the user want to re-find information or to find new information? In the former case, the search engine should ignore all not viewed text parts of the documents in the search process. In the latter case, it should only consider not viewed or roughly skimmed text parts.
Searching for similar attention patterns
Such a filter function would also be useful in a lawyer scenario: Some lawyers have very large collections of law comments on their computers (i.e. comments on how the different laws should be interpreted). Such collections are normally used as reference books. Therefore, a search engine that could explicitly distin- guish between searches for content that has been read before and not viewed content could be very helpful. The former case could be useful when the lawyer wants to make sure a remembered fact. The latter case might
It is also conceivable that gaze-based annotations can be used for finding documents that other people have used with a similar interest. For example in an enter- prise one could think of a central information system that hosts a lot of documents (say, several thousands, e.g. scientific papers) that might be interesting for the employees. From time to time it happens that an em- ployee has a similar interest like another employee be- fore. Considering a research institute as an example, one researcher, researcher 2, might want to get an overview of how a certain technique has been used in a specific domain. But he or she does not know that some weeks ago, a colleague, researcher 1, had exactly the same interest, looked through several papers, and found some interesting text parts. Hence the current researcher cannot take advantage of the first re- searcher’s experiences and has to get an own, inde- pendent overview.
Personalized Search Result Abstracts
3049
  CHI 2008 Proceedings · Works In Progress
April 5-10, 2008 · Florence, Italy
However, having kept track of both people’s eye movements on the opened papers, the central informa- tion system could identify a similarity in the usage of the papers by both researchers. I.e., it could notice that there is a certain similarity in the currently viewed document parts by researcher 2 and 1. Having noticed such a similarity, the system could suggest those pa- pers and paper parts to researcher 2 that researcher 1 has read intensely but that researcher 2 has not viewed, yet In that way, researcher 2 could also be alerted if he or she opened and skimmed an article but missed the section being most interesting to researcher 1.
for gaze-enhanced applications that we believe would make office work much more effective. As eye trackers will become cheaper and more precise, they could be convincing applications of the future.
In this scenario, gaze-based document annotations are useful for identifying similar viewing patterns and for calling the user’s attention to probably important sec- tions that would otherwise be missed.
Citations
Conclusion
[2] Brooks, P., Phang, K. Y., Bradley, R., Oard, D., White, R., and Guimbretire, F. Measuring the utility of gaze detec- tion for task modeling: A preliminary study. IUI’06. 2006.
In this paper, we described a prototypical system that interprets eye movements recorded by an eye tracker and uses this information for automatic gaze-based annotations of documents. By incorporating ORC (opti- cal character recognition) techniques our method is able to determine the text lines that have been read or skimmed even if the eye tracker provides imprecise and biased gaze location data. Read and skimmed lines that have been detected as such can be annotated in a wiki system.
[3] Buscher, G., Dengel, A., van Elst, L. Eye Movements as Implicit Relevance Feedback. CHI ’08 extended ab- stracts (accepted for publication), 2008.
Gaze-based document annotations open up a wide range of possibilities of how to enhance document- centered work. Since the eye is always involved while working with documents and reflects the user’s atten- tion in a way, such annotations are of great value for different kinds of applications. We described some ideas
[7] Maglio, P.P, Barrett, R., Campbell, C.S., Selker, T. SUITOR: An attentive information system. IUI’00. 2000.
Acknowledgements
We thank Daniel Keysers and Christian Kofler for pro- viding a customized version of OCRopus. This work was supported by the German Federal Ministry of Education, Science, Research and Technology (bmb+f), (Grant 01 IW F01, Project Mymory: Situated Documents in Per- sonal Information Spaces).
[1] Beymer, D., and Russell, D.M. WebGazeAnalyzer: a system for capturing and analyzing web reading behavior using eye gaze. Proc. CHI '05. 2005.
[4] Campbell, C.S., and Maglio, P.P. A robust algorithm for reading detection. Proc. PUI ’01, 2001.
[5] Hyrskykari, A. Eyes in Attentive Interfaces: Experi- ences from Creating iDict, a Gaze-Aware Reading Aid. PhD thesis. University of Tampere, Dept. of CS. 2006.
[6] Kiesel, M. Kaukolu – hub of the semantic corporate intranet. Proc. Semantic Wiki Workshop at ESWC 2006.
[8] OCRopus, an open source document analysis and OCR system (http://code.google.com/p/ocropus/).
[9] Puolamäki, K., Salojärvi, J., Savia, E., Simola, J., and Kaski, S. Combining eye movements and collaborative filtering for proactive information retrieval. Proc. SIGIR ‘05. ACM Press (2005), 146–153.
3050
     Atlantis Briefs in Artificial Intelligence
Series Editors: Henrik Christensen · Bernhard Nebel · Qiang Yang
Introduction to Text Visualization
Nan Cao Weiwei Cui

Atlantis Briefs in Artificial Intelligence Volume 1
Series editors
Henrik Christensen, Atlanta, USA Bernhard Nebel, Freiburg, Germany Qiang Yang, Hong Kong, China
More information about this series at htttp://www.atlantis-press.com
Nan Cao • Weiwei Cui
Introduction to Text Visualization

Nan Cao
IBM T. J. Watson Research Center Yorktown Heights, NY
USA
Weiwei Cui
Microsoft Research Asia Beijing
China
Atlantis Briefs in Artificial Intelligence
ISBN 978-94-6239-185-7 ISBN 978-94-6239-186-4 (eBook) DOI 10.2991/978-94-6239-186-4
Library of Congress Control Number: 2016950403
© Atlantis Press and the author(s) 2016
This book, or any parts thereof, may not be reproduced for commercial purposes in any form or by any means, electronic or mechanical, including photocopying, recording or any information storage and retrieval system known or to be invented, without prior permission from the Publisher.
Printed on acid-free paper
Acknowledgements
We would like to thank Prof. Yu-Ru Lin from University of Pittsburgh for her initial efforts on discussing the outline and the content of this book. We also would like to thank Prof. Qiang Yang from the Hong Kong University of Science and Technology who invited us to write the book.
v
Contents
1 Introduction.............................................. 1 1.1 InformationVisualization................................ 1 1.2 TextVisualization...................................... 8 1.3 BookOutline......................................... 9 References................................................ 10
2 OverviewofTextVisualizationTechniques..................... 11
2.1 ReviewScopeandTaxonomy ............................ 11
2.2 VisualizingDocumentSimilarity .......................... 13
2.2.1 ProjectionOrientedTechniques...................... 13
2.2.2 SemanticOrientedTechniques ...................... 15
2.3 RevealingTextContent ................................. 16
2.3.1 SummarizingaSingleDocument .................... 16
2.3.2 ShowingContentattheWordLevel.................. 18
2.3.3 VisualizingTopics ............................... 21
2.3.4 ShowingEventsandStoryline ...................... 24
2.4 VisualizingSentimentsandEmotions....................... 28
2.5 DocumentExplorationTechniques......................... 31
2.5.1 DistortionBasedApproaches ....................... 32
2.5.2 Exploration Based on Document Similarity. . . . . . . . . . . . . 32
2.5.3 HierarchicalDocumentExploration................... 33
2.5.4 SearchandQueryBasedApproaches................. 33
2.6 SummaryoftheChapter................................. 34
References................................................ 35
3 DataModel.............................................. 41
3.1 DataStructuresattheWordLevel......................... 43 3.1.1 BagofWordsandN-Gram......................... 43 3.1.2 WordFrequencyVector........................... 43
3.2 DataStructuresattheSyntactical-Level..................... 44
vii
viii
Contents
4
5
VisualizingDocumentSimilarity ............................. 49
4.1 ProjectionBasedApproaches............................. 49 4.1.1 LinearProjections................................ 50 4.1.2 Non-linearProjections............................. 51
4.2 SemanticOrientedTechniques............................ 54
4.3 Conclusion........................................... 55
References................................................ 55
VisualizingDocumentContent............................... 57
5.1 “WhatWeSay”:Word.................................. 58 5.1.1 Frequency...................................... 59 5.1.2 FrequencyTrend................................. 67
5.2 “HowWeSay”:Structure................................ 74
5.2.1 Co-occurrenceRelationships........................ 75
5.2.2 ConcordanceRelationships......................... 78
5.2.3 GrammarStructure............................... 79
5.2.4 RepetitionRelationships........................... 82
5.3 “WhatCanBeInferred”:Substance........................ 84
5.3.1 Fingerprint ..................................... 84
5.3.2 Topics......................................... 87
5.3.3 TopicEvolutions................................. 90
5.3.4 Event.......................................... 93
5.4 SummaryoftheChapter................................. 96
References................................................ 97
3.3
DataModelsattheSemanticLevel ........................ 45 3.3.1 NetworkOrientedDataModels...................... 45 3.3.2 Multifaceted Entity-Relational Data Model . . . . . . . . . . . . . 46 SummaryoftheChapter................................. 48
3.4
References................................................ 48
6 VisualizingSentimentsandEmotions ......................... 103
6.1 Introduction .......................................... 103
6.2 VisualAnalysisofCustomerComments .................... 107
6.3 VisualizingSentimentDiffusion........................... 109
6.4 Visualizing Sentiment Divergence in Social Media . . . . . . . . . . . . 111
6.5 Conclusion........................................... 113
References................................................ 113
Chapter 1 Introduction
Abstract Text is one of the greatest inventions in our history and is a major approach to recording information and knowledge, enabling easy information sharing across both space and time. For example, the study of ancient documents and books are still a main approach for us to studying the history and gaining knowledge from our predecessors. The invention of the Internet at the end of the last century significantly speed up the production of the text data. Currently, millions of websites are gen- erating extraordinary amount of online text data everyday. For example, Facebook, the world’s largest social media platform, with the help of over 1 billion monthly active users, is producing billions of posting messages everyday. The explosion of the data makes seeking information and understanding it difficult. Text visualization techniques can be helpful for addressing these problems. In particular, various visu- alizations have been designed for showing the similarity of text documents, revealing and summarizing text content, showing sentiments and emotions derived from the text data, and helping with big text data exploration. This book provides a system- atical review of existing text visualization techniques developed for these purposes. Before getting into the review details, in this chapter we introduce the background of information visualization and text visualization.
1.1 Information Visualization
In 1755, the French philosopher Denis Diderot made the following prophecy:
As long as the centuries continue to unfold, the number of books will grow continually, and one can predict that a time will come when it will be almost as difficult to learn anything from books as from the direct study of the whole universe. It will be almost as convenient to search for some bit of truth concealed in nature as it will be to find it hidden away in an immense multitude of bound volumes. – Denis Diderot
About two and a half centuries later, this prophecy has come true. We are facing a situation of Information Overload, which refers to the difficulty a person may have in understanding an issue and making decisions because of the presence of too much information. However, Information Overload is not mainly caused by the growth of books but mainly by the advent of the Internet.
© Atlantis Press and the author(s) 2016 1 C. Nan and W. Cui, Introduction to Text Visualization, Atlantis Briefs
in Artificial Intelligence 1, DOI 10.2991/978-94-6239-186-4_1
2 1 Introduction
Several reasons could be cited for the Internet accelerating the process of infor- mation overload process. First, with the Internet, the generation, duplication, and transmission of information has never been easier. Blogging, Twitter, and Facebook provide ordinary people the ability to efficiently produce information, which could be instantaneously accessed by the whole world. More and more people are considered active writers and viewers because of their participation. With the contribution of users, the volume of Internet data has become enormous. For example, 161 exabytes of information were created or replicated in the Internet in 2006, which were already more than that the generated information in the past 5000 years [6]. In addition, the information on the Internet is constantly updated. For example, news websites pub- lish new articles even every few minute; Twitter users post millions of tweets every day, and old information hardly leaves the Internet. For this kind of huge amount of information, analysis requires digging through historical data, which clearly compli- cates understanding and decision making. Furthermore, information on the Internet is usually uncontrolled, which likely causes high noise ratio, contradictions, and inaccuracies in available information on the Internet. Bad information quality will also disorientate people, thereby causing the information overload.
Understanding patterns in a large amount of data is a difficult task. Sophisticated technologies have been explored to address such an issue. The entire research field of data mining and knowledge discovery are dedicated to extracting useful informa- tion from large datasets or databases [5], for which data analysis tasks are usually performed entirely by computers. The end users, on the other hand, are normally not involved in the analysis process and passively accept the results provided by computers.
These issues could be addressed via information visualization techniques whose primary goal is to assist users see information, explore data, understand insightful data patterns, and finally supervise the analysis procedure. Research in this filed are motivated by the study of perceptions in psychology. Scientists have shown that our brains are capable of effectively processing huge amounts of information and signals in a parallel way when they are properly visually represented. By turning huge and abstract data, such as demographic data, social networks, and document corpora, into visual representations, information visualization techniques help users discover patterns buried inside the data or verify the analysis results.
Various definitions of information visualization exist [1, 3, 7] in current literature. One of the most commonly adopted definitions is that of Card et al. [2]: “the use of computer-supported, interactive visual representations of abstract to amplify cogni- tion”. This definition highlights how visualization techniques help with data analysis, i.e., the computer roughly processes the data and displays one or some visual rep- resentations; we, the end users, perform the actual data analysis by interacting with the representations.
A good visualization design is able to convey a large amount of information with minimal cognitive effort. Considered as a major advantage of visualization techniques, this feature is informally described by the old saying “A picture is worth
1.1 Information Visualization 3
a thousand words”.1 Communicating visually is more effective than using text based on the following reasons:
• The human brain can process several visual features, such as curvature and color, much faster than symbolic information [12]. For example, we can easily identify a blue dot in a large number of red dots, even before consciously noticing it.
• Information visualization takes advantage of the high bandwidth of the human perceptual system in rapidly processing huge amounts of data in a parallel way [13].
• Information visualization could change the nature of a task by providing external memory aids, and by providing information that could be directly perceived and used without being interpreted and formulated explicitly [14]. These external aids are described by Gestalt laws [8], which summarize how humans perceive visual patterns.
Another major advantage of visualization is “discovering the unexpected” [11, 13]. Normal data mining or knowledge discovery methods require a priory question or hypothesis before starting the analysis. Without any priory knowledge to a data, we will have to enumerate all possibilities, which is time-consuming and insecure. Meanwhile, information visualization is an ideal solution for starting the analysis without assumptions, and can facilitates the formation of new hypotheses [13].
A popular example for “discovering the unexpected” is Anscombe’s quartet (see Fig. 1.1a), which consists of four sets of data. If we only look at the statistics that describe each of them (see Table 1.1), we may easily jump to the conclusion that these four datasets are very similar. However, this idea has been disproven. Figure1.1b shows the scatter plots for each data set. Everyone can tell that these four datasets are not alike at all. Once people see the differences, they then may have a better idea on choosing the correct statistical metrics. In addition, information visualization requires less intense data analysis compared with data mining techniques. There- fore, by faithfully visualizing information in the way, the data are collected, instead of overcooking it, and data errors or artifacts may have higher chances to reveal themselves [13].
The development of computer hardware, particularly the human computer inter- face equipment and advanced graphics cards in the mid-1990s, has further facilitated people’s exploration of global and local data features [13]. In 1996, Ben Shneider- man systematically presented the visual information seeking mantra [10] to design information visualization systems: “overview first, zoom and filter, then details-on- demand”. This mantra categorizes a good visualization process into three steps, which allow users to explore datasets by using various level-of-detail visual repre- sentations. At the first step, visualization should provide the general information on entire datasets. Without holding assumptions, we could have a less-biased perspec- tive of the entire data, understand its overall structure, and finally identify interesting
1This phrase “A picture is worth a thousand words” was first used by Fred R. Barnard in an advertisement entitled “One Look is Worth A Thousand Words.” in 1921. In another advertisement designed by Barnard in 1927, he attached a Chinese proverb to make the ad more serious. Sadly, there is no evidence shows that the attached Chinese proverb really exists in Chinese literature.

  4 1 Introduction
 Fig. 1.1 Anscombe’s quartet: a four different datasets; b scatter plots corresponding to each data set in (a)
areas to explore. At the second step of “zoom and filter”, we isolate the areas of inter- est and generate reasonable hypotheses regarding patterns that draw our attentions. Finally, the details of the data should be displayed for further analysis, and eventually confirming or disprove our hypotheses. Given that details are extracted from an iso- lated area, we could avoid being overwhelmed by the large quantity of information

1.1 Information Visualization
5
Table 1.1 Same statistics in Anscombe’s quartet Property (in each set)
Mean of x
Variance of x
Mean of y
Variance of y
Correlation between x and y Linear regression line
Value
9.0
10.0
7.50
3.75
0.898
y = 0.5x + 3.0
               and analyze the details more efficiently. Many visual analysis approaches follow the same mantra, offering general users an interactive and intuitive way to explore their data.
In addition, real world data usually have multiple data dimensions that show information from different aspects. For example, user profiles in a social network are usually multidimensional in describing the age, sex, and location of users. The description of product features is multidimensional; even an unstructured text corpora is multidimensional because of it consists of multiple topics. Increasing the data vari- ables and dimensions leads to a more precise description of information. However, this feature significantly increases the difficulties of data analysis. Understanding a four dimensional dataset already exceeds the human capability of perception, let alone detects the multidimensional data patterns by analysis. This challenge has attracted great attentions in the past decades. Efforts have been made to reveal rela- tional patterns such as correlations, co-occurrences, similarities, dissimilarities, and various sematic relations such as friendships in a social network and causalities of news events.
Many sophisticated data mining techniques, such as correlation analysis, clus- tering, classification, and association rule detection have been developed to detect relational patterns among data entities based on their attributes. For example, data entities within the same cluster or class show similarity of certain attributes. Although these techniques have been successfully applied to analyze large multidimensional datasets, such as documents and databases, they have critical limitations.
First, data mining techniques are designed to solve or verify predefined tasks or assumptions of datasets. These techniques always lead to expected analysis results. For example, the correlation of two data variables A and B could be tested by corre- lation analysis. A cluster analysis divides data entities into a predefined number of groups. These techniques are highly driven by user experiences in data analysis and their priori domain knowledge about the datasets. Therefore, a challenging problem that requires new techniques is the detection of unexpected data patterns of common users, who have minimal data analysis experiences, using limited prior knowledge.
Second, training sets or ground truths of a dataset are unavailable, which makes machine learning and evaluation difficult. Lacking training sets disables supervised learning, such as classification. Moreover, lacking of ground truth causes difficulty
6 1 Introduction
in evaluating the analysis results of unsupervised learning, such as cluster analysis. To explain this point effectively, we examine the detailed process of typical cluster analysis. Cluster analysis is a widely used analytical method to group data entities into subsets called clusters, such that the entities in each cluster are similar in a certain way.
In this process, users are only required to choose a distance function (e.g., Euclidean distance) that measures similarity of two data items in a feature space, as well as other parameters, such as the number of clusters or a maximum cluster diameter. These predefined parameters are critical in the analysis and challenging to decide. For example, users must provide the number of clusters (i.e., k) for the well-known K-means algorithm. However, selecting a proper k value is difficult when the underlying given data are unknown ground truth. Therefore, algorithms such as K -means might group together entities that are semantically different (when k is smaller than the real number of clusters) or separate entities that are semanti- cally similar (when k is larger than the real number of clusters). Therefore, without a ground truth, correct evaluation of analysis result is difficult.
Third, the analysis results of multidimensional dataset could be difficult for users to interpret because of several reasons. First, datasets may contain heterogeneous dimensions with different types. The inconsistency of the dimensions makes the analysis results difficult to understand. For example, a challenging task is to under- stand the changing of topic trends in a text flow given that the process requires extracting textual and temporal information from the data. Second, interpretation is still difficult even if all dimensions are of the same type. For example, cluster analysis of multivariate datasets may generate results that are difficult to understand. Specif- ically, in multivariate clusters, data items are grouped together if they are close in the multidimensional feature space. However, their similarities may be mainly due to their closeness in a subset of dimensions instead of all dimensions. Understanding these abstract relationships is challenging. Moreover, a cluster may contain several different sub-clusters that could have different meanings for users. This sub-cluster structure is usually difficult to detect. Third, complete datasets are heterogeneous when collected from multiple data sources. For example, to analyze the process of information diffusion in microblogs, we need to collect social connections between users, critical events as triggers of diffusion, and places where information is spread from data sources, such as Twitter, online news, and Google maps. Understanding such dataset could be difficult given that all pieces of data need to be seamlessly assembled to provide unique and meaningful information.
Information visualization could be of great value when addressing these problems. From the very beginning, information visualization has been one of the most impor- tant techniques to facilitate interpretation, comparison, and inspection of analytical results as well as the underlying raw data. In the 1990s, information visualization techniques started to be used for analyzing multidimensional datasets. Techniques, such as the pixel-oriented database visualization designed by Keim and Kriegel [9, 10] and the parallel coordinates for data correlation analysis designed by Inselberg [7] were introduced to review simple relational patterns. These early visualizations have been extensively studied over the past years, and various novel visualizations
1.1 Information Visualization 7
with rich interactions are designed to represent the multidimensional datasets from different perspectives. Several visualizations focus on visualizing the relationships of individual items and their attributes while others focus on representing the overview of data distributions. Based on these interactive visualizations, explorative visual analysis becomes a popular approach for detecting relational data patterns. Visual analysis collects user feedback through interactions to refine the underlying analysis models. This step allows for highly correct results and precise analysis controls that could be achieved. Particularly, the motivation behind interactive multidimensional visualizations is to help with automatic data analysis for testing assumptions, select- ing analysis factors, models, and estimators, as well as revealing data patterns and detecting outliers. To visualize intuitively the relationships inside a multidimensional dataset, three primary challenges have emerged.
The first challenge is the process of intuitively encoding data that contain multi- ple dimensions to reveal their innate relations. High dimensions add difficulty when visual primitives, such as nodes, and lines with visual features, such as colors, shapes, textures and positions, are limited. The limitation of such data makes encoding diffi- cult. In addition, several visual features conflict with each other and could not be used simultaneously (e.g., color and texture). In addition, several features are unsuitable for precise representation of certain types of data (e.g., color and area could not be used to represent numerical values precisely). Thus, intuitive encoding is required in determining a set of visual primitives with proper visual attributes as well as encod- ing data dimensions and their relations while minimizing visual clutter. In certain situations, data are split by dimensions and visualized in different views, such as PaperLens [9] because of the difficulty involved in encoding multiple dimensions within single visualization. In this case, the challenge is the seamless connection of these views to reveal the different aspects of data while preserving user mental maps and reducing the training process.
The second challenge is detecting relational patterns in the multidimensional datasets based on their visual representations. These patterns could be complicated given their inclusion of substructures and correlations over various dimensions, clus- ters over data entities, similarities of clusters, and various semantic relationships across heterogeneous data pieces. Identifying various relationships could be diffi- cult because raw datasets or analysis results are complex and difficult to represent. Moreover, automatic analysis may generate misleading results. Thus, visualizations should provide additional aid in representing the results and in error detection and evaluation.
Finally, the third challenge is the way interactions could refine analysis results and the underlying analysis models. For example, a critical problem occurs when the user states his/her constraints on visualization. Limited research has been conducted in this area. This challenge depends on new interaction techniques, and more importantly on several hybrid techniques, such as active learning and incremental mining.
The above challenges could be trickier when data are large and dimensions are high. In these situations, visual clutter, such as line crossings and node overlaps, are usually unavoidable. The computation performance could be another problem given that most visualization techniques require efficient computation to provide online
8 1 Introduction
layout and analysis for interaction to be conducted. The final and trickiest problem is the limitation of the cognitive capability of human beings. Therefore, users could understand how the visual representation of huge and high dimensional data becomes a problem. Therefore, many techniques have been designed to tackle these problems. For example, various visual clutter reduction methods [4] are introduced for differ- ent data types and visualizations. Statistical embedding techniques are designed to represent huge datasets based on statistical aggregation of individual data items. This design allows data overview to be clearly represented in chat applications. Dimen- sion reduction methods, such as projection, are introduced to map high-dimensional information spaces into low-dimension plains. Several visualizations are carefully designed based on the aforementioned techniques to provide intuitiveness of huge and high-dimensional datasets, which are surveyed in the following section.
1.2 Text Visualization
Large collections of text documents have become ubiquitous in the digital age. In areas ranging from scholarly reviews of digital libraries to legal analysis of large email databases and online social media and news data, people are increasingly faced with the daunting task of needing to understand the content of unfamiliar documents. However, this task is made challenging because of the extremely large text corpus and dynamical change in data over time, as well as information coming from multiple information facets. For example, online publication records contain information on authors, references, publication dates, journals, and topics. The var- ied data cause challenges in understanding how documents relate with one another within or across different information facets. To address this issue, interactive visual analysis techniques that help visualize such content-wised information in an intuitive manner have been designed and developed. These techniques enable the discovery of actionable insights.
Existing text visualization techniques were largely designed to deal with the fol- lowing three major forms of text data:
• Documents. A text document refers to the data, such as a paper, a news article, or an online webpage. Visualizations designed to represent documents usually focus on illustrating how various documents are related or on summarizing the content or linguistic features of a document to facilitate an effective understanding or comparison of various documents.
• Corpus. A corpus indicates a collection of documents. Visualizations designed to represent the corpus usually focus on revealing statistics, such as topics, of the entire dataset.
• Streams.Atextstream,suchasthepostingorretweetingofmessagesonTwitter,is a text data source in which text data are continuously produced. Visually displaying such kind of text stream helps illustrate the overall trend of data over time.
1.2 Text Visualization 9
Based on these types of data, visualizations are designed to assist in analysis tasks for various purposes and application domains. For example, techniques have been proposed to analyze topics, discourse, events, and sentiments, which can be further grouped into the following four categories:
• Showing Similarity. Techniques in this category are developed to illustrate content-wised similarities of different documents. Various similarity measure- ments have been proposed based on two major types of techniques, which are projection-based and semantic-oriented.
• ShowingContent.Mosttextvisualizationtechniqueshavebeenproposedtoillus- trate different aspects of the content of text data, such as summarizing the content of a single document and showing the topics of a corpus.
• Showing Opinions and Emotions. This category includes techniques that sum- marizing the sentiment or emotional profiles of persons based on the text data they produced.
• ExploringtheCorpus.Manytextdataexplorationsystemshavebeendevelopedto help analysts or end users to efficiently explore text data. Many of these techniques are based on information retrieval techniques and enable a visual query approach to retrieve information based on user interests.
1.3 Book Outline
This book presents a systematic review of existing text visualization techniques, from the elementary to the profound, and covers most of the critical aspects of visually representing unstructured text data.
The book consists of seven chapters in total. In particular, we first provide an overview of the entire text visualization field to our readers in Chap. 2 by introduc- ing a taxonomy based on existing techniques and briefly survey and introduce the typical techniques in each category. Transforming the unstructured text data into a structured form is a typical approach and the first step for visualizing data. In Chap. 3, we introduce typical data structures and models used for organizing the text data and the corresponding approaches of data transformation. Starting in Chap. 4, we introduce the detailed text visualization techniques following the taxonomy intro- duced in Chap. 2. In particular, we introduce the techniques for visualizing document similarity in Chap. 4, for showing document content in Chap. 5, and for visualizing sentiments and emotions in Chap. 6. Finally, we conclude the book in Chap. 7.
We suggest that readers finish the first three chapters one by one before reading the rest of the book. This suggestion will help the readers obtain basic ideas on text visualization as well as the current techniques and research trends in the field. Thereafter, readers could choose to read Chaps. 4–7 based on their interests.
10 1 Introduction
References
1. Averbuch, M., Cruz, I., Lucas, W., Radzyminski, M.: As You Like It: Tailorable Information Visualization. Tufts University, Medford (2004)
2. Card,S.,Mackinlay,J.,Shneiderman,B.:ReadingsinInformationVisualization:UsingVision to Think. Morgan Kaufmann, Los Altos (1999)
3. Chen, C.: Top 10 unsolved information visualization problems. IEEE Comput. Graph. Appl. 12–16 (2005)
4. Ellis,G.,Dix,A.:Ataxonomyofclutterreductionforinformationvisualisation.IEEETrans. Vis. Comput. Graph. 13(6), 1216–1223 (2007)
5. Hand,D.,Mannila,H.,Smyth,P.:PrinciplesofDataMining.TheMITPress,Cambridge(2001)
6. Hersh,W.:InformationRetrieval:AHealthandBiomedicalPerspective.Springer,Berlin(2009)
7. Keim,D.,Mansmann,F.,Schneidewind,J.,Ziegler,H.:Challengesinvisualdataanalysis.In:
Tenth International Conference on Information Visualization, 2006. IV 2006, pp. 9–16. IEEE
(2006)
8. Koffka,K.:PrinciplesofGestaltPsychology.PsychologyPress,MiltonPark(1999)
9. Lee, B., Czerwinski, M., Robertson, G., Bederson, B.B.: Understanding research trends in
conferences using paperlens. In: CHI’05 Extended Abstracts on Human Factors in Computing
Systems, pp. 1969–1972. ACM (2005)
10. Shneiderman,B.:Theeyeshaveit:ataskbydatatypetaxonomyforinformationvisualizations.
In: IEEE Symposium on Visual Languages, 1996. Proceedings, pp. 336–343. IEEE (1996)
11. Thomas,J.,Cook,K.:IlluminatingthePath:TheResearchandDevelopmentAgendaforVisual
Analytics. IEEE Computer Society, Los Alamitos (2005)
12. Treisman, A.: Preattentive processing in vision. Comput. Vis. Graph. Image Process. 31(2),
156–177 (1985)
13. Ware, C.: Information Visualization: Perception for Design. Morgan Kaufmann, Los Altos
(2004)
14. Zhang,J.:Thenatureofexternalrepresentationsinproblemsolving.Cogn.Sci.21(2),179–217
(1997). doi:10.1016/S0364-0213(99)80022-6. http://www.sciencedirect.com/science/article/ B6W48-3Y2G07V-S/2/7f030b21efbc2de49b719126601212a5
Chapter 2
Overview of Text Visualization Techniques
Abstract The increasing availability of electronic document archives, such as web- pages, online news sites, blogs, and various publications and articles, provides an unprecedented amount of information. This situation introduces a new challenge, which is the discovery of useful knowledge from large document collections effec- tively without completely going through the details of each document in the collec- tion. Information visualization techniques provide a convenient means to summarize documents in visual forms that allow users to fully understand and memorize data insights. In turn, this process facilitates data comparison and pattern recognition. Many text visualization techniques have been extensively studied and developed for different purposes since the 1990s. In this chapter, we briefly review these techniques to provide an overview of text visualization. Our survey is based on studies sum- marized in the online text visualization browser (http://textvis.lnu.se/). We classify different text visualization techniques regarding their design goals, which largely group existing techniques into five categories. These categories include techniques developed (1) for visualizing document similarity, (2) for revealing content, (3) for visualizing sentiments and emotions of the text, (4) for exploring document corpus, and (5) for analyzing various domain-specific rich-text corpus, such as social media data, online news, emails, poetry, and prose. Based on this taxonomy, we introduce the details of the primary text visualization research topics in the following chapters of this book.
2.1 Review Scope and Taxonomy
In this book, we have reviewed over 200 papers summarized in the Text Visualization Browser [61] (Fig. 2.1), which was developed by the ISOVIS Research Group from Linnaeus University in Sweden. This online tool provides the most comprehensive and up-to-date summary of text visualization that has been published. This browser enables a user to filter these techniques interactively according to tasks, data, appli- cation domain, and visual design styles, which greatly support the writing of this book, especially this chapter.
© Atlantis Press and the author(s) 2016 11 C. Nan and W. Cui, Introduction to Text Visualization, Atlantis Briefs
in Artificial Intelligence 1, DOI 10.2991/978-94-6239-186-4_2
12 2 Overview of Text Visualization Techniques
 Fig. 2.1 TextVis Browser, an interactive online browser of existing text visualization techniques
Among all the studies listed in the Text Visualization Browser, a set of 120 core text visualization papers were reviewed; these were published in key conferences and journals in three different but highly related areas, namely, visualization, data mining, and human-computer interaction. In particular, our review focuses on the related papers published in (1) visualization-related conferences and journals, such as IEEE International Conference on Information Visualization, IEEE International Conference on Visual Analytics Science and Technology, IEEE Transactions on Visualization and Computer Graphics, IEEE Computer Graphics and Applications, Computer Graphics Forum, and Information Visualization; (2) data mining-related conferences and journals, such as ACM sigKDD Conference on Knowledge Dis- covery and Data Mining, IEEE Transactions on Knowledge and Data Engineering, SIAM International Conference on Data Mining, IEEE International Conference on Data Engineering, and ACM International Conference on Information and Knowl- edge Management; (3) human-computer-interaction related conferences and journals including ACM sigCHI Conference on Human Factors in Computing Systems and ACM International Conference on Intelligent User Interfaces.
Text Visualization Browser provides four different ways to categorize existing techniques, i.e., categorizing by task (either analysis or visualization), by data to be visualized, by application domain, and by style of visualization design. How- ever, providing a clear taxonomy with minimum overlap among different technique categories is difficult. To address this issue, we provide a simple yet clear taxon- omy based on development goals and purposes of the existing works. In particular, we first separate the techniques into two parts, namely, visualization or interaction technique and various systems developed for different domains by employing these
2.1 Review Scope and Taxonomy 13
techniques. In particular, in terms of visualization technique, we classify related research into three categories based on goals. These categories include techniques developed for (1) visualizing document similarity, (2) revealing the content of the text data, and (3) showing sentiments and emotions. In this way, we discuss different types of techniques in this book clearly, as well as illustrate their applications and show examples of using these techniques together for solving application problems in different domains.
The rest of this chapter and the book is presented by following this taxonomy. We first briefly review existing techniques and applications in this chapter and describe the detail of major and important techniques in the subsequent chapters.
2.2 Visualizing Document Similarity
Representing content similarities at the document level is one of the most tradi- tional visualization techniques produced for summarizing document collections. These visualizations share a similar representation in which documents are visu- alized as points on a low-dimensional (2D or 3D) visualization plane. The distance between each pair of points represents the similarities between the corresponding two documents, and follows the rule of the closer, the more similar. Many similar techniques have been extensively studied and have been categorized as either (1) projection-oriented or (2) semantic-oriented.
2.2.1 Projection Oriented Techniques
Projection oriented techniques visualize documents through a dimension reduction procedure. In these techniques, a document is represented as a bag of words and formally described by an N-dimensional feature vector. To compute this vector, a set of most informative words W (|W| = N) that best differentiates each docu- ment (i.e., best captures the features of different documents) is ranked out from the entire document collection based on “Term Frequency Inverse Document Frequency (TF-IDF)”. As a well-known numerical statistic method designed to help extractive word features for document classification [78], the process computes a TF-IDF score for each word in the document collection. The word with a higher score is considered to be more informative, i.e., more useful than other words for classifying different documents. Before computing TF-IDF, stop words are removed and word stems are extracted to ensure the informativeness of each word and the correctness of the word frequency calculation in TF-IDF. Based on these words, an N-dimensional feature vector is produced for a document with each field indicating a word with top-ranking TF-IDF score and the field value indicating its frequency in the given document. Based on this feature vector, projection-oriented techniques visualize a document from the N-dimensional feature space into a 2D or 3D visualization space via a dimension reduction algorithm.
14 2 Overview of Text Visualization Techniques
 Fig. 2.2 Comparison of different Non-Linear Projection Techniques (also known as manifold learning). The results shown in this figure are is produced based on scikit-lear, which illustrates the results of projecting a “S-shaped” three dimensional dataset (the left most figure) onto a 2D plane. The visualization results illustrate that how the original distance in 3D space is preserved in a 2D plane via different algorithms
Generally, the projection oriented techniques can be further separated into (1) linear projections (i.e., linear dimension reduction) and (2) non-linear projections (also known as manifold learning or non-linear dimension reduction). Representa- tive techniques in linear projection include Principle Component Analysis (PCA) [51] and Linear Discriminant Analysis [5]. Both techniques could be formulated in con- sistent form in which pair-wise distances between data items are maximized and guided by weights that indicate the importance of separating pairs of points in the results [58]. These techniques, although effective in terms of computation, usually fail to capture data similarities when they are non-linear. Therefore, many non-linear projection techniques have been developed and extensively studied. Existing methods could be further classified into (1) distance-oriented techniques and (2) probabilis- tic formulation-based techniques. A comparison of different techniques is shown in Fig. 2.2.
Specifically, the distance oriented non-linear projection techniques such as Mul- tidimensional Scaling (MDS) [60] and Locally Linear embedding (LLE) [92] intro- duce different methods to preserve the distances in the high-dimensional feature space in a low (2D or 3D) dimensional visualization space. The probabilistic for- mulation based techniques such as Stochastic Neighbor Embedding (SNE) [45] and t-Distributed Stochastic Neighbor Embedding (t-SNE) [73] formulate document sim- ilarity via statistical models, in which the similarity between two documents i and
j is captured by the conditional probability of P(j|i), i.e., given that document i the
2.2 Visualizing Document Similarity 15
probability of document j are in the neighborhood of i in the feature space. Com- pared with distance oriented techniques, these probabilistic based approaches can be more effectively computed and can produce results with improved quality [73].
2.2.2 Semantic Oriented Techniques
These approaches represent document similarity via latent topics extracted from text data. Studies in this direction are mainly inspired by topic modeling techniques such as Probabilistic Latent Semantic Analysis (PLSA) [46], Latent Dirichlet Allo- cation (LDA) [6], Spherical Topic Model (SAM) [87], and Non-Negative Matrix Factorization (NMF) [64]. Although widely used for analysis, these topic modeling techniques are not designed for visualization purpose, thereby directly showing the analysis results is usually difficult for users to interpret. For example, PLSA and LDA analyze topics in a simplex space which is shown as a triangle on the 2D Euclid- ean visualization plane. Therefore, these methods cannot display more than three topics a time (Fig.2.3d). Semantic oriented techniques have proposed to produce improved visual representations. These techniques were pioneered by Probabilistic Latent Semantic Visualization (PLSV ) [47] which embeds the latent topics and docu-
Fig. 2.3 Visualizing document similarity based on semantic oriented techniques (a, d, e) and non- linear projection (b, c). Each point in the diagram is a document colored by their primary topics that are extracted based on topic modeling. The distances between documents encode their similarities, following the rule of “the more similar, the closer”. This figure represent visualizations of the same data, i.e., papers published in NIPS, which was first published in [47]

16 2 Overview of Text Visualization Techniques
ments in the generic Euclidean space at the same time as the distances directly encode similarities among documents, regarding their shared topics (Fig. 2.3a). Following this work, other techniques such as Spherical Semantic Embedding (SSE) [63] were also developed, which provides improved approximations of similarities among doc- uments. When compared to the projection oriented techniques (Fig. 2.3b, c), semantic based approaches usually produce more meaningful results that are easier for users to understand.
2.3 Revealing Text Content
Visually representing the content of a text document is one of the most important tasks in the filed of text visualization. Specifically, visualization techniques have been developed to show the content of documents from different aspects and at different levels of details, including summarizing a single document, showing the words and topics, detecting events, and creating storylines.
2.3.1 Summarizing a Single Document
Existing visualization techniques summarize a document through two main aspects: (1) content such as words and figures and (2) features such as average sentence length and number of verbs.
In terms of showing the content of a document, Collins et al. [19] introduce DocBurst which decompose a document into a tree via its innate structures such as sections, paragraphs, and sentences that are illustrated in a SunBurst visualiza- tion [97] (Fig. 2.4). Rusu et al. [93] visualize the content of a document via a node-link diagram based on a semantic graph extracted from the document. Strobelt et al. [99] introduce a system that transforms a document into cards, in which the content of the document is summarized via keywords and critical figures that are extracted from the document (Fig.2.5). Stoffel et al. [98] propose a technique for producing the thumbnail of a document based on keyword distortion. This technique produces a focus+context representation of document at the page level. In particular, on each page of the document, important words are shown in a larger font whereas the rest ones are suppressed as the context, thereby compressing the entire page into a small thumbnail without losing the key information of each document page.
Despite the aforementioned studies, document fingerprint [50, 55, 80] is another typical visualization technique that is developed to summarize a single document. Instead of showing words and figures, this technique captures the key features of a document from multiple aspects through a heatmap visualization in which each cell represents a text block (e.g., a paragraph or a sentence) with color showing its feature value (Fig.2.6). A set of linguistic features were used to measure the document from different aspects, as summarized in [55], including (1) statistical
  2.3 Revealing Text Content 17
 Fig. 2.4 DocuBurst visualization of a science textbook rooted at idea. A search query for words starting with pl has been performed. Nodes matching the query are highlighted in gold
features such as average word length, average number of syllables per word and average sentence length, (2) vocabulary features such as the frequencies of specific words and vocabulary richness measured by Simpson Index, and (3) syntax features computed based on syntax trees [54].

18 2 Overview of Text Visualization Techniques
  Fig. 2.5 Summarization of the IEEE InfoVis 2008 proceedings corpus in Document Cards (a portion). Referring to [98] for the complete visual summarization of the whole proceeding
Fig. 2.6 The literature fingerprint visualization showing the feature “average sentence length” of books written by Jack London and Mark Twain
2.3.2 Showing Content at the Word Level
Directly illustrating the keywords of a document is the most intuitive approach to present document content. Existing visualization techniques in this category are largely developed to address three general problems: (1) how to represent the words esthetically in a visual form to clearly depict the content of the text; (2) how to

2.3 Revealing Text Content 19
 Fig. 2.7 Wordle visualization of a bag of words extracted from text data
summarize and represent the semantic relationships such as “A is B” and “A of B” between words in the text, and (3) how to reveal word-level patterns such as repetitions and co-occurrences.
TagCloud [53] is one of the most intuitive and commonly used techniques for visualizing words. It illustrates a bag of words that summarize the content of the input text data in a cloud form, in which words, with font size indicating their importance, are packed together without any overlap. Traditional TagCloud aligns words line by line, which is most commonly used in webpages to show the content of, for example, the current web. Different packing strategies will result in various types of TagClouds [13, 14, 20, 35, 104, 114], among which Wordle is the state-of-the-art technique that produces aesthetic word packing results by precisely calculating the word boundary and randomly inserting the word into empty spaces guiding by a spiral line (Fig. 2.7). Despite these static techniques, dynamic word clouds [24] are also developed for showing the changes of the text content of a streaming corpus such as Twitter and publication dataset over time.
Although widely used, TagClouds fail to uncover the word relationship as usually the words are randomly placed. Therefore, many tree or graph based visualization techniques are introduced. For example, WordTree [109] (Fig. 2.8) summarizes text data via a syntax tree in which sentences are aggregated by their sharing words and split into branches at a place where the corresponding words in the sentences are divergent. Another example is the PhraseNet [103] (Fig. 2.9). This visualization employs a node-link diagram, in which graph nodes are keywords and links repre- sent relationships among keywords which are determined by a regular expression indicated by users. For example, as shown in Fig. 2.9, a user can select a predefined regular expression from a list to extract a relationship such as “X and / is / of Y” or by inputting the regular expression by their own.
Despite relationships, visualizations are also developed to illustrate highly detailed patterns such as word co-occurrences and repetitions [4, 49, 108]. For example, Wattenberg introduced the Arc diagram (Fig. 2.10), which is one of the earliest visu-
20
2 Overview of Text Visualization Techniques
  Fig. 2.8
Word tree of the King James Bible showing all occurrences of love the
  Fig. 2.9
The Phrase Net user interface applied to James Joyce Portrait of the Artist as a Young Man. The user can select a predefined pattern from the list of patterns on the left or define a custom pattern in the box below. This list of patterns simultaneously serves as a legend, a list of presets and an interactive training mechanism for regular expressions. Here the user has selected X and Y, revealing two main clusters, one almost exclusively consisting of adjectives, the other of verbs and nouns. The highlighted clusters of terms have been aggregated by our edge compression algorithm [103]
2.3 Revealing Text Content 21
 Fig. 2.10 The arc diagram visualization of a HTML webpage in which repeated strings (e.g., words or sentences) are connected by arcs
alizations designed to illustrate repetition patterns. It represents an input string in a raw and connects the repeated symbols or words via arcs, thereby illustrating a clear visual pattern when repetition occurs.
2.3.3 Visualizing Topics
Accompanying the rapid development of topic analysis techniques, visualizing topics has become a highly interesting research direction in the field of text visualization in recent years. Compared with word-level visualizations, showing topics helps to capture more semantics of the data, thereby producing text visualizations that are easier to interpret. A growing number of visualization techniques are developed to (1) summarize and explore static topic information, (2) illustrate the topic dynamics over time, (3) help with topic comparison, and (4) illustrate events and storylines.
Even before the invention of modern topic modeling techniques such as PLSA [46] and LDA [6], visualization systems such as Topic IslandTM [76] and IN-SPIRETM1 had been introduced to illustrate and explore static topic themes extracted from text data. Research in this direction is significantly accelerated by the development of topic analysis techniques. Many attempts have been made to represent the topic analysis results. In particular, Cao et al. introduced ContexTour [67] (Fig.2.11), FacetAtals [17], and SolarMap [15] in a row based on a so-called “multifaceted entity-relational data model”. In particular, they decompose the text corpus into this data model based on a series of text analysis approached including (1) topic analysis, (2) name entity identification, and (3) word co-occurrence detection. The result- ing visualization illustrates static topics and their corresponding relationships from
1http://in-spire.pnnl.gov/.

  22 2 Overview of Text Visualization Techniques
 Fig. 2.11 Visualizing research topics in a publication dataset of papers published in the computer science conferences and journals in 2005 in ContexTour, in which the background contour produces a density map based on kernel density estimation, showing the underlying distribution of the words. Topics are shown as TagCoulds on top of the contour map
multiple information facets. Following these works, many similar techniques such as VisClustering [65] and TopicPanoram [69] were also developed. When compared to the aforementioned techniques, these works employed similar visual designs and provided similar functions in terms of topic representation and exploration, but were developed to focus on different analysis tasks (Fig. 2.12).
Capturing the topic dynamics is another research direction that attracts great atten- tion in the field of text visualization. In particular, ThemeRiever [42] is one of the earliest techniques developed to show how the frequency of the keywords are changed over time. It visualizes a set of keywords as stripes (i.e., themes) whose thicknesses change over time, indicating the change of frequencies of the corresponding key- words, which are shown on top of the stripe. This design was later extended by Liu

2.3 Revealing Text Content 23
  Fig. 2.12 ThemeRiver [42]: keywords in a document collection are shown as colored “stripes”, i.e. themes, with width indicating the occurrence frequency of keywords at different times
Fig. 2.13 RoseRiver, a visual analytics system for exploring evolutionary hierarchical topics. a Overview of the Prism scandal (June 5–Aug 16, 2013). Four colors represent the four major topics. Topics are displayed as vertical bars. The color stripes represent the evolving relationships between topics. b Comparison of the prominent keywords in tweets and news articles of the topic. The arc lengths encode the news article and tweet numbers (in log scale). c The new layout generated by splitting the gray topic
et al. [71] by using strips to represent changing topics, on top of which topic key- words are visualized as TagClouds. Following this research direction, Cui et al. [22] introduce TextFlow, the state-of-the-art design that captures the dynamics of topic evolution patterns such as splitting and merging over time. Subsequently, the authors refined their original design and algorithm to produce a hierarchical topic flow [23] that supports different levels of detail. This technique precisely captures the topic evolution patterns as shown in Fig. 2.13. Despite showing the overview of topic evo- lution trend, visualization techniques were also introduced to capture other temporal patterns in text data. For example, Gad et al. [34] introduced ThemeDelta, which integrates with topic modeling algorithms to identify change points when significant shifts in topics occurred. Liu et al. [68] introduced a technique for exploring topic lead-lag, i.e. a pattern illustrating how a former topic result in the occurrence of a latter topic

24 2 Overview of Text Visualization Techniques
When multiple document collections are visualized together at the same time, a spontaneous analysis task is to compare to find their common and distinct topics. To this end, many visualization techniques are introduced. Diakopoulos et al. [25] develop Compare Clouds, which is a TagCloud visualization designed to compare the topic keywords of two sets of input documents. Oelke et al. [81] also introduce a visual analysis system for comparing and distinguishing different document collections. In particular, it detects discriminative and common topics and visualizes each topic in a circular glyph, in which topic keywords are shown as a TagCloud. Glyphs are laid out based on topic similarities, i.e., the glyphs of similar topics are placed close to each other whereas the dissimilar ones are separated apart. In this way, common topics are placed at the center of the view and the discriminative ones are clearly separated into different topic regions, thereby forming a visualization that facilitates topic differentiations and comparisons.
2.3.4 Showing Events and Storyline
Finding topics in a collection of documents is generally a clustering approach. Doc- uments that have similar contents (essentially using similar words) are clustered together to constitute a topic or a theme. By contrast, event analysis focuses on a dif- ferent type of information that has time and space as the primary attributes [48]. An event is generally considered as an occurrence at a given space-time that is perceived by an observer to have a beginning and an end. Human beings are gifted with the ability to perceive and make sense of real-world activities as consisting of discrete events with orderly relations [116]. Thus, when visualizing events, researchers more focus on understanding the four Ws that characterize these events: who, what, when, and where.
A large amount of structured or semi-structured textual data explicitly contain event information, such as crime incidents or accidents recorded by police depart- ments [11, 66], patient records [21, 32, 39, 86, 111–113], and customer purchase logs [12]. For these datasets, the primary goal of visualization is to provide visual summaries and to support efficient queries. For example, as an early work, Life- Lines [86] provides a general visualization that allows users to explore details of a patient’s clinical records (Fig.2.14). A subsequent version, LifeLines2 [106] enhances the visualization and introduces three general operators, namely, align, rank, and filter, for interactive exploration of categorical, health-related datasets. PatternFinder [32] is designed to help users visually formulate queries and find tem- poral event patterns in medical record datasets. LifeFlow [112] and Outflow [111] aggregate multiple event sequences into tree or graph visualizations and provide users with highly scalable overviews (Fig. 2.15).
In real-world situations, people also often segment activities into events at multi- ple timescales [62]. Small events are grouped to constitute a large and complex event, such as acts in a play. In addition, events may also share elements, such as participants
2.3 Revealing Text Content 25
  Fig. 2.14 Screenshot of LifeLines [86]: colored horizontal bars show the time of occurrence and duration of clinical events for a patient, such as medical incidents, treatments, and rehabilitation. Additional information is encoded by the height and color of individual bars. Multiple facets of the records, such as notes and tests, are stacked vertically, and can be expanded and collapsed as needed
Fig. 2.15 Screenshot of Outflow [111] visualization that shows the scores of Manchester United in the 2010–2011 season. Green indicates pathways of winning, while red shows pathways of losing
and locations, with one another. To represent relationships between events, Burch et al. [12] use a horizontally oriented tree layout to represent the hierarchical relation- ships of transaction sequences along the timeline. André et al. [2] present Continuum to visually organize large amounts of hierarchical events and their relationships.

26 2 Overview of Text Visualization Techniques
Recently, significant research has been conducted to extract and explore events in unstructured textual data, such as news articles [28, 95] and microblogs [26, 75]. Since event information in these data is generally implicit, it requires text mining techniques, such as topic detection and tracking, are necessary to extract events for further visualization. For example, EventRiver [72] presents events in a river- metaphor based on event-based text analysis. In EventRiver visualization, an event is represented by a bubble that floats on a horizontal river of time. The shape of the bubble indicates the intensity and duration of the corresponding event. The color and vertical position of the bubble are used to indicate relationships among differ- ent events. Krstajic et al. [59] propose a visualization technique that incrementally detects clusters and events from multiple time series. To explore events in microblogs, Marcus et al. [75] describe a visual analytics system that allows users to specify a keyword of interest, and then visually summarizes events related to the query. DÖrk et al. [26] also propose an approach called visual backchannel that integrates text, images, and authors extracted from Twitter data. Their system does not require key- words of interest. Instead, it monitors evolving online discussions on major events, such as political speeches, natural disasters, and sport games. Twitter posts are sum- marized as a temporally adjusted stacked graph. Related authors and images are also visualized as a People Spiral and an Image Cloud, respectively, to help users track the event evolutions. LeadLine (Fig. 2.16) combines topic analysis and event detection techniques to extract events from social media data streams. Various information, such as topic, person, location, and time, is identified to help users reason about the topic evolutions.
Recently, storyline visualization has emerged and attracted significant attention. Unlike events and topics that focus on activities and themes, storyline visualizations switch the focus to entities and relationships between entities. In a typical storyline visualization, x-axis represents time, and an entity is represented as a line that extends
Fig. 2.16 Screenshot of LeadLine visualization that summarizes CNN news stories from Aug 15, 2011 to Nov 5, 2011. Bottom right locations that are related to President Obama are marked on the map. Left events that are related to the president are highlighted by color-coded bursts

2.3 Revealing Text Content 27
horizontally from left to right. The relationships between entities, which may change over time, are encoded by the vertical distances between the lines. Figure 1.1 shows an example of such visualizations. The figure shows the main story in the book of Lord of the Rings. Although this chart is manually made by the author, it inspires a set of approaches that aim to automatically generate similar visualizations. For exam- ple, Ogievetsky [83] builds an online tool to allow users to interactively generate and adjust a storyline visualization. However, to find a visually satisfying arrange- ment of lines is the key issue for this visualization. To solve this problem, Ogawa and Ma [82] raise several criteria and propose a rule-based algorithm to automati- cally generated a storyline-like visualization to help experts track software evolution (Fig.2.18). Later, Tanahashi and Ma [101] further formulate the storyline layout process as an optimization problem and solve the problem with a genetic method. Although time-consuming, their algorithm can generate results that are comparable to those handmade by XKCD. Based on their work, another optimization process is proposed by Liu et al. [70]; This process is time efficient enough to support real- time interaction and still be able to maintain the same level of aesthetics (Fig. 2.19). Recently, Tanahashi et al. [100] have extended storyline visualizations to streaming data, and further provide users with the ability to follow and reason dynamic data.
Fig. 2.17 XKCD’s movie narrative chart of Lord of the Rings [77]
Fig. 2.18 Storyline that shows the development of Python source codes [82]

28 2 Overview of Text Visualization Techniques
  Fig. 2.19 Reproduction of the same chat in Fig. 2.17 using StoryFlow algorithm [82]
2.4 Visualizing Sentiments and Emotions
Many visualization techniques have been developed to illustrate the change of sen- timents over time regarding to a given streaming text corpus such as news corpus, review comments, and Twitter streams. This goal can be achieved, as shown in Fig.2.20, by showing the sentiment dynamics in a time-series diagram, in which the time-series curve illustrates the change of sentiment scores computed across the entire dataset at different time points. However this simple visualization is too abstract to display information details such as the causes behind the sentiment shifts. Therefore, many other more advanced techniques have been introduced to illustrate and interpret the sentiment dynamics from different perspectives.
Most techniques are developed to compute and visualize the sentiments of a focal group of people based on the text data produced by them. The resulting visualization forms a “happiness indicator” that captures the sentiment change of the focal group over time. For example, Brew et al. [9] introduce SentireCrowds, which represents the sentiment changes of a group of twitter users from the same city in a timeline view
Fig. 2.20 Sentiment Indexing of Twitter data in a time-series diagram. This figure shows that the public sentiment may change dramatically regarding to different events in our real-life

2.4 Visualizing Sentiments and Emotions 29
and summarizes the potential underlying event that causes the changes in a multi-level TagCloud designed based on Treemap. Zhao et al. [118] introduce PEARL, which visualizes the change of a person’s emotion or mood profile derived from his tweets in a compound belt visualization. The belt groups a set of emotion bands, each indicating a type of emotion differentiated by colors. The thickness of the band changes over time, representing the portion of the corresponding emotion at different time. Guzman et al. [40] visualize the change of emotions of groups of different developers in various software development projects. Hao et al. [41] analyze sentiments via geo-temporal term associations based on a streaming dataset of customer’s feedback. Kempter et al. [57] introduce a fine-grained, multi-category emotion model to classify users’ emotional reactions to public events overtime and to visualize the results in a radar diagram, called EmotionWatch, as shown in Fig. 2.22.
Despite the preceding visualizations, some visual analysis systems have also been developed to assist with dynamic sentiment analysis. For example, Wanner et al. [107] develop a small multiple visualization view to conduct a semi-automatic sentiment analysis of large news feeds. In this work, a case study on news regarding the US presidential election in 2008 shows how visualization techniques will help analysts draw meaningful conclusions without exerting effort to read the content of the news. Brooks et al. [10] introduce Agave, a collaborative visual analysis system for explor- ing events and sentiment over time in large Twitter datasets. The system employs multiple co-ordinated views in which a streamgraph (Fig. 2.21) is used to summa- rize the changes of the sentiments of a subset of tweets queried based on users’ preferences. Zhang et al. [117] introduce a spacial-temporal view for visualizing the sentiment scores of micro-blog data based on an electron cloud model intruded in physics. The resulting visualization maps a single sentiment score to a position inside a circular visualization display.
More sophisticated systems are also developed to analyze the change of senti- ments based on streaming text data. For example, Rohrdantz et al. [91] introduce a visual analysis system to help users to detect interesting portions of text streams,
Fig. 2.21 Sentiment streamgraphs for the keyword search Flacco, the Super Bowl MVP in a Twitter dataset using Agave [10]. Negative is red, neutral is gray, and positive is blue. Top overall frequency of tweets, divided by sentiment type. Bottom sentiment as percent of overall volume

30 2 Overview of Text Visualization Techniques
 Fig. 2.22 Comparison of two emotion profiles of Roger Federer and Andy Murray (two tennis athletes) after a tennis game in EmotionWatch [57]; (A) the EmotionWatches, (B) timelines showing the two emotion flows, and (C) video
regarding to the change of sentiments, data density, and context coherence based on a set of features extracted from the text. Wang et al. [105] introduce SentiView, which employs advanced sentiment analysis techniques as well as visualization designs to analyze the change of public sentiments regarding popular topics on the Internet. Other systems are designed for analyzing sentiment divergences (i.e., conflicting of opinions) that occur between two groups of people. For example, Chen et al. [18] introduce the first work in this topic based on a simple time-series design that sum- marizes the overall conflicting opinions based on the Amazon review data. Following this topic, Cao et al. [16] introduce a more advanced technique called SocialHelix, which extracts two groups of people having the most significant sentiment divergence over time from Twitter data and illustrates their divergence in a Helix visualization to show how the divergence occurred, evolved, and terminated.
In terms of application, a large set of such techniques are developed to repre- sent the customer’s sentiments based on the review data. Alper et al. [1] introduce OpinionBlocks, an interactive visualization tool to improve people’s understanding of customer reviews. The visualization progressively discloses text information at different granularities from the keywords to the phrases in which the keywords are used, and to the reviews containing the phrases. The information is displayed within two horizontal regions, representing two types (positive and negative) of different sentiments. Gamon et al. [36] introduce Pulse for mining topics and sentiment orien- tation jointly from free text customer feedback. This system enables the exploration of large quantities of customer review data and was used for visually analyzing a database for car reviews. Through this system, users can examine customer opinion at a glance or explore the data at a finer level of detail. Oelke et al. [79] analyze to determine customers’ positive and negative opinions through the comments or rat-
2.4 Visualizing Sentiments and Emotions 31
 Fig. 2.23 Summary Report of printers: each row shows the attribute performances of a specific printer. Blue color represents comparatively positive user opinions and red color comparatively negative ones (see color scale). The size of an inner rectangle indicates the amount of customers that commented on an attribute. The larger the rectangle the more comments have been provided by the customers
ings posted by the customers. This system visualize the analysis results in a heatmap view showing both volume of comments and the summarized sentiments (Fig. 2.23). Wu et al. [115] introduce OpinionSeer, which employs subjective logic [52] to ana- lyze customer opinions on hotel rooms based on their review data inside a simplex space, which is visualized in a triangle surrounded by context about the customers such as their ages and their countries of origin. More generic systems are also devel- oped. For example, Wensel [110] introduce VIBES, which extracts the important topics from a blog, and measures the emotions associated with those topics that are illustrated through a range of different visualization views. Makki et al. [74] introduce an interactive visualization to engage the user in the process of polarity assignment to improve the quality of the generated lexicon used for sentiment or emotion analysis via minimal user effort.
2.5 Document Exploration Techniques
With a large document collection, how to effectively explore the data to find use- ful information or insightful data patterns is always a challenge that attracts many research attentions. Many visualization systems are designed to support an effective exploration of big text corpus. Many studies are focused on inventing or improv- ing the text data exploration techniques. A large category of them are query-based systems in which full text indices are built so that users can query to retrieve data based on their interests. Based on these techniques, many systems are developed to explore text collected from various application domains. In this section, we review these exploration techniques as well as their applications.
32 2 Overview of Text Visualization Techniques
 Fig. 2.24
2.5.1
Data Mountain visualization shows 100 webpages
Distortion Based Approaches
As early as the 1990s, some distortion based techniques have been developed to assist in text data exploration. For example, Robertson and Mackinlay introduced Docu- ment Lens [90] which introduced a focus+context design inspired by the magnifier lens. In this visualization, the focused content of a document is shown in details in the view center, surrounded by other parts of the content that provides an overall impression of the text data. Despite showing the content, a similar idea is also used to visualize documents. For example, Data Mountain [89] employs a focus+context view based on a prospective projection, in which the focused documents are shown in a larger size with additional details in front of other documents, whereas the unfocused ones are shown at the back side in a smaller size (Fig. 2.24). Users can interactively switch between the focus and context by clicking on the documents.
2.5.2 Exploration Based on Document Similarity
Exploring documents in a similarity view is another early but popular technique that is extensively used in many text visualizations. These systems, such as InfoSky [3] and IN-SPIRETM [44] and ForceSPIRE [31], use an overview that summarizes
2.5 Document Exploration Techniques 33
the entire document collection based on document similarities (see, Sect.2.1) and employs a multiple coordinated view to reveal the document details from various aspects such as keywords and topics. Users can navigate through the similarity view based on interactions such as zooming and panning, thereby showing different levels of details [30].
2.5.3 Hierarchical Document Exploration
Exploring big document collection based on hierarchical clustering is another com- monly used approach. For example, Paulovich and Minghim introduce HiPP [85], which lays out documents via a hierarchical circle packing algorithm, in which a document is shown as a circle. Dou et al. [29], Brehmer et al. [8], as well as Pascual- Cid and Kaltenbrunner [84] develop different types of document exploration systems that are all based on hierarchical clustering. In these systems, documents are hierar- chically clustered based on their topic similarity and the cluster results are shown in a tree view to guide the data navigation.
2.5.4 Search and Query Based Approaches
Full text search and document query are also widely used to support document exploration since the very beginning of the text visualization [43, 94, 96]. Instead of showing a ranked list of related documents regarding the query keywords, most of the existing visualization techniques transform the search results into a visual representation to illustrate the insight of content relationships among documents. Graph layout and projection-based approach are commonly used to represent the search and query results showing the relationships among documents [7], or text snippets such as words [88] or collections of topic keywords [33, 37]. More advanced techniques are also developed. For example, Isaacs introduced Footprints to support an effective and interactive procedure to retrieve information in a desired subject from a large document collection. In this system, the topics queried by a user are visually summarized in an iconic representation as shown in Fig. 2.25. The user can click the topic to load a collection of related document in the document list and the content of a selected document can be further shown in the document viewer. A set of filters also helps users extract the most interesting data.
Exploration in Coordinated Views. The aforementioned document exploration techniques are usually combined with multiple coordinated visualization views that illustrate different aspects of the input document collection. Thew views are usually connected together through interactions such as “Linking and Brushing”.2 In these
2“The idea of linking and brushing is to combine different visualization methods to overcome the shortcomings of single techniques. Interactive changes made in one visualization are automatically

34 2 Overview of Text Visualization Techniques
 Fig. 2.25 Footprints, a topics-based document search tool, which supports exploratory search to help analysts a retrieve unknown information the goal of query is missing, b track the query results from multiple aspects to avoid missing important information and help to determine when should the query procedure be stopped
techniques, topic analysis is one of the most important aspects. For example, Dou et al. [27] introduce ParallelTopics, which guides the document exploration via multi- ple views. Specifically, it employs a parallel coordinates to illustrate how a document distributed in different topics, employs a TagCloud view to illustrate the keywords inside each topic, and employs the theme river to show the change of topics over time. Many other similar systems are available, such as Jigsaw [38] and IVEA [102], which are discussed in details in Chap. 4.
2.6 Summary of the Chapter
In this chapter, we reviewed more than 200 papers in the field of text visualization to provide an overview. This chapter provides readers a brief idea of what is text visualization and what is the research focus of this field. In particular, we summa- rize the existing works into three major categories based on the type of informa- tion to be shown in a visualization. Specifically, these techniques include those for (1) visualizing document similarities, (2) revealing text content, and (3) visualizing
(Footnote 2 continued)
reflected in the other visualizations. Note that connecting multiple visualizations through interactive linking and brushing provides more information than considering the component visualizations independently.” [56].

2.6 Summary of the Chapter 35
semantics and emotions. We also reviewed the most commonly used text exploration techniques, including (1) distortion-based approaches, (2) exploration based on doc- ument similarity, (3) hierarchical document exploration, (4) search and query-based approaches, and (5) exploration in coordinated views. In the following chapters, we focus on detailed techniques.
References
1. Alper,B.,Yang,H.,Haber,E.,Kandogan,E.:Opinionblocks:visualizingconsumerreviews. In: IEEE VisWeek 2011 Workshop on Interactive Visual Text Analytics for Decision Making (2011)
2. André, P., Wilson, M.L., Russell, A., Smith, D.A., Owens, A., et al.: Continuum: designing timelines for hierarchies, relationships and scale. In: Proceedings of the 20th Annual ACM Symposium on User interface Software and Technology, pp. 101–110. ACM (2007)
3. Andrews, K., Kienreich, W., Sabol, V., Becker, J., Droschl, G., Kappe, F., Granitzer, M., Auer, P., Tochtermann, K.: The infosky visual explorer: exploiting hierarchical structure and document similarities. Inf. Vis. 1(3–4), 166–181 (2002)
4. Angus, D., Smith, A., Wiles, J.: Conceptual recurrence plots: revealing patterns in human discourse. IEEE Trans. Vis. Comput. Graph. 18(6), 988–997 (2012)
5. Balakrishnama,S.,Ganapathiraju,A.:LinearDiscriminantAnalysis—ABriefTutorial,vol. 18. Institute for Signal and information Processing, Starkville (1998)
6. Blei,D.M.,Ng,A.Y.,Jordan,M.I.:LatentDirichletallocation.J.Mach.Learn.Res.3,993– 1022 (2003)
7. Bradel,L.,North,C.,House,L.:Multi-modelsemanticinteractionfortextanalytics.In:2014 IEEE Conference on Visual Analytics Science and Technology (VAST), pp. 163–172. IEEE (2014)
8. Brehmer,M.,Ingram,S.,Stray,J.,Munzner,T.:Overview:thedesign,adoption,andanalysis of a visual document mining tool for investigative journalists. IEEE Trans. Vis. Comput. Graph. 20(12), 2271–2280 (2014)
9. Brew, A., Greene, D., Archambault, D., Cunningham, P.: Deriving insights from national happiness indices. In: 2011 IEEE 11th International Conference on Data Mining Workshops (ICDMW), pp. 53–60. IEEE (2011)
10. Brooks,M.,Robinson,J.J.,Torkildson,M.K.,Aragon,C.R.,etal.:Collaborativevisualanaly- sis of sentiment in Twitter events. In: Cooperative Design, Visualization, and Engineering, pp. 1–8. Springer, Berlin (2014)
11. Buetow, T., Chaboya, L., OToole, C., Cushna, T., Daspit, D., Petersen, T., Atabakhsh, H., Chen, H.: A spatio temporal visualizer for law enforcement. In: Intelligence and Security Informatics, pp. 181–194. Springer, Berlin (2003)
12. Burch,M.,Beck,F.,Diehl,S.:Timelinetrees:visualizingsequencesoftransactionsininforma- tion hierarchies. In: Proceedings of the Working Conference on Advanced Visual Interfaces, pp. 75–82. ACM (2008)
13. Burch,M.,Lohmann,S.,Beck,F.,Rodriguez,N.,DiSilvestro,L.,Weiskopf,D.:Radcloud: visualizing multiple texts with merged word clouds. In: 2014 18th International Conference on Information Visualisation (IV), pp. 108–113. IEEE (2014)
14. Burch,M.,Lohmann,S.,Pompe,D.,Weiskopf,D.:Prefixtagclouds.In:201317thInterna- tional Conference on Information Visualisation (IV), pp. 45–50. IEEE (2013)
15. Cao,N.,Gotz,D.,Sun,J.,Lin,Y.R.,Qu,H.:Solarmap:multifacetedvisualanalyticsfortopic exploration. In: IEEE International Conference on Data Mining, pp. 101–110. IEEE (2011)
16. Cao,N.,Lu,L.,Lin,Y.R.,Wang,F.,Wen,Z.:Socialhelix:visualanalysisofsentimentdiver- gence in social media. J. Vis. 18(2), 221–235 (2015)
36 2 Overview of Text Visualization Techniques
17. Cao, N., Sun, J., Lin, Y.R., Gotz, D., Liu, S., Qu, H.: Facetatlas: multifaceted visualization for rich text corpora. IEEE Trans. Vis. Comput Graph. 16(6), 1172–1181 (2010)
18. Chen,C.,Ibekwe-SanJuan,F.,SanJuan,E.,Weaver,C.:Visualanalysisofconflictingopinions. In: 2006 IEEE Symposium on Visual Analytics Science and Technology, pp. 59–66. IEEE (2006)
19. Collins,C.,Carpendale,S.,Penn,G.:Docuburst:visualizingdocumentcontentusinglanguage structure. Comput. Graph. Forum 28(3), 1039–1046 (2009)
20. Collins,C.,Viegas,F.B.,Wattenberg,M.:Paralleltagcloudstoexploreandanalyzefaceted text corpora. In: IEEE Symposium on Visual Analytics Science and Technology, 2009. VAST 2009, pp. 91–98. IEEE (2009)
21. Cousins, S.B., Kahn, M.G.: The visual display of temporal information. Artifi. Intell. Med. 3(6), 341–357 (1991)
22. Cui, W., Liu, S., Tan, L., Shi, C., Song, Y., Gao, Z.J., Qu, H., Tong, X.: Textflow: towards better understanding of evolving topics in text. IEEE Trans. Vis. Comput. Graph. 17(12), 2412–2421 (2011)
23. Cui,W.,Liu,S.,Wu,Z.,Wei,H.:Howhierarchicaltopicsevolveinlargetextcorpora.IEEE Trans. Vis. Comput. Graph. 20(12), 2281–2290 (2014)
24. Cui,W.,Wu,Y.,Liu,S.,Wei,F.,Zhou,M.X.,Qu,H.:Contextpreservingdynamicwordcloud visualization. In: IEEE Symposium on Pacific Visualization, pp. 121–128 (2010)
25. Diakopoulos,N.,Elgesem,D.,Salway,A.,Zhang,A.,Hofland,K.:Compareclouds:visual- izing text corpora to compare media frames. In: Proceedings of IUI Workshop on Visual Text Analytics (2015)
26. Dörk, M., Gruen, D., Williamson, C., Carpendale, S.: A visual backchannel for large-scale events. IEEE Trans. Vis. Comput. Graph. 16(6), 1129–1138 (2010)
27. Dou,W.,Wang,X.,Chang,R.,Ribarsky,W.:Paralleltopics:aprobabilisticapproachtoexplor- ing document collections. In: 2011 IEEE Conference on Visual Analytics Science and Tech- nology (VAST), pp. 231–240. IEEE (2011)
28. Dou,W.,Wang,X.,Skau,D.,Ribarsky,W.,Zhou,M.X.:Leadline:interactivevisualanalysis of text data through event identification and exploration. In: 2012 IEEE Conference on Visual Analytics Science and Technology (VAST), pp. 93–102. IEEE (2012)
29. Dou,W.,Yu,L.,Wang,X.,Ma,Z.,Ribarsky,W.:Hierarchicaltopics:visuallyexploringlarge text collections using topic hierarchies. IEEE Trans. Vis. Comput. Graph. 19(12), 2002–2011 (2013)
30. Endert,A.,Burtner,R.,Cramer,N.,Perko,R.,Hampton,S.,Cook,K.:Typograph:multiscale spatial exploration of text documents. In: 2013 IEEE International Conference on Big Data, pp. 17–24. IEEE (2013)
31. Endert,A.,Fiaux,P.,North,C.:Semanticinteractionforvisualtextanalytics.In:Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 473–482. ACM (2012)
32. Fails, J.A., Karlson, A., Shahamat, L., Shneiderman, B.: A visual interface for multivariate temporal data: finding patterns of events across multiple histories. In: 2006 IEEE Symposium on Visual Analytics Science and Technology, pp. 167–174. IEEE (2006)
33. Forbes, A.G., Savage, S., Höllerer, T.: Visualizing and verifying directed social queries. In: IEEE Workshop on Interactive Visual Text Analytics, Seattle, WA (2012)
34. Gad, S., Javed, W., Ghani, S., Elmqvist, N., Ewing, T., Hampton, K.N., Ramakrishnan, N.: Themedelta: dynamic segmentations over temporal topic models. IEEE Trans. Vis. Comput. Graph. 21(5), 672–685 (2015)
35. Gambette,P.,Véronis,J.:Visualisingatextwithatreecloud.In:ClassificationasaToolfor Research, pp. 561–569. Springer, Berlin (2010)
36. Gamon,M.,Aue,A.,Corston-Oliver,S.,Ringger,E.:Pulse:miningcustomeropinionsfrom free text. In: Advances in Intelligent Data Analysis VI, pp. 121–132. Springer, Berlin (2005)
37. Gomez-Nieto,E.,SanRoman,F.,Pagliosa,P.,Casaca,W.,Helou,E.S.,deOliveira,M.C.F., Nonato, L.G.: Similarity preserving snippet-based visualization of web search results. IEEE
Trans. Vis. Comput. Graph. 20(3), 457–470 (2014)
References 37
38. Gorg,C.,Liu,Z.,Kihm,J.,Choo,J.,Park,H.,Stasko,J.:Combiningcomputationalanalyses and interactive visualization for document exploration and sensemaking in jigsaw. IEEE Trans. Vis. Comput. Graph. 19(10), 1646–1663 (2013)
39. Gschwandtner,T.,Aigner,W.,Kaiser,K.,Miksch,S.,Seyfang,A.:Carecruiser:exploringand visualizing plans, events, and effects interactively. In: Visualization Symposium (PacificVis), 2011 IEEE Pacific, pp. 43–50. IEEE (2011)
40. Guzman,E.:Visualizingemotionsinsoftwaredevelopmentprojects.In:IEEEWorkingCon- ference on Software Visualization, pp. 1–4. IEEE (2013)
41. Hao,M.C.,Rohrdantz,C.,Janetzko,H.,Keim,D.A.,etal.:Visualsentimentanalysisofcus- tomer feedback streams using geo-temporal term associations. Inf. Vis. 12(3–4), 273 (2013)
42. Havre,S.,Hetzler,B.,Nowell,L.:Themeriver:visualizingthemechangesovertime.In:IEEE
Symposium on Information Visualization, 2000. InfoVis 2000, pp. 115–123. IEEE (2000)
43. Hearst, M.A., Karadi, C.: Cat-a-cone: an interactive interface for specifying searches and viewing retrieval results using a large category hierarchy. ACM SIGIR Forum 31(SI), 246–
255 (1997)
44. Hetzler,E.,Turner,A.:Analysisexperiencesusinginformationvisualization.IEEEComput.
Graph. Appl. 24(5), 22–26 (2004)
45. Hinton,G.E.,Roweis,S.T.:Stochasticneighborembedding.In:Advancesinneuralinforma-
tion processing systems, pp. 833–840 (2002)
46. Hofmann, T.: Probabilistic latent semantic indexing. In: Proceedings of International ACM
SIGIR Conference on Research and Development in Information Retrieval, pp. 50–57. ACM
(1999)
47. Iwata, T., Yamada, T., Ueda, N.: Probabilistic latent semantic visualization: topic model for
visualizing documents. In: Proceedings of SIGKDD International Conference on Knowledge
Discovery and Data Mining, pp. 363–371. ACM (2008)
48. Jain, R.: Out-of-the-box data engineering events in heterogeneous data environments. In:
Proceedings. 19th International Conference on Data Engineering, 2003, pp. 8–21. IEEE (2003)
49. Jänicke, S., Geßner, A., Büchler, M., Scheuermann, G.: Visualizations for text re-use.
GRAPP/IVAPP, pp. 59–70 (2014)
50. Jankowska,M.,Keselj,V.,Milios,E.:Relativen-gramsignatures:documentvisualizationat
the level of character n-grams. In: 2012 IEEE Conference on Visual Analytics Science and
Technology (VAST), pp. 103–112. IEEE (2012)
51. Jolliffe,I.:PrincipalComponentAnalysis.WileyOnlineLibrary(2002)
52. Jøsang, A.: The consensus operator for combining beliefs. Artif. Intell. 141(1), 157–170
(2002)
53. Kaser,O.,Lemire,D.:Tag-clouddrawing:algorithmsforcloudvisualization.arXivpreprint
cs/0703109 (2007)
54. Kaster,A.,Siersdorfer,S.,Weikum,G.:Combiningtextandlinguisticdocumentrepresenta-
tions for authorship attribution. In: SIGIR Workshop: Stylistic Analysis of Text for Information
Access (2005)
55. Keim,D.,Oelke,D.,etal.:Literaturefingerprinting:anewmethodforvisualliteraryanalysis.
In: IEEE Symposium on Visual Analytics Science and Technology, 2007. VAST 2007, pp.
115–122. IEEE (2007)
56. Keim,D.,etal.:Informationvisualizationandvisualdatamining.IEEETrans.Vis.Comput.
Graph. 8(1), 1–8 (2002)
57. Kempter,R.,Sintsova,V.,Musat,C.,Pu,P.:Emotionwatch:visualizingfine-grainedemotions
in event-related tweets. In: International AAAI Conference on Weblogs and Social Media
(2014)
58. Koren,Y.,Carmel,L.:Visualizationoflabeleddatausinglineartransformations.In:Proceed-
ings of IEEE Symposium on Information Visualization, pp. 121–128 (2003)
59. Krstajic ́,M.,Bertini,E.,Keim,D.A.:Cloudlines:compactdisplayofeventepisodesinmul-
tiple time-series. IEEE Trans. Vis. Comput. Graph. 17(12), 2432–2439 (2011)
60. Kruskal,J.B.:Multidimensionalscalingbyoptimizinggoodnessoffittoanonmetrichypoth-
esis. Psychometrika 29(1), 1–27 (1964)
38 2 Overview of Text Visualization Techniques
61. Kucher,K.,Kerren,A.:Textvisualizationbrowser:avisualsurveyoftextvisualizationtech- niques. Poster Abstracts of IEEE VIS (2014)
62. Kurby,C.A.,Zacks,J.M.:Segmentationintheperceptionandmemoryofevents.TrendsCogn Sci 12(2), 72–79 (2008)
63. Le, T., Lauw, H.W.: Semantic visualization for spherical representation. In: Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1007–1016. ACM (2014)
64. Lee, D.D., Seung, H.S.: Algorithms for non-negative matrix factorization. In: Advances in Neural Information Processing Systems, pp. 556–562 (2001)
65. Lee,H.,Kihm,J.,Choo,J.,Stasko,J.,Park,H.:ivisclustering:aninteractivevisualdocument clustering via topic modeling. Comput. Graph. Forum 31(3pt3), 1155–1164 (2012)
66. Levine, N., et al.: Crimestat iii: a spatial statistics program for the analysis of crime inci- dent locations (version 3.0). Ned Levine & Associates, Houston/National Institute of Justice, Washington (2004)
67. Lin,Y.R.,Sun,J.,Cao,N.,Liu,S.:Contextour:contextualcontourvisualanalysisondynamic multi-relational clustering. In: SIAM Data Mining Conference. SIAM (2010)
68. Liu,S.,Chen,Y.,Wei,H.,Yang,J.,Zhou,K.,Drucker,S.M.:Exploringtopicallead-lagacross corpora. IEEE Trans. Knowl. Data Eng. 27(1), 115–129 (2015)
69. Liu,S.,Wang,X.,Chen,J.,Zhu,J.,Guo,B.:Topicpanorama:afullpictureofrelevanttopics. In: 2014 IEEE Conference on Visual Analytics Science and Technology (VAST), pp. 183–192. IEEE (2014)
70. Liu, S., Wu, Y., Wei, E., Liu, M., Liu, Y.: Storyflow: tracking the evolution of stories. IEEE Trans. Vis. Comput. Graph. 19(12), 2436–2445 (2013)
71. Liu,S.,Zhou,M.X.,Pan,S.,Song,Y.,Qian,W.,Cai,W.,Lian,X.:Tiara:interactive,topic- based visual text summarization and analysis. ACM Trans. Intell. Syst. Technol. (TIST) 3(2), 25 (2012)
72. Luo, D., Yang, J., Krstajic, M., Ribarsky, W., Keim, D.: Eventriver: visually exploring text collections with temporal references. IEEE Trans. Vis. Comput. Graph. 18(1), 93–105 (2012)
73. Van der Maaten, L., Hinton, G.: Visualizing data using t-sne. J. Mach. Learn. Res. 9(2579–
2605), 85 (2008)
74. Makki,R.,Brooks,S.,Milios,E.E.:Context-specificsentimentlexiconexpansionviaminimal
user interaction. In: Proceedings of the International Conference on Information Visualization
Theory and Applications (IVAPP), pp. 178–186 (2014)
75. Marcus, A., Bernstein, M.S., Badar, O., Karger, D.R., Madden, S., Miller, R.C.: Twitinfo:
aggregating and visualizing microblogs for event exploration. In: Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems, pp. 227–236. ACM (2011)
76. Miller, N.E., Wong, P.C., Brewster, M., Foote, H.: Topic islands TM—a wavelet-based text visualization system. In: IEEE Symposium on Information Visualization, pp. 189–196. IEEE
(1998)
77. Munroe,R.:Movienarrativecharts.http://xkcd.com/657/.AccessedJan2016
78. Neto,J.L.,Santos,A.D.,Kaestner,C.A.,Freitas,A.A.:Documentclusteringandtextsumma-
rization. In: Proceedings of the International Conference Practical Applications of Knowledge
Discovery and Data Mining, pp. 41–55. The Practical Application Company (2000)
79. Oelke, D., Hao, M., Rohrdantz, C., Keim, D., Dayal, U., Haug, L.E., Janetzko, H., et al.: Visual opinion analysis of customer feedback data. In: IEEE Symposium on Visual Analytics
Science and Technology, 2009. VAST 2009, pp. 187–194. IEEE (2009)
80. Oelke, D., Kokkinakis, D., Keim, D.A.: Fingerprint matrices: uncovering the dynamics of
social networks in prose literature. Comput. Graph. Forum 32(3pt4), 371–380 (2013)
81. Oelke, D., Strobelt, H., Rohrdantz, C., Gurevych, I., Deussen, O.: Comparative exploration of document collections: a visual analytics approach. Comput. Graph. Forum 33(3), 201–210
(2014)
82. Ogawa,M.,Ma,K.L.:Softwareevolutionstorylines.In:Proceedingsofthe5thInternational
Symposium on Software Visualization, pp. 35–42. ACM (2010)
83. Ogievetsky,V.:PlotWeaver.http://ogievetsky.com/PlotWeaver/.AccessedJan2016
References 39
84. Pascual-Cid,V.,Kaltenbrunner,A.:Exploringasynchronousonlinediscussionsthroughhier- archical visualisation. In: 2009 13th International Conference on Information Visualisation, pp. 191–196. IEEE (2009)
85. Paulovich, F.V., Minghim, R.: Hipp: a novel hierarchical point placement strategy and its application to the exploration of document collections. IEEE Trans. Vis. Comput. Graph. 14(6), 1229–1236 (2008)
86. Plaisant, C., Mushlin, R., Snyder, A., Li, J., Heller, D., Shneiderman, B.: Lifelines: using visualization to enhance navigation and analysis of patient records. In: Proceedings of the AMIA Symposium, p. 76. American Medical Informatics Association (1998)
87. Reisinger,J.,Waters,A.,Silverthorn,B.,Mooney,R.J.:Sphericaltopicmodels.In:Proceed- ings of International Conference on Machine Learning, pp. 903–910 (2010)
88. Riehmann,P.,Gruendl,H.,Potthast,M.,Trenkmann,M.,Stein,B.,Froehlich,B.:Wordgraph: keyword-in-context visualization for netspeak’s wildcard search. IEEE Trans. Vis. Comput. Graph. 18(9), 1411–1423 (2012)
89. Robertson, G., Czerwinski, M., Larson, K., Robbins, D.C., Thiel, D., Van Dantzich, M.: Data mountain: using spatial memory for document management. In: Proceedings of the 11th Annual ACM Symposium on User Interface Software and Technology, pp. 153–162. ACM (1998)
90. Robertson, G.G., Mackinlay, J.D.: The document lens. In: Proceedings of the 6th Annual ACM Symposium on User Interface Software and Technology, pp. 101–108. ACM (1993)
91. Rohrdantz,C.,Hao,M.C.,Dayal,U.,Haug,L.E.,Keim,D.A.:Feature-basedvisualsentiment analysis of text document streams. ACM Trans. Intell. Syst. Technol. (TIST) 3(2), 26 (2012)
92. Roweis, S.T., Saul, L.K.: Nonlinear dimensionality reduction by locally linear embedding.
Science 290(5500), 2323–2326 (2000)
93. Rusu,D.,Fortuna,B.,Mladenic ́,D.,Grobelnik,M.,Sipos,R.:Documentvisualizationbased
on semantic graphs. In: 13th International Conference Information Visualisation, pp. 292–297.
IEEE (2009)
94. Sebrechts, M.M., Cugini, J.V., Laskowski, S.J., Vasilakis, J., Miller, M.S.: Visualization of
search results: a comparative evaluation of text, 2d, and 3d interfaces. In: Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 3–10. ACM (1999)
95. Smith, D.A.: Detecting events with date and place information in unstructured text. In: Pro- ceedings of the 2nd ACM/IEEE-CS Joint Conference on Digital Libraries, pp. 191–196. ACM (2002)
96. Spoerri,A.:Infocrystal:avisualtoolforinformationretrieval&management.In:Proceedings of the Second International Conference on Information and Knowledge Management, pp. 11– 20. ACM (1993)
97. Stasko,J.,Zhang,E.:Focus+contextdisplayandnavigationtechniquesforenhancingradial, space-filling hierarchy visualizations. In: IEEE Symposium on Information Visualization, 2000. InfoVis 2000, pp. 57–65. IEEE (2000)
98. Stoffel, A., Strobelt, H., Deussen, O., Keim, D.A.: Document thumbnails with variable text scaling. Comput. Graph. Forum 31(3pt3), 1165–1173 (2012)
99. Strobelt, H., Oelke, D., Rohrdantz, C., Stoffel, A., Keim, D., Deussen, O., et al.: Document cards: a top trumps visualization for documents. IEEE Trans. Vis. Comput. Graph. 15(6), 1145–1152 (2009)
100. Tanahashi,Y.,Hsueh,C.H.,Ma,K.L.:Anefficientframeworkforgeneratingstorylinevisu- alizations from streaming data. IEEE Trans. Vis. Comput. Graph. 21(6), 730–742 (2015)
101. Tanahashi,Y.,Ma,K.L.:Designconsiderationsforoptimizingstorylinevisualizations.IEEE Trans. Vis. Comput. Graph. 18(12), 2679–2688 (2012)
102. Thai, V., Handschuh, S., Decker, S.: Tight coupling of personal interests with multi- dimensional visualization for exploration and analysis of text collections. In: International Conference on Information Visualisation, pp. 221–226. IEEE (2008)
103. VanHam,F.,Wattenberg,M.,Viégas,F.B.:Mappingtextwithphrasenets.IEEETrans.Vis. Comput. Graph. 15(6), 1169–1176 (2009)
40 2 Overview of Text Visualization Techniques
104. Viegas, F.B., Wattenberg, M., Feinberg, J.: Participatory visualization with wordle. IEEE Trans. Vis. Comput. Graph. 15(6), 1137–1144 (2009)
105. Wang, C., Xiao, Z., Liu, Y., Xu, Y., Zhou, A., Zhang, K.: Sentiview: sentiment analysis and visualization for internet popular topics. IEEE Trans. Hum. Mach. Syst 43(6), 620–630 (2013)
106. Wang,T.D.,Plaisant,C.,Shneiderman,B.,Spring,N.,Roseman,D.,Marchand,G.,Mukher- jee, V., Smith, M.: Temporal summaries: supporting temporal categorical searching, aggre-
gation and comparison. IEEE Trans. Vis. Comput. Graph. 15(6), 1049–1056 (2009)
107. Wanner,F.,Rohrdantz,C.,Mansmann,F.,Oelke,D.,Keim,D.A.:Visualsentimentanalysisof rss news feeds featuring the us presidential election in 2008. In: Workshop on Visual Interfaces
to the Social and the Semantic Web (VISSW) (2009)
108. Wattenberg, M.: Arc diagrams: visualizing structure in strings. In: IEEE Symposium on
Information Visualization, pp. 110–116. IEEE (2002)
109. Wattenberg,M.,Viégas,F.B.:Thewordtree,aninteractivevisualconcordance.IEEETrans.
Vis. Comput. Graph. 14(6), 1221–1228 (2008)
110. Wensel,A.M.,Sood,S.O.:Vibes:visualizingchangingemotionalstatesinpersonalstories.In:
Proceedings of the 2nd ACM International Workshop on Story Representation, Mechanism
and Context, pp. 49–56. ACM (2008)
111. Wongsuphasawat, K., Gotz, D.: Exploring flow, factors, and outcomes of temporal event
sequences with the outflow visualization. IEEE Trans. Vis. Comput Graph. 18(12), 2659–
2668 (2012)
112. Wongsuphasawat, K., Guerra Gómez, J.A., Plaisant, C., Wang, T.D., Taieb-Maimon, M.,
Shneiderman, B.: Lifeflow: visualizing an overview of event sequences. In: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 1747–1756. ACM (2011)
113. Wongsuphasawat,K.,Shneiderman,B.:Findingcomparabletemporalcategoricalrecords:a similarity measure with an interactive visualization. In: IEEE Symposium on Visual Analytics Science and Technology, 2009. VAST 2009, pp. 27–34. IEEE (2009)
114. Wu, Y., Provan, T., Wei, F., Liu, S., Ma, K.L.: Semantic-preserving word clouds by seam carving. Comput. Graph. Forum 30(3), 741–750 (2011)
115. Wu, Y., Wei, F., Liu, S., Au, N., Cui, W., Zhou, H., Qu, H.: Opinionseer: interactive visu- alization of hotel customer feedback. IEEE Trans. Vis. Comput. Graph. 16(6), 1109–1118 (2010)
116. Zacks,J.M.,Tversky,B.:Eventstructureinperceptionandconception.Psychol.Bull.127(1), 3 (2001)
117. Zhang, C., Liu, Y., Wang, C.: Time-space varying visual analysis of micro-blog sentiment. In: Proceedings of the 6th International Symposium on Visual Information Communication and Interaction, pp. 64–71. ACM (2013)
118. Zhao,J.,Gou,L.,Wang,F.,Zhou,M.:Pearl:aninteractivevisualanalytictoolforunderstand- ing personal emotion style derived from social media. In: 2014 IEEE Conference on Visual Analytics Science and Technology (VAST), pp. 203–212. IEEE (2014)
Chapter 3 Data Model
Abstract In text data, the semantic relationships among keywords, sentences, paragraphs, sections, chapters, and documents are usually implicit. A reader must go through the entire corpus to capture insights such as relationships among characters in a novel, event causalities in news reports, and evolution of topics in research articles. The difficulties usually arise from the unstructured nature of text data as well as the low information acquisition efficiency of reading these unstructured texts. Therefore, how to convert unstructured text data into a structured form to facilitate understand- ing and cognition becomes an important problem that has attracted considerable research interest. In this chapter, we introduce the data models that are frequently used in current text visualization techniques. We review low-level data structures such as bag of words, the structures at the syntactic level such as the syntax tree, as well as the network-oriented data structures at the semantic level. We introduce these data models (i.e., structures) together with detailed visualization examples that show how the structures are used to represent and summarize the unstructured text data.
Converting unstructured documents into a structured form helps produce addi- tional context regarding data relationships, thereby further helping users to quickly capture the insight of the document content as well as complex relationships among keywords, paragraphs, and sections in the documents at the semantic level. For exam- ple, in the field of data visualization, Watternberg et al. [14] introduced WordTree (Fig. 2.8), which used a tree structure to decompose sentences into text to investigate how a collection of sentences extracted from the documents share similar structures. Van Ham et al. [12] introduced PhraseNets (Fig. 2.9), which employs a graph based representation to capture co-occurrences of keywords based on textual patterns such as “X of Y” or “A’s B” in document collections. Based on these techniques, many interesting patterns are illustrated, helping users to summarize, reason, compare, and memorize the content and the corresponding semantics of the input text data. In addition, organizing and storing documents in structured form also aids to the data analysis process. Many advanced analysis algorithms such as clustering and classification are developed to deal with structured data described by feature vec- tors and relationships. Given all these factors, converting a collection of text data into a structured data model is one of the most important steps for text analysis and visualization.
© Atlantis Press and the author(s) 2016 41 C. Nan and W. Cui, Introduction to Text Visualization, Atlantis Briefs
in Artificial Intelligence 1, DOI 10.2991/978-94-6239-186-4_3
42 3 Data Model
However, designing a structured data model to capture the insight structures of text data is not an easy task. Many challenges exist. First, the text data itself contains rich information that can be difficult in designing a comprehensive model to capture all the information at the same time. For example, in a digital publication dataset, each publication is a document that contains information on authors, content in different topics, and publishing venue and time. Thus, different publications in a dataset may be related with each other from different aspects in terms of topics, authors, and the corresponding venues and time. Capturing all these information at the same time is not an easy task. Second, the text data may dynamically changed overtime, thus capturing the corresponding information dynamics in the model is challenging. Third, the data corpus could be extremely large, thereby requiring a scalable model to store the data. For example, Twitter produces millions or even billions of tweet everyday which are large and dynamic.
To address these challenges, a well-designed data model should consider many issues to capture all kinds of key attributes of the text data at the same time. First, as a fundamental requirement, the model should be able to store the content of the inputting documents. Second, to facilitate data understanding and reasoning, the model should capture the innate relationships of the key elements extracted from the text. These elements could be the representative keywords captured in the topic models [1], or the name entities detected in the process of name entity recognition [10]. These textual elements are usually related to each other in different ways such as through co-occurrences in documents or paragraphs that belonging to the same topic, or through semantic similarities. Third, in order to further clarify the content for summarization purpose, the model should also be able to capture both content and relationships from different information facets to produce a mutli- perspective representation of the data. With these considerations in mind, next, we will introduce data models that are widely used for text analysis and visualizations.
Based on the aforementioned considerations, many data models and structures have been developed for text data analysis and visualization, including the simplest Bag-of-Words model,1 more advanced tree and graph models, and the sophisticated multifaceted entity-relational data model [3, 4]. In the following subsections, we will describe the details of these data models one by one to introduce the related con- cepts, their advantages and disadvantages, as well as the corresponding visualization examples based on them.
 1Bag of words model. http://en.wikipedia.org/wiki/Bag-of-words_model.
3.1 Data Structures at the Word Level 43
3.1 Data Structures at the Word Level
3.1.1 Bag of Words and N-Gram
This is the simplest representation of documents, which is widely used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregard- ing grammar and even word order but maintaining multiplicity. The bag-of-words model is commonly used in methods of document classification methods where the (frequency of) occurrence of each word is used as a feature to train a classifier. Documents modeled by bag of words can be directly visualized by tag clouds or Wordle [13] as illustrated in Fig. 2.7.
In recent years, growing attention has focused on analyzing and visualizing streaming text data produced by various media systems such as online news, microblogs, and email exchanges. To dealing with these data, the most commonly used approach is to produce a dynamic bag-of-words model in which the words are changed over time corresponding to the change of the underlying text. The most famous visualization based on this data model is the ThemeRiever and many of its variants [7, 8] as shown in Fig. 2.12.
The Bag-of-Words data model is the simplest and the most intuitive representa- tion of the text data. When the proper words are extracted, this model successfully summarizes the content of the text. However, it usually fails to capture the semantic relationships among different words. Intuitively, in the tag cloud visualization, the words are usually randomly laid out or following some heuristic strategies such as putting the high frequency words in the middle surrounded by low frequency ones. In these visualizations, the screen distances between words usually are meaningless, i.e., two nearby wards do not mean they are not necessarily similar or related to each other, thereby making the visualization sometimes difficult to read and understand. To solve this problem, more advanced tree and graph models are developed, which are introduced in the following.
3.1.2 Word Frequency Vector
When computing the feature vectors of each document, usually, only the terms that best differentiate various documents are selected. The term extraction usually follows a standard procedure in which the documents are first split into words, and then a set of predefined stop words are removed and finally the remaining words are ranked based on their importance and computed based on their frequencies. In particular, TF-IDF is the most commonly used method to rank the words, which ensures that the selected high-ranking words are able to differentiate the documents. In this case, TF is the term frequency, which measures how frequently a term t occurs in a document
44 3 Data Model
d, which is usually normalized by document length (i.e., the total number of terms in the document). Formally, TF is defined as:
TF(t,d)= freq(t,d)/lens(d)
where freq(·) computes the term frequency in the given document d and len(·) indicates the document length. IDF refers to the inverse document frequency, which measures how important a term is in the document collection, which is formally defined as:
I DF(t, D) = ln(|D|/|{t ∈ d|d ∈ D}|)
where D is the document collection. Intuitively, IDF weights down the frequent terms that widely occur in many documents such as “this” and “that”, but scale up the ones that high frequently exist in one of some of the documents, thereby ranking out the words that best differentiate the documents. The TF-IDF score is defined as
TF-IDF(t, d, D) = T F(t, d) · I DF(t, D)
Sometimes, stemming is applied to extract word radicals so that the frequency can be more correctly computed and an n-gram is computed to increase the amount of information of each term before the ranking procedure.
3.2 Data Structures at the Syntactical-Level
In text analysis, the tree structure has been widely used to capture the relationships of the text content. It is generally used in two approaches. First, it is used in the procedure of hierarchical text classification [11], and topic modeling [6] to produce document classes or topic clusters with multiple granularity levels. Analysis in this category, usually accompanied with a zoomable representation of the results, allows users to interactively zooming into different levels of information detail. In these applications, the tree data model is used to guide the interactive data navigation. Second, in the field of nature language processing, the tree data model is also used for syntactic analysis [9]. A corresponding visualization, Word Tree [14] is also designed for this purpose, which represents the syntactic structure of a collection of sentences (Fig. 2.8).
Compared with the Bag-of-Words model, the tree structure is advanced and is able to guide the navigation and capture the structure of sentences. However, the higher level relationships, i.e., the relationships at the semantic level, are still obscure in the tree model. For example, various relationships among different name entities, such as authors and topic keywords in a publication dataset, can not be clearly captured in a tree structure.
3.3 Data Models at the Semantic Level 45
3.3 Data Models at the Semantic Level
3.3.1 Network Oriented Data Models
To address the aforementioned limitations, various types of network oriented struc- tures are frequently used to capture the semantics of the text data, in which nodes (i.e., entities) indicate concepts, name entities, or keywords, showing the content of corpus and links (i.e., relation) indicates the their relationships. It is one of the most flexible structures designed to capture relationships among data elements. In text analysis, the network oriented data model is widely used to represent high level relationships among documents such as citing and referencing among publications and hyperlinks in online webpages. This data model can also be used to capture the textual relationships at the content level by representing the relationships among topic keywords or name entities that are extracted from the documents. Phrase Nets is a visualization which is designed based on the network data model to illustrate relationships among keywords. In this visualization, a user can choose relationships based on different grammar pattern. For example, “A s ́ B” or “B of A” indicates the relationship of B belonging to A. Once the pattern is chosen (Fig.2.9), the text is tokenized by “ ́’’ or “of” and the high frequency keywords in the text and their cor- responding relationships are extracted which are visualized in a graph visualization.
Recently, a directed network model is introduced to capture the evolution of topics over time. As shown in Fig. 3.1, in this model, each node represents a topic computed based on the underlying text data that are collected at different time. In this case, the nodes from left to right indicate topics respectively computed at time t1, t2, and t3. The directed links indicate the transition with weights indicating the corresponding portion of the content transited from one topic to another topic. Obviously, this data model focuses on capturing the transition trend, i.e., how topics are merged or split over time instead of the detailed changes of the keywords. For example, in Fig. 3.1, the topics T1-1 and T1-2 at t1 are merged into a new topic T2-1 at time t2. When visualizing this directed network model in TextFlow [5] as illustrated in Fig. 2.13, the dynamic topic transition trend are clearly shown.
Fig. 3.1 The directed network data model for modeling topic evolutions over time

46 3 Data Model
3.3.2 Multifaceted Entity-Relational Data Model
Although flexible, the aforementioned network oriented data models are fail to cap- ture both content and relationships from different information facets to produce a mutli-perspective representation of the data. To meet this requirement, the multi- faceted entity-relational data model is developed based on the network data model. This model is a more sophisticated design that decomposes text data into several key elements from different aspects to facilitate analysis and visualization. This model is used in most of the algorithms and visualizations introduced in this book and is the most comprehensive as well as the most complicated data model for representing text data among all the models that we introduced.
Generally speaking, the Multifaceted Entity-Relational data model (as shown in Fig.3.2) decomposes the text data into three low-level components including entities, relations, and facets based on which the data model reveals high level patterns such as clusters, topics, and the corresponding transition trend of the text data over time. Specifically, we interpret the key components and the corresponding high level patterns as follows:
• Entitiesareinstances(e.g.keywordsornameentities)ofaparticularconcept(e.g., topics) extracted from the underlying text data. An entity is considered as the atom element in the model that cannot be further divided.
• Facetsareclassesofentitiesthatshareauniqueattributeorattributes.Forexample, “topic” is a facet that contains a set of topic keywords (i.e., entities) together describing a unique semantic meaning.
• Relations are connections between pairs of entities. In the Multifaceted-Entity Relational Data model, there are two types of relations. Internal relations are connections between entities or entity groups within the same information facet. An example is the co-occurrence relationships of the keywords within the same topic. External relations are connections between entities of different facets. For example the co-occurrence relationships of the keywords cross different topics.
Fig. 3.2 The multifaceted entity relational data model and the data transformation pipeline. The DBLP data are used as an example

3.3 Data Models at the Semantic Level 47
• Clusters are groups of similar entities within a single facet. It is a concept that further divides entities within the same facet into groups based on their similarities such as sub-topics under a primary topic.
• TemporalTrenddescribestheinformationdynamicsofbothentitiesandrelation- ships over time. As shown in Fig. 3.2, this data model captures the change of the data by producing the multifaceted entity-relational structures based on the data collected at different times.
An example of using the data model to store a publication dataset is illustrated in Fig. 3.2. In this example, the publications are divided into three information facets— Conferences (where the paper published in), Authors, and Keywords (describe the topics of the papers)—each represented as a separate layer in Fig.3.2. Nodes on each layer represent entities within the corresponding facet. Edges within a layer are internal relations, while edges across layers are external relations. This model also captures the annual changes on the dataset.
Data Transformation. Compared with the aforementioned data models, the Mul- tifaceted Entity-Relational data model is relatively complex. Transforming text data into this model usually requires to performing an entire data preprocessing procedure that consists of multiple key steps, including facet segmentation, entity extraction, relation building, and temporal reordering.
• Facet Segmentation. In this step, documents are divided into information facets. Usually, no standard approach for doing this exists because it highly depends on the input documents. Sometimes, the information facets can be identified easily. In the aforementioned example of the publication dataset, conference, author, and keywords are three obvious information facets that can be easily extracted from the data. Segmenting online descriptions of diseases that are usually written in a standard format, as well as describing the corresponding symptoms, diagnosis, and treatment are also easy. However, in most cases, the documents are not well organized in a standard format. To segment these data, highly advanced techniques need to be used. Typically, we employ a topic modeling technique such as LDA [2] to compute topics of the input text and to take each topic as a facet.
• Entity Extraction. Entities are further extracted from each of the information facet identified in the above procedure. These entities can be keywords ranked by topic models or name-entities identified in a name entity recognition procedure.
• RelationBuilding.Inthisstep,weestablishrelationshipsamongentitieswithinor across different information facets. Among different entities, there are many types of relationships such as co-occurrences in documents, or various relationships (e.g., A s ́ B or B of A, where A and B are textual entities) in sentences.
• TemporalOrdering.Inthefinalstepofthedatatransformation,allthedataentities and their corresponding relations are sorted in the temporal order, thus producing a data stream that can be further segmented by time windows with either fixed or adaptive sizes.
48 3 Data Model
3.4 Summary of the Chapter
In this chapter, we introduced the data models and the corresponding methods to convert the unstructured text data into a structured form to facilitate text analysis and visualization. We first introduce the background of the problem, including moti- vations and corresponding considerations and requirements. Thereafter, we discuss four most frequently used data models, namely, bag-of-words, tree network, and multifaceted entity relational data model to represent text data in a structured form. We describe these models one by one in detail and introduce the related concepts, their advantages and disadvantages, as well as the corresponding visualization exam- ples. Among all these data models, the last, which is the most comprehensive one that enables a multifaceted representation of the text data, is the focus of our study. This model is also the foundation of most of the algorithms and visualizations intro- duced in this book. Therefore, gaining an in-depth understanding of this data model is important in reading the following chapters.
References
1. Blei,D.M.:Probabilistictopicmodels.Commun.ACM55(4),77–84(2012)
2. Blei, D.M., Ng, A.Y., Jordan, M.I.: Latent Dirichlet allocation. J. Mach. Learn. Res. 3, 993–
1022 (2003)
3. Cao,N.,Gotz,D.,Sun,J.,Lin,Y.R.,Qu,H.:Solarmap:multifacetedvisualanalyticsfortopic
exploration. In: IEEE International Conference on Data Mining, pp. 101–110. IEEE (2011)
4. Cao,N.,Sun,J.,Lin,Y.R.,Gotz,D.,Liu,S.,Qu,H.:Facetatlas:multifacetedvisualizationfor
rich text corpora. IEEE Trans. Vis. Comput. Graph. 16(6), 1172–1181 (2010)
5. Cui,W.,Liu,S.,Tan,L.,Shi,C.,Song,Y.,Gao,Z.J.,Qu,H.,Tong,X.:Textflow:towardsbetter understanding of evolving topics in text. IEEE Trans. Vis. Comput. Graph. 17(12), 2412–2421
(2011)
6. Griffiths,D.:Tenenbaum:hierarchicaltopicmodelsandthenestedChineserestaurantprocess.
Adv. Neural Inf. Process. Syst. 16, 17 (2004)
7. Havre,S.,Hetzler,B.,Nowell,L.:Themeriver:visualizingthemechangesovertime.In:IEEE
Symposium on Information Visualization, 2000. InfoVis 2000, pp. 115–123. IEEE (2000)
8. Liu,S.,Zhou,M.X.,Pan,S.,Song,Y.,Qian,W.,Cai,W.,Lian,X.:TIARA:interactive,topic- based visual text summarization and analysis. ACM Trans. Intell. Syst. Technol. (TIST) 3(2),
25 (2012)
9. Manning,C.D.,Schütze,H.:FoundationsofStatisticalNaturalLanguageProcessing,vol.999.
MIT Press, Cambridge (1999)
10. Nadeau, D., Sekine, S.: A survey of named entity recognition and classification. Lingvist.
Investig. 30(1), 3–26 (2007)
11. Sun,A.,Lim,E.P.:Hierarchicaltextclassificationandevaluation.In:IEEEInternationalCon-
ference on Data Mining, pp. 521–528. IEEE (2001)
12. Van Ham, F., Wattenberg, M., Viégas, F.B.: Mapping text with phrase nets. IEEE Trans. Vis.
Comput. Graph. 15(6), 1169–1176 (2009)
13. Viegas,F.B.,Wattenberg,M.,Feinberg,J.:Participatoryvisualizationwithwordle.IEEETrans.
Vis. Comput. Graph. 15(6), 1137–1144 (2009)
14. Wattenberg, M., Viégas, F.B.: The word tree, an interactive visual concordance. IEEE Trans.
Vis. Comput. Graph. 14(6), 1221–1228 (2008)
Chapter 4
Visualizing Document Similarity
Abstract A large category of text visualization techniques were developed to illustrate similarities of document files in a corpus. This is the most traditional research direction in this field. These techniques produce visualizations in a sim- ilar form in which document files are represented as points on the display sized by their importance and colored or shaped by their semantics (such as topics). The screen distance between any pair of points indicates the similarities of the correspond- ing documents that are captured by different measures in different analysis models, following the rule of the closer, the more similar. Two major types of approaches: pro- jection (i.e., dimension reduction)-based methods and semantic-oriented document visualizations, are proposed to produce such a document overview; these approaches are developed based on different data and analysis models. In this section, we inves- tigate these techniques and the corresponding document visualization systems.
4.1 Projection Based Approaches
One of the major method for visualizing document similarity is the projection based approaches. In these techniques, a document is represented by an N-dimensional feature vector in which each filed represents the number of a term in a keyword collection extracted from the entire document corpus. The visualization is produced by projecting the document from the high dimensional feature space into a low dimensional (2D or 3D) visualization space.
Formally, a projection procedure is defined as a follows: let X = {x1, x2, . . . , xk } be a set of n-dimensional data items (i.e., documents), with dn (xi , x j ) as a defined distance measure between two items xi and xj. Let Y = {y1, y2,..., yn} also be a set of points defined in an m-dimensional space, with M ∈ {1, 2, 3}, and dm (xi , x j ) indicates the distance between two data items i and j in the m-dimensional visualiza- tion space. In this way, a dimension reduction technique is described as an injective function f : X → Y that attempts to best preserve a specified relationship between dn and dm. This definition treats each projection as an optimization problem that has two advantages [19]: (1) the iterative nature of the optimization allows users to observe the details of the projection process, and (2) the incremental nature of the
© Atlantis Press and the author(s) 2016 49 C. Nan and W. Cui, Introduction to Text Visualization, Atlantis Briefs
in Artificial Intelligence 1, DOI 10.2991/978-94-6239-186-4_4
50 4 Visualizing Document Similarity
optimization allows users to add new data items efficiently. Many such projection techniques have been described in the literature. Overall, they can be classified into two major categories, based on whether function f is linear or non-linear.
In the following, we first describe the methods developed for selecting proper features for each document, followed by a brief introductions of both linear and non-linear projection techniques.
4.1.1 Linear Projections
These techniques project high dimensional data based on linear combinations of the dimensions, sharing many advantages than non-linear projections [14]. These advan- tages include the following: (1) it is assured for linear projections to show genuine properties of the data, thereby making them reliable; (2) the linear combinations of the original axes make the new axes generated by projection meaningful; (3) the technique is an incremental process in which new data items can be easily added based on the already computed linear transformation; (4) computing a linear projec- tion is easy and efficient. Thus, the linear projection attracts serious attentions in the field of information visualization and many of them have been used to represent the overview of document similarities.
Koren and Kermal [14] first introduced the general form of linear projection into the visualization community. In particular, the linear projection can be formulated as the following optimization problem:

max dm(xi,xj)2 i<j
which seeks an m-dimensional projection that best maximizes the distances between everytwopointsxi,xj inthen-dimensionalspace.Choosingdifferentdistancemet- rics will result in different algorithms. In this study, we show two of them, Principle Component Analysis and Linear Discriminant Analysis, as examples. More discus- sions can be found in [14].
Principal Component Analysis (PCA). When setting dm = ||xi − x j || (i.e., the L2-norm) in the aforementioned model, we have

i<j
which is the objective of Principal Component Analysis (PCA) [11], a well-know linear dimension reduction technique. Intuitively, this technique aims to preserve and maximize data variance so that each point in the data can be easily differentiated from others when shown on the screen while the projection is being computed. This opti- mization problem can be effectively solved by decomposing the covariance matrix
1 XT X of the document collection (X = {x1,...,xk} into orthogonal eigenvectors n
max
||xi −xj||2

4.1 Projection Based Approaches 51
and taking the first m eigenvectors y1 , y2 , . . . , y p with the largest eigenvalues as the m-dimensional visualization coordinates. Following this definition, the so-called weighted PCA is defined as:

i<j
where ωi j measures the importance of separating two data items xi and x j . It has many variances designed for different purpose.
Forexample,whenωij =1/dn(xi,xj),theprecedingobjectivecomputesanor- malized PCA [11] that transforms the distances computed under different features into the same scale, thereby improving the robustness of the algorithm.
Whensettingωij =t·ωij,theaforementionedmodelisabletorepresentandinter- pret labeled multidimensional data (e.g., classification results or documents labeled with different topic tags in the context of text visualization) by adjusting the value of t. In particular, setting 0 ≤ t ≤ 1 if both xi and xj share the same label (i.e., in the same class) otherwise setting t = 1 if xi , x j are in different classes, helps to differentiate different data classes in the resulting visualization.
Linear Discriminant Analysis (LDA). When applying the following distance met- ric to the above linear projection model, the resulting objective indicates of Fisher’s Linear Discriminant Analysis (LDA) [2], another type of linear projection that is developed to illustrate the labeled multivariate data (e.g., classification results).
  1 − 1 ifitemiandjarebothintheclassC dm(i,j)= k2 k·|C|
1 if item i and j are in different classes k2
where k indicates the total number of data items in the collection and C indicates a class whose size (i.e., the number of containing items) is |C|. Similar to the above weighted PCA with the weight determining the label information, this distance metric also helps to separate different classes in the data and considers the size of each class.
4.1.2 Non-linear Projections
The aforementioned linear projection techniques, although efficient and has many advantages, fail to capture the higher order relationship of the data items that are usually non-linear. To address this issue, many non-linear projection techniques are also introduced [9, 15, 17, 20, 21]. Based on the ways in which data relationships are measured, these techniques can be largely organized into two categories including, (1) those that use pairwise distances such as multidimensional scaling (MDS) [15], locally linear embedding (LLE) [20] and isomap [21] and (2) those that employ prob- abilistic formulations such as stochastic neighbor embedding (SNE) [7], parametric
max
ωij||xi − xj||2

52 4 Visualizing Document Similarity
embedding (PE) [9] and t-SNE [17]. Next, we will introduce the one representative technique in each category as follows.
Distance Oriented Techniques
Multidimensional Scaling (MDS) [15] is the most well-known distance oriented projection technique that has been commonly used for visualizing multidimensional data. It focuses on placing data items (i.e., documents in case of text visualization) in the m-dimensional (m = 2 or 3) visualization space so that the high-dimensional distance between any pair of data items is well preserved. MDS achieves this goal by optimizing the following objective:

min ||dm(xi,xj)−dn(xi,xj)||2 i<j
where dm (xi , x j ) and dn (xi , x j ), which is consistent with the previous definition, respectively indicates the screen distance and feature distance between the data items i and j. Usually, L2-norm is used. In other sophisticated usages of MDS such as Isomap [3], the connections (i.e., relationships) between data points are estimated based on the nearest Euclidean neighbors in high-dimensional space and the distance is thus defined based on the shortest paths in the resulting graph.
MDS usually generates meaningful results, revealing patterns such as clusters and outliers. Therefore, it has been widely used in the field of data visualization to produce an overview of the entire document corpus [1]. However, achieving the aforementioned objective requires calculating the pairwise distance of data points, which is time consuming with a time complexity of O(n2). To address this issue, many approximation methods have been developed [5, 12, 18, 22], which intro- duces methods for computing approximate MDS projection results with a linear or near-linear time complexity. However, in these techniques, the projection quality is sacrificed.
The preceding objective can also be generalized in the following weighted form:

min ωij||dm(xi,xj)−dn(xi,xj)||2 i<j
whereωij indicatestheimportanceofpreservingthedistancebetweenaspecifiedpair of data items i and j in the low dimensional space. A different ωi j leads to different projection models. In particular, when setting ωi j = 1/dmk where k ∈ [1, m ], it becomes the Kamada-Kawai force-directed layout [13], a well-known graph drawing algorithm that can be bettered solved based on stress majorization [6].
Probabilistic Formulation
Among all probabilistic formulation oriented non-linear projection techniques such as stochastic neighbor embedding (SNE) [7] and parametric embedding (PE) [9], t-SNE [17], a variant of SNE technique, is considered as a landmark technique that produces the best visualization results in most cases, which preserves the details of the data’s local structure in the low dimensional visualization space. Compared
4.1 Projection Based Approaches 53
with SNE, t-SNE overcomes the crowding problem and produces a visualization that effectively reveals cluster patterns.
Specifically, SNE minimizes the sum of Kullback-Leibler divergences [16] over all data points via the gradient descent method as follows:
   pj|i min pj|ilogqj|i
ij
where pj|i and qj|i are conditional probabilities that capture the similarity between data point xi and xj in high dimensional feature space and low dimensional visu- alization space respectively. Intuitively, they can be understood as when given data point xi , the probability of choosing x j as its neighbor in the feature space ( p j |i ) and visualization space (q j |i ). In an ideal projection in which all the features are pre- served, p j |i and q j |i should be the same, which provides the design rationale of SNE: it computes the projection by minimizing the differences (measured by Kullback- Leibler divergence in the aforementioned optimization objective) between these two probabilities over all data points.
Although the preceding optimization problem is well motivated and defined, the visualization results produced by SNE usually fail to capture data clusters, which is known as the crowding problem [17]. t-SEN addresses this issue from two aspects: first, it employs the joint probability pi j and qi j instead of the conditional probability to provide a symmetric measurement of the similarity between data point xi and x j in both the feature and visualization space. Second, t-SEN defines qi j based on the Student’s t-distributions instead of Gaussian distribution. As shown in Fig.4.1, the resulting model:
Fig. 4.1 Visualizations of 6000 handwritten digits from the MNIST dataset based on SNE (left) and t-SEN (right)

54 4 Visualizing Document Similarity
   pij min pijlogqij
ij
better preserves local structures and successfully preserves the high-dimensional data clusters in the visualization.
4.2 Semantic Oriented Techniques
Unlike the aforementioned projection-based visualization approaches, semantic- oriented techniques represent document similarity based on latent topic informa- tion. Research in this direction is inspired by traditional topic modeling such as PLSA [8] and Latent Dirichlet allocation (LDA) [4]. Although widely used and is able to extract a low dimensional representation of a document, these techniques can not represent more than three topics in 2D visualization space as the visualization has to be produced inside a simplex space, which is usually shown as a triangle on the 2D Euclidean visualization plane as shown in Fig. 4.2d. To effectively visualize document similarity based on multiple topics, many semantic-oriented techniques are developed. Probabilistic Latent Semantic Visualization (PLSV) [10] is the first and the most impactful work in this direction. Compared with PLSA and LDA, PLSV can represent any number of topics within a two-dimensional display, and it embeds documents in the Euclidean space instead of the simplex space, thus produc-
Fig. 4.2 Comparison of PLSV with different existing techniques [10]

4.2 Semantic Oriented Techniques 55
ing an intuitive visual representation in which distances directly encode document similarities regarding to their topics.
In particular, PLSV computes the visualization coordinates of the input docu- ments, X = xi ik=1, based on a probabilistic topic model that captures the topic pro- portional of each document via Euclidean distance. Formally, the topic proportion of a document is defined as:
exp −1||xi −φz||2  2
 P(z|xi,Φ)= Z   1
exp − ||xi −φz′
||
2
 z′=1 2
 where z ∈ Z is a latent topic and φz ∈ Φ is its corresponding visualization coordi- nates. || · || is the L2-norm in the visualization space. Therefore, when a document xi is close to a topic φz, the topic proportion is high, which indicates that the doc- ument has a higher probability to contain the content in the z-th topic. In this way, documents close to each other share a similar set of topics. The visualization results based on PLSV are shown in Fig. 4.2a and are comparison with the results visualized based on PLSA (Fig. 4.2d).
4.3 Conclusion
In this chapter, we introduce the techniques developed to illustrate the similarities between document files in a corpus. We have reviewed They produce visualizations in a similar form in which document files are represented as points on the display sized by importance and colored or shaped by semantics (such as topics). The screen distance between any pair of points indicates the similarities of the correspond- ing documents that are captured by various measures in different analysis models, following the rule of the closer, the more similar. Two major types of approaches are available: projection (i.e., dimension reduction)-based methods and semantic- oriented document visualizations; these visualizations are proposed for producing such a document overview that is developed based on different data and analysis mod- els. In this section, we investigate these techniques and the corresponding document visualization systems.
References
1. Andrews,K.,Kienreich,W.,Sabol,V.,Becker,J.,Droschl,G.,Kappe,F.,Granitzer,M.,Auer,P., Tochtermann, K.: The infosky visual explorer: exploiting hierarchical structure and document similarities. Inf. Vis. 1(3/4), 166–181 (2002)
2. Balakrishnama, S., Ganapathiraju, A.: Linear Discriminant Analysis—A Brief Tutorial, vol. 18. Institute for Signal and Information Processing, Starkville (1998)
3. Balasubramanian,M.,Schwartz,E.L.:Theisomapalgorithmandtopologicalstability.Science 295(5552), 7–7 (2002)
56 4 Visualizing Document Similarity
4. Blei,D.M.,Ng,A.Y.,Jordan,M.I.:Latentdirichletallocation.J.Mach.Learn.Res.3,993–1022 (2003)
5. Brandes,U.,Pich,C.:Eigensolvermethodsforprogressivemultidimensionalscalingoflarge data. In: Graph Drawing, pp. 42–53. Springer, Berlin (2007)
6. Gansner, E., Koren, Y., North, S.: Graph drawing by stress majorization. In: Graph Drawing, pp. 239–250 (2005)
7. Hinton,G.E.,Roweis,S.T.:Stochasticneighborembedding.In:AdvancesinNeuralInforma- tion Processing Systems, pp. 833–840 (2002)
8. Hofmann, T.: Probabilistic latent semantic indexing. In: Proceedings of International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 50–57. ACM (1999)
9. Iwata, T., Saito, K., Ueda, N., Stromsten, S., Griffiths, T.D., Tenenbaum, J.B.: Parametric embedding for class visualization. Neural Comput. 19(9), 2536–2556 (2007)
10. Iwata, T., Yamada, T., Ueda, N.: Probabilistic latent semantic visualization: topic model for visualizing documents. In: Proceedings of SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 363–371. ACM (2008)
11. Jolliffe,I.:PrincipalComponentAnalysis.WileyOnlineLibrary(2002)
12. Jourdan, F., Melangon, G.: Multiscale hybrid mds. In: Proceedings of the IEEE Symposium
on Information Visualization, pp. 388–393 (2004)
13. Kamada,T.,Kawai,S.:Analgorithmfordrawinggeneralundirectedgraphs.Inf.Process.Lett.
31(1), 7–15 (1989)
14. Koren,Y.,Carmel,L.:Visualizationoflabeleddatausinglineartransformations.In:Proceed-
ings of IEEE Symposium on Information Visualization, pp. 121–128 (2003)
15. Kruskal,J.B.:Multidimensionalscalingbyoptimizinggoodnessoffittoanonmetrichypoth-
esis. Psychometrika 29(1), 1–27 (1964)
16. Kullback,S.,Leibler,R.A.:Oninformationandsufficiency.Ann.Math.Stat.79–86(1951)
17. Van der Maaten, L., Hinton, G.: Visualizing data using t-SNE. J. Mach. Learn. Res. 9(2579–
2605), 85 (2008)
18. Morrison,A.,Chalmers,M.:Apivot-basedroutineforimprovedparent-findinginhybridMDS.
Inf. Vis. 3(2), 109–122 (2004)
19. Paulovich, F.V., Oliveira, M.C.F., Minghim, R.: The projection explorer: a flexible tool for
projection-based multidimensional visualization. In: Computer Graphics and Image Process-
ing, pp. 27–36. IEEE (2007)
20. Roweis, S.T., Saul, L.K.: Nonlinear dimensionality reduction by locally linear embedding.
Science 290(5500), 2323–2326 (2000)
21. Tenenbaum, J.B., De Silva, V., Langford, J.C.: A global geometric framework for nonlinear
dimensionality reduction. Science 290(5500), 2319–2323 (2000)
22. Williams, M., Munzner, T.: Steerable, progressive multidimensional scaling. In: Proceedings
of the IEEE Symposium on Information Visualization, pp. 57–64 (2004)
Chapter 5
Visualizing Document Content
Abstract Text is primarily made of words and always meant to contain content for information delivery. Content analysis is the earliest established method of text analysis (Holsti et al., The handbook of social psychology, vol 2, pp 596–692, 1968 [55]). Although studied extensively and systematically by linguists, related disci- plines are roughly divided into two categories, structure and substance, according to their subjects of study (Ansari, Dimensions in discourse: elementary to essentials. Xlibris Corporation, Bloomington, 2013 [9]). Structure is about the surface charac- teristics that are visible for a valid text, such as word co-occurrence, text reuse, and grammar structure. On the other hand, substance is the umbrella term for all infor- mation that needs to be inferred from text, such as fingerprinting, topics, and events. Various techniques have been proposed to analyze these aspects. In this chapter, we will briefly review these techniques and the corresponding visualization systems.
According to literary theory, written text situates syntactic symbols, such as words and punctuations, in a certain arrangement in order to produce a message. This arrangement of syntactic symbols is considered the message’s content [71], rather than other physical factors, such as the appearance of the symbols or material on which it is written. Content analysis is the earliest established method of text analy- sis [55]. In the early years of content analysis, content was mainly viewed as the result of a communication process: “Who says what in which channel to whom and with what effect” [65]. Nowadays, this term is extended dramatically, covering from the communicative context of texts to their linguistic form. Although text con- tent has been studied extensively and systematically by linguists, related disciplines are roughly divided into two categories, structure and substance, according to their subjects of study [9].
Structure is about all the defining characteristics that are visible at the surface of a valid text. Werlich [114] defined a text as
an extended structure of syntactic unit such as words, groups, and clauses and textual units that is marked by both coherence among the elements and completion where as a non-text consists of random sequences of linguistic units such as sentences, paragraphs or sections in any temporal and/or spatial extension.
© Atlantis Press and the author(s) 2016 57 C. Nan and W. Cui, Introduction to Text Visualization, Atlantis Briefs
in Artificial Intelligence 1, DOI 10.2991/978-94-6239-186-4_5
58 5 Visualizing Document Content
All the units and the structure collectively contribute to the content of the text. At the structural level, researchers study how the syntactic units are organized with a strong logic of linguistic connections.
On the other hand, substance, which is a more subjective term, focuses on the information that can be inferred from the text, or on what remains after having “read” the text. The information is either intentionally delivered, such as the main idea of a document, or unintentionally delivered, such as the author of an anony- mous blackmail note. Research in this category has largely been concerned with the ways people build up meaning in various contexts, with minimal attention paid to grammatical rules or structures. For example, discourse analysis aims to produce valid and trustworthy inferences via contextualized interpretations of messages pro- duced in a communication process. Plagiarism detection is the process of detecting plagiarism instances within one or multiple documents by comparing particular pat- terns of language use, such as vocabulary, collocations, pronunciation, spelling, and grammar.
In this chapter, two sections are dedicated to analysis and visualization techniques for structure and substance, respectively. However, before these two sections, we decide to add one more for word visualization. There are two main reasons that we separate and put it first. One is that word is the most important syntactic unit for higher level analysis of structure and substance. It is part of text, but does not strictly belong to either structure or substance. The other reason is that bag-of-words is the fundamental model used in the text mining field. In this model, text content is treated as a set of words, disregarding grammar and even word order but keeping frequencies of occurrences. Since it is a very popular and successful model in mining, various visualizations have been specifically proposed for it. Therefore, before we introduce more sophisticated visualization solutions for structure and substance, it is necessary and meaningful to cover visualization techniques that are designed for the basic building block: word.
To summarize, this chapter mainly consists of three levels of visual analytics (from concrete to abstract): word, structure, and substance, which basically agrees with Justin Johnson’s saying about text:
not only what we say and how we say it, but also what we do not say which can be inferred from what we say.
5.1 “What We Say”: Word
Besides words, it is clear that there are more things we can use during communica- tions, such as tones and punctuations. However, we narrow our scope to words in this section due to its dominant position in content.
A word is the smallest language element that can deliver literal or practical mean- ings. As a consequence, the bag-of-words model is widely adopted to represent text contents. Intuitively, a bag of words contains the occurrences of words in the
5.1 “What We Say”: Word 59 corresponding text regardless their position in it. The concept of “bag of words” is
first mentioned by linguist Zellig Harris in 1954 [49]:
...for language is not merely a bag of words but a tool with particular properties
which have been fashioned in the course of its use.
It is clear that Mr. Harris does not favor the idea of “bag of words”. However after decades of research and testing, the bag-of-words model has proven successful in various research domains, such as content analysis, text mining, natural language processing, and information retrieval. It is generally considered a success of statis- tics: linguistic items with similar distributions have similar meanings. Based on this basic distributional hypothesis, researchers exploit the fast processing capability of computers and the massive text data available on the Internet to achieve good results:
Every time we fire a phonetician/linguist, the performance of our system goes up. — Roger K. Moore, 1985.
Simple and powerful as it is, some information is clearly lost when using the bag- of-words model. First, each word may have multiple meanings that can only be understood as a part of different sentences. However, since the context of individual occurrences is ignored in the bag-of-words model, the meanings are treated equally. Second, the meaning of a whole sentence generally cannot be considered a summation of the meanings of its component words taken individually. Thus, the sophisticated information delivered by sentences are also completely lost in the bag-of-words representation.
Nonetheless, the bag-of-words model is still a popular and successful way to help people gain a quick overview about the text. In this section, we will look into visualizations that are specifically designed for the bag-of-words model.
5.1.1 Frequency
In the bag-of-words model, a valid text content is generally considered a set of words with occurrence frequencies in the corresponding text regardless of their positions in it (a bag of words). Thus, the data to be visualized for the bag-of-words model are essentially a set of [word, frequency] pairs, and a good visualization should deliver these two pieces of information effectively and intuitively.
In written language, words already have a well-accepted visualization form in written language, which are called glyphs. In typography, a glyph is a visual mark within a predefined set of marks intended to represent a character for the purpose of writing. For example, the letter a may appear differently using different font types, such as Arial and Times New Roman. However, all the appearances are valid visual- izations of the letter a. In other words, glyphs are considered marks that collectively add up to visualize a word. Thus, we believe no visualizations can be more familiar,
60 5 Visualizing Document Content
precise, or intuitive than directly showing the words (the data itself). For conve- nience, the term word may refer to the glyphs that visually represent the word or the semantic meaning behind it based on the term’s context in the rest of this section.
Regarding the frequency information in the bag-of-words model, it is only nat- ural to map it to other visual attributes of glyphs, such as font size, typeface, font weight, text color. As a matter of fact, several visual attributes have been adopted to encode additional information (not necessarily occurrences) of words in practice. For example, Amazon uses color intensity to tell users about how recently a tag is used. Del.icio.us uses red and blue to show if a tag is shared or not. However, among all the visual attributes, font size is most popular for representing a large range of numbers [12], such as occurrence frequencies. This group of visualizations that are composed of a set of words with size encoding one extra attribute, commonly the frequency value, are called word clouds. Word clouds are the dominant method for visualizing a bag of words, and thus, it is our focus in this section.
Background of Word Clouds
The concept of word clouds can be roughly traced back to Milgram’s psychological map of Pairs [79]. In 1976, The social psychologist Stanley Milgram asked people to name landmarks in Paris, and drew a map with words with font size encoding the number of votes for the corresponding landmark (Fig. 5.1).
However, it was the photo sharing website Flickr that popularized the use of word clouds worldwide. When people upload photos to Flickr, tags are also attached to
Fig. 5.1 Stanley Milgram’s mental map of Pairs [79]. Each word is roughly placed at the location of the corresponding attraction of interest on the map

5.1 “What We Say”: Word 61
 Fig. 5.2 Example of a Flickr tag cloud (https://www.flickr.com/photos/tags)
label the photos. For example, a photo taken at a beach may have one or more tags, such as “beach”, “sunset”, “sea”, or “scenery”, based on the judgment of the person who uploads it. Flickr summarizes and displays some of the most popular tags using word clouds on its webpage,1 which is probably why people used to call it tag cloud back then. With the help of tag clouds, users can glance and see what tags are hot and click to browse all the photos that carry the same tag (Fig. 5.2).
Nowadays people generally use “word cloud” and “tag cloud” interchangeably. Although visually the same, we still like to think they are slightly different in concept according to data sources: predefined meta-data or freeform text content. Tags in a tag cloud are semantically equal and equally meaningful. For example, tags in the tag clouds generated by Flickr represent different groups of photos, and are not necessary related to one another. By contract, words in a word cloud are often semantically related to one another, since they share the same content context. Thus, to best understand a word cloud, users may need to consider many words in the word cloud collectively. In addition, due to the roles of different words in the content, the words in a word cloud are also of different importance for making sense of the whole content. For example, the functional word is is clearly less useful than the noun table if they appear in the same word cloud.
Following the footsteps of Flickr, several other famous websites, such as Techno- rati and del.icio.us, also deploy word clouds to facilitate navigation. Word clouds have quickly become ubiquitous and abused on the Internet, no matter they are helpful or not. As a consequence, people rapidly grow tired of them. In fact, Flickr apologized for starting the craze about word clouds in 2006.
Just when word clouds are about to fade out on the Internet due to the abuse, researchers and software developers have found another arena, in which word clouds can be particularly useful: visualizing text contents.
There are reasons for word clouds being user friendly in general. Unlike other visu- alizations, word clouds need no additional visual artifacts and are self-explanatory. Users read the words and compare their sizes to know which one is more salient.
1https://www.flickr.com/photos/tags.

62 5 Visualizing Document Content Simple as it is, word clouds are an excellent way to provide users with a high-level
overview for casual explorations. Detailed tasks [89] for word clouds include:
• Search: Finding the size and location of a specific word.
• Browse: Finding words of interest for further exploration.
• Present: Forming a general idea about the content.
• Recognize/Match: Recognizing the subject that is described by the word cloud.
For example, one tedious task in text classification is to label the training data. Researchers have shown that using word clouds, users can label documents twice as fast but still as accurately as using full-text documents [93].
On the other hand, there are also some situations where word clouds are not appropriate. For example, people complain that word clouds cannot help make sense of or gain insight into a news article or a book [48]. Breaking text into word is like decomposing an electronic device into atoms. By checking the composing elements, although we can see that it is likely an electronic device, we certainly have no idea whether it is a television or an oven. Sometimes, word clouds may even mislead. For example, when we see a big word nice in the word cloud, we may consider it a positive impression on something. However, it is quite possible that the word nice in the content may have another word not co-occurring with it. The main reason is the context for each word is totally lost in word clouds. However, to be fair, we cannot completely blame word clouds for not being more meaningful. In fact, we believe that the issue is inherited from the bag-of-words model. Clearly, the context information is already lost at the data preparation process, so it is really impossible to recover at the visualization stage. If we just consider word clouds as a visualization technique to represent a bag of words, we would argue that there is no better way than word clouds. In the rest parts of this chapter, we will introduce more informative visualization techniques to represent text content. As far as word clouds, we need to be very careful about what tasks it is useful for.
Layout of Word Clouds
Generally, there are three ways to generate a word cloud:
• Sorted: Words are displayed as a sorted list based on dictionary order or frequency values.
• Random: Words are randomly placed in a 2D space.
• Cluster:Wordsareplacedina2Dspacetoreflecttheclusterrelationshipsbetween
them via Euclidean distance.
Technically, cluster-based layout is not a visualization technique for a bag of words, since it requires cluster relationships between words that certainly are not contained by a pure bag of words. In fact, a cluster relationship is more like a type of structure information that indicates how words are organized. Therefore, we will mainly focus on the first two in this section, and introduce the third one in later sections.
Sorted-based layout is the oldest and easiest way to draw word clouds. It is com- monly seen on websites like Flickr for navigation purposes. There are no sophisticated
5.1 “What We Say”: Word 63
 Fig. 5.3 Examples of sorted-based tag clouds [90]: (left) based on alphabetical order and (right) based on frequency value
algorithms required to generated sorted-based word clouds. Words are simply dis- played in a sequence with their font sizes equal to the relative importance or frequency values. Since there are only two pieces of information, i.e., word and frequency, for each item in a bag of words, we can only sort words based on alphabetic or based on frequency (Fig. 5.3). However, due to the irregular font sizes, wasted white space is inevitable between lines and words. All the unintentional white spaces are not aesthetically pleasing.
Unlike sorting-based methods that place all words in a sequence, random-based approaches freely place words on a plane and pack them together to generate a dense layout, which essentially falls into the category of geometric packing problems. Many algorithms have been designed, such as force-directed [32], seam-carving [115], and greedy [107], to remove the empty space between words. Intuitively, the first two algorithms start with a sparse, overlap-free layout of words, and then try to remove the empty space between words as much as possible. Different intuitions can be leveraged to achieve this goal. For example, Cui et al. [32] assume that there are attractive forces between words and use a force simulation to eliminate the white space between words. On the other hand, Wu et al. [115] construct an energy field based on the word distribution and remove the empty space that has lowest energy. The third category, greedy-based methods, is a little different. It directly adds words one-by-one to an empty canvas. Given a new word, the algorithm tests a set of candidate locations to place the word one-by-one, and checks if the word overlaps with the ones that have been added. Then, the algorithm adds the word to the canvas when a legit place is found, or rejects the word when all locations have been tested.
No matter which method is adopted, one key issue is efficiently detecting whether two word glyphs overlap, which essentially is a collision detection problem. One simple solution, which is also the most often adopted, is bounding box approxima- tion (Fig.5.4(left)). In this case, testing whether two words wa and wb overlap is equivalent to testing if their minimum bounding boxes  a and  b overlap, which can be efficiently obtained by checking the following value:
( a.L <  b.R) ∧ ( a.R >  b.L) ∧ ( a.T >  b.B) ∧ ( a.B <  b.T),
64 5 Visualizing Document Content
 Fig. 5.4 Two ways to pack words [32]: (left) treating each word as a rectangle and (right) treating each word as a complex shape
where edges of  a and  b are parallel to x-axis or y-axis, respectively. L, R, T , and B represent the left, right, top, and bottom bounds of the corresponding rectangle, respectively.
However, it still will not be really compact (Fig. 5.4(left)), since the word glyphs likely has ascent and descent empty spaces. Therefore, we need a more accurate and efficient way to detect the collision between word glyphs. Generally, there are two approaches, quadtree-based and mask-based, that are popular for achieving this goal.
A quadtree is a tree structure used to partition a 2d space by recursively sub- dividing it into four regions. When applying it to approximate the shape of a word, we first need to obtain the bounding box of the word. Then we recursively divide the bounding box into small four quadrants until one of the following conditions is met:
• there is no drawing of the word in the quadrant, which is considered clean; or • the quadrant is full of drawing of the word, which is considered dirty; or
• the size of the quadrant has reached a threshold, which is also considered dirty.
Although building a quadtree for a word is expensive, the cost is reclaimed by an order of magnitude during the collision detection step. When testing for collisions between word wa and wb, we only need to recursively traverse down into the inter- secting rectangles from the corresponding treea and treeb. The two words overlap if and only if two dirty leaves are found during the traversal. By pruning a lot of branches during the traversal, the results can be obtained very efficiently (Fig. 5.5).
For the other mask-based method, we also treat the drawings of words as images, each pixel is encoded with “1” and “0” to indicate whether it is occupied. Then we convert the matrix that is filled with ones and zeros to integers. By doing so, checking if two drawings becoming doing bitwise-and operation between integers, which can be efficiently performed in CPUs (Fig. 5.6).
Extending Word Clouds
Although intuitive, word clouds have limited capability of expression. Researchers have explored various paths to add additional values to it, such as supporting more interactions and encoding more information.
For example, ManiWordle [61] allows users to manually control the layout results produced by Wordle [107], a greedy algorithm automatically generates a static word
5.1 “What We Say”: Word 65
  Fig. 5.5 Example of quadtree representation: (left) the construction of the quadtree for the shape example and (right) the quadtree result of the shape of letter A
Fig. 5.6 Example of mask representation. Two drawings are converted to integers. Then bitwise- and operation is performed on the integers to check whether these drawings overlap
cloud in which words are randomly placed to achieve good compactness. In a typical random-based layout algorithm, users have no control over the final locations of any words in the generated layout. However, there are often scenarios when users have a preference for the final layout, such as putting two words side-by-side. ManiWordle is designed to accommodate this need. In a layout generated by Wordle, users may drag or rotate a word, ManiWordle will efficiently re-arrange the overlapped words and generate another version of compact word cloud accordingly (Fig. 5.7). Gener- ally, their implementation also follows the basic idea of Wordle, such as collision detection, and spiral-based arrange strategy. Their efficient implementation makes sure the manual adjustments can be done in real-time. Another important issue during this interaction is animation. Since user interactions may trigger relocation of many words, animation is necessary to help users track these changes, such as location changes and angle changes.
RadCloud [15] is another technique targets at merging multiple word clouds into one. This technique is particular useful for comparison tasks. For example, different medias, such as news, blogs, and micro-blogs, may cover different aspects of the

66 5 Visualizing Document Content
  Fig. 5.7 Example of ManiWordle [61]. (Left) The word data is rotated by user interaction; all the other words that overlap with the rotated data are pushed away to the closest legit locations to avoid overlapping. Empty space may appear during the process. (Right) All the words whose locations are not specified by users are re-arranged to generate a packed layout again
same social event. By comparing word clouds generated from individual sources, people may quickly grasp the difference between them without going through a tedious reading process. One intuitive idea is to generate word clouds independently for each source and display them side-by-side (Fig. 5.8(left)). However, finding the same and different words between these word clouds would be a nightmare. Instead, merging these clouds into one can help users quickly identify the shared words and unique words owned by individual word clouds. Figure 5.8(right) shows an example of RadCloud design. All words are placed inside the circle that consists of four colored arcs representing four different data sources. A word is initially placed at the center of the circle. Then it is moved towards different arcs based on its weight in the corresponding data sources. For example, the word tag appears in both green and orange data sources. Thus, two vectors are calculated accordingly and added together to decide the final location of the word tag, which is in the lower part of the circle. In addition, it is moved slightly to the left, since its size in the green source is bigger than that in the orange source. For words that are uniquely owned by a source, such as the word word, they are placed far from the circle center and close to the corresponding arc. The stacked bars chart placed under each word explicitly indicate how the corresponding word is shared by all sources.
Sometimes, word clouds are also placed on top of maps to expose text information that is tied to a specific location on a map. Thus, tag maps can be considered to be tag clouds grounded in real geographical spaces.
Fig. 5.8 (Left) Word clouds extracted from four text sources. (Right) The RadCloud representation that merges the four word clouds into one [15]

5.1 “What We Say”: Word 67
 Fig. 5.9 Example of Tag Map [97]. (Left) Tag map and (right) tag cloud of the top 20 business directory searches, centered on South Manhattan
For example, Tag maps [57] are tag clouds overlaid on maps, in which the position of words is based upon real geographical space (Fig. 5.9). For an online map, zoom- ing and panning are two basic and common operations to explore maps at different granularity. In this case, since the tags are constrained and tied to specific geograph- ical locations, the main issue for this type of word clouds is to ensure word legibility during zooming and panning operations. During the interaction, words need to be selected based on their prominences and screen real estate in real time as part of the exploration process.
5.1.2 Frequency Trend
In a typical bag of words, all words and their associated frequency values are fixed. However, it is also easy to imagine that the bag of words can evolve as the data source change over time. During the evolution, new words may appear in the bag while some words may disappear. But, most likely, the frequencies of existing words change over time. Tracking the evolution can sometimes reveal interesting patterns and insights. One good example is Google Ngram Viewer, which is an online search engine that shows frequencies of any set of words or phrases using a yearly count in Google’s text corpora between 1500 and 2008. This is a huge collection, 450 million words from millions of books, for users to freely search. For example, users can easily track and compare the popularities of Albert Einstein, Sherlock Holmes, and Frankenstein (Fig. 5.10). Simple as it is, the tool has been a huge help to socialists and historians in discovering cultural and historical patterns from the graphs of word
68 5 Visualizing Document Content
 Fig. 5.10 Example of Google Ngram Viewer: the yearly counts of Albert Einstein, Sherlock Holmes, and Frankenstein in Google’s text corpora between 1800 and 2000
usages. For more details about Google Ngram Viewer, please refer to the TED talk of Aiden [5].
Essentially, it is a classic multi time series visualization problem, in which a time series represents the frequency change of a word. William Payfair has studied this visualization by inventing multi-line charts in 1786 [43]. Figure 5.10 shows an example of such a visualization, which is so popular that everybody is used to it nowadays. However, there are limitations to applying line charts directly to a bag of words with changing frequency values. First, building connections between words and individual visual elements is less intuitive. Typically, words are either placed close to the corresponding lines (Fig. 5.10) or displayed as a legend next to the chart. However, neither of these ways is as user friendly as word clouds. Second, when there are many words in the bag, the line chart plot will become very cluttered and less appealing/clear. Visualization is about showing data aesthetically and informatively. Therefore, researchers have tried several alternatives to line charts for visualizing word frequency trends. We basically categorize existing attempts into three groups: small multiples, animation, and timeline.
Showing Frequency Trends with Small Multiples
Small multiples, also known as trellis chart, grid chart, panel chart, or lattice chart, are a visualization concept popularized by Edward Tufte [104], who described them as:
Illustrations of postage-stamp size are indexed by category or a label, sequenced over time like the frames of a movie, or ordered by a quantitative variable not used in the single image itself.
Intuitively, small multiples use similar graphs or charts to show different parti- tions of a dataset. To enable efficient comparison, these charts are usually placed side-by-side and share the same measure, scale, size, and shape. If the sequence of charts follows a chronological order like comic books, they can be used to show changes. Therefore, it is easy to extend static word clouds to show frequency trends of individual words. For example, critical time points can be analyzed and extracted
5.1 “What We Say”: Word 69
from the whole evolution. Then word clouds can be generated for all time points and displayed together as small multiples to see the trends (Fig. 5.11).
The advantage of this approach is simplicity. However, the readability of this visualization is rather low. In small multiples, users need to locate the same word in every word cloud to interpret its frequency changes. Thus, we cannot follow popular random-based algorithms, such as Wordle [107], to generate individual word clouds, since these algorithms make the locations of the same word in different word clouds unpredictable. Instead, new algorithms need to be designed to avoid the random- arrangement issue. The basic idea is that if some words are placed together in one layout, they are most likely placed together in another one.
Cui et al. [32] are the first to propose a context-preserving word cloud generation approach to achieve this goal. Figure 5.12 illustrates the pipeline of their algorithm. First, words are extracted from all text content in disregard their appearances on the time line (Fig. 5.12a). One big sparse word cloud is generated from these words accordingly, which largely defines the relative positions between words at all small multiples (Fig. 5.12b). To generate the word cloud for a specific time point, irrelevant words are temporally removed from the big sparse word cloud, which results in an even sparser word cloud (Fig. 5.12c1). Then, a triangle control mesh is constructed to connect all words in the filtered, overlap-free word cloud (Fig. 5.12d1). The mesh is the key to removing empty spaces and keeping relative positions stable. Abstractive forces are exerted on the mesh edges to make sure words move close to one another. During the move process, collision detection is applied to avoid word overlapping. In addition, the mesh are consistently checked for planarity status. As long as the mesh
Fig. 5.11 Example of showing trends with small multiples [32]. (Top center) The curve shows the content fluctuation extracted from a collection of news articles related to Apple Inc. The x-axis encodes the time and the y-axis encodes the significance of the word clouds at individual time points. Five word clouds, a–e, are created using the algorithm presented in [32] for five selected time points where high significance values are observed

70
5
Visualizing Document Content
         (a)
(b)
(c1)
(c2)
Word Filtering
(d1)
(d2)
Force Setup
(e1)
(e2)
Layout Adjustment
                                   Word Extraction
Initial Placement
Fig. 5.12 The pipeline for context-preserving word cloud generation [32]
Fig. 5.13 The pipeline for seam-carving based word cloud generation [115]
  stays planar, we believe the relative position of words are kept (Fig. 5.12e1). When generating the word cloud for another time point, the process is repeated (Fig. 5.12c2– e2). Since the relative positions of words are kept during the space-removal process, the layout stability can be preserved between different word clouds.
As another example, Wu et al. [115] use seam-carving techniques to achieve this goal. Originally, seam-carving [11] is a content-aware image resizing algo- rithm. It first estimates an energy field based on the content of the image, and then repeatedly selects a seam of the lowest energy crossing the image (left-right or top- bottom) to duplicate or remove. In this case of word clouds, the empty space between words is considered to have the lowest energy for removal. Figure5.13 shows the basic process. First, a sparse word cloud is obtained, just like the one obtained in Fig. 5.12c1. The red background indicates the Gaussian importance field based on the word arrangement (Fig. 5.13a). Then, the layout is divided into grids according to the bounding boxes of the words (Fig. 5.13b). An optimal seam (marked in blue) that crosses the layout (left to right in this case) is selected (Fig. 5.13c). To prevent distorting of the layout, the seam is pruned to have a uniform width (yellow seam in Fig. 5.13d). Figure 5.13e shows the word cloud layout after the seam is carved. The compacted word cloud result is obtained when there are no more seams to be carved (Fig. 5.13f).
Showing Frequency Trends with Animation
The second way is animation, which has been used to show changes for many years [91]. When the frequency of a word changes, it is natural to see its size change accordingly as the size is used to encode the frequency of the word. Although animation may work well with individual words, things become complicated when
5.1 “What We Say”: Word 71
animating a word cloud. First, since word clouds are usually compact, directly chang- ing word sizes without moving the words will cause them to overlap. Thus, the key issue in animating a word cloud is resolving the overlap during size animation. One possible solution is using key-frames. We can generate a sequence of overlapping- free word clouds representing several key points on the time line. For example, we may consider Fig. 5.11a–e as keyframes and make an animation by interpolating the word size and locations in them. However, although there is no overlapping in each key-frame, words still may overlap during the interpolation process, which may cause some readability and aesthetic issues. To solve this issue and achieve better animation effect, rigid body dynamics [22] can be used to ensure word clouds are overlap-free during whole animation. Rigid body dynamics do not assume keyframes. Instead, each word is regarded as a rigid body that can collide with others during the size changing process and moves based on two momentums, linear and angular. The dynamics start with an initial configuration, and then a numerical solver is used to track the change of the state of each word during the animation process. To ensure a smooth and pleasing visualization, several constraints are proposed and can be optionally integrated into the dynamics.
• Contacting: used to avoid word overlapping by assigning reaction forces to con- tacting words.
• Boundary: used to specify the legit space to arrange words and help ensure the compactness. This is achieved by viewing the boundary as a fixed wall. All words that touch the wall are totally rebounded.
• Orientation:usedtoensureallwordshavethesameangle.Thisisachievedbyspec- ifying a uniform rotation matrix to all words and ignoring their angular momen- tums.
• Position:usedtofixthelocationofspecificwordsbydirectlysettingtheirpositions and ignoring their linear momentums.
Showing Frequency Trends with a Timeline
In the third category, time is generally encoded as an axis in the display. As a matter of fact, the multi-line representation invented by William Playfair [43] falls into this category. However, many variations have been designed to alleviate the limitations of traditional line charts (Fig. 5.14).
The first alternative is generally called stacked graphs, a type of graph that show time series data by using stacked layers, originally invented by William Playfair [85]. The primary advantage of stacked graphs is allowing users to see individual time series in the context of the aggregation of all data. Only recently have versions that can scale up to large number to time series been created. ThemeRiver [50] is probably the first attempt to enhance the power of stacked graphs to visualize a bag of words/tags that evolves over time. A layer in the ThemeRiver representation represents the frequency trend of a specific term (or “theme”) in a news feed. The novelty of ThemeRiver is two-fold. First, the frequency values at different time points are interpolated to generate a smooth stripe, in contrast to long polygons in a
72 5 Visualizing Document Content
  Fig. 5.14 Example of rigid body dynamics result [22]: a morphable word cloud shows human evolution. The word frequencies changed, and the words are arranged inside a shape that morphs from Australopithecine to Homo sapiens. In particular, the word Human Evolution is fixed during animation using the position constraint
traditional stack graph, so that users can easily track the change. Second, layers are not stacked on the x-axis in ThemeRiver, but rather in a symmetrical shape, which also greatly improves the aesthetics of the visualization. As a follow-up to ThemeRiver, Streamgraph [16] further discusses the aesthetic limitations of stacked graphs, and proposes a new layout algorithm that aims to reduce the layer wiggles (Fig. 5.15).
Compared with traditional multi-line chart, ThemeRiver-like visualizations have better scalability, and allow users to intuitively see a layout with the context of layer aggregation. However, the labeling issue still exists in stack graphs. Since each word is converted into a stripe, the label information is lost in this visual encoding. For a layer that is thick enough, the corresponding word can be placed inside the layers. However, for those layers that do not have enough space, interactions are generally adopted to help users build the connections between words and stripes.
Parallel Tag Cloud (PTC) [26] is another way to visualize the word frequency change directly. In a PTC visualization, the x-axis also represents time. Words that are extracted from the documents at each time slot are arranged vertically. The words are sorted alphabetically with sizes encoding the word frequencies at individual slots. Thus, the same words may not be aligned horizontally, such as the highlighted
Fig. 5.15 Streamgraph visualization of a person’s music listening history in the last.fm service [16]. The x-axis encodes time and each layer represents an artist. The varying width of a layer indicates how many times the person has listened to songs by a given artist over time

5.1 “What We Say”: Word 73
word patent in Fig. 5.16. However, users can interact with the system to explicitly see the word connected to see their trends. For those words that are not currently focused, small lines segments are placed to on both sides of the word to indicate the connections, which is used to avoid extensive visual clutter.
A third popular example is called SparkClouds [67]. In this work, the authors integrate sparklines into word clouds to show the trends for words. A sparkline [105] is generally a line chart that piggybacks on another visual element to indicate the trend in some measurement of the piggybacked item in a simple and highly condensed way. It is typically small and drawn without axes or coordinates (Fig. 5.17). Lee et al. [67] test different alternative arrangements of sparklines and evaluate their performance in six tasks related to trend comparison or interpolation. Their results have shown that this simple technique is intuitive and better than other techniques, such as multi-line charts and stacked bar charts.
Fig. 5.16 Parallel Tag Cloud (PTC) visualization [26]. X-axis represents time. When users hover over a word, its appearances at different time points are linked together to show trends
Fig. 5.17 SparkClouds visualization [67]. The word sizes may either encode the total frequencies over the entire time period or the frequencies at a specific time point. The sparklines beneath words illustrate the overall frequency trends of corresponding words over time

74 5 Visualizing Document Content
Which Method is the Best?
Here we briefly discuss the pros and cons of the aforementioned methods, since none of them is perfect for all scenarios. Showing trends with small multiples is a natural extension of static word clouds. It is storyboard style visualization that is familiar to everyone and easy to understand. However, tracking words between different word clouds is the biggest pain for this type of methods. Although techniques have been proposed to alleviate this issue by ensuring word relative relations stable across different word clouds, users still find it not easy. Another issue is the scalability. To make sure words are legible, each word cloud cannot be too small. Thus, small multiples of word clouds are also limited by screen real estate. For example, in a typical display with a resolution of 1920 by 1080, only five or ten word clouds can be displayed side by side to ensure effective comparisons.
For animation-based approaches, they also inherit the drawback of animation, confusing people in certain scenarios. Previous perception research [86] has sug- gested that people can only track a subset of up to 5 objects simultaneously in order to distinguish a change in a target from that in a distractor. A good animation should ensure a clear and clean story. Thus, we can expect that if all words are changing their sizes and locations chaotically, users can easily get confused and lose focus. Although animations are generally fun and engaging, finding frequency trend or patterns using animation often requires repeatedly playing, pausing, or rewinding the animation. Robertson et al. [91] formally compare the performances of animated and static visu- alizations for trend tracking tasks. Their results show that the core value of animation is for presentation. However, users still slightly prefer using static visualizations for analysis tasks.
The third category is the most popular and commonly seen in many visual ana- lytics systems [30, 38, 74], which are not limited to visualizing time-varying word clouds, but time series data in general. First, it uses statistic representation, which avoids the drawback of animation. Second, it provides strong visual cues to help users track changes, avoiding the drawback of small multiples. However, since one dimen- sion (x-axis) is used to encode time, only one dimension can be exploit to encode additional information. For example, words can no longer be arranged free in a 2D space, like what typical word clouds do. In certain scenarios [32, 115], where spa- tial relationships between words have meanings such as semantic distance between words, timeline-based approaches are certainly less expressive then the other two.
5.2 “How We Say”: Structure
The word “text” originates from the famous statement in Quintilian’s book on speeches:
“after you have chosen your words, they must be weaved together into a fine and delicate fabric,” – with the Latin for fabric being textum.
5.2 “How We Say”: Structure 75
Thus, a text is always expected to have a well-defined structure. To deliver sophis- ticated messages, different words must be arranged appropriately and correspond to one another in a manner conditioned by logic and grammar.
There are various ways a word or syntactic unit can correspond to another. For example, one of the most studied relationships is known as co-occurrence, which may refer to words appearing in the same document, in the same sentence, following the same word, etc. According to the definition of co-occurrence, the distance and the order between words are irrelevant. Considering the aforementioned bag-of-words model, if all words are extracted from a document, the corresponding word cloud will be a visualization for document-wise co-occurring relationships. However, for more sophisticated relationships or structures, such as concatenation and grammar relationships, word clouds would certainly be incapable.
In this section, we formally review and summarize existing work that visualize different types of structure in text.
5.2.1 Co-occurrence Relationships
Co-occurrence is the most common relationship studied in text mining and nat- ural language processing applications. They are linguistic assumptions behind co- occurrence: if two words co-occur, they are semantically related, neglecting the distance and order between one another. This assumption makes using the bag-of- words model to represent the content in a document reasonable. Document-wise co-occurrence has been widely used in many applications, such as content analysis, text mining, construction of thesauruses and ontologies [76].
Although document-wise co-recurrence is the most adopted in existing literature, co-occurrence can be defined at other granularities. For example, content sometimes may dramatically change within a document. Thus, we may also investigate co- occurrence in smaller units, such as paragraphs or sentences. Besides defining co- occurrence with such linguistic units, we can consider the words in a fixed-width window surrounding the target word. Since the window width is a parameter, different values may apply. In general practice, 5, 20, and 100–200 word windows approximate phrase, sentence, and paragraph co-occurrence, respectively. In certain scenarios, co-occurrence is even considered within a syntactic structure. This grammar-aware co-occurrence can help cluster terms more delicately, such as finding all nouns that follow the verb “drink”: water, milk, bear, etc.
Statistics of co-occurrence may be represented in two ways. For example, let us compute the co-occurrence number of the word to and be in the context of the sentence “to be or not to be”. The simplest way to check if the sentence contains both words in disregard of their appearance number. In this case, the co-occurrence value Cto·be = Cbe·to = 1. Alternatively, we may also consider the two to’s and two be’s aredifferent,addingtheirco-occurrencetogether,whichgiveusCto·be =Cbe·to =4. In the later case, people imply that the contexts that contain repeated target words are different from the contexts that do not. Given a set of contexts, e.g., all sentences in a
76 5 Visualizing Document Content
 Fig. 5.18 Example of a co-occurrence between words
documents, a co-occurrence matrix can be then obtained by aggregating all pair-wise co-occurrence values together (Fig. 5.18).
Co-occurrence matrices tell people whether two specific words co-occur or how strongly they co-occur. More importantly, they help people find correlations between word pairs and/or similarities of meaning among/within word patterns. The underly- ing assumption of statistical semantics was established by Harris [49] and popularized by Firth [41]:
A word is characterized by the company it keeps.
To be more specific, two words that tend to have similar co-occurrence patterns, tend to be positioned closer together in semantic space and resemble each other in mean- ing. Mathematically, each column or row in a co-occurrence matrix is considered the feature vector defined by the co-occurrence pattern between the corresponding word and all other words. Therefore, if two words “drink” and “eat” exist in the same context, we would not be surprised to see them have similar vector values since they resemble each other in meaning.
As mentioned before, word clouds can be used to visualize co-occurrence relation- ships. For example, a word cloud consisting of words from a document can be con- sidered a document-wise co-occurrence visualization. In particular, Vuillemot [108] proposes a system, POSvis, to help users search specific words and explore the surrounding words (Fig. 5.19).
However, it is also natural to use graphs or links to explicitly describe co- occurrence relationships. For example, users may have one or more focused key- words and show the other terms that co-occur with the focused keywords explicitly (Fig. 5.20(left)). When users have no particular focused keywords, it is also helpful to show a general graph that depicts strongly co-occurred word pairs extracted from content (Fig. 5.20(right)).
5.2 “How We Say”: Structure 77
  Fig. 5.19 POSvis visualizing Gertrude Stein’s The Making of Americans [108]. The focused word Martha is selected by users. The co-occurred verbs and nouns are displayed in the centered main window as a word cloud
Fig. 5.20 Examples of using links to represent co-occurrence relationships. (left) Terms that co- occur with either “martha” or “eddy” are connecting to these two words explicitly with links [108]. (right) Keyword pairs that have co-occurrences passing a given threshold are connected together as a general graph [100]

78 5 Visualizing Document Content
5.2.2 Concordance Relationships
While co-occurrence relationships are mainly exploited to infer the similarity between different words, concatenation concerns more about the ways in which the words are connected within a consecutive sequence. Formally, Sinclair [96] describes this relationship as concordance in 1991:
A concordance is a collection of the occurrences of a word-form, each in its own textual environment. In its simplest form it is an index. Each word-form is indexed and a reference is given to the place of occurrence in a text.
Originally, a concordance is a manually prepared list of words found in a text along with references to their locations in the text, which is a tedious work requiring a lot of human effort to build such an index. For example, as the first recorded concordance in history, Correctio Biblie was built by Hugh of St-Cher with the help of hundreds of Dominican monks in Paris in the year 1230’.
To demonstrate the concept of concordance, Sinclair [96] gives an example of one-word context concordance of a short text: “The cat sat on the mat.” (Fig. 5.21). All word occurrences are placed in the middle column of the table. The previous and next words (if existing) are placed in the first and third columns, respectively.
The concordance relationships are originally recorded to provide scholars with the information about where words were used in a closed set of text and a source of insight and illumination for a wider readership. With the capability of generat- ing concordances in the blink of an eye, computers can support a wider range of applications than its paper counterpart.
To begin with, it can help with better author profiling. Idiolect already shows that linguistic variation not only exist at the word level, but also at the phrase level or sentence level. For example, some may say “Use your feelings, Obi-Wan, and find him you will,” while the others prefer saying “Obi-Wan, use your feelings, and you will find him”. Thus, concordances can preserve more clues/information for classification models to better identifying authorships. In this case, the bag-of-words model will certainly fail, since the data are identical.
Another major application of concordance relationships is word prediction, which can be used in any tasks where we have to identify words in noisy/ambiguous input, such as speech recognition, handwriting recognition, spelling correction, or machine translation. The problem is generally considered as: in a document collection, given some preceding words, what is the most likely word to follow? The likelihood is
Fig. 5.21 One-word context of all words extracted from “The cat sat on the mat.” [96]

5.2 “How We Say”: Structure 79
generally captured with simple statistical method based on Markov assumption: the probability of a word depends only on the probability of a limited history.
The keyword in context (KWIC) visualization (Fig. 5.22) is the first designed and most commonly used tool for concordance analysis. The visualization consists of a list of text fragments with the searched keyword aligned vertically in the view. Thus, users can easily compare or inspect linguistic properties of the words before or after the keyword, which is usually highlighted with a different color.
However, the traditional KWIC visualization generally shows concordances in great detail, and is not intuitive for statistical information, such as variation and repetition in context. Recently, Word Tree [112] and Double Tree [33] have been proposed to address this issue. Word Tree visualizes all contexts of a keyword using a tree layout. Considering the occurrences of adjacent words, Word Tree expands tree branches to encode the recurring phrase patterns. Compared with Word Tree that only shows one side of context, Double Tree shows both sides (Fig. 5.23).
Unlike traditional KWIC-like visualizations that focus on the surrounding words of a specific words, Phrase Nets [106] focuses on visualizing specific concordance relationships, such as is- and of-relationships. Specifically, Phrase Nets builds a graph, in which nodes represent words and edges represent a user specific concor- dance relationships (Fig. 5.24).
5.2.3 Grammar Structure
Text, although seeming linearly developed, consists of a complicated network con- structed by grammar. Since concordance related models only concern the ways in which words are connected into a sequence, they are often criticized for lacking
Fig. 5.22 KWIC visualization for the word rabbit in Alice in Wonderland

80 5 Visualizing Document Content
  Fig. 5.23
of love the
grammatical dependency information. Thus, in this section, let us move one step further into complicated relationships and consider grammar structure in text.
So, what is grammar? The term grammar is derived from the Greek word gram- matike ̄, where gram means something written, and tike ̄ derives from techne ̄ and meant art. Hence, grammar is the art of writing, which has a long history tracing back to ancient Greece and Rome. After thousands of years and going through several stages of development, such as prescriptive grammar, non-structural descriptive gram- mar, structural descriptive grammar, and generative grammar, the term nowadays is
Word Tree visualization [112] of the King James Bible showing the contexts to the right
Phrase Nets visualization [106] of X begat Y relationships in Bible, revealing a network of family structures
  Fig. 5.24
5.2 “How We Say”: Structure 81
generally defined as the set of structural rules governing the composition of clauses, phrases, and words in any given natural languages.
Although various grammar frameworks have been proposed, there are usually two ways to describe sentence structure in natural languages, constituency and depen- dency. Although proven to be strongly equivalent [45], they focus on different aspects of grammar.
Traced back to the ancient Stoics [77], constituency grammar emphasizes part- whole relationships between words and greater units, and now becomes the basis of grammar model in computer science. The basic structure for constituency grammar analysis is subject-predicate. A typical sentence is divided into two phrases: sub- ject, i.e., noun phrase, and predicate, i.e., verb phrase. The phrases are then further subdivided into more one-to-one or one-to-more correspondence. Thus constituency relationships can be visualized as a tree structure (Fig. 5.25(left)). Every leaf in the tree corresponds to a word in the sentence. And a parent node is a bigger or more complex syntactic unit by combining its child nodes. Therefore, for each word in the sentence, there are one or more nodes containing it.
Dependency grammar [51, 94], on the other hand, is a relatively new class of mod- ern linguistic theories that focuses on explaining any agreements, case assignments, or semantic relationships between words. Compared with constituency grammar, it emphasizes head-dependent relationships between individual words, and there are no phrases or clauses in the structure. All units in the dependency grammars are words. Each relationship involves exactly two words, i.e., a head and a dependent. In general, the head determines the behavior of the pair, while the dependent is the modifier or complement. Thus, the dependency relationships between words are usu- ally visualized as curved arrows placed on top of the sentence (Fig. 5.25(top right)). Each arrow points from a head to its dependent. However, the hierarchy of the depen- dency relationships is unclear in the arc visualizations. Thus, trees (Fig. 5.25(bottom
Fig. 5.25 Examples of grammar visualizations [27]: (left) a tree visualization of constituency grammar structures, (top right) a arrow visualization of dependency grammar structures, and (bottom right) a tree visualization of dependency grammar structures

82 5 Visualizing Document Content
right)) are also often used to help researchers see the dependency relationships and the hierarchy simultaneously.
5.2.4 Repetition Relationships
Repetition is another interesting and common pattern in content, which serves dif- ferent purposes and inspires further investigation into interpretations of the text. In a narrative or speech, for example, authors may use repetition to emphasize themes or ideas worth paying close attention to. Repetition may also occur between two or multiple texts. One typical example is plagiarism, which has gain much attention recently due to the availability of large, online document archives. On the other hand, the repetition can also appear in different ways. In many cases, the repetition may be exact or near exact by using the same words or phrases, such as direct quotes. In other cases, the recurrences may occur at a more subtle level, such as themes, motifs, and metaphors.
The most obvious and common repeated elements are words and phrases. Read- ers can easily spot repeated words in a document simply by reading it. However, recurrences of function words like “the” or “is” are trivial and meaningless. For effective understanding of text content, choosing the correct words and analyzing their repetition patterns in the content can help users generate high-level interpreta- tions. A good visualization for repetition patterns should simultaneously show the total number of occurrences and the gaps between occurrences. For example, arc diagrams [111] are proposed to represent complex repetition patterns in string data (Fig. 5.26). Intuitively, identical subsequences in the whole sequences are connected by thick semicircular arcs. The thickness of the arcs is equal to the length of the repeated unit. The height of the arc is thus proportional to the distances between the two units. Since only consecutive pairs of identical subsequences are connected with arcs, users can easily follow the path of arcs and understand the distribution of a specific repeated subsequence.
Fig. 5.26 Examples of arc diagrams [111]: (left) two arcs connect the three occurrences of a subsequence and (right) the arc diagram visualization of Beethoven’s Für Elise

5.2 “How We Say”: Structure 83
As another example, FeatureLens [37] focuses on frequent item sets of n-grams in a text collection. Figure 5.27 shows a screenshot of FeatureLens visualizing The Making of Americans. The left panel shows a list of precomputed n-grams for users to explore. Users can select one or more from the list (three in this screenshot), the views in the middle present the occurrences of these n-grams in individual speeches. A rectangle represents a speech, and the height of the rectangle encodes the number of paragraphs in the corresponding section. Each colored line in a rectangle indicates that the paragraph at the corresponding location contains the word. Darker lines suggest more occurrences of the corresponding words. Thus, users can compare the occurrences patterns of different features of interest. For example, users can easily identify the paragraphs where selected features occur together.
However, in many scenarios, the recurrences may not always happen at the copy- paste level. Similar ideas or concepts can be evoked or repeated using different words, which is impossible to capture by finding repeated words or phrases. Thus, a higher level of abstraction is required to capture conceptual similarity. To quantitatively capture the similarity between two fragments of text, many conceptual similarity algorithms have been proposed, such as Leximancer [98], Latent Semantic Analy- sis [64], and Latent Dirichlet Allocation [14]. For example, Picapica [1], a text reuse search engine, defines at least seven types of plagiarism, from complete plagiarism to translation plagiarism. Based on the search engine, Riehmann et al. [88] build a visual analysis tool to help users effectively assess and verify alleged cases of plagiarism.
Fig. 5.27 FeatureLens visualization [37] showing three n-grams: in the house, governess, and redfern

84 5 Visualizing Document Content
 Fig. 5.28 Screenshot of the visual analysis tool built by Riehmann et al. [88]
Figure 5.28 shows a screenshot of their system. An overview of the distribution of alleged spots in the document is displayed in the left panel, along with their lengths, types, and relationships with the source documents. In the right view, glyphs are used to emphasize the relationships between the alleged fragments and the source text, which aims to help users quickly form a decision without looking at the actual text.
5.3 “What Can Be Inferred”: Substance
In the two previous sections, we focus on dissecting the surface of text, which is essentially a set of expressions made of words. However, expressions are meant to deliver information. Some delivered information is explicit, while some other part of the information is implicit. Understanding the information is essential for many analysis tasks. Thus, in this section, we move beyond the surface of text and dig deeper into the semantic aspect of text.
5.3.1 Fingerprint
Linguists believe that every person has their own distinct way to use language, namely idiolect, which can be understood through distinctive choices in text. Thus, a cat- egory of methods called linguistic fingerprinting is developed to capture linguistic
5.3 “What Can Be Inferred”: Substance 85
impressions from a text, just like a signature, to represent the text and identify the authors.
Linguistic fingerprinting is originally used for authorship. Since Jan Svartvik published The Evans Statements: A Case For Forensic Linguistics [102] in 1968, lawyers and courts have been repeatedly demonstrated its success in cases of disputed authorship of suicide notes, anonymous letters, confession statements, etc.
Recently, the rapid growth of the Internet has made it the largest public repository of anonymous information, such as emails [35] and forum messages [2, 69], and provided an unprecedented opportunity and challenge to authorship analysis. Various techniques have been proposed, which generally focus on two aspects, namely, style feature extraction and classification techniques.
Zheng et al. [123] summarize four categories of writing style features that can be extracted from a text for authorship identification, namely, lexical, syntactic, structural, and content-specific features. For example, lexical features include word counts, words per sentence, paragraph lengths, vocabulary richness, word distribu- tion, etc. Syntax and structural features refer to the patterns or rules used to construct sentences and paragraphs, such as punctuations, stop words, and the use of greetings. Content-specific features are defined as keywords that are important for specific con- texts. All these features are captured at the surface level of text, which is discussed in previous sections. However, the meaning the content is not longer important to linguistic fingerprinting. Instead, combined with machine learning techniques, such as support vector machines, neural networks, and decision trees, these fea- tures have shown great potential and accuracy in determining authorships of content [2, 35, 69].
One common concept in these approaches is document fingerprint, which aims to generate a compact, comparable description for each document. This concept is first proposed by Manber [75]. In this work, a fingerprint is defined as a collection of inte- gers that encode some key content in a document. Each integer, also called minutia, is generated by selecting a representative subsequence from the text and applying a hashing-like function to it. All the minutiae are combined together and indexed to support quick query and identification of similar documents. The fingerprint concept is then extended and evolved in many approaches [52, 95], but there are two basic considerations that may greatly influence the accuracy of these approaches: granu- larity and resolution [54]. The size of text to generate a minutia is considered the fingerprint granularity, while the number of minutiae is considered the resolution.
Researchers have devoted great effort to creating visualization tools [3, 59, 82] to facilitate fingerprint analysis. In these approaches, style features, such as word frequency, average sentence length, and number of verbs, are extracted and visually integrated together to provide users with a high-level signature for the document. For example, Keim and Oelke [59] introduce literature fingerprinting, a pixel-based representation of text features. In this visualization, the document is segmented into blocks of different granularities, such as sections and chapters, and features are computed for each block, encoded as a colored pixel. The pixels are packed and arranged based on the document blocks to collectively provide a compact and scalable visualization of the document (Fig. 5.29). Their technique is then further extended to
86 5 Visualizing Document Content
 Fig. 5.29 Literature fingerprinting visualizations [59] of Jack London’s The Iron Heel and Children of the Frost. The color is used to indicate the vocabulary richness
help users analyze why particular paragraphs and sentences are difficult to read and understand [83].
Literature fingerprinting allows users to define different types of features at differ- ent levels, such as average word length, average number sentence length, and vocab- ulary richness. The computed feature values are shown in a heatmap (Fig. 5.29), in which each pixel represents a text block and its color indicates the values of one of the selected features whose range is represented by a legend at the side of the visu- alization. Although very simple, the compact visualization is deliberately designed as an image or a signature that helps users quickly compare to find (dis)similarities between multiple text content. Following the same design, Oelke et al. [82] introduce a fingerprint matrix to help understand a dynamic network extracted from the text.
Following a similar idea, Jankowska et al. [58] introduce the document signature which summarizes a document based on character n-grams. The character n-grams are consecutive sequences of n characters extracted from the given text. For example, we can extract four distinctive character 5-grams from the text “the book”: “the b”, “he bo”, “e boo”, and “ book”. Character n-grams have been widely used to han- dle text classification problems, such as language recognition [20], and authorship analysis [56]. In this work, Jankowska et al. [58] analyze and visualize the difference of n-gram patterns between two or multiple documents. For a pair of documents, namely a base document and a target document, a rectangle visualization, called a relative signature, is constructed to represent the difference between the most com- mon n-grams between these documents. The n-grams are represented by horizontal stripes. The stripe colors indicate the difference of frequency patterns of the corre- sponding n-gram between these two documents. Intuitively, the white stripe means that the n-gram has similar or identical frequency patterns between these two docu- ment. The blue stripes at the top of each rectangle indicate that these n-grams only appear in the target document. The red-yellow scale indicate the frequency of the corresponding n-gram appear more frequently in the base document then in the target document (Fig. 5.30).
From the above examples, we can see that one of the major applications of the above fingerprint-based technique is to help with document comparison. Besides

5.3 “What Can Be Inferred”: Substance 87
 Fig. 5.30 The relative signatures of nine books with L. Carroll’s Alice’s Adventures in Wonderland as the base document [58]. It is clear that the signature of Carroll’s Through the Looking Glass is much lighter and shorter than those of the other books, which indicates that the book is much more similar to the base book than the other books are. In addition, users can select an n-gram (“OULD” in this case) to compare its frequencies in different book
fingerprinting, several other techniques are also developed for the similar purpose. For example, Rohrer et al. [92] introduce a glyph design to summarize the text data by illustrating the features via shapes, which was used to study Shakespeare’s works. Many visual analysis systems are also developed for plagiarism detection [87, 88] or authorship identification [3], or showing the change of language [53].
5.3.2 Topics
With the unprecedented growth of text data accessible on the Internet, people now more and more likely to run into tasks to digest of a large collection of documents. In this scenario, individual documents are no longer important. Instead, people need to step back from individual documents, construct an overview of the content in the corpus, and look at the larger pattern among all documents, which traditionally can only be obtained by going through a tedious reading and note-taking process.
To assist effective understanding of document collections, one intuitive way is to perform clustering on the documents [4, 17, 42, 72, 81, 118, 119]. Clustering of documents can automatically organize documents in a meaningful structure (either hierarchical or partitioning) based on their content to help users build a coherent con- ceptual overview of a document corpus [84]. On one hand, clustering has been widely studied as a fundamental aspect of machine learning, and many general clustering techniques, such as k-means, have been applied to document clustering. On the other hand, more specific clustering techniques have been proposed taking advantage of
88 5 Visualizing Document Content
the characteristics of documents. Examples include latent semantic indexing [36], non-negative matrix factorization [119], and concept factorization [17]. These meth- ods generally transform document features from the noisy and sparse term space to a concise latent space, which in turn, to help improve the clustering quality. Highly related to document clustering, topic modeling techniques have been developed to help people discover and summarize categories and patterns. In this section, we focus on the topic modeling and visualization techniques.
So what exactly does the term topic mean in the context of topic modeling? Intu- itively, a topic is a short description of a group of semantically-related documents. In the domain of topic modeling, a topic is often represented by a set of words that fre- quently appear in documents but less frequently in other documents. The idea is also based on the distributional hypothesis of words. For example, “Jordan” and “basket- ball” may appear more often in documents about Michael Jordan, while “Jackson” and “music” may appear more often in articles about Michael Jackson. However, “Michael” may have no significant difference between both document sets. Thus, “Michael” is not an appropriate distinguishing topic term for either of the document sets. After years of development, a topic is now formally defined a probability dis- tribution of words in a vocabulary, which aims to indicate the probability of each individual word appearing in a document on a specific topic [99].
Topic modeling is an umbrella term for machine learning techniques used to find topics in textural data. Its history can be traced back to the early 90s, when Deerwester et al. [36] proposed latent semantic analysis for clustering semantically-related words for natural language processing research. However, it is the Latent Dirichlet Allo- cation (LDA) model, proposed by Blei et al. [14] in 2003, that popularizes topic modeling in practice. The basic intuition of LDA is that a document usually cannot be neatly categorized into one single topic, but can relate to a set of topics. In other words, a document is a mixture of several topics with different weights.
The goal of LDA and many related topic modeling techniques is to identify these topics and quantify the proportions of these topics for every document. To achieve this goal, these methods consider this problem the other way around and assume a generative model for a text corpus. To begin with, we assume all the topics in the text corpus are known and the corpus is empty. To generate a document in the corpus, we first randomly choose a distribution over topics. Then, we repeatedly choose a topic based on the topic distribution. From each picked topic, we randomly pick a word based on the corresponding word distribution.
However, the reality is the opposite. We have the text corpus ready but unknown topics to be decided. Therefore, what a LDA-like topic modeling method tries to do is to estimate the parameters that make the model have a high likelihood to generate a corpus similar to the given corpus via the aforementioned generative process. The process of estimation is the key and differs various topic modeling approaches. For example, the original LDA model [14] is based on a process called variational inference. Gibbs sampling [47] is also very popular technique to discover hidden topics in a large text corpus. An excellent survey about probabilistic topic models can be found at [34].
5.3 “What Can Be Inferred”: Substance 89
The output of a typical topic modeling method is usually complicated. First, a set of topics is estimated from the corpus, and each topic consists of ranked probabilities of words. Similarly, each document is also summarized as the ranked probabilities of topics. Then, the topic modeling method can further cluster documents based on these distributions to identify representative documents for each topic. Although the text corpus is summarized to a certain extent, the complicated output is still challenging to evaluate and interpret. Thus, various topic visualization techniques have been developed specifically to help users with this task.
To start with, Chuang et al. [25] propose a matrix-style visualization called Termite to help users organize and evaluate term distributions associated with topics generated by LDA model (Fig. 5.31). In this work, the authors define a saliency measure and a seriation method to arrange the terms in the matrix to emphasize clustering structures in the dataset.
Many other visualizations are built upon projection techniques [18, 19, 24, 46, 68, 70]. For example, TopicNets [46] generates a document-topic graph by using an LDA model [10]. Then, the topics are represented as nodes that are projected onto a 2D surface via a multidimensional scaling technique [28]. Once the topic nodes are fixed, a force-directed layout algorithm [44] is applied to the document-topic graph and fix the positions for each document node (Fig. 5.32).
UTOPIAN [24] is another visualization that highly integrates with a Nonnegative Matrix Factorization (NMF) topic model. It also uses a projection-based visualiza- tion to help users explore the topic space. In many cases, the results generated by a topic model are not perfect. Since NMF is a deterministic model that can generate
Fig. 5.31 Termite visualization [25] of a LDA result. The term-topic matrix is displayed to the left. When a topic is selected in the matrix (Topic 17 in this case), the related terms are highlighted and their frequency distribution is displayed in the middle. In addition, the most representative documents are listed to the right

90 5 Visualizing Document Content
 Fig. 5.32 TopicNets visualization [46] of the document-topic graph generated from CS-related NFS grants that are awarded to University of California campuses. The topic nodes are labeled with the top two frequent terms and colored based their fields. The document nodes are displayed as small circles filled with the corresponding colors
a consistent result from multiple runs, UTOPIAN allows users to progressively edit and refine the topic results by interactively changing the algorithm settings. In the visualization, UTOPIAN uses colored dots to represent documents in different top- ics, exploiting a technique called t-distributed stochastic neighborhood embedding (t-SNE) to generate the 2D layout to reflects the pairwise similarities between doc- uments (Fig. 5.33).
5.3.3 Topic Evolutions
Many text corpus have time stamps associated to individual document in it, such as news articles, forum posts, email achieves, and research papers. For these text corpora, the temporal patterns of the topics are the key in many analysis tasks. Thus, related mining and visualization techniques have been proposed specifically for this goal.
One simple solution is to first ignore the time stamps of documents and generate topics as usual with the aforementioned topic modeling techniques [40, 113]. Once the topics are generated, the documents that related to each topic can be split based on their time stamps to help users to understand how the topics are distributed on the time dimension. However, for streaming data, it is impossible for a topic model
5.3 “What Can Be Inferred”: Substance 91
 Fig. 5.33 UTOPIAN visualization [24] of the topics in the collection of InfoVis papers (1995– 2010) and VAST (2006–2010). The topic result can be refined via a rich set of interactions, such as topic merging/splitting and topic inducing
to know all data in advance. Thus, the data are generally processed in batches. For example, evolutionary clustering algorithms [21, 23, 29, 31, 80, 117] aim to find a clustering result for the new batch of the streaming data, such that the result is coherent with those of the arrived batches and appropriately reflect the structure in the new batch. Topic detection and tracking (TDT) [6, 121] is designed to detect new topics or find documents that belong to already detected topics from a document stream. The LDA model is also extended to handle streaming data [13, 109, 110].
Similar to the bag-of-words model, the attribute of time also dramatically changes the visualizations of topics. Most of current dynamic topic visualizations are based on stacked graphs [29, 31, 101, 113, 116]. For example, TIARA [113] first applies LDA model to a text corpus to generate topics, and then uses a probability threshold to assign one or more topics to each document in it. In the TIARA visualization, each topic is visualized as a layer in a stacked graph. The x-axis encodes time, and the thickness of a layer at a time point is proportional to the number of documents that belong to the topic and have the corresponding time stamps (Fig. 5.34).
Cui et al. [29] further design TextFlow, a river-based visualization, to convey relationships between evolving topics (Fig. 5.35). In contrast to stacked graphs that show individual stripes evolving independently, TextFlow allows stripes to split or merge during evolution, which can intuitively deliver the complex splitting/merging patterns between topics. In addition, relationships between words are also visualized using a thread-weaving metaphor. Users can selectively show words as threads over- laid on top of the stripes. If two or more words co-occur during a specific time span in the same topic, the corresponding threads also intertwine to generate a weaving pattern in the visualization to convey such information.
The expressive visualization cannot be achieved without proper support of the topic modeling and visualization techniques. On the backend of TextFlow, an
92 5 Visualizing Document Content
  Fig. 5.34 TIARA visualization [113] of over 23,000 clinical records (2002–2003). Each layer represents a topic discovered by the LDA model. The overlaid labels represents the top key words in the documents of the corresponding time and topic
Fig. 5.35 TextFlow visualization [29] of selected topics in InfoVis papers (2001–2010). Three main topics are visualized as three stripes that have splitting/merging relationships. Several key- words are visualized as threads that weave together to convey their co-occurrence relationships
incremental hierarchical Dirichlet process model [103] is adopted to learn the split- ting/merging patterns between topics. In addition, a set of critical events and key- words are ranked and indexed to represent the content at different time points. Finally, a three-level directed acyclic graph layout algorithm is design to integrate and present the mining results in a coherent and visually pleasing visualization.
Similar to TextFlow, RoseRiver [31] follows the same river metaphor to depict the splitting/merging patterns between evolving topics. However, in this work, the authors focus on the scalability issue of datasets. In contrast to TextFlow that builds flat topic structures, RoseRiver builds a hierarchical topic tree for each time point and makes sure the topic trees are coherent at neighboring time points. This approach has two advantages over the TextFlow implementation. First, different users may have different interests or focuses for the same corpus. RoseRiver can provide a “one

5.3 “What Can Be Inferred”: Substance 93
 Fig. 5.36 RoseRiver visualization [31] of news topics related to the Prism scandal (June 5–Aug 16, 2013). Four major topics are highlighted with red, purple, green, and blue, respectively. If users are interested in one topic at a specific time point (marked with D), they can split and examine smaller topics that contained by it (see c)
shoe fits all” solution to them. For topics that are not of interest, users can choose a coarse granularity and hide details to avoid distractions. In contrast to RoseRiver, TextFlow may need to generate different flat topic structures to meet specific needs of different users. Second, since the visualization is supported by topic hierarchies, users can freely refine and alter the topic granularities during the exploration process by interacting with the system (Fig. 5.36).
5.3.4 Event
Cognition studies [60, 120] have shown that people are often used to perceive and make sense of a continuously observed activity by segmenting it into a sequence of related discrete events. However, the definition of event is controversial. For example, the Merriam-Webster dictionary generally defines it as “something that happens,” “a noteworthy happening,” or “a social occasion or activity”. Zacks and Tversky [120] define it as “a segment of time at a given location that is perceived by an observer to have a beginning and an end.” Kim [60] considers an event a structure consisting of three elements: object, a property, and time (or a temporal interval). In the topic detection and tracking field, an event is something that has a specific topic, time, and location. Although expressed differently, these definitions all similarly reflect people’s intuition about events, but certainly do not exhaust the common conception of events. Ironically, many researchers also admit that none of these features are essential for an event.
In this section, we do not intend to contribute to this debate about the accurate definition of event. Instead, we look at events from the perspective of text analysis, and try to understand how people use events to efficiently make sense of a dynamic
94 5 Visualizing Document Content
text corpus. When following a narrative or an evolving topic, readers often encounter plots or occurrences that impact or change the course of development. These plots or occurrences, which may present themselves at multiple levels of granularity [63], are generally critical to helping readers comprehend the logic and reasoning behind the development. Thus, they are often considered the primary type of events to detect and analyze in many text visualization approaches.
As one of the first attempts, topic detection and tracking [7, 8] was a DARPA- sponsored initiative investigating techniques to find and follow new events in news streams back in 1996. The basic idea is straightforward. Their algorithm first extracts features from a newly arrived article to form a query. Then, the query is compared against earlier processed queries. If no earlier queries are triggered by exceeding a certain threshold, the new article is marked as containing a new event. Otherwise, the new article is marked as containing an earlier event. There are many follow-up methods based on the same idea. An excellent survey on topic detection and tracking approaches can be found in [6]. For example, Mei and Zhai [78] use a language model to extract and analyze the temporal structure of events. Luo et al. [73] assume that news articles are generated based on real-word events. In the assumption, once a worthy real-world event happens, news agencies will generate a set of articles that have closely-related contents in a continuous time period while the event draws continuous attention. Thus, they believe that a big temporal-locality cluster of news articles that have highly related contents may indicate and represent a real-world event that motivate the documents in it. The authors propose a two-stage temporal- locality clustering algorithm to extract these events, and further cluster events into groups to represent long-term stories.
In many other approaches, researchers will first apply topic models to organize a text corpus into topics that perpendicular to the time dimension, then segment each topic into a sequence of events to extract critical changes in the topic. For example, Zhao and Mitra [122] first extract a hierarchy of topics from the content of social text streams, such as emails. Then, given the sequence of emails within a topic, they also construct a hierarchy of temporal segments to represent events at different levels of granularity. In addition, the author information of these emails are also exploited to help users group and explore these events with the context of social communities. Dou et al. [39] define an event in a text stream as “an occurrence causing change in the volume of text data that discusses the associated topic at a specific time”. Based on this definition, the authors apply the accumulative sum control chart [66] to locate volume bursts, which are treated as events, in individual topic streams. Intuitive, the algorithm tracks a accumulative sum of volume at each timespan. Once the sum exceeds the predefined threshold, the corresponding timespan will be considered as an event. In each event, name entities including people and location are also extracted to help users explore relationships between events.
Once the events are successfully extracted from text streams, their visualizations are rather unanimous in different approaches. For example, EventRiver [73] visualize an event (a temporal-local cluster of articles) as a bubble in a river of time (Fig. 5.37). The varying width and the size of the bubble indicate the attention it draws over time
5.3 “What Can Be Inferred”: Substance 95
  Fig. 5.37 EvenRiver visualization [73] of CNN news (29,211 articles from Aug 1 to Aug 24, 2006)
and in total, respectively. A novel layout algorithm is also used to pack event bubbles of similar contents together to form a long-term story.
CloudLines [62] defines events as “time-stamped data points that arrive in data streams at non-equidistant time intervals.” For example, the authors apply this con- cept to news streams and consider a news article mentioning a specific person as an event. Then, they group events by person and visualize each group as a ribbon on top of the time-axis (Fig. 5.38). The density and width of a ribbon can help users quickly identify important event episodes for further detailed analysis.
Similar to CloudLine, LeadLine [39] first extracts topics from a dynamic text corpus and visualizes them as ribbons with width encoding the topic strength. Based on this familiar visual metaphor, the authors use the accumulative sum control chart
Fig. 5.38 CloudLines visualization [62] of events related to 16 politicians who appeared in news articles in Feb 2011

96 5 Visualizing Document Content
 Fig. 5.39 CloudLines visualization [62] of events related to 16 politicians who appeared in news articles in Feb 2011
to locate time spans in each topic, and highlight them with colors and contour lines as critical events in the corresponding topic. In addition to the detected events, the authors also extract name entities of people and location from the events, and visualize them in two juxtaposed views to help users make sense of these events (Fig. 5.39).
5.4 Summary of the Chapter
In this chapter, we focus on techniques that are widely used for analyzing text content. From simple and concrete to complicate and abstract, we progressively review the basic content analysis techniques in three steps.
The first step is words, the fundamental building element of text. We focus on the bag-of-words model and related visualization techniques in this step. Although the basic idea is simple, the bag-of-words model has been proven effective and successful in many application domains. Due to its simplicity, the related visualizations, such as Wordle [107], have also become popular and been accepted by more and more people on the Internet.
The second step is structures. In the bag-of-words model, words are treated equally and independently. However, text content is much more than the meanings of individ- ual words combined. In this second step, we take a step further to explore relationships between words, and look into the methods that people use to understand how words are constructed to build text content. There are various ways we can examine text structures. For example, grammar structure is obvious one way to understand word relationships. However, grammar structures are mainly exploited to help computers make sense of texts, and not very helpful when shown in visualizations. On the other hand, other statistical relationships are more helpful, such as word co-occurrences, word concordances, and text reuses. The statistics of these relationships can help us not only to see what words are used, but also understand how they are used. In
5.4 Summary of the Chapter 97
addition, more contexts of individual words are provided and help us make sense of contents more easily.
The third step is substance. In this step, we leave the surface of text behind and dig deeper into the information that can be inferred from texts. First, we look at the linguistic fingerprinting. In fingerprinting analysis, the actual text content is no longer important. Instead, researchers focus on developing techniques to capture linguistic impressions from text expressions, just like signatures, to represent the text, differentiate linguistic styles, and identify the authors. We then introduce techniques that aim to provide high-level summarizations of large text corpora. Specifically, we focus on machine learning models and visualizations that help us discover and understand topics and events in text corpora. These techniques ignore individual documents and minute details. Instead, they extract and organize the most important and essential information, and then use it to construct a concise overview of a text corpus, which traditionally can only obtained through a tedious and intensive close reading process.
Content analysis is a large area, and we just briefly cover the most common and popular aspects in these sections. Many technical details are skipped, and many rich possibilities remain open to explore and catalog. As we enter the era of big data, it is more and more challenging to perform traditional content analysis: reading. Thus, we are never more convinced that computed-aided content analysis is the future, and hope readers can draw inspiration from our discussions and encourage further study of this area.
References
1. Picapica.www.picapica.org(2014).[Online;accessedJan.2016]
2. Abbasi,A.,Chen,H.:Applyingauthorshipanalysistoextremist-groupwebforummessages.
Intelligent Systems, IEEE 20(5), 67–75 (2005)
3. Abbasi,A.,Chen,H.:Visualizingauthorshipforidentification.In:IntelligenceandSecurity
Informatics, pp. 60–71. Springer (2006)
4. Aggarwal, C.C., Zhai, C.: A survey of text clustering algorithms. In: Mining Text Data, pp.
77–128. Springer (2012)
5. Aiden,E.L.,Michel,J.B.:Whatwelearnedfrom5millionbooks.https://www.ted.com/talks/
what_we_learned_from_5_million_books (2011). [Online; accessed Jan. 2016]
6. Allan,J.:Topicdetectionandtracking:event-basedinformationorganization,vol.12.Springer
Science & Business Media (2012)
7. Allan,J.,Carbonell,J.G.,Doddington,G.,Yamron,J.,Yang,Y.:Topicdetectionandtracking
pilot study final report (1998)
8. Allan,J.,Papka,R.,Lavrenko,V.:On-lineneweventdetectionandtracking.In:Proceedings
of the 21st annual international ACM SIGIR conference on Research and development in
information retrieval, pp. 37–45. ACM (1998)
9. Ansari,T.:DimensionsinDiscourse:ElementarytoEssentials.XlibrisCorporation(2013)
10. Asuncion,A.,Welling,M.,Smyth,P.,Teh,Y.W.:Onsmoothingandinferencefortopicmodels. In: Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pp. 27–34. AUAI Press (2009)
11. Avidan,S.,Shamir,A.:Seamcarvingforcontent-awareimageresizing.In:ACMTransactions on graphics (TOG), vol. 26, p. 10. ACM (2007)
98 5 Visualizing Document Content
12. Bateman,S.,Gutwin,C.,Nacenta,M.:Seeingthingsintheclouds:theeffectofvisualfeatures on tag cloud selections. Proceedings of the nineteenth ACM conference on Hypertext and hypermedia 4250, 193–202 (2008). doi:10.1145/1379092.1379130. URL http://portal.acm. org/citation.cfm?id=1379130
13. Blei, D.M., Lafferty, J.D.: Dynamic topic models. In: Proceedings of the 23rd international conference on Machine learning, pp. 113–120. ACM (2006)
14. Blei,D.M.,Ng,A.Y.,Jordan,M.I.:Latentdirichletallocation.the.JournalofmachineLearn- ing research 3, 993–1022 (2003)
15. Burch,M.,Lohmann,S.,Beck,F.,Rodriguez,N.,DiSilvestro,L.,Weiskopf,D.:Radcloud: Visualizing multiple texts with merged word clouds. In: Information Visualisation (IV), 2014 18th International Conference on, pp. 108–113. IEEE (2014)
16. Byron, L., Wattenberg, M.: Stacked graphs-geometry & aesthetics. Visualization and Com- puter Graphics, IEEE Transactions on 14(6), 1245–1252 (2008)
17. Cai, D., He, X., Han, J.: Locally consistent concept factorization for document clustering. Knowledge and Data Engineering, IEEE Transactions on 23(6), 902–913 (2011)
18. Cao, N., Gotz, D., Sun, J., Lin, Y.R., Qu, H.: SolarMap: Multifaceted Visual Analyt- ics for Topic Exploration. 2011 IEEE 11th International Conference on Data Mining pp. 101–110 (2011). doi:10.1109/ICDM.2011.135. URL http://ieeexplore.ieee.org/lpdocs/ epic03/wrapper.htm?arnumber=6137214
19. Cao, N., Sun, J., Lin, Y.R., Gotz, D., Liu, S., Qu, H.: FacetAtlas: Multifaceted visualization for rich text corpora. IEEE Transactions on Visualization and Computer Graphics 16(6), 1172–1181 (2010). doi:10.1109/TVCG.2010.154
20. Cavnar,W.B.,Trenkle,J.M.,etal.:N-gram-basedtextcategorization.AnnArborMI48113(2), 161–175 (1994)
21. Chakrabarti, D., Kumar, R., Tomkins, A.: Evolutionary clustering. In: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 554–560. ACM (2006)
22. Chi,M.T.,Lin,S.S.,Chen,S.Y.,Lin,C.H.,Lee,T.Y.:Morphablewordcloudsfortime-varying text data visualization. Visualization and Computer Graphics, IEEE Transactions on 21(12), 1415–1426 (2015)
23. Chi,Y.,Song,X.,Zhou,D.,Hino,K.,Tseng,B.L.:Evolutionaryspectralclusteringbyincor- porating temporal smoothness. In: Proceedings of the 13th ACM SIGKDD international con- ference on Knowledge discovery and data mining, pp. 153–162. ACM (2007)
24. Choo, J., Lee, C., Reddy, C.K., Park, H.: UTOPIAN: User-driven topic modeling based on interactive nonnegative matrix factorization. IEEE Transactions on Visualization and Com- puter Graphics 19(12), 1992–2001 (2013). doi:10.1109/TVCG.2013.212
25. Chuang, J., Manning, C.D., Heer, J.: Termite: Visualization techniques for assessing textual topic models. In: Proceedings of the International Working Conference on Advanced Visual Interfaces, pp. 74–77. ACM (2012)
26. Collins,C.,Viegas,F.B.,Wattenberg,M.:Paralleltagcloudstoexploreandanalyzefaceted text corpora. In: Visual Analytics Science and Technology, 2009. VAST 2009. IEEE Sympo- sium on, pp. 91–98. IEEE (2009)
27. Covington, M.A.: A fundamental algorithm for dependency parsing. In: Proceedings of the 39th annual ACM southeast conference, pp. 95–102. Citeseer (2001)
28. Cox,T.F.,Cox,M.A.:Multidimensionalscaling.CRCpress(2000)
29. Cui,W.,Liu,S.,Tan,L.,Shi,C.,Song,Y.,Gao,Z.,Tong,X.,Qu,H.:Textflow:Towardsbetter
understanding of evolving topics in text. IEEE Transactions on Visualization and Computer
Graphics 17(12), 2412–2421 (2011). doi:10.1109/TVCG.2011.239
30. Cui, W., Liu, S., Tan, L., Shi, C., Song, Y., Gao, Z.J., Qu, H., Tong, X.: Textflow: Towards
better understanding of evolving topics in text. Visualization and Computer Graphics, IEEE
Transactions on 17(12), 2412–2421 (2011)
31. Cui, W., Liu, S., Wu, Z., Wei, H.: How Hierarchical Topics Evolve in Large Text Cor-
pora. IEEE Transactions on Visualization and Computer Graphics 20(12), 2281–2290 (2014). doi:10.1109/TVCG.2014.2346433. URL http://research.microsoft.com/en-us/um/ people/weiweicu/images/roseriver.pdf
References 99
32. Cui,W.,Wu,Y.,Liu,S.,Wei,F.,Zhou,M.,Qu,H.:Context-preserving,dynamicwordcloud visualization. IEEE Computer Graphics and Applications 30(6), 42–53 (2010). doi:10.1109/ MCG.2010.102
33. Culy,C.,Lyding,V.:Doubletree:anadvancedkwicvisualizationforexpertusers.In:Infor- mation Visualisation (IV), 2010 14th International Conference, pp. 98–103. IEEE (2010)
34. Daud,A.,Li,J.,Zhou,L.,Muhammad,F.:Knowledgediscoverythroughdirectedprobabilistic topic models: a survey. Frontiers of computer science in China 4(2), 280–301 (2010)
35. DeVel,O.,Anderson,A.,Corney,M.,Mohay,G.:Mininge-mailcontentforauthoridentifi- cation forensics. ACM Sigmod Record 30(4), 55–64 (2001)
36. Deerwester,S.,Dumais,S.T.,Furnas,G.W.,Landauer,T.K.,Harshman,R.:Indexingbylatent semantic analysis. Journal of the American society for information science 41(6), 391 (1990)
37. Don, A., Zheleva, E., Gregory, M., Tarkan, S., Auvil, L., Clement, T., Shneiderman, B., Plaisant, C.: Discovering interesting usage patterns in text collections: integrating text mining with visualization. Main pp. 213–221 (2007). doi:10.1145/1321440.1321473. URL http://
portal.acm.org/citation.cfm?id=1321473
38. Dörk, M., Gruen, D., Williamson, C., Carpendale, S.: A visual backchannel for large-scale events. Visualization and Computer Graphics, IEEE Transactions on 16(6), 1129–1138 (2010)
39. Dou,W.,Wang,X.,Skau,D.,Ribarsky,W.,Zhou,M.X.:LeadLine:Interactivevisualanaly- sis of text data through event identification and exploration. Visual Analytics Science and Technology (VAST), 2012 IEEE Conference on pp. 93–102 (2012). doi:10.1109/VAST.2012.
6400485. URL http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6400485
40. Dou, W., Yu, L., Wang, X., Ma, Z., Ribarsky, W.: Hierarchicaltopics: Visually exploring large text collections using topic hierarchies. Visualization and Computer Graphics, IEEE
Transactions on 19(12), 2002–2011 (2013)
41. Firth,J.R.:Asynopsisoflinguistictheory,1930-1955(1957)
42. Forsati,R.,Mahdavi,M.,Shamsfard,M.,Meybodi,M.R.:Efficientstochasticalgorithmsfor
document clustering. Information Sciences 220, 269–291 (2013)
43. Friendly,M.,Denis,D.J.:Milestonesinthehistoryofthematiccartography,statisticalgraph-
ics, and data visualization. URL http://www.datavis.ca/milestones (2001)
44. Fruchterman, T.M., Reingold, E.M.: Graph drawing by force-directed placement. Software:
Practice and experience 21(11), 1129–1164 (1991)
45. Gaifman, H.: Dependency systems and phrase-structure systems. Information and control
8(3), 304–337 (1965)
46. Gretarsson,B.,Odonovan,J.,Bostandjiev,S.,Höllerer,T.,Asuncion,A.,Newman,D.,Smyth,
P.: Topicnets: Visual analysis of large text corpora with topic modeling. ACM Transactions
on Intelligent Systems and Technology (TIST) 3(2), 23 (2012)
47. Griffiths,T.L.,Steyvers,M.:Findingscientifictopics.ProceedingsoftheNationalAcademy
of Sciences 101(suppl 1), 5228–5235 (2004)
48. HARRIS, J.: Word clouds considered harmful. www.niemanlab.org/2011/10/word-clouds-
considered-harmful/ (2011). [Online; accessed Jan. 2016]
49. Harris,Z.S.:Distributionalstructure.Word10(23),146–162(1954)
50. Havre, S., Hetzler, E., Whitney, P., Nowell, L.: Themeriver: Visualizing thematic changes
in large document collections. Visualization and Computer Graphics, IEEE Transactions on
8(1), 9–20 (2002)
51. Hays,D.G.:Dependencytheory:Aformalismandsomeobservations.Languagepp.511–525
(1964)
52. Heintze, N., et al.: Scalable document fingerprinting. In: 1996 USENIX workshop on elec-
tronic commerce, vol. 3 (1996)
53. Hilpert,M.:Dynamicvisualizationsoflanguagechange:Motionchartsonthebasisofbivari-
ate and multivariate data from diachronic corpora. International Journal of Corpus Linguistics
16(4), 435–461 (2011)
54. Hoad,T.C.,Zobel,J.:MethodsforIdentifyingVersionedandPlagiarisedDocuments.Journal
of the ASIS&T 54, 203–215 (2003). doi:10.1002/asi.10170
55. Holsti,O.R.,etal.:Contentanalysis.Thehandbookofsocialpsychology2,596–692(1968)
100 5 Visualizing Document Content
56. Houvardas,J.,Stamatatos,E.:N-gramfeatureselectionforauthorshipidentification.In:Arti- ficial Intelligence: Methodology, Systems, and Applications, pp. 77–86. Springer (2006)
57. Jaffe,A.,Naaman,M.,Tassa,T.,Davis,M.:Generatingsummariesandvisualizationforlarge
collections of geo-referenced photographs. In: Proceedings of the 8th ACM international
workshop on Multimedia information retrieval, pp. 89–98. ACM (2006)
58. Jankowska,M.,Keselj,V.,Milios,E.:Relativen-gramsignatures:Documentvisualizationat the level of character n-grams. In: Visual Analytics Science and Technology (VAST), 2012
IEEE Conference on, pp. 103–112. IEEE (2012)
59. Keim,D.,Oelke,D.,etal.:Literaturefingerprinting:Anewmethodforvisualliteraryanalysis.
In: Visual Analytics Science and Technology, 2007. VAST 2007. IEEE Symposium on, pp.
115–122. IEEE (2007)
60. Kim,J.:Causation,nomicsubsumption,andtheconceptofevent.TheJournalofPhilosophy
pp. 217–236 (1973)
61. Koh, K., Lee, B., Kim, B., Seo, J.: Maniwordle: Providing flexible control over wordle.
Visualization and Computer Graphics, IEEE Transactions on 16(6), 1190–1197 (2010)
62. Krstajic ́,M.,Bertini,E.:Keim,D.a.:Cloudlines:Compactdisplayofeventepisodesinmultiple time-series. IEEE Transactions on Visualization and Computer Graphics 17(12), 2432–2439
(2011). doi:10.1109/TVCG.2011.179
63. Kurby, C.A., Zacks, J.M.: Segmentation in the perception and memory of events. Trends in
cognitive sciences 12(2), 72–79 (2008)
64. Landauer,T.K.,Foltz,P.W.,Laham,D.:Anintroductiontolatentsemanticanalysis.Discourse
processes 25(2–3), 259–284 (1998)
65. Lasswell,H.D.:Describingthecontentsofcommunication.Propaganda,communicationand
public opinion pp. 74–94 (1946)
66. Leavenworth, R.S., Grant, E.L.: Statistical quality control. Tata McGraw-Hill Education
(2000)
67. Lee, B., Riche, N.H., Karlson, A.K., Carpendale, S.: Sparkclouds: Visualizing trends in tag
clouds. Visualization and Computer Graphics, IEEE Transactions on 16(6), 1182–1189 (2010)
68. Lee,H.,Kihm,J.,Choo,J.,Stasko,J.,Park,H.:ivisclustering:Aninteractivevisualdocument clustering via topic modeling. In: Computer Graphics Forum, vol. 31, pp. 1155–1164. Wiley
Online Library (2012)
69. Li, J., Zheng, R., Chen, H.: From fingerprint to writeprint. Communications of the ACM
49(4), 76–82 (2006)
70. Liu,S.,Wang,X.,Chen,J.,Zhu,J.,Guo,B.:Topicpanorama:afullpictureofrelevanttopics.
In: Visual Analytics Science and Technology (VAST), 2014 IEEE Conference on, pp. 183–
192. IEEE (2014)
71. Lotman,I.:Thestructureoftheartistictext
72. Lu, Y., Mei, Q., Zhai, C.: Investigating task performance of probabilistic topic models: an
empirical study of plsa and lda. Information Retrieval 14(2), 178–203 (2011)
73. Luo, D., Yang, J., Krstajic, M., Ribarsky, W., Keim, D.: Event river: Visually exploring text collections with temporal references. IEEE Transactions on Visualization and Computer Graphics 18(1), 93–105 (2012). doi:10.1109/TVCG.2010.225. URL http://ieeexplore.ieee.
org/xpls/abs_all.jsp?arnumber=5611507
74. Luo,D.,Yang,J.,Krstajic,M.,Ribarsky,W.,Keim,D.:Eventriver:Visuallyexploringtextcol- lections with temporal references. Visualization and Computer Graphics, IEEE Transactions on 18(1), 93–105 (2012)
75. Manber, U.: Finding similar files in a large file system. In: 1994 Winter USENIX Technical Conference, vol. 94, pp. 1–10 (1994)
76. Manning,C.D.,Raghavan,P.,Schütze,H.,etal.:Introductiontoinformationretrieval,vol.1. Cambridge university press Cambridge (2008)
77. Mates,B.:Stoiclogic(1953)
78. Mei,Q.,Zhai,C.:Discoveringevolutionarythemepatternsfromtext:anexplorationoftem-
poral text mining. In: Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pp. 198–207. ACM (2005)
References 101
79. Milgram,S.:Psychologicalmapsofparis,theindividualinasocialworld(1977)
80. Mukhopadhyay,A.,Maulik,U.,Bandyopadhyay,S.:Asurveyofmultiobjectiveevolutionary
clustering. ACM Computing Surveys (CSUR) 47(4), 61 (2015)
81. Ng, A.Y., Jordan, M.I., Weiss, Y., et al.: On spectral clustering: Analysis and an algorithm.
Advances in neural information processing systems 2, 849–856 (2002)
82. Oelke, D., Kokkinakis, D., Keim, D.A.: Fingerprint matrices: Uncovering the dynamics of
social networks in prose literature 32(3pt4), 371–380 (2013)
83. Oelke,D.,Spretke,D.,Stoffel,A.,Keim,D.A.:Visualreadabilityanalysis:Howtomakeyour
writings easier to read. Visualization and Computer Graphics, IEEE Transactions on 18(5),
662–674 (2012)
84. Pirolli,P.,Schank,P.,Hearst,M.,Diehl,C.:Scatter/gatherbrowsingcommunicatesthetopic
structure of a very large text collection. In: Proceedings of the SIGCHI conference on Human
factors in computing systems, pp. 213–220. ACM (1996)
85. Playfair,W.:Commercialandpoliticalatlasandstatisticalbreviary(1786)
86. Pylyshyn,Z.W.,Storm,R.W.:Trackingmultipleindependenttargets:Evidenceforaparallel
tracking mechanism*. Spatial vision 3(3), 179–197 (1988)
87. Ribler,R.L.,Abrams,M.:Usingvisualizationtodetectplagiarismincomputerscienceclasses.
In: Proceedings of the IEEE Symposium on Information Vizualization, p. 173. IEEE Computer
Society (2000)
88. Riehmann,P.,Potthast,M.,Stein,B.,Froehlich,B.:VisualAssessmentofAllegedPlagiarism
Cases. Computer Graphics Forum 34(3), 61–70 (2015). doi:10.1111/cgf.12618. URL http://
doi.wiley.com/10.1111/cgf.12618
89. Rivadeneira,a.W.,Gruen,D.M.,Muller,M.J.,Millen,D.R.:GettingOurHeadintheClouds: Toward Evaluation Studies of Tagclouds. 25th SIGCHI Conference on Human Factors in Computing Systems, CHI 2007 pp. 995–998 (2007). doi:10.1145/1240624.1240775
90. Rivadeneira,A.W.,Gruen,D.M.,Muller,M.J.,Millen,D.R.:Gettingourheadintheclouds: toward evaluation studies of tagclouds. In: Proceedings of the SIGCHI conference on Human factors in computing systems, pp. 995–998. ACM (2007)
91. Robertson, G., Fernandez, R., Fisher, D., Lee, B., Stasko, J.: Effectiveness of animation in trend visualization. Visualization and Computer Graphics, IEEE Transactions on 14(6), 1325–1332 (2008)
92. Rohrer, R.M., Ebert, D.S., Sibert, J.L.: The shape of shakespeare: visualizing text using implicit surfaces. In: Information Visualization, 1998. Proceedings. IEEE Symposium on, pp. 121–129. IEEE (1998)
93. Seifert,C.,Ulbrich,E.,Granitzer,M.:Wordcloudsforefficientdocumentlabeling.In:Dis- covery Science, pp. 292–306. Springer (2011)
94. Sgall,P.:Dependency-basedformaldescriptionoflanguage.TheEncyclopediaofLanguage and Linguistics 2, 867–872 (1994)
95. Shivakumar,N.,Garcia-Molina,H.:Findingnear-replicasofdocumentsontheweb.In:The World Wide Web and Databases, pp. 204–212. Springer (1998)
96. Sinclair,J.:Corpus,concordance,collocation.OxfordUniversityPress(1991)
97. Slingsby, A., Dykes, J., Wood, J., Clarke, K.: Interactive tag maps and tag clouds for the multiscale exploration of large spatio-temporal datasets. In: Information Visualization, 2007.
IV’07. 11th International Conference, pp. 497–504. IEEE (2007)
98. Smith, A.E., Humphreys, M.S.: Evaluation of unsupervised semantic mapping of natural
language with leximancer concept mapping. Behavior Research Methods 38(2), 262–279
(2006)
99. Steyvers,M.,Griffiths,T.:Probabilistictopicmodels.Handbookoflatentsemanticanalysis
427(7), 424–440 (2007)
100. Subašic ́, I., Berendt, B.: Web Mining for Understanding Stories through Graph Visuali-
sation. 2008 Eighth IEEE International Conference on Data Mining pp. 570–579 (2008). doi:10.1109/ICDM.2008.138. URL http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm? arnumber=4781152
102 5 Visualizing Document Content
101. Sun,G.,Wu,Y.,Liu,S.,Peng,T.Q.,Zhu,J.J.H.,Liang,R.:EvoRiver:VisualAnalysisofTopic Coopetition on Social Media. Visualization and Computer Graphics, IEEE Transactions on PP(99), 1 (2014). doi:10.1109/TVCG.2014.2346919
102. Svartvik,J.:TheEvansstatements.UniversityofGoteburg(1968)
103. Teh, Y.W., Jordan, M.I., Beal, M.J., Blei, D.M.: Hierarchical dirichlet processes. Journal of
the american statistical association (2012)
104. Tufte,E.R.:Envisioninginformation.Optometry&VisionScience68(4),322–324(1991)
105. Tufte,E.R.:Beautifulevidence.NewYork(2006)
106. VanHam,F.,Wattenberg,M.,Viégas,F.B.:Mappingtextwithphrasenets.IEEETransactions
on Visualization & Computer Graphics 6, 1169–1176 (2009)
107. Viégas, F.B., Wattenberg, M., Feinberg, J.: Participatory visualization with wordle. IEEE
Transactions on Visualization and Computer Graphics 15(6), 1137–1144 (2009). doi:10.1109/
TVCG.2009.171
108. Vuillemot,R.,Clement,T.,Plaisant,C.,Kumar,A.:What’sbeingsaidnearmartha?exploring name entities in literary text collections. In: Visual Analytics Science and Technology, 2009. VAST 2009. IEEE Symposium on, pp. 107–114. IEEE (2009)
109. Wang, C., Blei, D., Heckerman, D.: Continuous time dynamic topic models. arXiv preprint arXiv:1206.3298 (2012)
110. Wang,X.,McCallum,A.:Topicsovertime:anon-markovcontinuous-timemodeloftopical trends. In: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 424–433. ACM (2006)
111. Wattenberg, M.: Arc diagrams: visualizing structure in strings. Information Visualization Proceedings 2002(2002), 110–116 (2002). doi:10.1109/INFVIS.2002.1173155
112. Wattenberg,M.,Viégas,F.B.:Thewordtree,aninteractivevisualconcordance.IEEETransac- tions on Visualization and Computer Graphics 14(6), 1221–1228 (2008). doi:10.1109/TVCG. 2008.172
113. Wei,F.,Liu,S.,Song,Y.,Pan,S.,Zhou,M.X.,Qian,W.,Shi,L.,Tan,L.,Zhang,Q.:Tiara:a visual exploratory text analytic system. In: Proceedings of the 16th ACM SIGKDD interna- tional conference on Knowledge discovery and data mining, pp. 153–162. ACM (2010)
114. Werlich,E.:AtextgrammarofEnglish.Quelle&Meyer(1976)
115. Wu, Y., Provan, T., Wei, F., Liu, S., Ma, K.L.: Semantic-Preserving Word Clouds by Seam
Carving. Computer Graphics Forum 30(3), 741–750 (2011). doi:10.1111/j.1467-8659.2011.
01923.x. URL http://doi.wiley.com/10.1111/j.1467-8659.2011.01923.x
116. Xu, P., Wu, Y., Wei, E., Peng, T.Q., Liu, S., Zhu, J.J.H., Qu, H.: Visual analysis of topic competition on social media. IEEE Transactions on Visualization and Computer Graphics
19(12), 2012–2021 (2013). doi:10.1109/TVCG.2013.221
117. Xu,T.,Zhang,Z.,Yu,P.S.,Long,B.:Evolutionaryclusteringbyhierarchicaldirichletprocess
with hidden markov state. In: Data Mining, 2008. ICDM’08. Eighth IEEE International Con-
ference on, pp. 658–667. IEEE (2008)
118. Xu,W.,Gong,Y.:Documentclusteringbyconceptfactorization.In:Proceedingsofthe27th
annual international ACM SIGIR conference on Research and development in information
retrieval, pp. 202–209. ACM (2004)
119. Xu,W.,Liu,X.,Gong,Y.:Documentclusteringbasedonnon-negativematrixfactorization.
In: Proceedings of the 26th annual international ACM SIGIR conference on Research and
development in informaion retrieval, pp. 267–273. ACM (2003)
120. Zacks,J.M.,Tversky,B.:Eventstructureinperceptionandconception.Psychologicalbulletin
127(1), 3 (2001)
121. Zhang, J., Ghahramani, Z., Yang, Y.: A probabilistic model for online document clustering
with application to novelty detection. In: Advances in Neural Information Processing Systems,
pp. 1617–1624 (2004)
122. Zhao,Q.,Mitra,P.:EventDetectionandVisualizationforSocialTextStreams.EventLondon
pp. 26–28 (2007). URL http://www.icwsm.org/papers/3--Zhao-Mitra.pdf
123. Zheng, R., Li, J., Chen, H., Huang, Z.: A framework for authorship identification of online messages: Writing-style features and classification techniques. Journal of the American Soci-
ety for Information Science and Technology 57(3), 378–393 (2006)
Chapter 6
Visualizing Sentiments and Emotions
Abstract Sentiment analysis, also known as opinion mining, is one of the most important text mining tasks and has been widely used for analyzing, for example, reviews or social media data for various of applications, including marketing and customer service. In general, “sentiment analysis aims to determine the attitude of a speaker or a writer with respect to some topic or the overall contextual polarity of a document. The attitude may be his or her judgment or evaluation, affective state (i.e., the emotional state of the author when writing), or the intended emotional communication (i.e., the emotional effect the author wishes to have on the reader)” (Wikipedia, Sentiment analysis—Wikipedia, the free encyclopedia, 2006. https://en. wikipedia.org/wiki/Sentiment_analysis [17]). The sentiment analysis result is usu- ally a score that ranges from −1 to 1 (after normalization) with −1 indicating the most negative, 1 indicating the most positive, and 0 indicating neutral. This sentiment score is usually treated as an attribute of the corresponding text, which can be intu- itively differentiated by colors that range from, for example, red (−1) to green (1). In this chapter, we introduce the state-of-the-art sentiments visualization techniques that can be largely classified into two categories including (1) the techniques for summarizing the sentiment dynamics over time, and (2) the techniques for assisting sentiment analysis.
6.1 Introduction
The design goal of a large category of sentiment visualization techniques is to visu- ally summarize the change of sentiments over time regarding to in a given streaming dataset such as news corpus, review comments, and Twitter streams. This goal can be approached, as shown in Fig. 6.1, by showing the sentiment dynamics in a time- series diagram where the time-series curve illustrates the change of sentiment scores computed across the entire dataset at different time points. However this simple visu- alization is too abstract to display information details such as the causes behind the sentiment shifts. Therefore, many other highly advanced techniques have been intro- duced to illustrate and interpret the sentiment dynamics from different prospectives.
© Atlantis Press and the author(s) 2016 103 C. Nan and W. Cui, Introduction to Text Visualization, Atlantis Briefs
in Artificial Intelligence 1, DOI 10.2991/978-94-6239-186-4_6
104 6 Visualizing Sentiments and Emotions
 Fig. 6.1 Sentiment indexing of Twitter data in a time-series diagram. This figure shows that the public sentiment regarding different events in real-life may change dramatically
Most of the techniques are developed to compute and visualize the sentiments of a group of focused people based on the text data produced by them. The resulting visualization forms a “happiness indicator” that captures the sentiment change of the focal group over time. For example, Brew et al. [2] introduced SentireCrowds, which represents the sentiment changes of a group of Twitter users from the same city in a timeline view and summarizes the potential underlying event that causes the changes in a multi-level TagCloud designed on the basis of Treemap. Guzman et al. [7] visu- alizes the change of emotions of groups of different developers in various software development projects. Hao et al. [8] analyzes sentiments through geo-temporal term associations based on a streaming dataset of customers’ feedback. Kempter et al. [10] introduced a fine-grained, multi-category emotion model to classify the emotional reactions of users in public events overtime and to visualize the results in a radar diagram, called EmotionWatch, as shown in Fig. 6.3.
Some visual analysis systems have also been developed to support dynamic senti- ment analysis. For example, Wanner et al. [15] developed a small multiple visualiza- tion view to conduct a semi-automatic sentiment analysis of large news feeds. In this work, a case study on news regarding to the US presidential election in 2008 shows how visualization techniques can help analysts to draw meaningful conclusions with- out existing effort to read the news content. Brooks et al. [3] introduced Agave, a collaborative visual analysis system for exploring events and sentiment over time in large Twitter datasets. The system employs multiple co-ordinated views in which a streamgraph (Fig. 6.2) is used to summarize the changes of the sentiments of a sub- set of tweets queried on the basis of user preferences. Zhang et al. [20] introduced a spatial-temporal view for visualizing the sentiment scores of microblog data based on an electron cloud model intruded in physics. The resulting visualization maps a single sentiment score to a position inside a circular visualization display (Fig. 6.3).
6.1 Introduction 105
  Fig. 6.2 Sentiment streamgraphs for the keyword search Flacco, the Super Bowl MVP in a Twitter dataset using Agave [3]. Red indicates negative, gray indicates neutral, and blue indicates positive. Top overall frequency of tweets, divided by sentiment type. Bottom sentiment as percentage of overall volume
Fig. 6.3 Comparison of two emotion profiles of Roger Federer and Andy Murray (two tennis athletes) after a tennis game in EmotionWatch [10]; (A) the EmotionWatches, (B) timelines showing the two emotion flows, and (C) video
In terms of application, a large set of techniques has been developed to repre- sent the customer’s sentiments based on the review data. Alper et al. [1] introduced OpinionBlocks, an interactive visualization tool to enhance understanding of cus- tomer reviews. The visualization progressively discloses text information at differ- ent granularities from the keywords to the snippets the keywords are used in, and to the reviews containing those snippets. This text information is displayed within two horizontally regions separated by their sentiments. Gamon et al. [6] introduced Pulse for mining topics and sentiment orientation jointly from free text customer feedback. This system enables the exploration of large quantities of customer review data and was used to visually analyze a database of car reviews. Through this sys-

106 6 Visualizing Sentiments and Emotions
 Fig. 6.4 Summary report of printers: each row shows the attribute performances of a specific printer. Blue color represents comparatively positive user opinions and red color comparatively negative ones (see color scale). The size of an inner rectangle indicates the amount of customers that commented on an attribute. The larger the rectangle the more comments have been provided by the customers
tem, the users can examine customer opinion at a glance or explore the data at a finer level of detail. Oelke et al. [12] analyzed to determine customers’ positive and neg- ative opinions via the comments or ratings posted by the customers and visualized the analysis results in a heatmap view showing both volume of comments and the summarized sentiments (Fig. 6.4). More generic systems were also developed. For example, Wensel [16] introduced VIBES, which extracts the important topics from a blog, measures the emotions associated with those topics, and represents topics in the context of emotions based on multiple coordinated views. Makki et al. [11] introduced an interactive visualization to engage the user in the process of polarity assignment to improve the quality of the generated lexicon used for sentiment or emotion analysis via minimal user effort.
Despite the aforementioned techniques in which standard visualizations such as line charts, streaming graphs, and multiple coordinated views are employed. More sophisticated systems with advanced visualization designs were also proposed and developed to analyze the change of sentiments based on the streaming text data. For example, Wang et al. [14] introduced SentiView, which employs advanced sentiment analysis techniques as well as visualization designs to analyze the change of public sentiments regarding popular topics on the Internet. Other systems were designed to analyze sentiment divergences (i.e., conflicting of opinions) that occur between two groups of people. For example, Chen et al. [5] introduced the first work on this topic based on a simple time-series design that summarizes the overall conflicting of opinions based on the Amazon review data. Following this topic, Cao et al. [4] introduced a more advanced technique called SocialHelix, which extracts two groups of people that have the most significant sentiment divergence over time from Twitter data and visualizes their divergence in a Helix visualization as shown in Fig. 6.12, which illustrates how the divergence occurred, evolved, and terminated. Wu et al. [19] introduced OpinionSeer (Fig.6.5), a visualizaiton design that employs subjective logic [9] to analyze customer opinions about hotels based on their review data inside a simplex space, which is visualized in a triangle surrounded by the context about the customers such as their ages and their countries of origin. Zhao et al. [21] introduced PEARL, which visualizes the change of a person’s emotion or mood profiles derived
6.1 Introduction 107
 Fig. 6.5 OpinionSeer visualization showing customers’ opinions on hotel rooms. The center trian- gle is a ternary plot which illustrates the distribution of customers in the three dimensional opinion space computed based on subjective logic. In particular a dot in the triangle illustrate a customer, each triangle vertex indicates a type of opinion, the distances between the dot and triangle vertex indicate how strong the customer holds the opinion (i.e., closer is stronger). The surrounding bar charts and arcs showing context information such as the number of people with similar opinions over time and across different space
from his tweets in a compound belt visualization. The belt groups a set of emotion bands, each one indicating a type of emotion differentiated by colors. The thickness of the band changes over time depending on the portion of the corresponding emotion at different times.
In the rest of this chapter, we describe several well-designed visualizations and the corresponding visual analysis systems that were proposed to reveal and analyze sentiments extracted from various types of text corpus. From these examples, we demonstrate how people’s sentiments can be meaningfully visualized and how these sentiment oriented data patterns are revealed.
6.2 Visual Analysis of Customer Comments
In this section, we introduce the visual analysis system developed by Rohrdantz et al. [13] to reveal temporal sentiment patterns in a text stream. The proposed system integrates methods and techniques from text mining, sentiment analysis, and visual analysis together to help analysts to perform a temporal analysis of customers’ comments collected from an online web survey. The proposed system helps users to detect interesting portions of an input text streams (e.g., customer reviews/comments), regarding the change of sentiments, data density, and context coherence calculated based on the features (i.e., representative keywords) extracted from the text stream.
108 6 Visualizing Sentiments and Emotions
In the proposed system, a pixel map is introduced to illustrate the sentiment dynamics of each document (Fig. 6.6) over time. The map provides an overview of the entire corpus by arranging text documents into a pixel based calendar, in which, each pixel indicates a document with the color showing the overall-sentiment of the document. In this case, red indicates negative, green indicates positive, and yellow indicates neutral. The x-axis bins in the background are days and y-axis bins are years with months.
To reveal the change of sentiments, data density, and context coherence, a time density plot is also introduced in the system. As shown in Fig. 6.7, a set of comments that contain the noun “password” (i.e., a keyword feature) extracted from the corpus containing over 50,000 comments, collected over 2 years, are sequentially displayed in a row. In this view, each comment is shown as a vertical bar with color indicating whether the noun password has been mentioned in a positive (green), negative (red) or neutral (gray) context. The height of the bar shows the uncertainty involved in the sentiment analysis following the rule of “the higher the bar, the more certain the result”. When hover on a comment bar, the content is displayed in a tooltip. All nouns have background colors to illustrate the sentiments context. The curve plot below these comment bars illustrate the data density over time. An algorithm is also designed in the system to help highlight the data portion that is potentially interesting to the users. With all these visual designs and the pattern detection algorithm, many issues regarding the company have been found from the user comments as shown in Fig. 6.8.
Fig. 6.6 Pixel map calendar: each pixel indicates a document with the color encoding its overall- sentiment, i.e., the average of all contained feature sentiments. Here, red indicates negative, green indicates positive, and yellow indicates neutral. In the background, the x-axis bins are days and y-axis bins are years with months

6.3 Visualizing Sentiment Diffusion 109
  Fig. 6.7 Time density plots visualization that illustrates all the comments related to the feature “password” with associated terms shown at the bottom and automatically annotated example com- ments shown on top. Each comment is visualized as a vertical bar with color indicates whether the noun password has been mentioned in a positive (green), negative (red) or neutral (gray) context. The height of a bar encodes the uncertainty involved in the sentiment analysis. The curve plotted below the comment sequence illustrate the data density over time
Fig. 6.8 The top issues discovered by the pattern detection algorithm and visualized in the time density plot
6.3 Visualizing Sentiment Diffusion
Understanding and tracing the diffusion process of public opinions has attracted significant attentions in recent years. For example, the government may want to know people’s opinion about, for example, a new policy. The sales man in a company may also want to understand how effective a users’ opinion will affect others in terms

110 6 Visualizing Sentiments and Emotions
of choosing a product. However, tracing the diffusion of opinion or sentiment is not an easy task because of the rapid spreading and great diversity of public opinions on social media. To address this problem, Wu et al. [18] introduced OpinionFlow, a visual analysis system designed and developed to analyze opinion diffusion on Social Media.
Specifically, the system starts by analyzing a set of tweets posted by users regard- ing to a predefined topic. For each user u at a given time t, the system computes the sentiment score, denoted as s(u, t), to reveal his/her opinion (either positive or neg- ative) at the moment indicated by t based on the tweets the focal user involved (i.e., post or retweed) in. The system tracks the spreading of opinions by inferring how a single opinion spreads from one user to another by detecting the opinion propaga- tion as a trend formulated by a group of users sharing similar spreading patterns. A statistical analysis model is introduced in the system to estimate the probability that an opinion initiated by user u would propagate to another user v given a predefined topic. The diffusion under a group of multiple topics is calculated and illustrated in a flow-based visualization simultaneously.
In particular, OpinionFlow employs a Sankey diagram based design to represent user flows across multiple topics because of its simplicity and the intuitiveness of
Fig. 6.9 The a adapted and b rotated Gaussian kernel function used for representing the direction and density for representing the opinion diffusion process
Fig. 6.10 The visualization of PRISM data based on Opinion Flow which indicates five major topics. A and D illustrate the spreading paths of the media user “FoxNews” with strong and weak opinions. B and C illustrate the diffusion paths of a common user with strong and weak opinions, respectively

6.3 Visualizing Sentiment Diffusion 111
representing information flows. On top of this view, a density map is employed to help illustrate the diffusion of opinions among users over time. Specifically in order to reveal the diffusion trend, an altered Gaussian kernel is used as shown in Fig. 6.9. The resulting visualization reveals many interesting diffusion pattern as shown in Fig. 6.10. In this view, a node-link diagram is also used to specifically highlight the individual diffusion path on top of the density map, showing more details regarding the diffusion.
6.4 Visualizing Sentiment Divergence in Social Media
SocialHelix [4] is the third example introduced in this chapter to demonstrate the usefulness of visualization techniques in terms of revealing sentiment oriented data patterns. SocialHelix was designed to help uncover the divergence of users’ opinions (i.e., sentiment divergence) inspired by an event in social media platforms such as Twitter. For example, in political campaigns, people who support different parties and candidates may engage in debates on social media based on their own political perspectives. Makers of competing products may also launch persuasion campaigns on social media to attract attention. From these examples, we can see that sentiment divergence is a complex social phenomenon that involves three key components, i.e., the communities of users who are involved in the divergence by holding different opinions, the topic context in which the divergence is occurred and developed, as well as various events or episodes that provoke the divergence. In this case, several primary characteristics of the components can be observed. For example, a divergence should at least involve two groups of people (i.e., communities) and these groups must hold different opinions regarding the same series of focal events or episodes over time. These components as well as the corresponding characteristics are used to detect divergence from the raw social media data and also to design visualizations to represent the divergence.
Specifically, the SocialHelix system detects and represents the temporal process of social divergence through a series of data processing steps. With a given set of streaming Twitter data, the SocialHelix system first extracts a group of active users who continuously post or retweeted a large number of tweets over time. In the following, the system computes their sentiments at different time points based on their posts.TheresultsformavectorS=[s1,s2,...,sn]wheresi isanumericalscorethat indicates the users’ sentiment at the i -th time point. This vector is then used for cluster analysis to group users with similar sentiment changing trend together. Among all the resulting clusters, the two with the most significant differences (i.e., with the largest sentiment divergence) are selected to be visualized in a novel helix based visualization design as shown in Fig. 6.11. This design is inspired by the structure of DNA helix given its perfect match with the aforementioned key components in a typical sentiment divergence procedure. In particular, in this design, the backbones of the DNA helix are used to illustrate two communities with the largest sentiment
112 6 Visualizing Sentiments and Emotions
  Fig. 6.11 The design of SocialHelix visualization employs a DNA helix-metaphor in which the backbones of the DNA helix is used to illiterate two communities with the largest sentiment diver- gence. The phosphates represent the groups of representative users in these communities at different time and the base pairs are used to illustrate the episode or events that inspired the divergence
Fig. 6.12 SocialHelix visualization summarizes the sentiment divergence of two group of Twitter users based on the sentiment analysis of their posts. This figure illustrates the divergence about “Obamacare” between two groups of people during the 2012 US presidential debate on October 3rd, 2012

6.5 Conclusion 113
divergence. The phosphates represent the groups of representative users in these communities at different times. The base pairs are used to illustrate the episodes or events that inspired the divergence.
Based on a Twitter dataset collected during four political debates during the 2012 presidential election in the US, the SocialHelix system revealed many interesting findings. One example is shown in Fig.6.12. This figure illustrates a divergence between two groups of people centered around the topic of “ObamaCare”, a new healthcare policy in USA proposed by President Obama. One group of users shows support but another is against this policy. The data were collected during the live debate show on TV occurred in the evening of Oct 3rd, 2012. This figure clearly illustrates the divergence started after the political debate (i.e. after 10:00 p.m.), when the viewers of the debate started to post their opinions on Twitter. An interesting finding is the emergence of turning point in which the group of users holding negative opinions shifted to positive while the group with positive opinions shifted to negative, which was suspicious point and worth a close investigation of the detailed tweets.
6.5 Conclusion
This chapter has introduced sentiment visualization techniques. After briefly review- ing the existing visualization and visual analysis techniques, we introduced three example systems and the corresponding visualization designs to help readers under- stand how visualization can help with the representation and analysis of sentiment- oriented information. Sentiment visualization is a highly interesting research topic that is under development. Although we have introduced many techniques in this chapter, challenges such as accuracy and scalability issues have not been fully addressed. In terms of visualization, a systematic approach or a well-established standard for representing sentiment-oriented information is lacking. The current visu- alization techniques are more application-driven and cannot be clearly categorized. Therefore, we believe that further study in this direction is necessary and promising.
References
1. Alper, B., Yang, H., Haber, E., Kandogan, E.: Opinionblocks: visualizing consumer reviews. In: IEEE VisWeek 2011 Workshop on Interactive Visual Text Analytics for Decision Making (2011)
2. Brew, A., Greene, D., Archambault, D., Cunningham, P.: Deriving insights from national happiness indices. In: 2011 IEEE 11th International Conference on Data Mining Workshops (ICDMW), pp. 53–60. IEEE (2011)
3. Brooks,M.,Robinson,J.J.,Torkildson,M.K.,Aragon,C.R.,etal.:Collaborativevisualanalysis of sentiment in twitter events. In: Cooperative Design, Visualization, and Engineering, pp. 1–8. Springer, Berlin (2014)
4. Cao,N.,Lu,L.,Lin,Y.R.,Wang,F.,Wen,Z.:Socialhelix:visualanalysisofsentimentdiver- gence in social media. J. Vis. 18(2), 221–235 (2015)
114 6 Visualizing Sentiments and Emotions
5. Chen,C.,Ibekwe-SanJuan,F.,SanJuan,E.,Weaver,C.:Visualanalysisofconflictingopinions. In: 2006 IEEE Symposium on Visual Analytics Science and Technology, pp. 59–66. IEEE (2006)
6. Gamon, M., Aue, A., Corston-Oliver, S., Ringger, E.: Pulse: Mining customer opinions from free text. In: Advances in Intelligent Data Analysis VI, pp. 121–132. Springer, Berlin (2005)
7. Guzman,E.:Visualizingemotionsinsoftwaredevelopmentprojects.In:IEEEWorkingCon- ference on Software Visualization, pp. 1–4. IEEE (2013)
8. Hao, M.C., Rohrdantz, C., Janetzko, H., Keim, D.A., et al.: Visual sentiment analysis of cus- tomer feedback streams using geo-temporal term associations. Inf. Vis. 12(3–4), 273 (2013)
9. Jøsang,A.:Theconsensusoperatorforcombiningbeliefs.Artif.Intell.141(1),157–170(2002)
10. Kempter,R.,Sintsova,V.,Musat,C.,Pu,P.:Emotionwatch:visualizingfine-grainedemotions in event-related tweets. In: International AAAI Conference on Weblogs and Social Media
(2014)
11. Makki,R.,Brooks,S.,Milios,E.E.:Context-specificsentimentlexiconexpansionviaminimal
user interaction. In: Proceedings of the International Conference on Information Visualization
Theory and Applications (IVAPP), pp. 178–186 (2014)
12. Oelke,D.,Hao,M.,Rohrdantz,C.,Keim,D.,Dayal,U.,Haug,L.E.,Janetzko,H.,etal.:Visual
opinion analysis of customer feedback data. In: IEEE Symposium on Visual Analytics Science
and Technology, 2009. VAST 2009, pp. 187–194. IEEE (2009)
13. Rohrdantz,C.,Hao,M.C.,Dayal,U.,Haug,L.E.,Keim,D.A.:Feature-basedvisualsentiment
analysis of text document streams. ACM Trans. Intell. Syst. Technol. (TIST) 3(2), 26 (2012)
14. Wang, C., Xiao, Z., Liu, Y., Xu, Y., Zhou, A., Zhang, K.: Sentiview: sentiment analysis and visualization for internet popular topics. IEEE Trans. Hum. Mach. Syst. 43(6), 620–630 (2013)
15. Wanner,F.,Rohrdantz,C.,Mansmann,F.,Oelke,D.,Keim,D.A.:Visualsentimentanalysisof rss news feeds featuring the us presidential election in 2008. In: Workshop on Visual Interfaces
to the Social and the Semantic Web (VISSW) (2009)
16. Wensel, A.M., Sood, S.O.: Vibes: visualizing changing emotional states in personal stories.
In: Proceedings of the 2nd ACM International Workshop on Story Representation, Mechanism
and Context, pp. 49–56. ACM (2008)
17. Wikipedia:Sentimentanalysis—Wikipedia,thefreeencyclopedia(2006).https://en.wikipedia.
org/wiki/Sentiment_analysis. Accessed 10 Nov 2015
18. Wu,Y.,Liu,S.,Yan,K.,Liu,M.,Wu,F.:Opinionflow:visualanalysisofopiniondiffusionon
social media. IEEE Trans. Vis. Comput. Graph. 20(12), 1763–1772 (2014)
19. Wu,Y.,Wei,F.,Liu,S.,Au,N.,Cui,W.,Zhou,H.,Qu,H.:Opinionseer:interactivevisualization
of hotel customer feedback. IEEE Trans. Vis. Comput. Graph. 16(6), 1109–1118 (2010)
20. Zhang,C.,Liu,Y.,Wang,C.:Time-spacevaryingvisualanalysisofmicro-blogsentiment.In: Proceedings of the 6th International Symposium on Visual Information Communication and
Interaction, pp. 64–71. ACM (2013)
21. Zhao,J.,Gou,L.,Wang,F.,Zhou,M.:Pearl:aninteractivevisualanalytictoolforunderstand-
ing personal emotion style derived from social media. In: 2014 IEEE Conference on Visual Analytics Science and Technology (VAST), pp. 203–212. IEEE (2014)














































































































































